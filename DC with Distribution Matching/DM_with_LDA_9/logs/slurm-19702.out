/data/happythgus/repos/lda/DatasetCondensation
/data/opt/anaconda3/bin/python
moana-r1
eval_it_pool:  [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000, 20000]
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz
  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 32768/170498071 [00:00<28:15, 100535.79it/s]  0%|          | 65536/170498071 [00:00<25:00, 113588.42it/s]  0%|          | 98304/170498071 [00:00<25:13, 112590.99it/s]  0%|          | 229376/170498071 [00:01<11:38, 243889.58it/s]  0%|          | 425984/170498071 [00:01<06:19, 448667.79it/s]  0%|          | 524288/170498071 [00:01<05:16, 536946.92it/s]  1%|          | 884736/170498071 [00:01<03:25, 826569.41it/s]  1%|          | 1572864/170498071 [00:02<02:33, 1102198.83it/s]  1%|▏         | 2490368/170498071 [00:02<01:32, 1817991.78it/s]  2%|▏         | 4096000/170498071 [00:02<00:48, 3436448.80it/s]  3%|▎         | 4915200/170498071 [00:02<00:46, 3598942.04it/s]  3%|▎         | 5373952/170498071 [00:03<00:46, 3515921.59it/s]  3%|▎         | 5931008/170498071 [00:03<00:44, 3662295.56it/s]  4%|▎         | 6356992/170498071 [00:03<00:48, 3412648.51it/s]  4%|▍         | 7012352/170498071 [00:03<00:45, 3599238.00it/s]  4%|▍         | 7405568/170498071 [00:03<00:47, 3443337.26it/s]  5%|▍         | 8093696/170498071 [00:03<00:44, 3689859.59it/s]  5%|▍         | 8486912/170498071 [00:03<00:52, 3069553.93it/s]  5%|▌         | 8912896/170498071 [00:04<00:49, 3258090.54it/s]  5%|▌         | 9273344/170498071 [00:04<01:27, 1836413.23it/s]  6%|▌         | 10584064/170498071 [00:04<00:46, 3466730.57it/s]  7%|▋         | 11173888/170498071 [00:04<00:51, 3111990.10it/s]  7%|▋         | 11665408/170498071 [00:04<00:46, 3389335.75it/s]  7%|▋         | 12156928/170498071 [00:05<00:53, 2970973.59it/s]  7%|▋         | 12550144/170498071 [00:05<00:55, 2868783.01it/s]  8%|▊         | 12910592/170498071 [00:05<00:54, 2909097.19it/s]  8%|▊         | 13336576/170498071 [00:05<00:49, 3174996.19it/s]  8%|▊         | 13729792/170498071 [00:05<00:55, 2810623.13it/s]  8%|▊         | 14155776/170498071 [00:05<00:50, 3087658.98it/s]  9%|▊         | 14516224/170498071 [00:06<00:55, 2811850.56it/s]  9%|▊         | 14909440/170498071 [00:06<00:51, 3012628.87it/s]  9%|▉         | 15237120/170498071 [00:06<00:53, 2886895.17it/s]  9%|▉         | 15597568/170498071 [00:06<00:51, 2997231.16it/s]  9%|▉         | 15990784/170498071 [00:06<00:48, 3205874.01it/s] 10%|▉         | 16351232/170498071 [00:06<00:52, 2914972.42it/s] 10%|▉         | 16744448/170498071 [00:06<00:48, 3148512.39it/s] 10%|█         | 17104896/170498071 [00:06<00:50, 3012461.06it/s] 10%|█         | 17465344/170498071 [00:06<00:49, 3096074.58it/s] 10%|█         | 17858560/170498071 [00:07<00:46, 3301563.47it/s] 11%|█         | 18219008/170498071 [00:07<00:50, 3003307.01it/s] 11%|█         | 18644992/170498071 [00:07<00:46, 3284303.47it/s] 11%|█         | 19005440/170498071 [00:07<00:49, 3072909.33it/s] 11%|█▏        | 19365888/170498071 [00:07<00:47, 3190914.81it/s] 12%|█▏        | 19791872/170498071 [00:07<00:44, 3385188.02it/s] 12%|█▏        | 20152320/170498071 [00:07<00:49, 3051903.90it/s] 12%|█▏        | 20545536/170498071 [00:07<00:45, 3261244.91it/s] 12%|█▏        | 20905984/170498071 [00:08<00:47, 3126901.65it/s] 12%|█▏        | 21266432/170498071 [00:08<00:46, 3241733.20it/s] 13%|█▎        | 21692416/170498071 [00:08<00:43, 3439048.59it/s] 13%|█▎        | 22052864/170498071 [00:08<00:47, 3120325.01it/s] 13%|█▎        | 22446080/170498071 [00:08<00:44, 3315514.32it/s] 13%|█▎        | 22806528/170498071 [00:08<00:46, 3170983.72it/s] 14%|█▎        | 23199744/170498071 [00:08<00:44, 3312112.49it/s] 14%|█▍        | 23592960/170498071 [00:08<00:42, 3467138.22it/s] 14%|█▍        | 23953408/170498071 [00:08<00:46, 3139442.58it/s] 14%|█▍        | 24346624/170498071 [00:09<00:44, 3285785.33it/s] 15%|█▍        | 24739840/170498071 [00:09<00:44, 3239992.68it/s] 15%|█▍        | 25100288/170498071 [00:09<00:43, 3317653.96it/s] 15%|█▍        | 25493504/170498071 [00:09<00:42, 3428340.79it/s] 15%|█▌        | 25853952/170498071 [00:09<00:45, 3162663.50it/s] 15%|█▌        | 26279936/170498071 [00:09<00:42, 3368463.53it/s] 16%|█▌        | 26673152/170498071 [00:09<00:44, 3268338.68it/s] 16%|█▌        | 27033600/170498071 [00:09<00:42, 3342452.59it/s] 16%|█▌        | 27426816/170498071 [00:10<00:42, 3404547.97it/s] 16%|█▋        | 27787264/170498071 [00:10<00:44, 3219660.54it/s] 17%|█▋        | 28213248/170498071 [00:10<00:41, 3433742.54it/s] 17%|█▋        | 28639232/170498071 [00:10<00:39, 3571823.37it/s] 17%|█▋        | 28999680/170498071 [00:10<00:43, 3257073.95it/s] 17%|█▋        | 29392896/170498071 [00:10<00:42, 3343619.00it/s] 17%|█▋        | 29753344/170498071 [00:10<00:44, 3168488.29it/s] 18%|█▊        | 30179328/170498071 [00:10<00:41, 3397535.72it/s] 18%|█▊        | 30539776/170498071 [00:10<00:41, 3368440.32it/s] 18%|█▊        | 30900224/170498071 [00:11<00:43, 3240125.68it/s] 18%|█▊        | 31260672/170498071 [00:11<00:43, 3221465.74it/s] 19%|█▊        | 31621120/170498071 [00:11<00:43, 3161003.76it/s] 19%|█▉        | 32047104/170498071 [00:11<00:40, 3420429.13it/s] 19%|█▉        | 32407552/170498071 [00:11<00:41, 3354827.27it/s] 19%|█▉        | 32768000/170498071 [00:11<00:42, 3253040.17it/s] 19%|█▉        | 33193984/170498071 [00:11<00:40, 3403370.09it/s] 20%|█▉        | 33587200/170498071 [00:11<00:43, 3178028.05it/s] 20%|█▉        | 34013184/170498071 [00:11<00:39, 3432039.75it/s] 20%|██        | 34373632/170498071 [00:12<00:40, 3357506.91it/s] 20%|██        | 34734080/170498071 [00:12<00:41, 3276909.36it/s] 21%|██        | 35160064/170498071 [00:12<00:39, 3407042.33it/s] 21%|██        | 35553280/170498071 [00:12<00:42, 3187098.97it/s] 21%|██        | 35979264/170498071 [00:12<00:39, 3424145.29it/s] 21%|██▏       | 36339712/170498071 [00:12<00:40, 3303232.43it/s] 22%|██▏       | 36700160/170498071 [00:12<00:42, 3184952.63it/s] 22%|██▏       | 37027840/170498071 [00:12<00:41, 3200427.59it/s] 22%|██▏       | 37355520/170498071 [00:13<00:41, 3219262.89it/s] 22%|██▏       | 37683200/170498071 [00:13<00:41, 3211494.69it/s] 22%|██▏       | 38010880/170498071 [00:13<00:41, 3206660.16it/s] 23%|██▎       | 38371328/170498071 [00:13<00:40, 3230363.56it/s] 23%|██▎       | 38699008/170498071 [00:13<00:40, 3242209.80it/s] 23%|██▎       | 39059456/170498071 [00:13<00:40, 3266593.45it/s] 23%|██▎       | 39387136/170498071 [00:13<00:40, 3239635.41it/s] 23%|██▎       | 39747584/170498071 [00:13<00:40, 3265361.17it/s] 24%|██▎       | 40108032/170498071 [00:13<00:39, 3273449.45it/s] 24%|██▎       | 40435712/170498071 [00:13<00:39, 3254531.67it/s] 24%|██▍       | 40763392/170498071 [00:14<00:40, 3232363.58it/s] 24%|██▍       | 41091072/170498071 [00:14<00:39, 3240285.51it/s] 24%|██▍       | 41418752/170498071 [00:14<00:40, 3226635.60it/s] 25%|██▍       | 41779200/170498071 [00:14<00:39, 3257312.31it/s] 25%|██▍       | 42106880/170498071 [00:14<00:39, 3242034.78it/s] 25%|██▍       | 42467328/170498071 [00:14<00:39, 3280509.57it/s] 25%|██▌       | 42827776/170498071 [00:14<00:38, 3291224.38it/s] 25%|██▌       | 43188224/170498071 [00:14<00:38, 3295210.46it/s] 26%|██▌       | 43548672/170498071 [00:14<00:38, 3295687.01it/s] 26%|██▌       | 43909120/170498071 [00:15<00:38, 3303646.71it/s] 26%|██▌       | 44269568/170498071 [00:15<00:37, 3332170.68it/s] 26%|██▌       | 44630016/170498071 [00:15<00:38, 3303184.20it/s] 26%|██▋       | 44990464/170498071 [00:15<00:38, 3300941.30it/s] 27%|██▋       | 45350912/170498071 [00:15<00:37, 3313366.54it/s] 27%|██▋       | 45711360/170498071 [00:15<00:37, 3284531.18it/s] 27%|██▋       | 46071808/170498071 [00:15<00:38, 3257363.06it/s] 27%|██▋       | 46399488/170498071 [00:15<00:38, 3220798.69it/s] 27%|██▋       | 46727168/170498071 [00:15<00:38, 3187031.62it/s] 28%|██▊       | 47054848/170498071 [00:16<00:38, 3174004.11it/s] 28%|██▊       | 47382528/170498071 [00:16<00:39, 3117346.21it/s] 28%|██▊       | 47710208/170498071 [00:16<00:39, 3135725.36it/s] 28%|██▊       | 48037888/170498071 [00:16<00:39, 3116643.92it/s] 28%|██▊       | 48365568/170498071 [00:16<00:39, 3091438.60it/s] 29%|██▊       | 48693248/170498071 [00:16<00:39, 3082631.97it/s] 29%|██▉       | 49020928/170498071 [00:16<00:39, 3062785.21it/s] 29%|██▉       | 49348608/170498071 [00:16<00:39, 3031466.25it/s] 29%|██▉       | 49676288/170498071 [00:16<00:39, 3057146.84it/s] 29%|██▉       | 50003968/170498071 [00:16<00:40, 3010372.45it/s] 30%|██▉       | 50331648/170498071 [00:17<00:39, 3031846.73it/s] 30%|██▉       | 50659328/170498071 [00:17<00:41, 2898056.59it/s] 30%|██▉       | 51019776/170498071 [00:17<00:38, 3064597.90it/s] 30%|███       | 51347456/170498071 [00:17<00:38, 3078816.50it/s] 30%|███       | 51675136/170498071 [00:17<00:38, 3088949.02it/s] 31%|███       | 52101120/170498071 [00:17<00:34, 3390318.50it/s] 31%|███       | 52461568/170498071 [00:17<00:34, 3391427.52it/s] 31%|███       | 52822016/170498071 [00:17<00:34, 3430748.98it/s] 31%|███       | 53248000/170498071 [00:17<00:32, 3607036.83it/s] 31%|███▏      | 53641216/170498071 [00:18<00:32, 3620566.50it/s] 32%|███▏      | 54034432/170498071 [00:18<00:31, 3656957.99it/s] 32%|███▏      | 54460416/170498071 [00:18<00:30, 3814912.66it/s] 32%|███▏      | 54853632/170498071 [00:18<00:30, 3806088.53it/s] 32%|███▏      | 55246848/170498071 [00:18<00:30, 3822082.05it/s] 33%|███▎      | 55705600/170498071 [00:18<00:28, 3968105.60it/s] 33%|███▎      | 56131584/170498071 [00:18<00:29, 3861918.13it/s] 33%|███▎      | 56524800/170498071 [00:18<00:33, 3394172.00it/s] 33%|███▎      | 56918016/170498071 [00:19<00:57, 1966482.42it/s] 34%|███▍      | 58556416/170498071 [00:19<00:25, 4349180.25it/s] 35%|███▍      | 59211776/170498071 [00:19<00:27, 4073839.34it/s] 35%|███▌      | 59768832/170498071 [00:19<00:32, 3421175.43it/s] 35%|███▌      | 60227584/170498071 [00:19<00:31, 3557079.71it/s] 36%|███▌      | 60686336/170498071 [00:20<00:33, 3256605.99it/s] 36%|███▌      | 61177856/170498071 [00:20<00:31, 3455278.35it/s] 36%|███▌      | 61571072/170498071 [00:20<00:33, 3210187.39it/s] 36%|███▋      | 62095360/170498071 [00:20<00:30, 3599290.55it/s] 37%|███▋      | 62521344/170498071 [00:20<00:33, 3199297.51it/s] 37%|███▋      | 62947328/170498071 [00:20<00:31, 3421560.19it/s] 37%|███▋      | 63340544/170498071 [00:20<00:31, 3377513.14it/s] 37%|███▋      | 63700992/170498071 [00:20<00:33, 3224550.78it/s] 38%|███▊      | 64192512/170498071 [00:21<00:29, 3595145.29it/s] 38%|███▊      | 64585728/170498071 [00:21<00:31, 3345051.53it/s] 38%|███▊      | 65044480/170498071 [00:21<00:28, 3641454.63it/s] 38%|███▊      | 65437696/170498071 [00:21<00:29, 3567749.83it/s] 39%|███▊      | 65830912/170498071 [00:21<00:30, 3384136.68it/s] 39%|███▉      | 66256896/170498071 [00:21<00:29, 3563369.87it/s] 39%|███▉      | 66650112/170498071 [00:21<00:29, 3475038.91it/s] 39%|███▉      | 67141632/170498071 [00:21<00:27, 3702030.16it/s] 40%|███▉      | 67600384/170498071 [00:22<00:26, 3818232.88it/s] 40%|███▉      | 67993600/170498071 [00:22<00:28, 3587244.54it/s] 40%|████      | 68452352/170498071 [00:22<00:26, 3781281.06it/s] 40%|████      | 68845568/170498071 [00:22<00:27, 3636260.70it/s] 41%|████      | 69271552/170498071 [00:22<00:26, 3776720.29it/s] 41%|████      | 69697536/170498071 [00:22<00:25, 3882779.21it/s] 41%|████      | 70090752/170498071 [00:22<00:26, 3756090.80it/s] 41%|████▏     | 70549504/170498071 [00:22<00:25, 3918238.34it/s] 42%|████▏     | 70975488/170498071 [00:22<00:24, 3993736.41it/s] 42%|████▏     | 71401472/170498071 [00:23<00:26, 3745430.25it/s] 42%|████▏     | 71892992/170498071 [00:23<00:24, 4006987.20it/s] 42%|████▏     | 72318976/170498071 [00:23<00:25, 3862890.70it/s] 43%|████▎     | 72744960/170498071 [00:23<00:24, 3929579.70it/s] 43%|████▎     | 73203712/170498071 [00:23<00:23, 4070375.45it/s] 43%|████▎     | 73629696/170498071 [00:23<00:24, 3945358.10it/s] 43%|████▎     | 74088448/170498071 [00:23<00:23, 4022937.20it/s] 44%|████▎     | 74514432/170498071 [00:23<00:24, 3923179.38it/s] 44%|████▍     | 74973184/170498071 [00:23<00:24, 3949966.03it/s] 44%|████▍     | 75431936/170498071 [00:24<00:23, 4073931.92it/s] 44%|████▍     | 75857920/170498071 [00:24<00:23, 4111985.92it/s] 45%|████▍     | 76283904/170498071 [00:24<00:23, 4074142.26it/s] 45%|████▍     | 76709888/170498071 [00:24<00:22, 4115844.95it/s] 45%|████▌     | 77135872/170498071 [00:24<00:23, 4046644.45it/s] 45%|████▌     | 77561856/170498071 [00:24<00:22, 4065503.41it/s] 46%|████▌     | 77987840/170498071 [00:24<00:22, 4099397.29it/s] 46%|████▌     | 78413824/170498071 [00:24<00:22, 4120402.61it/s] 46%|████▌     | 78839808/170498071 [00:24<00:22, 4142557.52it/s] 46%|████▋     | 79265792/170498071 [00:24<00:22, 4115192.92it/s] 47%|████▋     | 79691776/170498071 [00:25<00:21, 4150991.59it/s] 47%|████▋     | 80150528/170498071 [00:25<00:21, 4226685.17it/s] 47%|████▋     | 80576512/170498071 [00:25<00:22, 4047375.26it/s] 48%|████▊     | 81068032/170498071 [00:25<00:21, 4214968.64it/s] 48%|████▊     | 81526784/170498071 [00:25<00:21, 4217864.53it/s] 48%|████▊     | 81952768/170498071 [00:25<00:21, 4207886.45it/s] 48%|████▊     | 82378752/170498071 [00:25<00:21, 4115303.71it/s] 49%|████▊     | 82837504/170498071 [00:25<00:20, 4206904.24it/s] 49%|████▉     | 83296256/170498071 [00:25<00:21, 4118990.85it/s] 49%|████▉     | 83722240/170498071 [00:26<00:20, 4158346.98it/s] 49%|████▉     | 84148224/170498071 [00:26<00:20, 4148499.55it/s] 50%|████▉     | 84606976/170498071 [00:26<00:20, 4241013.50it/s] 50%|████▉     | 85032960/170498071 [00:26<00:20, 4231093.68it/s] 50%|█████     | 85458944/170498071 [00:26<00:20, 4196503.07it/s] 50%|█████     | 85917696/170498071 [00:26<00:19, 4283196.31it/s] 51%|█████     | 86376448/170498071 [00:26<00:19, 4277684.73it/s] 51%|█████     | 86835200/170498071 [00:26<00:20, 4137452.13it/s] 51%|█████     | 87293952/170498071 [00:26<00:20, 4130489.02it/s] 51%|█████▏    | 87719936/170498071 [00:26<00:20, 4128233.72it/s] 52%|█████▏    | 88211456/170498071 [00:27<00:18, 4345555.91it/s] 52%|█████▏    | 88670208/170498071 [00:27<00:19, 4140434.59it/s] 52%|█████▏    | 89096192/170498071 [00:27<00:19, 4103610.23it/s] 53%|█████▎    | 89554944/170498071 [00:27<00:19, 4236857.74it/s] 53%|█████▎    | 90013696/170498071 [00:27<00:19, 4167150.27it/s] 53%|█████▎    | 90472448/170498071 [00:27<00:19, 4152588.80it/s] 53%|█████▎    | 90963968/170498071 [00:27<00:18, 4357584.50it/s] 54%|█████▎    | 91422720/170498071 [00:27<00:18, 4255048.25it/s] 54%|█████▍    | 91881472/170498071 [00:27<00:18, 4290230.75it/s] 54%|█████▍    | 92340224/170498071 [00:28<00:18, 4159725.28it/s] 54%|█████▍    | 92766208/170498071 [00:28<00:19, 4017623.40it/s] 55%|█████▍    | 93257728/170498071 [00:28<00:18, 4253839.28it/s] 55%|█████▍    | 93716480/170498071 [00:28<00:18, 4154341.17it/s] 55%|█████▌    | 94208000/170498071 [00:28<00:17, 4308928.43it/s] 56%|█████▌    | 94666752/170498071 [00:28<00:17, 4338513.43it/s] 56%|█████▌    | 95125504/170498071 [00:28<00:17, 4272151.74it/s] 56%|█████▌    | 95584256/170498071 [00:28<00:17, 4193581.28it/s] 56%|█████▋    | 96043008/170498071 [00:28<00:17, 4166314.18it/s] 57%|█████▋    | 96468992/170498071 [00:29<00:18, 4014017.85it/s] 57%|█████▋    | 96993280/170498071 [00:29<00:17, 4207584.43it/s] 57%|█████▋    | 97419264/170498071 [00:29<00:18, 4050308.44it/s] 57%|█████▋    | 97943552/170498071 [00:29<00:16, 4371235.24it/s] 58%|█████▊    | 98402304/170498071 [00:29<00:18, 3991273.83it/s] 58%|█████▊    | 98926592/170498071 [00:29<00:17, 4209458.78it/s] 58%|█████▊    | 99450880/170498071 [00:29<00:15, 4471228.29it/s] 59%|█████▊    | 99909632/170498071 [00:29<00:17, 4071488.82it/s] 59%|█████▉    | 100433920/170498071 [00:29<00:16, 4360208.78it/s] 59%|█████▉    | 100892672/170498071 [00:30<00:17, 4023055.19it/s] 59%|█████▉    | 101384192/170498071 [00:30<00:16, 4247128.51it/s] 60%|█████▉    | 101842944/170498071 [00:30<00:17, 3856389.91it/s] 60%|██████    | 102301696/170498071 [00:30<00:16, 4021209.29it/s] 60%|██████    | 102727680/170498071 [00:30<00:17, 3907631.50it/s] 61%|██████    | 103251968/170498071 [00:30<00:16, 4133355.66it/s] 61%|██████    | 103776256/170498071 [00:30<00:15, 4385896.21it/s] 61%|██████    | 104235008/170498071 [00:30<00:16, 4138802.55it/s] 61%|██████▏   | 104660992/170498071 [00:31<00:16, 4057856.83it/s] 62%|██████▏   | 105218048/170498071 [00:31<00:15, 4306584.93it/s] 62%|██████▏   | 105676800/170498071 [00:31<00:15, 4306660.40it/s] 62%|██████▏   | 106135552/170498071 [00:31<00:14, 4337466.57it/s] 63%|██████▎   | 106594304/170498071 [00:31<00:15, 4016735.92it/s] 63%|██████▎   | 107085824/170498071 [00:31<00:15, 4211264.09it/s] 63%|██████▎   | 107675648/170498071 [00:31<00:13, 4653502.17it/s] 63%|██████▎   | 108167168/170498071 [00:31<00:14, 4238966.77it/s] 64%|██████▎   | 108625920/170498071 [00:31<00:14, 4303344.61it/s] 64%|██████▍   | 109084672/170498071 [00:32<00:15, 4031174.47it/s] 64%|██████▍   | 109608960/170498071 [00:32<00:14, 4327195.61it/s] 65%|██████▍   | 110166016/170498071 [00:32<00:13, 4632855.70it/s] 65%|██████▍   | 110657536/170498071 [00:32<00:13, 4279872.41it/s] 65%|██████▌   | 111116288/170498071 [00:32<00:14, 4234928.32it/s] 65%|██████▌   | 111640576/170498071 [00:32<00:13, 4215876.71it/s] 66%|██████▌   | 112164864/170498071 [00:32<00:13, 4466468.47it/s] 66%|██████▌   | 112656384/170498071 [00:32<00:12, 4582537.43it/s] 66%|██████▋   | 113147904/170498071 [00:32<00:13, 4325978.50it/s] 67%|██████▋   | 113606656/170498071 [00:33<00:13, 4347398.28it/s] 67%|██████▋   | 114196480/170498071 [00:33<00:11, 4703707.80it/s] 67%|██████▋   | 114688000/170498071 [00:33<00:12, 4421675.61it/s] 68%|██████▊   | 115146752/170498071 [00:33<00:14, 3877154.01it/s] 68%|██████▊   | 115572736/170498071 [00:33<00:15, 3654176.28it/s] 68%|██████▊   | 115965952/170498071 [00:34<00:27, 1999583.12it/s] 69%|██████▉   | 117800960/170498071 [00:34<00:11, 4503252.78it/s] 69%|██████▉   | 118489088/170498071 [00:34<00:12, 4085525.25it/s] 70%|██████▉   | 119078912/170498071 [00:34<00:13, 3750998.07it/s] 70%|███████   | 119570432/170498071 [00:34<00:13, 3827972.72it/s] 70%|███████   | 120061952/170498071 [00:34<00:14, 3549564.77it/s] 71%|███████   | 120520704/170498071 [00:34<00:13, 3726573.25it/s] 71%|███████   | 120946688/170498071 [00:35<00:14, 3329538.80it/s] 71%|███████   | 121470976/170498071 [00:35<00:13, 3718568.49it/s] 71%|███████▏  | 121896960/170498071 [00:35<00:13, 3595233.09it/s] 72%|███████▏  | 122290176/170498071 [00:35<00:13, 3673022.82it/s] 72%|███████▏  | 122748928/170498071 [00:35<00:12, 3855709.45it/s] 72%|███████▏  | 123174912/170498071 [00:35<00:13, 3418135.67it/s] 73%|███████▎  | 123699200/170498071 [00:35<00:12, 3829795.85it/s] 73%|███████▎  | 124125184/170498071 [00:35<00:12, 3850193.67it/s] 73%|███████▎  | 124551168/170498071 [00:36<00:12, 3787094.82it/s] 73%|███████▎  | 125042688/170498071 [00:36<00:11, 4014902.25it/s] 74%|███████▎  | 125468672/170498071 [00:36<00:12, 3556977.80it/s] 74%|███████▍  | 126025728/170498071 [00:36<00:11, 4014039.49it/s] 74%|███████▍  | 126451712/170498071 [00:36<00:10, 4057859.90it/s] 74%|███████▍  | 126877696/170498071 [00:36<00:11, 3894198.48it/s] 75%|███████▍  | 127401984/170498071 [00:36<00:10, 4207970.08it/s] 75%|███████▍  | 127860736/170498071 [00:36<00:11, 3709917.23it/s] 75%|███████▌  | 128450560/170498071 [00:37<00:10, 4185267.60it/s] 76%|███████▌  | 128909312/170498071 [00:37<00:10, 4033827.49it/s] 76%|███████▌  | 129368064/170498071 [00:37<00:09, 4155973.16it/s] 76%|███████▌  | 129892352/170498071 [00:37<00:09, 4388312.06it/s] 76%|███████▋  | 130351104/170498071 [00:37<00:10, 3834609.54it/s] 77%|███████▋  | 130940928/170498071 [00:37<00:09, 4337125.73it/s] 77%|███████▋  | 131399680/170498071 [00:37<00:09, 4322911.50it/s] 77%|███████▋  | 131858432/170498071 [00:37<00:09, 4146250.55it/s] 78%|███████▊  | 132415488/170498071 [00:37<00:08, 4518436.95it/s] 78%|███████▊  | 132907008/170498071 [00:38<00:09, 3927755.03it/s] 78%|███████▊  | 133496832/170498071 [00:38<00:08, 4317444.38it/s] 79%|███████▊  | 133988352/170498071 [00:38<00:08, 4322967.65it/s] 79%|███████▉  | 134447104/170498071 [00:38<00:08, 4331864.08it/s] 79%|███████▉  | 135004160/170498071 [00:38<00:07, 4575765.44it/s] 79%|███████▉  | 135495680/170498071 [00:38<00:08, 4057744.09it/s] 80%|███████▉  | 136085504/170498071 [00:38<00:07, 4459069.61it/s] 80%|████████  | 136577024/170498071 [00:38<00:07, 4386465.23it/s] 80%|████████  | 137035776/170498071 [00:39<00:07, 4275232.02it/s] 81%|████████  | 137658368/170498071 [00:39<00:06, 4752257.89it/s] 81%|████████  | 138149888/170498071 [00:39<00:07, 4080696.09it/s] 81%|████████▏ | 138772480/170498071 [00:39<00:06, 4596225.47it/s] 82%|████████▏ | 139264000/170498071 [00:39<00:07, 4046584.85it/s] 82%|████████▏ | 139952128/170498071 [00:39<00:06, 4654745.90it/s] 82%|████████▏ | 140476416/170498071 [00:39<00:06, 4478466.50it/s] 83%|████████▎ | 140967936/170498071 [00:39<00:06, 4371343.01it/s] 83%|████████▎ | 141459456/170498071 [00:40<00:06, 4494950.05it/s] 83%|████████▎ | 141950976/170498071 [00:40<00:06, 4204850.46it/s] 84%|████████▎ | 142639104/170498071 [00:40<00:05, 4808457.68it/s] 84%|████████▍ | 143163392/170498071 [00:40<00:05, 4571811.23it/s] 84%|████████▍ | 143654912/170498071 [00:40<00:06, 4458785.79it/s] 85%|████████▍ | 144146432/170498071 [00:40<00:05, 4555302.31it/s] 85%|████████▍ | 144637952/170498071 [00:40<00:06, 4243991.56it/s] 85%|████████▌ | 145326080/170498071 [00:40<00:05, 4864944.34it/s] 86%|████████▌ | 145850368/170498071 [00:41<00:05, 4364275.90it/s] 86%|████████▌ | 146374656/170498071 [00:41<00:05, 4583930.49it/s] 86%|████████▌ | 146866176/170498071 [00:41<00:05, 4652720.82it/s] 86%|████████▋ | 147357696/170498071 [00:41<00:05, 4166386.68it/s] 87%|████████▋ | 148078592/170498071 [00:41<00:04, 4875239.53it/s] 87%|████████▋ | 148602880/170498071 [00:41<00:04, 4493253.24it/s] 87%|████████▋ | 149127168/170498071 [00:41<00:04, 4652970.81it/s] 88%|████████▊ | 149618688/170498071 [00:41<00:04, 4567175.70it/s] 88%|████████▊ | 150110208/170498071 [00:41<00:04, 4197342.12it/s] 88%|████████▊ | 150831104/170498071 [00:42<00:04, 4848208.97it/s] 89%|████████▉ | 151355392/170498071 [00:42<00:04, 4587097.33it/s] 89%|████████▉ | 151879680/170498071 [00:42<00:03, 4730708.36it/s] 89%|████████▉ | 152371200/170498071 [00:42<00:03, 4610997.63it/s] 90%|████████▉ | 152862720/170498071 [00:42<00:04, 4211532.41it/s] 90%|█████████ | 153550848/170498071 [00:42<00:03, 4856447.83it/s] 90%|█████████ | 154075136/170498071 [00:42<00:03, 4574630.39it/s] 91%|█████████ | 154599424/170498071 [00:42<00:03, 4730082.03it/s] 91%|█████████ | 155090944/170498071 [00:43<00:03, 4606498.82it/s] 91%|█████████▏| 155582464/170498071 [00:43<00:03, 4217432.13it/s] 92%|█████████▏| 156270592/170498071 [00:43<00:02, 4852959.13it/s] 92%|█████████▏| 156794880/170498071 [00:43<00:02, 4574806.01it/s] 92%|█████████▏| 157319168/170498071 [00:43<00:02, 4714976.82it/s] 93%|█████████▎| 157810688/170498071 [00:43<00:02, 4592476.00it/s] 93%|█████████▎| 158302208/170498071 [00:43<00:02, 4229496.98it/s] 93%|█████████▎| 158990336/170498071 [00:43<00:02, 4721276.27it/s] 94%|█████████▎| 159481856/170498071 [00:43<00:02, 4582123.18it/s] 94%|█████████▍| 159973376/170498071 [00:44<00:02, 4626755.70it/s] 94%|█████████▍| 160464896/170498071 [00:44<00:02, 4622430.69it/s] 94%|█████████▍| 160956416/170498071 [00:44<00:02, 4240812.80it/s] 95%|█████████▍| 161677312/170498071 [00:44<00:01, 4932393.14it/s] 95%|█████████▌| 162201600/170498071 [00:44<00:01, 4574514.39it/s] 95%|█████████▌| 162693120/170498071 [00:44<00:01, 4398924.62it/s] 96%|█████████▌| 163250176/170498071 [00:44<00:01, 4616519.20it/s] 96%|█████████▌| 163741696/170498071 [00:44<00:01, 4363827.98it/s] 96%|█████████▋| 164331520/170498071 [00:45<00:01, 4736439.95it/s] 97%|█████████▋| 164823040/170498071 [00:45<00:01, 4560667.35it/s] 97%|█████████▋| 165314560/170498071 [00:45<00:01, 4403166.09it/s] 97%|█████████▋| 165937152/170498071 [00:45<00:00, 4841142.13it/s] 98%|█████████▊| 166461440/170498071 [00:45<00:00, 4406699.30it/s] 98%|█████████▊| 167051264/170498071 [00:45<00:00, 4776444.72it/s] 98%|█████████▊| 167575552/170498071 [00:45<00:00, 4598047.17it/s] 99%|█████████▊| 168067072/170498071 [00:45<00:00, 4452541.78it/s] 99%|█████████▉| 168689664/170498071 [00:45<00:00, 4856031.65it/s] 99%|█████████▉| 169213952/170498071 [00:46<00:00, 4448201.79it/s]100%|█████████▉| 169803776/170498071 [00:46<00:00, 4769756.23it/s]100%|█████████▉| 170295296/170498071 [00:46<00:00, 4601756.61it/s]100%|██████████| 170498071/170498071 [00:46<00:00, 3673280.63it/s]
/data/happythgus/repos/lda/DatasetCondensation/main_DM.py:94: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
/data/happythgus/repos/lda/DatasetCondensation/main_DM.py:94: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
/home/happythgus/.local/lib/python3.9/site-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/home/happythgus/.local/lib/python3.9/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/home/happythgus/.local/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Extracting data/cifar-10-python.tar.gz to data
Files already downloaded and verified

================== Exp 0 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5173a57f40>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-04 12:58:59] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}
[2023-10-04 12:59:25] Evaluate_00: epoch = 1000 train time = 23 s train loss = 0.015503 train acc = 0.9980, test acc = 0.4877
[2023-10-04 12:59:49] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.015771 train acc = 1.0000, test acc = 0.4786
[2023-10-04 13:00:14] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.004297 train acc = 1.0000, test acc = 0.4874
[2023-10-04 13:00:38] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002860 train acc = 1.0000, test acc = 0.4872
[2023-10-04 13:01:02] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.006542 train acc = 1.0000, test acc = 0.4821
[2023-10-04 13:01:27] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.006767 train acc = 1.0000, test acc = 0.4832
[2023-10-04 13:01:51] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.000798 train acc = 1.0000, test acc = 0.4898
[2023-10-04 13:02:16] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003273 train acc = 1.0000, test acc = 0.4848
[2023-10-04 13:02:40] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.005848 train acc = 1.0000, test acc = 0.4910
[2023-10-04 13:03:05] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002377 train acc = 1.0000, test acc = 0.4863
[2023-10-04 13:03:29] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.015463 train acc = 1.0000, test acc = 0.4943
[2023-10-04 13:03:54] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.005081 train acc = 1.0000, test acc = 0.4900
[2023-10-04 13:04:18] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011921 train acc = 1.0000, test acc = 0.4890
[2023-10-04 13:04:43] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.010131 train acc = 0.9980, test acc = 0.4868
[2023-10-04 13:05:07] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.007479 train acc = 1.0000, test acc = 0.4851
[2023-10-04 13:05:32] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.009137 train acc = 1.0000, test acc = 0.4888
[2023-10-04 13:05:56] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.006200 train acc = 1.0000, test acc = 0.4944
[2023-10-04 13:06:21] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.015387 train acc = 0.9980, test acc = 0.4937
[2023-10-04 13:06:45] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.015278 train acc = 1.0000, test acc = 0.4860
[2023-10-04 13:07:10] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.016406 train acc = 1.0000, test acc = 0.4865
Evaluate 20 random ConvNet, mean = 0.4876 std = 0.0039
-------------------------
[2023-10-04 13:07:10] iter = 00000, loss = 6.3116
[2023-10-04 13:07:11] iter = 00010, loss = 5.8270
[2023-10-04 13:07:12] iter = 00020, loss = 5.3840
[2023-10-04 13:07:13] iter = 00030, loss = 4.7029
[2023-10-04 13:07:14] iter = 00040, loss = 4.7243
[2023-10-04 13:07:14] iter = 00050, loss = 4.0763
[2023-10-04 13:07:15] iter = 00060, loss = 4.1813
[2023-10-04 13:07:16] iter = 00070, loss = 3.9236
[2023-10-04 13:07:17] iter = 00080, loss = 3.4308
[2023-10-04 13:07:18] iter = 00090, loss = 3.8434
[2023-10-04 13:07:19] iter = 00100, loss = 3.5015
[2023-10-04 13:07:20] iter = 00110, loss = 3.3326
[2023-10-04 13:07:21] iter = 00120, loss = 3.2300
[2023-10-04 13:07:22] iter = 00130, loss = 2.8241
[2023-10-04 13:07:23] iter = 00140, loss = 2.9618
[2023-10-04 13:07:24] iter = 00150, loss = 2.8562
[2023-10-04 13:07:25] iter = 00160, loss = 3.0156
[2023-10-04 13:07:26] iter = 00170, loss = 2.8020
[2023-10-04 13:07:27] iter = 00180, loss = 2.8236
[2023-10-04 13:07:27] iter = 00190, loss = 2.8844
[2023-10-04 13:07:28] iter = 00200, loss = 2.7772
[2023-10-04 13:07:29] iter = 00210, loss = 2.6148
[2023-10-04 13:07:30] iter = 00220, loss = 2.7037
[2023-10-04 13:07:31] iter = 00230, loss = 2.7343
[2023-10-04 13:07:32] iter = 00240, loss = 2.7774
[2023-10-04 13:07:33] iter = 00250, loss = 2.6166
[2023-10-04 13:07:34] iter = 00260, loss = 2.5053
[2023-10-04 13:07:35] iter = 00270, loss = 2.5719
[2023-10-04 13:07:36] iter = 00280, loss = 2.5317
[2023-10-04 13:07:36] iter = 00290, loss = 2.4370
[2023-10-04 13:07:37] iter = 00300, loss = 2.4465
[2023-10-04 13:07:38] iter = 00310, loss = 2.4726
[2023-10-04 13:07:39] iter = 00320, loss = 2.4393
[2023-10-04 13:07:40] iter = 00330, loss = 2.4188
[2023-10-04 13:07:41] iter = 00340, loss = 2.6220
[2023-10-04 13:07:42] iter = 00350, loss = 2.7144
[2023-10-04 13:07:43] iter = 00360, loss = 2.5447
[2023-10-04 13:07:44] iter = 00370, loss = 2.3283
[2023-10-04 13:07:45] iter = 00380, loss = 2.3883
[2023-10-04 13:07:46] iter = 00390, loss = 2.2990
[2023-10-04 13:07:47] iter = 00400, loss = 2.3563
[2023-10-04 13:07:47] iter = 00410, loss = 2.2733
[2023-10-04 13:07:48] iter = 00420, loss = 2.2270
[2023-10-04 13:07:49] iter = 00430, loss = 2.1927
[2023-10-04 13:07:50] iter = 00440, loss = 2.2086
[2023-10-04 13:07:51] iter = 00450, loss = 2.2515
[2023-10-04 13:07:52] iter = 00460, loss = 2.2934
[2023-10-04 13:07:53] iter = 00470, loss = 2.1987
[2023-10-04 13:07:54] iter = 00480, loss = 2.1774
[2023-10-04 13:07:55] iter = 00490, loss = 2.2899
[2023-10-04 13:07:56] iter = 00500, loss = 2.0967
[2023-10-04 13:07:57] iter = 00510, loss = 2.2902
[2023-10-04 13:07:58] iter = 00520, loss = 2.0732
[2023-10-04 13:07:58] iter = 00530, loss = 2.3269
[2023-10-04 13:07:59] iter = 00540, loss = 2.1478
[2023-10-04 13:08:00] iter = 00550, loss = 2.2268
[2023-10-04 13:08:01] iter = 00560, loss = 2.1574
[2023-10-04 13:08:02] iter = 00570, loss = 2.1689
[2023-10-04 13:08:03] iter = 00580, loss = 2.2850
[2023-10-04 13:08:04] iter = 00590, loss = 2.3139
[2023-10-04 13:08:05] iter = 00600, loss = 2.0987
[2023-10-04 13:08:06] iter = 00610, loss = 2.2137
[2023-10-04 13:08:07] iter = 00620, loss = 2.1092
[2023-10-04 13:08:08] iter = 00630, loss = 2.2508
[2023-10-04 13:08:09] iter = 00640, loss = 2.1349
[2023-10-04 13:08:10] iter = 00650, loss = 2.1031
[2023-10-04 13:08:11] iter = 00660, loss = 2.0258
[2023-10-04 13:08:12] iter = 00670, loss = 2.0001
[2023-10-04 13:08:13] iter = 00680, loss = 2.2492
[2023-10-04 13:08:14] iter = 00690, loss = 2.2259
[2023-10-04 13:08:14] iter = 00700, loss = 2.1037
[2023-10-04 13:08:15] iter = 00710, loss = 2.3083
[2023-10-04 13:08:16] iter = 00720, loss = 2.2969
[2023-10-04 13:08:17] iter = 00730, loss = 2.2336
[2023-10-04 13:08:18] iter = 00740, loss = 2.1680
[2023-10-04 13:08:19] iter = 00750, loss = 2.1342
[2023-10-04 13:08:20] iter = 00760, loss = 2.1058
[2023-10-04 13:08:21] iter = 00770, loss = 2.1312
[2023-10-04 13:08:22] iter = 00780, loss = 2.0403
[2023-10-04 13:08:23] iter = 00790, loss = 2.0020
[2023-10-04 13:08:24] iter = 00800, loss = 2.1022
[2023-10-04 13:08:24] iter = 00810, loss = 2.0832
[2023-10-04 13:08:25] iter = 00820, loss = 2.0610
[2023-10-04 13:08:26] iter = 00830, loss = 2.1777
[2023-10-04 13:08:27] iter = 00840, loss = 2.0351
[2023-10-04 13:08:28] iter = 00850, loss = 1.9161
[2023-10-04 13:08:29] iter = 00860, loss = 1.9868
[2023-10-04 13:08:30] iter = 00870, loss = 2.0872
[2023-10-04 13:08:31] iter = 00880, loss = 2.2500
[2023-10-04 13:08:32] iter = 00890, loss = 2.0706
[2023-10-04 13:08:33] iter = 00900, loss = 1.9689
[2023-10-04 13:08:34] iter = 00910, loss = 2.0510
[2023-10-04 13:08:35] iter = 00920, loss = 2.0787
[2023-10-04 13:08:36] iter = 00930, loss = 1.9534
[2023-10-04 13:08:37] iter = 00940, loss = 2.0444
[2023-10-04 13:08:38] iter = 00950, loss = 2.0547
[2023-10-04 13:08:39] iter = 00960, loss = 2.0575
[2023-10-04 13:08:39] iter = 00970, loss = 1.9198
[2023-10-04 13:08:40] iter = 00980, loss = 2.0208
[2023-10-04 13:08:41] iter = 00990, loss = 2.0131
[2023-10-04 13:08:42] iter = 01000, loss = 2.1212
[2023-10-04 13:08:43] iter = 01010, loss = 1.9577
[2023-10-04 13:08:44] iter = 01020, loss = 2.0059
[2023-10-04 13:08:45] iter = 01030, loss = 1.9684
[2023-10-04 13:08:46] iter = 01040, loss = 2.0608
[2023-10-04 13:08:47] iter = 01050, loss = 2.0191
[2023-10-04 13:08:48] iter = 01060, loss = 1.9806
[2023-10-04 13:08:49] iter = 01070, loss = 1.8190
[2023-10-04 13:08:49] iter = 01080, loss = 2.0133
[2023-10-04 13:08:50] iter = 01090, loss = 2.2677
[2023-10-04 13:08:51] iter = 01100, loss = 2.1053
[2023-10-04 13:08:52] iter = 01110, loss = 1.8706
[2023-10-04 13:08:53] iter = 01120, loss = 2.0581
[2023-10-04 13:08:54] iter = 01130, loss = 2.0932
[2023-10-04 13:08:55] iter = 01140, loss = 1.9499
[2023-10-04 13:08:56] iter = 01150, loss = 1.9334
[2023-10-04 13:08:57] iter = 01160, loss = 1.8891
[2023-10-04 13:08:58] iter = 01170, loss = 1.9975
[2023-10-04 13:08:59] iter = 01180, loss = 1.8022
[2023-10-04 13:09:00] iter = 01190, loss = 1.8946
[2023-10-04 13:09:01] iter = 01200, loss = 1.9584
[2023-10-04 13:09:02] iter = 01210, loss = 1.9979
[2023-10-04 13:09:03] iter = 01220, loss = 1.9482
[2023-10-04 13:09:04] iter = 01230, loss = 1.9403
[2023-10-04 13:09:04] iter = 01240, loss = 1.9291
[2023-10-04 13:09:05] iter = 01250, loss = 2.0251
[2023-10-04 13:09:06] iter = 01260, loss = 1.9418
[2023-10-04 13:09:07] iter = 01270, loss = 2.1248
[2023-10-04 13:09:08] iter = 01280, loss = 2.0056
[2023-10-04 13:09:09] iter = 01290, loss = 2.0182
[2023-10-04 13:09:10] iter = 01300, loss = 1.9086
[2023-10-04 13:09:11] iter = 01310, loss = 1.7825
[2023-10-04 13:09:12] iter = 01320, loss = 2.0632
[2023-10-04 13:09:13] iter = 01330, loss = 1.9195
[2023-10-04 13:09:14] iter = 01340, loss = 2.0459
[2023-10-04 13:09:14] iter = 01350, loss = 1.9324
[2023-10-04 13:09:15] iter = 01360, loss = 1.8644
[2023-10-04 13:09:16] iter = 01370, loss = 1.8061
[2023-10-04 13:09:17] iter = 01380, loss = 1.9128
[2023-10-04 13:09:18] iter = 01390, loss = 1.8967
[2023-10-04 13:09:19] iter = 01400, loss = 1.9207
[2023-10-04 13:09:20] iter = 01410, loss = 2.0291
[2023-10-04 13:09:21] iter = 01420, loss = 1.9535
[2023-10-04 13:09:22] iter = 01430, loss = 1.8651
[2023-10-04 13:09:23] iter = 01440, loss = 1.9439
[2023-10-04 13:09:23] iter = 01450, loss = 1.7220
[2023-10-04 13:09:24] iter = 01460, loss = 1.8498
[2023-10-04 13:09:25] iter = 01470, loss = 1.8751
[2023-10-04 13:09:26] iter = 01480, loss = 1.7958
[2023-10-04 13:09:27] iter = 01490, loss = 1.8429
[2023-10-04 13:09:28] iter = 01500, loss = 1.9744
[2023-10-04 13:09:29] iter = 01510, loss = 1.8833
[2023-10-04 13:09:30] iter = 01520, loss = 1.9596
[2023-10-04 13:09:31] iter = 01530, loss = 1.9542
[2023-10-04 13:09:32] iter = 01540, loss = 2.1002
[2023-10-04 13:09:33] iter = 01550, loss = 1.9790
[2023-10-04 13:09:34] iter = 01560, loss = 1.9805
[2023-10-04 13:09:35] iter = 01570, loss = 1.7741
[2023-10-04 13:09:35] iter = 01580, loss = 1.8493
[2023-10-04 13:09:36] iter = 01590, loss = 1.8047
[2023-10-04 13:09:37] iter = 01600, loss = 2.0131
[2023-10-04 13:09:38] iter = 01610, loss = 1.8820
[2023-10-04 13:09:39] iter = 01620, loss = 1.8346
[2023-10-04 13:09:40] iter = 01630, loss = 1.9950
[2023-10-04 13:09:41] iter = 01640, loss = 1.9326
[2023-10-04 13:09:42] iter = 01650, loss = 1.8469
[2023-10-04 13:09:43] iter = 01660, loss = 1.9147
[2023-10-04 13:09:44] iter = 01670, loss = 1.9999
[2023-10-04 13:09:44] iter = 01680, loss = 1.8278
[2023-10-04 13:09:45] iter = 01690, loss = 1.7567
[2023-10-04 13:09:46] iter = 01700, loss = 1.7945
[2023-10-04 13:09:47] iter = 01710, loss = 1.8924
[2023-10-04 13:09:48] iter = 01720, loss = 2.2116
[2023-10-04 13:09:49] iter = 01730, loss = 1.9812
[2023-10-04 13:09:50] iter = 01740, loss = 1.8010
[2023-10-04 13:09:51] iter = 01750, loss = 1.8454
[2023-10-04 13:09:52] iter = 01760, loss = 1.8472
[2023-10-04 13:09:53] iter = 01770, loss = 1.9718
[2023-10-04 13:09:54] iter = 01780, loss = 1.8766
[2023-10-04 13:09:55] iter = 01790, loss = 1.9284
[2023-10-04 13:09:56] iter = 01800, loss = 1.9149
[2023-10-04 13:09:57] iter = 01810, loss = 1.8460
[2023-10-04 13:09:58] iter = 01820, loss = 1.8691
[2023-10-04 13:09:59] iter = 01830, loss = 1.8763
[2023-10-04 13:10:00] iter = 01840, loss = 1.8476
[2023-10-04 13:10:01] iter = 01850, loss = 1.7406
[2023-10-04 13:10:02] iter = 01860, loss = 1.9054
[2023-10-04 13:10:02] iter = 01870, loss = 1.7062
[2023-10-04 13:10:03] iter = 01880, loss = 1.7974
[2023-10-04 13:10:04] iter = 01890, loss = 2.0086
[2023-10-04 13:10:05] iter = 01900, loss = 1.9007
[2023-10-04 13:10:06] iter = 01910, loss = 1.8739
[2023-10-04 13:10:07] iter = 01920, loss = 1.8224
[2023-10-04 13:10:08] iter = 01930, loss = 1.7425
[2023-10-04 13:10:09] iter = 01940, loss = 1.7985
[2023-10-04 13:10:10] iter = 01950, loss = 1.7893
[2023-10-04 13:10:11] iter = 01960, loss = 1.9310
[2023-10-04 13:10:12] iter = 01970, loss = 1.8558
[2023-10-04 13:10:13] iter = 01980, loss = 1.7292
[2023-10-04 13:10:14] iter = 01990, loss = 1.9026
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 14983}
[2023-10-04 13:10:39] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005733 train acc = 1.0000, test acc = 0.5824
[2023-10-04 13:11:03] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.013070 train acc = 1.0000, test acc = 0.5930
[2023-10-04 13:11:28] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.012938 train acc = 1.0000, test acc = 0.5814
[2023-10-04 13:11:53] Evaluate_03: epoch = 1000 train time = 23 s train loss = 0.001932 train acc = 1.0000, test acc = 0.5787
[2023-10-04 13:12:17] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.001676 train acc = 1.0000, test acc = 0.5747
[2023-10-04 13:12:42] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.000928 train acc = 1.0000, test acc = 0.5833
[2023-10-04 13:13:06] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002419 train acc = 1.0000, test acc = 0.5857
[2023-10-04 13:13:31] Evaluate_07: epoch = 1000 train time = 23 s train loss = 0.003708 train acc = 1.0000, test acc = 0.5834
[2023-10-04 13:13:56] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.007886 train acc = 1.0000, test acc = 0.5801
[2023-10-04 13:14:20] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005096 train acc = 1.0000, test acc = 0.5781
[2023-10-04 13:14:45] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002661 train acc = 1.0000, test acc = 0.5841
[2023-10-04 13:15:10] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.009328 train acc = 1.0000, test acc = 0.5852
[2023-10-04 13:15:34] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011190 train acc = 1.0000, test acc = 0.5840
[2023-10-04 13:15:59] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003291 train acc = 1.0000, test acc = 0.5886
[2023-10-04 13:16:23] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.009853 train acc = 1.0000, test acc = 0.5841
[2023-10-04 13:16:48] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.008817 train acc = 0.9980, test acc = 0.5809
[2023-10-04 13:17:12] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.001456 train acc = 1.0000, test acc = 0.5847
[2023-10-04 13:17:37] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003390 train acc = 1.0000, test acc = 0.5898
[2023-10-04 13:18:01] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.005999 train acc = 1.0000, test acc = 0.5872
[2023-10-04 13:18:26] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003295 train acc = 1.0000, test acc = 0.5804
Evaluate 20 random ConvNet, mean = 0.5835 std = 0.0041
-------------------------
[2023-10-04 13:18:26] iter = 02000, loss = 1.7561
[2023-10-04 13:18:27] iter = 02010, loss = 1.8044
[2023-10-04 13:18:28] iter = 02020, loss = 1.7882
[2023-10-04 13:18:29] iter = 02030, loss = 1.8857
[2023-10-04 13:18:30] iter = 02040, loss = 1.7705
[2023-10-04 13:18:31] iter = 02050, loss = 1.8375
[2023-10-04 13:18:32] iter = 02060, loss = 1.8406
[2023-10-04 13:18:33] iter = 02070, loss = 1.8363
[2023-10-04 13:18:34] iter = 02080, loss = 1.9073
[2023-10-04 13:18:35] iter = 02090, loss = 1.7625
[2023-10-04 13:18:35] iter = 02100, loss = 1.8181
[2023-10-04 13:18:36] iter = 02110, loss = 1.7808
[2023-10-04 13:18:37] iter = 02120, loss = 1.9022
[2023-10-04 13:18:38] iter = 02130, loss = 1.7852
[2023-10-04 13:18:39] iter = 02140, loss = 1.7612
[2023-10-04 13:18:40] iter = 02150, loss = 1.8840
[2023-10-04 13:18:41] iter = 02160, loss = 1.7186
[2023-10-04 13:18:42] iter = 02170, loss = 1.8642
[2023-10-04 13:18:43] iter = 02180, loss = 1.7104
[2023-10-04 13:18:44] iter = 02190, loss = 1.9495
[2023-10-04 13:18:45] iter = 02200, loss = 1.7211
[2023-10-04 13:18:45] iter = 02210, loss = 1.7222
[2023-10-04 13:18:46] iter = 02220, loss = 1.8229
[2023-10-04 13:18:47] iter = 02230, loss = 1.8445
[2023-10-04 13:18:48] iter = 02240, loss = 1.8128
[2023-10-04 13:18:49] iter = 02250, loss = 1.6594
[2023-10-04 13:18:50] iter = 02260, loss = 1.7322
[2023-10-04 13:18:51] iter = 02270, loss = 1.8301
[2023-10-04 13:18:52] iter = 02280, loss = 1.8015
[2023-10-04 13:18:53] iter = 02290, loss = 1.7190
[2023-10-04 13:18:54] iter = 02300, loss = 1.9750
[2023-10-04 13:18:55] iter = 02310, loss = 1.8640
[2023-10-04 13:18:56] iter = 02320, loss = 1.7662
[2023-10-04 13:18:57] iter = 02330, loss = 1.7571
[2023-10-04 13:18:57] iter = 02340, loss = 1.7177
[2023-10-04 13:18:58] iter = 02350, loss = 1.8911
[2023-10-04 13:18:59] iter = 02360, loss = 1.7875
[2023-10-04 13:19:00] iter = 02370, loss = 1.6567
[2023-10-04 13:19:01] iter = 02380, loss = 1.7146
[2023-10-04 13:19:02] iter = 02390, loss = 1.8496
[2023-10-04 13:19:03] iter = 02400, loss = 1.7360
[2023-10-04 13:19:04] iter = 02410, loss = 1.8887
[2023-10-04 13:19:05] iter = 02420, loss = 1.7183
[2023-10-04 13:19:06] iter = 02430, loss = 1.7929
[2023-10-04 13:19:07] iter = 02440, loss = 1.9363
[2023-10-04 13:19:08] iter = 02450, loss = 1.7503
[2023-10-04 13:19:09] iter = 02460, loss = 1.8205
[2023-10-04 13:19:10] iter = 02470, loss = 1.9101
[2023-10-04 13:19:10] iter = 02480, loss = 1.7250
[2023-10-04 13:19:11] iter = 02490, loss = 1.6852
[2023-10-04 13:19:12] iter = 02500, loss = 1.6618
[2023-10-04 13:19:13] iter = 02510, loss = 1.7675
[2023-10-04 13:19:14] iter = 02520, loss = 1.9277
[2023-10-04 13:19:15] iter = 02530, loss = 1.6842
[2023-10-04 13:19:16] iter = 02540, loss = 1.7540
[2023-10-04 13:19:17] iter = 02550, loss = 1.6812
[2023-10-04 13:19:18] iter = 02560, loss = 1.6348
[2023-10-04 13:19:19] iter = 02570, loss = 1.6984
[2023-10-04 13:19:20] iter = 02580, loss = 1.8368
[2023-10-04 13:19:20] iter = 02590, loss = 1.9182
[2023-10-04 13:19:21] iter = 02600, loss = 1.7048
[2023-10-04 13:19:22] iter = 02610, loss = 1.7502
[2023-10-04 13:19:23] iter = 02620, loss = 1.7065
[2023-10-04 13:19:24] iter = 02630, loss = 1.6987
[2023-10-04 13:19:25] iter = 02640, loss = 1.7236
[2023-10-04 13:19:26] iter = 02650, loss = 1.8058
[2023-10-04 13:19:27] iter = 02660, loss = 2.0285
[2023-10-04 13:19:28] iter = 02670, loss = 1.7748
[2023-10-04 13:19:29] iter = 02680, loss = 1.6216
[2023-10-04 13:19:29] iter = 02690, loss = 1.7131
[2023-10-04 13:19:30] iter = 02700, loss = 1.7271
[2023-10-04 13:19:31] iter = 02710, loss = 1.7280
[2023-10-04 13:19:32] iter = 02720, loss = 1.7364
[2023-10-04 13:19:33] iter = 02730, loss = 1.6704
[2023-10-04 13:19:34] iter = 02740, loss = 1.7438
[2023-10-04 13:19:35] iter = 02750, loss = 1.6284
[2023-10-04 13:19:36] iter = 02760, loss = 1.6191
[2023-10-04 13:19:37] iter = 02770, loss = 1.6729
[2023-10-04 13:19:38] iter = 02780, loss = 1.8226
[2023-10-04 13:19:39] iter = 02790, loss = 1.8023
[2023-10-04 13:19:40] iter = 02800, loss = 1.8059
[2023-10-04 13:19:40] iter = 02810, loss = 1.8146
[2023-10-04 13:19:41] iter = 02820, loss = 1.8300
[2023-10-04 13:19:42] iter = 02830, loss = 1.7078
[2023-10-04 13:19:43] iter = 02840, loss = 1.6177
[2023-10-04 13:19:44] iter = 02850, loss = 1.7481
[2023-10-04 13:19:45] iter = 02860, loss = 1.6791
[2023-10-04 13:19:46] iter = 02870, loss = 1.7154
[2023-10-04 13:19:47] iter = 02880, loss = 1.6933
[2023-10-04 13:19:48] iter = 02890, loss = 1.7695
[2023-10-04 13:19:49] iter = 02900, loss = 1.6814
[2023-10-04 13:19:50] iter = 02910, loss = 1.7094
[2023-10-04 13:19:50] iter = 02920, loss = 1.7440
[2023-10-04 13:19:51] iter = 02930, loss = 1.7940
[2023-10-04 13:19:52] iter = 02940, loss = 1.7517
[2023-10-04 13:19:53] iter = 02950, loss = 1.7963
[2023-10-04 13:19:54] iter = 02960, loss = 1.7068
[2023-10-04 13:19:55] iter = 02970, loss = 1.7826
[2023-10-04 13:19:56] iter = 02980, loss = 1.6408
[2023-10-04 13:19:57] iter = 02990, loss = 1.7018
[2023-10-04 13:19:58] iter = 03000, loss = 1.7470
[2023-10-04 13:19:59] iter = 03010, loss = 1.7056
[2023-10-04 13:20:00] iter = 03020, loss = 1.6966
[2023-10-04 13:20:01] iter = 03030, loss = 1.6867
[2023-10-04 13:20:01] iter = 03040, loss = 1.6067
[2023-10-04 13:20:03] iter = 03050, loss = 1.6324
[2023-10-04 13:20:03] iter = 03060, loss = 1.9805
[2023-10-04 13:20:04] iter = 03070, loss = 1.8391
[2023-10-04 13:20:05] iter = 03080, loss = 1.6387
[2023-10-04 13:20:06] iter = 03090, loss = 1.8123
[2023-10-04 13:20:07] iter = 03100, loss = 1.6300
[2023-10-04 13:20:08] iter = 03110, loss = 1.7833
[2023-10-04 13:20:09] iter = 03120, loss = 2.0278
[2023-10-04 13:20:10] iter = 03130, loss = 1.7899
[2023-10-04 13:20:11] iter = 03140, loss = 1.8695
[2023-10-04 13:20:12] iter = 03150, loss = 1.7714
[2023-10-04 13:20:13] iter = 03160, loss = 1.6538
[2023-10-04 13:20:14] iter = 03170, loss = 1.8392
[2023-10-04 13:20:15] iter = 03180, loss = 1.7020
[2023-10-04 13:20:16] iter = 03190, loss = 1.6232
[2023-10-04 13:20:17] iter = 03200, loss = 1.7271
[2023-10-04 13:20:17] iter = 03210, loss = 1.5933
[2023-10-04 13:20:18] iter = 03220, loss = 1.7078
[2023-10-04 13:20:19] iter = 03230, loss = 1.7644
[2023-10-04 13:20:20] iter = 03240, loss = 1.7390
[2023-10-04 13:20:21] iter = 03250, loss = 1.7390
[2023-10-04 13:20:22] iter = 03260, loss = 1.8193
[2023-10-04 13:20:23] iter = 03270, loss = 1.6470
[2023-10-04 13:20:24] iter = 03280, loss = 1.8595
[2023-10-04 13:20:25] iter = 03290, loss = 1.6238
[2023-10-04 13:20:26] iter = 03300, loss = 1.6727
[2023-10-04 13:20:27] iter = 03310, loss = 1.5771
[2023-10-04 13:20:27] iter = 03320, loss = 1.6736
[2023-10-04 13:20:28] iter = 03330, loss = 1.6899
[2023-10-04 13:20:29] iter = 03340, loss = 1.6432
[2023-10-04 13:20:30] iter = 03350, loss = 1.6614
[2023-10-04 13:20:31] iter = 03360, loss = 1.8018
[2023-10-04 13:20:32] iter = 03370, loss = 1.7450
[2023-10-04 13:20:33] iter = 03380, loss = 1.7199
[2023-10-04 13:20:34] iter = 03390, loss = 1.8343
[2023-10-04 13:20:35] iter = 03400, loss = 1.7655
[2023-10-04 13:20:36] iter = 03410, loss = 1.6214
[2023-10-04 13:20:36] iter = 03420, loss = 1.7417
[2023-10-04 13:20:37] iter = 03430, loss = 1.8938
[2023-10-04 13:20:38] iter = 03440, loss = 1.6367
[2023-10-04 13:20:39] iter = 03450, loss = 1.7578
[2023-10-04 13:20:40] iter = 03460, loss = 1.6533
[2023-10-04 13:20:41] iter = 03470, loss = 1.8139
[2023-10-04 13:20:42] iter = 03480, loss = 1.6309
[2023-10-04 13:20:43] iter = 03490, loss = 1.5588
[2023-10-04 13:20:44] iter = 03500, loss = 1.8284
[2023-10-04 13:20:45] iter = 03510, loss = 1.7080
[2023-10-04 13:20:46] iter = 03520, loss = 1.8327
[2023-10-04 13:20:47] iter = 03530, loss = 1.6864
[2023-10-04 13:20:48] iter = 03540, loss = 1.7269
[2023-10-04 13:20:48] iter = 03550, loss = 1.6694
[2023-10-04 13:20:49] iter = 03560, loss = 1.7929
[2023-10-04 13:20:50] iter = 03570, loss = 1.6875
[2023-10-04 13:20:51] iter = 03580, loss = 1.7225
[2023-10-04 13:20:52] iter = 03590, loss = 1.7263
[2023-10-04 13:20:53] iter = 03600, loss = 1.7020
[2023-10-04 13:20:54] iter = 03610, loss = 1.6517
[2023-10-04 13:20:55] iter = 03620, loss = 1.8006
[2023-10-04 13:20:56] iter = 03630, loss = 1.5737
[2023-10-04 13:20:57] iter = 03640, loss = 1.7362
[2023-10-04 13:20:58] iter = 03650, loss = 1.6594
[2023-10-04 13:20:59] iter = 03660, loss = 1.7072
[2023-10-04 13:21:00] iter = 03670, loss = 1.6989
[2023-10-04 13:21:00] iter = 03680, loss = 1.7529
[2023-10-04 13:21:01] iter = 03690, loss = 1.6181
[2023-10-04 13:21:02] iter = 03700, loss = 1.7224
[2023-10-04 13:21:03] iter = 03710, loss = 1.6289
[2023-10-04 13:21:04] iter = 03720, loss = 1.7136
[2023-10-04 13:21:05] iter = 03730, loss = 1.6357
[2023-10-04 13:21:06] iter = 03740, loss = 1.7555
[2023-10-04 13:21:07] iter = 03750, loss = 1.5745
[2023-10-04 13:21:07] iter = 03760, loss = 1.7002
[2023-10-04 13:21:08] iter = 03770, loss = 1.5919
[2023-10-04 13:21:09] iter = 03780, loss = 1.7878
[2023-10-04 13:21:10] iter = 03790, loss = 1.7135
[2023-10-04 13:21:11] iter = 03800, loss = 1.6949
[2023-10-04 13:21:12] iter = 03810, loss = 1.6519
[2023-10-04 13:21:13] iter = 03820, loss = 1.7169
[2023-10-04 13:21:14] iter = 03830, loss = 1.5890
[2023-10-04 13:21:15] iter = 03840, loss = 1.6969
[2023-10-04 13:21:16] iter = 03850, loss = 1.6511
[2023-10-04 13:21:17] iter = 03860, loss = 1.7198
[2023-10-04 13:21:18] iter = 03870, loss = 1.6629
[2023-10-04 13:21:18] iter = 03880, loss = 1.6968
[2023-10-04 13:21:19] iter = 03890, loss = 1.6571
[2023-10-04 13:21:20] iter = 03900, loss = 1.8733
[2023-10-04 13:21:21] iter = 03910, loss = 1.6074
[2023-10-04 13:21:22] iter = 03920, loss = 1.7809
[2023-10-04 13:21:23] iter = 03930, loss = 1.5738
[2023-10-04 13:21:24] iter = 03940, loss = 1.8392
[2023-10-04 13:21:25] iter = 03950, loss = 1.7241
[2023-10-04 13:21:26] iter = 03960, loss = 1.5514
[2023-10-04 13:21:27] iter = 03970, loss = 1.5617
[2023-10-04 13:21:28] iter = 03980, loss = 1.7808
[2023-10-04 13:21:29] iter = 03990, loss = 1.7350
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 89906}
[2023-10-04 13:21:54] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.009389 train acc = 0.9940, test acc = 0.6076
[2023-10-04 13:22:18] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010057 train acc = 1.0000, test acc = 0.5983
[2023-10-04 13:22:43] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.012830 train acc = 1.0000, test acc = 0.6050
[2023-10-04 13:23:07] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.015442 train acc = 0.9980, test acc = 0.5972
[2023-10-04 13:23:32] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.012618 train acc = 1.0000, test acc = 0.5954
[2023-10-04 13:23:56] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.015480 train acc = 1.0000, test acc = 0.6017
[2023-10-04 13:24:21] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012051 train acc = 1.0000, test acc = 0.6063
[2023-10-04 13:24:45] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.013390 train acc = 0.9980, test acc = 0.6021
[2023-10-04 13:25:10] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002924 train acc = 1.0000, test acc = 0.6020
[2023-10-04 13:25:35] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.006094 train acc = 1.0000, test acc = 0.6052
[2023-10-04 13:25:59] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013399 train acc = 1.0000, test acc = 0.5986
[2023-10-04 13:26:24] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.013987 train acc = 1.0000, test acc = 0.5988
[2023-10-04 13:26:48] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.005253 train acc = 0.9980, test acc = 0.5968
[2023-10-04 13:27:13] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003969 train acc = 1.0000, test acc = 0.5953
[2023-10-04 13:27:37] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.017077 train acc = 1.0000, test acc = 0.5967
[2023-10-04 13:28:02] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.029673 train acc = 1.0000, test acc = 0.5962
[2023-10-04 13:28:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.010591 train acc = 1.0000, test acc = 0.5989
[2023-10-04 13:28:51] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004571 train acc = 1.0000, test acc = 0.6031
[2023-10-04 13:29:15] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.009026 train acc = 1.0000, test acc = 0.5946
[2023-10-04 13:29:40] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004772 train acc = 1.0000, test acc = 0.5978
Evaluate 20 random ConvNet, mean = 0.5999 std = 0.0039
-------------------------
[2023-10-04 13:29:40] iter = 04000, loss = 1.7002
[2023-10-04 13:29:41] iter = 04010, loss = 1.7561
[2023-10-04 13:29:42] iter = 04020, loss = 1.7811
[2023-10-04 13:29:43] iter = 04030, loss = 1.6352
[2023-10-04 13:29:44] iter = 04040, loss = 1.6143
[2023-10-04 13:29:45] iter = 04050, loss = 1.6110
[2023-10-04 13:29:46] iter = 04060, loss = 1.6593
[2023-10-04 13:29:46] iter = 04070, loss = 1.5872
[2023-10-04 13:29:47] iter = 04080, loss = 1.7278
[2023-10-04 13:29:48] iter = 04090, loss = 1.8630
[2023-10-04 13:29:49] iter = 04100, loss = 1.6176
[2023-10-04 13:29:50] iter = 04110, loss = 1.7461
[2023-10-04 13:29:51] iter = 04120, loss = 1.6194
[2023-10-04 13:29:52] iter = 04130, loss = 1.6074
[2023-10-04 13:29:53] iter = 04140, loss = 1.6865
[2023-10-04 13:29:54] iter = 04150, loss = 1.7659
[2023-10-04 13:29:55] iter = 04160, loss = 1.6680
[2023-10-04 13:29:56] iter = 04170, loss = 1.6720
[2023-10-04 13:29:57] iter = 04180, loss = 1.6250
[2023-10-04 13:29:58] iter = 04190, loss = 1.5831
[2023-10-04 13:29:58] iter = 04200, loss = 1.7059
[2023-10-04 13:29:59] iter = 04210, loss = 1.6463
[2023-10-04 13:30:00] iter = 04220, loss = 1.6853
[2023-10-04 13:30:01] iter = 04230, loss = 1.6544
[2023-10-04 13:30:02] iter = 04240, loss = 1.6367
[2023-10-04 13:30:03] iter = 04250, loss = 1.6976
[2023-10-04 13:30:04] iter = 04260, loss = 1.7869
[2023-10-04 13:30:05] iter = 04270, loss = 1.6918
[2023-10-04 13:30:06] iter = 04280, loss = 1.6786
[2023-10-04 13:30:07] iter = 04290, loss = 1.6777
[2023-10-04 13:30:08] iter = 04300, loss = 1.6186
[2023-10-04 13:30:09] iter = 04310, loss = 1.7990
[2023-10-04 13:30:10] iter = 04320, loss = 1.6284
[2023-10-04 13:30:11] iter = 04330, loss = 1.6826
[2023-10-04 13:30:11] iter = 04340, loss = 1.7103
[2023-10-04 13:30:12] iter = 04350, loss = 1.6092
[2023-10-04 13:30:13] iter = 04360, loss = 1.5566
[2023-10-04 13:30:14] iter = 04370, loss = 1.7125
[2023-10-04 13:30:15] iter = 04380, loss = 1.6932
[2023-10-04 13:30:16] iter = 04390, loss = 1.6483
[2023-10-04 13:30:17] iter = 04400, loss = 1.7030
[2023-10-04 13:30:18] iter = 04410, loss = 1.8236
[2023-10-04 13:30:19] iter = 04420, loss = 1.7202
[2023-10-04 13:30:20] iter = 04430, loss = 1.8090
[2023-10-04 13:30:21] iter = 04440, loss = 1.6091
[2023-10-04 13:30:22] iter = 04450, loss = 1.6346
[2023-10-04 13:30:23] iter = 04460, loss = 1.5984
[2023-10-04 13:30:24] iter = 04470, loss = 1.5715
[2023-10-04 13:30:25] iter = 04480, loss = 1.6168
[2023-10-04 13:30:25] iter = 04490, loss = 1.5447
[2023-10-04 13:30:26] iter = 04500, loss = 1.7868
[2023-10-04 13:30:27] iter = 04510, loss = 1.6635
[2023-10-04 13:30:28] iter = 04520, loss = 1.7480
[2023-10-04 13:30:29] iter = 04530, loss = 1.6143
[2023-10-04 13:30:30] iter = 04540, loss = 1.6272
[2023-10-04 13:30:31] iter = 04550, loss = 1.5161
[2023-10-04 13:30:32] iter = 04560, loss = 1.5736
[2023-10-04 13:30:33] iter = 04570, loss = 1.7083
[2023-10-04 13:30:34] iter = 04580, loss = 1.7042
[2023-10-04 13:30:35] iter = 04590, loss = 1.5623
[2023-10-04 13:30:35] iter = 04600, loss = 1.6086
[2023-10-04 13:30:36] iter = 04610, loss = 1.6303
[2023-10-04 13:30:37] iter = 04620, loss = 1.6286
[2023-10-04 13:30:38] iter = 04630, loss = 1.5897
[2023-10-04 13:30:39] iter = 04640, loss = 1.5283
[2023-10-04 13:30:40] iter = 04650, loss = 1.6252
[2023-10-04 13:30:41] iter = 04660, loss = 1.6416
[2023-10-04 13:30:42] iter = 04670, loss = 1.8222
[2023-10-04 13:30:43] iter = 04680, loss = 1.7981
[2023-10-04 13:30:44] iter = 04690, loss = 1.4955
[2023-10-04 13:30:45] iter = 04700, loss = 1.5345
[2023-10-04 13:30:46] iter = 04710, loss = 1.6024
[2023-10-04 13:30:47] iter = 04720, loss = 1.4967
[2023-10-04 13:30:47] iter = 04730, loss = 1.5640
[2023-10-04 13:30:48] iter = 04740, loss = 1.7755
[2023-10-04 13:30:49] iter = 04750, loss = 1.6610
[2023-10-04 13:30:50] iter = 04760, loss = 1.7793
[2023-10-04 13:30:51] iter = 04770, loss = 1.7158
[2023-10-04 13:30:52] iter = 04780, loss = 1.6078
[2023-10-04 13:30:53] iter = 04790, loss = 1.6224
[2023-10-04 13:30:54] iter = 04800, loss = 1.7828
[2023-10-04 13:30:55] iter = 04810, loss = 1.6517
[2023-10-04 13:30:56] iter = 04820, loss = 1.5191
[2023-10-04 13:30:57] iter = 04830, loss = 1.7020
[2023-10-04 13:30:57] iter = 04840, loss = 1.9363
[2023-10-04 13:30:58] iter = 04850, loss = 1.5748
[2023-10-04 13:30:59] iter = 04860, loss = 1.6527
[2023-10-04 13:31:00] iter = 04870, loss = 1.6356
[2023-10-04 13:31:01] iter = 04880, loss = 1.6097
[2023-10-04 13:31:02] iter = 04890, loss = 1.6584
[2023-10-04 13:31:03] iter = 04900, loss = 1.6899
[2023-10-04 13:31:04] iter = 04910, loss = 1.6280
[2023-10-04 13:31:05] iter = 04920, loss = 1.6602
[2023-10-04 13:31:06] iter = 04930, loss = 1.5873
[2023-10-04 13:31:07] iter = 04940, loss = 1.6362
[2023-10-04 13:31:07] iter = 04950, loss = 1.6848
[2023-10-04 13:31:08] iter = 04960, loss = 1.5929
[2023-10-04 13:31:09] iter = 04970, loss = 1.6542
[2023-10-04 13:31:10] iter = 04980, loss = 1.5720
[2023-10-04 13:31:11] iter = 04990, loss = 1.8550
[2023-10-04 13:31:12] iter = 05000, loss = 1.6031
[2023-10-04 13:31:13] iter = 05010, loss = 1.5463
[2023-10-04 13:31:14] iter = 05020, loss = 1.5844
[2023-10-04 13:31:15] iter = 05030, loss = 1.5944
[2023-10-04 13:31:16] iter = 05040, loss = 1.5484
[2023-10-04 13:31:17] iter = 05050, loss = 1.7820
[2023-10-04 13:31:17] iter = 05060, loss = 1.6418
[2023-10-04 13:31:18] iter = 05070, loss = 1.6881
[2023-10-04 13:31:19] iter = 05080, loss = 1.7940
[2023-10-04 13:31:20] iter = 05090, loss = 1.5043
[2023-10-04 13:31:21] iter = 05100, loss = 1.5753
[2023-10-04 13:31:22] iter = 05110, loss = 1.6491
[2023-10-04 13:31:23] iter = 05120, loss = 1.8357
[2023-10-04 13:31:24] iter = 05130, loss = 1.7185
[2023-10-04 13:31:25] iter = 05140, loss = 1.6324
[2023-10-04 13:31:26] iter = 05150, loss = 1.6394
[2023-10-04 13:31:27] iter = 05160, loss = 1.5092
[2023-10-04 13:31:28] iter = 05170, loss = 1.6438
[2023-10-04 13:31:29] iter = 05180, loss = 1.7242
[2023-10-04 13:31:30] iter = 05190, loss = 1.7506
[2023-10-04 13:31:31] iter = 05200, loss = 1.8065
[2023-10-04 13:31:31] iter = 05210, loss = 1.8576
[2023-10-04 13:31:32] iter = 05220, loss = 1.5855
[2023-10-04 13:31:33] iter = 05230, loss = 1.4801
[2023-10-04 13:31:34] iter = 05240, loss = 1.7385
[2023-10-04 13:31:35] iter = 05250, loss = 1.6390
[2023-10-04 13:31:36] iter = 05260, loss = 1.5492
[2023-10-04 13:31:37] iter = 05270, loss = 1.7106
[2023-10-04 13:31:38] iter = 05280, loss = 1.5219
[2023-10-04 13:31:39] iter = 05290, loss = 1.4954
[2023-10-04 13:31:39] iter = 05300, loss = 1.5628
[2023-10-04 13:31:41] iter = 05310, loss = 1.6518
[2023-10-04 13:31:41] iter = 05320, loss = 1.6085
[2023-10-04 13:31:42] iter = 05330, loss = 1.6240
[2023-10-04 13:31:43] iter = 05340, loss = 1.6369
[2023-10-04 13:31:44] iter = 05350, loss = 1.6263
[2023-10-04 13:31:45] iter = 05360, loss = 1.6352
[2023-10-04 13:31:46] iter = 05370, loss = 1.5716
[2023-10-04 13:31:47] iter = 05380, loss = 1.5784
[2023-10-04 13:31:48] iter = 05390, loss = 1.7377
[2023-10-04 13:31:49] iter = 05400, loss = 1.5800
[2023-10-04 13:31:50] iter = 05410, loss = 1.6298
[2023-10-04 13:31:51] iter = 05420, loss = 1.6916
[2023-10-04 13:31:51] iter = 05430, loss = 1.5236
[2023-10-04 13:31:52] iter = 05440, loss = 1.6771
[2023-10-04 13:31:53] iter = 05450, loss = 1.5842
[2023-10-04 13:31:54] iter = 05460, loss = 1.5659
[2023-10-04 13:31:55] iter = 05470, loss = 1.6896
[2023-10-04 13:31:56] iter = 05480, loss = 1.5505
[2023-10-04 13:31:57] iter = 05490, loss = 1.4374
[2023-10-04 13:31:58] iter = 05500, loss = 1.6511
[2023-10-04 13:31:59] iter = 05510, loss = 1.5371
[2023-10-04 13:32:00] iter = 05520, loss = 1.5847
[2023-10-04 13:32:01] iter = 05530, loss = 1.6335
[2023-10-04 13:32:02] iter = 05540, loss = 1.7240
[2023-10-04 13:32:03] iter = 05550, loss = 1.5728
[2023-10-04 13:32:03] iter = 05560, loss = 1.6048
[2023-10-04 13:32:04] iter = 05570, loss = 1.7487
[2023-10-04 13:32:05] iter = 05580, loss = 1.6091
[2023-10-04 13:32:06] iter = 05590, loss = 1.5767
[2023-10-04 13:32:07] iter = 05600, loss = 1.5775
[2023-10-04 13:32:08] iter = 05610, loss = 1.6397
[2023-10-04 13:32:09] iter = 05620, loss = 1.7066
[2023-10-04 13:32:10] iter = 05630, loss = 1.5883
[2023-10-04 13:32:11] iter = 05640, loss = 1.6242
[2023-10-04 13:32:12] iter = 05650, loss = 1.6415
[2023-10-04 13:32:13] iter = 05660, loss = 1.5879
[2023-10-04 13:32:14] iter = 05670, loss = 1.7098
[2023-10-04 13:32:15] iter = 05680, loss = 1.5177
[2023-10-04 13:32:16] iter = 05690, loss = 1.5160
[2023-10-04 13:32:17] iter = 05700, loss = 1.6122
[2023-10-04 13:32:17] iter = 05710, loss = 1.6785
[2023-10-04 13:32:18] iter = 05720, loss = 1.6299
[2023-10-04 13:32:19] iter = 05730, loss = 1.6183
[2023-10-04 13:32:20] iter = 05740, loss = 1.6044
[2023-10-04 13:32:21] iter = 05750, loss = 1.5757
[2023-10-04 13:32:22] iter = 05760, loss = 1.5459
[2023-10-04 13:32:23] iter = 05770, loss = 1.6197
[2023-10-04 13:32:24] iter = 05780, loss = 1.7437
[2023-10-04 13:32:25] iter = 05790, loss = 1.7409
[2023-10-04 13:32:26] iter = 05800, loss = 1.6478
[2023-10-04 13:32:27] iter = 05810, loss = 1.6590
[2023-10-04 13:32:28] iter = 05820, loss = 1.6527
[2023-10-04 13:32:29] iter = 05830, loss = 1.6160
[2023-10-04 13:32:29] iter = 05840, loss = 1.5786
[2023-10-04 13:32:30] iter = 05850, loss = 1.6248
[2023-10-04 13:32:31] iter = 05860, loss = 1.4863
[2023-10-04 13:32:32] iter = 05870, loss = 1.5160
[2023-10-04 13:32:33] iter = 05880, loss = 1.5772
[2023-10-04 13:32:34] iter = 05890, loss = 1.5770
[2023-10-04 13:32:35] iter = 05900, loss = 1.7258
[2023-10-04 13:32:36] iter = 05910, loss = 1.6547
[2023-10-04 13:32:37] iter = 05920, loss = 1.6670
[2023-10-04 13:32:38] iter = 05930, loss = 1.4417
[2023-10-04 13:32:39] iter = 05940, loss = 1.5717
[2023-10-04 13:32:39] iter = 05950, loss = 1.7135
[2023-10-04 13:32:40] iter = 05960, loss = 1.5483
[2023-10-04 13:32:41] iter = 05970, loss = 1.6424
[2023-10-04 13:32:42] iter = 05980, loss = 1.7143
[2023-10-04 13:32:43] iter = 05990, loss = 1.5350
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 64623}
[2023-10-04 13:33:09] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.009332 train acc = 1.0000, test acc = 0.6142
[2023-10-04 13:33:33] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010866 train acc = 1.0000, test acc = 0.6059
[2023-10-04 13:33:58] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010680 train acc = 1.0000, test acc = 0.5992
[2023-10-04 13:34:22] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.008755 train acc = 1.0000, test acc = 0.6114
[2023-10-04 13:34:47] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.018352 train acc = 1.0000, test acc = 0.6098
[2023-10-04 13:35:11] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011097 train acc = 1.0000, test acc = 0.6110
[2023-10-04 13:35:36] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.011140 train acc = 1.0000, test acc = 0.6128
[2023-10-04 13:36:00] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001254 train acc = 1.0000, test acc = 0.6050
[2023-10-04 13:36:25] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003194 train acc = 1.0000, test acc = 0.6137
[2023-10-04 13:36:50] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013069 train acc = 1.0000, test acc = 0.6017
[2023-10-04 13:37:14] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.011014 train acc = 1.0000, test acc = 0.6077
[2023-10-04 13:37:39] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.006134 train acc = 1.0000, test acc = 0.6108
[2023-10-04 13:38:04] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.012787 train acc = 1.0000, test acc = 0.6059
[2023-10-04 13:38:28] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003334 train acc = 1.0000, test acc = 0.6104
[2023-10-04 13:38:53] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001864 train acc = 1.0000, test acc = 0.6054
[2023-10-04 13:39:17] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003707 train acc = 1.0000, test acc = 0.6061
[2023-10-04 13:39:42] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003566 train acc = 1.0000, test acc = 0.6090
[2023-10-04 13:40:06] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.008962 train acc = 1.0000, test acc = 0.6100
[2023-10-04 13:40:31] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.016709 train acc = 1.0000, test acc = 0.6086
[2023-10-04 13:40:56] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.010105 train acc = 1.0000, test acc = 0.6052
Evaluate 20 random ConvNet, mean = 0.6082 std = 0.0038
-------------------------
[2023-10-04 13:40:56] iter = 06000, loss = 1.6474
[2023-10-04 13:40:57] iter = 06010, loss = 1.5338
[2023-10-04 13:40:58] iter = 06020, loss = 1.6339
[2023-10-04 13:40:59] iter = 06030, loss = 1.6692
[2023-10-04 13:40:59] iter = 06040, loss = 1.6608
[2023-10-04 13:41:00] iter = 06050, loss = 1.7328
[2023-10-04 13:41:01] iter = 06060, loss = 1.6849
[2023-10-04 13:41:02] iter = 06070, loss = 1.6130
[2023-10-04 13:41:03] iter = 06080, loss = 1.6411
[2023-10-04 13:41:04] iter = 06090, loss = 1.5326
[2023-10-04 13:41:05] iter = 06100, loss = 1.6207
[2023-10-04 13:41:06] iter = 06110, loss = 1.6919
[2023-10-04 13:41:06] iter = 06120, loss = 1.6290
[2023-10-04 13:41:07] iter = 06130, loss = 1.5863
[2023-10-04 13:41:08] iter = 06140, loss = 1.8212
[2023-10-04 13:41:09] iter = 06150, loss = 1.6850
[2023-10-04 13:41:10] iter = 06160, loss = 1.6474
[2023-10-04 13:41:11] iter = 06170, loss = 1.6519
[2023-10-04 13:41:12] iter = 06180, loss = 1.7183
[2023-10-04 13:41:13] iter = 06190, loss = 1.5788
[2023-10-04 13:41:14] iter = 06200, loss = 1.5216
[2023-10-04 13:41:15] iter = 06210, loss = 1.6708
[2023-10-04 13:41:16] iter = 06220, loss = 1.6336
[2023-10-04 13:41:17] iter = 06230, loss = 1.6491
[2023-10-04 13:41:18] iter = 06240, loss = 1.6540
[2023-10-04 13:41:19] iter = 06250, loss = 1.7264
[2023-10-04 13:41:19] iter = 06260, loss = 1.5638
[2023-10-04 13:41:20] iter = 06270, loss = 1.8672
[2023-10-04 13:41:21] iter = 06280, loss = 1.5906
[2023-10-04 13:41:22] iter = 06290, loss = 1.5558
[2023-10-04 13:41:23] iter = 06300, loss = 1.7488
[2023-10-04 13:41:24] iter = 06310, loss = 1.5756
[2023-10-04 13:41:25] iter = 06320, loss = 1.7105
[2023-10-04 13:41:26] iter = 06330, loss = 1.6891
[2023-10-04 13:41:27] iter = 06340, loss = 1.6894
[2023-10-04 13:41:28] iter = 06350, loss = 1.5115
[2023-10-04 13:41:29] iter = 06360, loss = 1.7123
[2023-10-04 13:41:30] iter = 06370, loss = 1.4891
[2023-10-04 13:41:31] iter = 06380, loss = 1.5932
[2023-10-04 13:41:32] iter = 06390, loss = 1.6049
[2023-10-04 13:41:33] iter = 06400, loss = 1.6367
[2023-10-04 13:41:34] iter = 06410, loss = 1.5935
[2023-10-04 13:41:35] iter = 06420, loss = 1.6226
[2023-10-04 13:41:35] iter = 06430, loss = 1.6040
[2023-10-04 13:41:36] iter = 06440, loss = 1.5588
[2023-10-04 13:41:37] iter = 06450, loss = 1.4762
[2023-10-04 13:41:38] iter = 06460, loss = 1.5783
[2023-10-04 13:41:39] iter = 06470, loss = 1.7699
[2023-10-04 13:41:40] iter = 06480, loss = 1.6485
[2023-10-04 13:41:41] iter = 06490, loss = 1.6616
[2023-10-04 13:41:42] iter = 06500, loss = 1.5900
[2023-10-04 13:41:43] iter = 06510, loss = 1.6971
[2023-10-04 13:41:44] iter = 06520, loss = 1.5527
[2023-10-04 13:41:44] iter = 06530, loss = 1.6336
[2023-10-04 13:41:45] iter = 06540, loss = 1.5659
[2023-10-04 13:41:46] iter = 06550, loss = 1.6417
[2023-10-04 13:41:47] iter = 06560, loss = 1.6453
[2023-10-04 13:41:48] iter = 06570, loss = 1.6276
[2023-10-04 13:41:49] iter = 06580, loss = 1.5798
[2023-10-04 13:41:50] iter = 06590, loss = 1.5629
[2023-10-04 13:41:51] iter = 06600, loss = 1.5065
[2023-10-04 13:41:52] iter = 06610, loss = 1.5994
[2023-10-04 13:41:53] iter = 06620, loss = 1.4856
[2023-10-04 13:41:54] iter = 06630, loss = 1.6029
[2023-10-04 13:41:54] iter = 06640, loss = 1.6289
[2023-10-04 13:41:55] iter = 06650, loss = 1.6955
[2023-10-04 13:41:56] iter = 06660, loss = 1.6651
[2023-10-04 13:41:57] iter = 06670, loss = 1.4894
[2023-10-04 13:41:58] iter = 06680, loss = 1.6674
[2023-10-04 13:41:59] iter = 06690, loss = 1.6060
[2023-10-04 13:42:00] iter = 06700, loss = 1.5075
[2023-10-04 13:42:01] iter = 06710, loss = 1.6348
[2023-10-04 13:42:02] iter = 06720, loss = 1.7154
[2023-10-04 13:42:03] iter = 06730, loss = 1.5036
[2023-10-04 13:42:04] iter = 06740, loss = 1.6084
[2023-10-04 13:42:05] iter = 06750, loss = 1.6406
[2023-10-04 13:42:05] iter = 06760, loss = 1.5379
[2023-10-04 13:42:06] iter = 06770, loss = 1.6938
[2023-10-04 13:42:07] iter = 06780, loss = 1.5273
[2023-10-04 13:42:08] iter = 06790, loss = 1.5011
[2023-10-04 13:42:09] iter = 06800, loss = 1.6963
[2023-10-04 13:42:10] iter = 06810, loss = 1.7876
[2023-10-04 13:42:11] iter = 06820, loss = 1.5489
[2023-10-04 13:42:12] iter = 06830, loss = 1.5672
[2023-10-04 13:42:13] iter = 06840, loss = 1.6535
[2023-10-04 13:42:14] iter = 06850, loss = 1.6703
[2023-10-04 13:42:14] iter = 06860, loss = 1.7555
[2023-10-04 13:42:15] iter = 06870, loss = 1.6376
[2023-10-04 13:42:16] iter = 06880, loss = 1.6005
[2023-10-04 13:42:17] iter = 06890, loss = 1.4971
[2023-10-04 13:42:18] iter = 06900, loss = 1.5376
[2023-10-04 13:42:19] iter = 06910, loss = 1.6646
[2023-10-04 13:42:20] iter = 06920, loss = 1.6329
[2023-10-04 13:42:21] iter = 06930, loss = 1.4665
[2023-10-04 13:42:22] iter = 06940, loss = 1.5485
[2023-10-04 13:42:23] iter = 06950, loss = 1.5795
[2023-10-04 13:42:24] iter = 06960, loss = 1.5102
[2023-10-04 13:42:25] iter = 06970, loss = 1.6489
[2023-10-04 13:42:26] iter = 06980, loss = 1.8170
[2023-10-04 13:42:27] iter = 06990, loss = 1.6051
[2023-10-04 13:42:27] iter = 07000, loss = 1.5993
[2023-10-04 13:42:28] iter = 07010, loss = 1.5176
[2023-10-04 13:42:29] iter = 07020, loss = 1.5232
[2023-10-04 13:42:30] iter = 07030, loss = 1.6385
[2023-10-04 13:42:31] iter = 07040, loss = 1.5083
[2023-10-04 13:42:32] iter = 07050, loss = 1.6888
[2023-10-04 13:42:33] iter = 07060, loss = 1.4893
[2023-10-04 13:42:34] iter = 07070, loss = 1.4897
[2023-10-04 13:42:35] iter = 07080, loss = 1.5030
[2023-10-04 13:42:36] iter = 07090, loss = 1.5293
[2023-10-04 13:42:37] iter = 07100, loss = 1.5321
[2023-10-04 13:42:38] iter = 07110, loss = 1.6514
[2023-10-04 13:42:38] iter = 07120, loss = 1.6510
[2023-10-04 13:42:39] iter = 07130, loss = 1.6444
[2023-10-04 13:42:40] iter = 07140, loss = 1.6294
[2023-10-04 13:42:41] iter = 07150, loss = 1.5932
[2023-10-04 13:42:42] iter = 07160, loss = 1.7248
[2023-10-04 13:42:43] iter = 07170, loss = 1.5933
[2023-10-04 13:42:44] iter = 07180, loss = 1.5253
[2023-10-04 13:42:45] iter = 07190, loss = 1.4964
[2023-10-04 13:42:46] iter = 07200, loss = 1.5829
[2023-10-04 13:42:47] iter = 07210, loss = 1.4833
[2023-10-04 13:42:48] iter = 07220, loss = 1.4697
[2023-10-04 13:42:49] iter = 07230, loss = 1.6388
[2023-10-04 13:42:49] iter = 07240, loss = 1.6568
[2023-10-04 13:42:50] iter = 07250, loss = 1.4458
[2023-10-04 13:42:52] iter = 07260, loss = 1.6407
[2023-10-04 13:42:52] iter = 07270, loss = 1.4694
[2023-10-04 13:42:53] iter = 07280, loss = 1.6719
[2023-10-04 13:42:54] iter = 07290, loss = 1.5313
[2023-10-04 13:42:55] iter = 07300, loss = 1.5185
[2023-10-04 13:42:56] iter = 07310, loss = 1.6424
[2023-10-04 13:42:57] iter = 07320, loss = 1.6246
[2023-10-04 13:42:58] iter = 07330, loss = 1.4689
[2023-10-04 13:42:59] iter = 07340, loss = 1.5370
[2023-10-04 13:43:00] iter = 07350, loss = 1.5472
[2023-10-04 13:43:01] iter = 07360, loss = 1.5652
[2023-10-04 13:43:02] iter = 07370, loss = 1.6333
[2023-10-04 13:43:03] iter = 07380, loss = 1.5414
[2023-10-04 13:43:03] iter = 07390, loss = 1.4595
[2023-10-04 13:43:04] iter = 07400, loss = 1.6164
[2023-10-04 13:43:05] iter = 07410, loss = 1.5514
[2023-10-04 13:43:06] iter = 07420, loss = 1.5662
[2023-10-04 13:43:07] iter = 07430, loss = 1.5472
[2023-10-04 13:43:08] iter = 07440, loss = 1.5191
[2023-10-04 13:43:09] iter = 07450, loss = 1.5112
[2023-10-04 13:43:10] iter = 07460, loss = 1.5327
[2023-10-04 13:43:11] iter = 07470, loss = 1.7052
[2023-10-04 13:43:12] iter = 07480, loss = 1.6196
[2023-10-04 13:43:13] iter = 07490, loss = 1.6136
[2023-10-04 13:43:14] iter = 07500, loss = 1.6615
[2023-10-04 13:43:15] iter = 07510, loss = 1.5515
[2023-10-04 13:43:16] iter = 07520, loss = 1.7867
[2023-10-04 13:43:16] iter = 07530, loss = 1.5229
[2023-10-04 13:43:17] iter = 07540, loss = 1.6125
[2023-10-04 13:43:18] iter = 07550, loss = 1.5830
[2023-10-04 13:43:19] iter = 07560, loss = 1.5231
[2023-10-04 13:43:20] iter = 07570, loss = 1.4994
[2023-10-04 13:43:21] iter = 07580, loss = 1.6673
[2023-10-04 13:43:22] iter = 07590, loss = 1.5590
[2023-10-04 13:43:23] iter = 07600, loss = 1.5958
[2023-10-04 13:43:24] iter = 07610, loss = 1.5263
[2023-10-04 13:43:25] iter = 07620, loss = 1.6617
[2023-10-04 13:43:26] iter = 07630, loss = 1.5291
[2023-10-04 13:43:27] iter = 07640, loss = 1.5238
[2023-10-04 13:43:28] iter = 07650, loss = 1.5658
[2023-10-04 13:43:28] iter = 07660, loss = 1.4978
[2023-10-04 13:43:29] iter = 07670, loss = 1.5182
[2023-10-04 13:43:30] iter = 07680, loss = 1.4978
[2023-10-04 13:43:31] iter = 07690, loss = 1.5402
[2023-10-04 13:43:32] iter = 07700, loss = 1.5774
[2023-10-04 13:43:33] iter = 07710, loss = 1.6505
[2023-10-04 13:43:34] iter = 07720, loss = 1.5847
[2023-10-04 13:43:35] iter = 07730, loss = 1.5979
[2023-10-04 13:43:36] iter = 07740, loss = 1.6322
[2023-10-04 13:43:37] iter = 07750, loss = 1.6397
[2023-10-04 13:43:37] iter = 07760, loss = 1.6253
[2023-10-04 13:43:38] iter = 07770, loss = 1.4583
[2023-10-04 13:43:39] iter = 07780, loss = 1.6329
[2023-10-04 13:43:40] iter = 07790, loss = 1.5322
[2023-10-04 13:43:41] iter = 07800, loss = 1.5766
[2023-10-04 13:43:42] iter = 07810, loss = 1.4738
[2023-10-04 13:43:43] iter = 07820, loss = 1.8005
[2023-10-04 13:43:44] iter = 07830, loss = 1.5178
[2023-10-04 13:43:45] iter = 07840, loss = 1.4491
[2023-10-04 13:43:45] iter = 07850, loss = 1.5661
[2023-10-04 13:43:47] iter = 07860, loss = 1.6016
[2023-10-04 13:43:47] iter = 07870, loss = 1.6174
[2023-10-04 13:43:48] iter = 07880, loss = 1.6047
[2023-10-04 13:43:49] iter = 07890, loss = 1.5321
[2023-10-04 13:43:50] iter = 07900, loss = 1.5232
[2023-10-04 13:43:51] iter = 07910, loss = 1.6002
[2023-10-04 13:43:52] iter = 07920, loss = 1.6044
[2023-10-04 13:43:53] iter = 07930, loss = 1.6089
[2023-10-04 13:43:54] iter = 07940, loss = 1.4390
[2023-10-04 13:43:55] iter = 07950, loss = 1.5747
[2023-10-04 13:43:56] iter = 07960, loss = 1.6689
[2023-10-04 13:43:57] iter = 07970, loss = 1.4945
[2023-10-04 13:43:58] iter = 07980, loss = 1.5630
[2023-10-04 13:43:58] iter = 07990, loss = 1.5787
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 39702}
[2023-10-04 13:44:24] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.014948 train acc = 1.0000, test acc = 0.6164
[2023-10-04 13:44:48] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.016998 train acc = 1.0000, test acc = 0.6101
[2023-10-04 13:45:13] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.001550 train acc = 1.0000, test acc = 0.6158
[2023-10-04 13:45:38] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.011191 train acc = 1.0000, test acc = 0.6161
[2023-10-04 13:46:02] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.023792 train acc = 1.0000, test acc = 0.6056
[2023-10-04 13:46:26] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003907 train acc = 1.0000, test acc = 0.6050
[2023-10-04 13:46:51] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.014988 train acc = 1.0000, test acc = 0.6165
[2023-10-04 13:47:16] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.007849 train acc = 1.0000, test acc = 0.6094
[2023-10-04 13:47:40] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.009198 train acc = 1.0000, test acc = 0.6079
[2023-10-04 13:48:04] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002561 train acc = 1.0000, test acc = 0.6017
[2023-10-04 13:48:29] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.011569 train acc = 1.0000, test acc = 0.6086
[2023-10-04 13:48:53] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.017586 train acc = 1.0000, test acc = 0.6162
[2023-10-04 13:49:18] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.014620 train acc = 1.0000, test acc = 0.6156
[2023-10-04 13:49:43] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.001794 train acc = 1.0000, test acc = 0.6129
[2023-10-04 13:50:07] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.021537 train acc = 1.0000, test acc = 0.6118
[2023-10-04 13:50:32] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002023 train acc = 1.0000, test acc = 0.6103
[2023-10-04 13:50:56] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.014488 train acc = 1.0000, test acc = 0.6130
[2023-10-04 13:51:21] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004174 train acc = 1.0000, test acc = 0.6145
[2023-10-04 13:51:45] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003310 train acc = 1.0000, test acc = 0.6047
[2023-10-04 13:52:10] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.015096 train acc = 0.9980, test acc = 0.6105
Evaluate 20 random ConvNet, mean = 0.6111 std = 0.0044
-------------------------
[2023-10-04 13:52:10] iter = 08000, loss = 1.6536
[2023-10-04 13:52:11] iter = 08010, loss = 1.6027
[2023-10-04 13:52:12] iter = 08020, loss = 1.6206
[2023-10-04 13:52:13] iter = 08030, loss = 1.4704
[2023-10-04 13:52:13] iter = 08040, loss = 1.5405
[2023-10-04 13:52:14] iter = 08050, loss = 1.6333
[2023-10-04 13:52:15] iter = 08060, loss = 1.5478
[2023-10-04 13:52:16] iter = 08070, loss = 1.5809
[2023-10-04 13:52:17] iter = 08080, loss = 1.6121
[2023-10-04 13:52:18] iter = 08090, loss = 1.5415
[2023-10-04 13:52:19] iter = 08100, loss = 1.6860
[2023-10-04 13:52:20] iter = 08110, loss = 1.4825
[2023-10-04 13:52:21] iter = 08120, loss = 1.5507
[2023-10-04 13:52:22] iter = 08130, loss = 1.6567
[2023-10-04 13:52:23] iter = 08140, loss = 1.5576
[2023-10-04 13:52:24] iter = 08150, loss = 1.5305
[2023-10-04 13:52:25] iter = 08160, loss = 1.5129
[2023-10-04 13:52:25] iter = 08170, loss = 1.5198
[2023-10-04 13:52:26] iter = 08180, loss = 1.5094
[2023-10-04 13:52:27] iter = 08190, loss = 1.4926
[2023-10-04 13:52:28] iter = 08200, loss = 1.5183
[2023-10-04 13:52:29] iter = 08210, loss = 1.6813
[2023-10-04 13:52:30] iter = 08220, loss = 1.4611
[2023-10-04 13:52:31] iter = 08230, loss = 1.5080
[2023-10-04 13:52:32] iter = 08240, loss = 1.6111
[2023-10-04 13:52:33] iter = 08250, loss = 1.5268
[2023-10-04 13:52:34] iter = 08260, loss = 1.5722
[2023-10-04 13:52:35] iter = 08270, loss = 1.6379
[2023-10-04 13:52:36] iter = 08280, loss = 1.4661
[2023-10-04 13:52:37] iter = 08290, loss = 1.7309
[2023-10-04 13:52:38] iter = 08300, loss = 1.4918
[2023-10-04 13:52:38] iter = 08310, loss = 1.6388
[2023-10-04 13:52:39] iter = 08320, loss = 1.6147
[2023-10-04 13:52:40] iter = 08330, loss = 1.4805
[2023-10-04 13:52:41] iter = 08340, loss = 1.5157
[2023-10-04 13:52:42] iter = 08350, loss = 1.5839
[2023-10-04 13:52:43] iter = 08360, loss = 1.4094
[2023-10-04 13:52:44] iter = 08370, loss = 1.6603
[2023-10-04 13:52:45] iter = 08380, loss = 1.7015
[2023-10-04 13:52:46] iter = 08390, loss = 1.6002
[2023-10-04 13:52:47] iter = 08400, loss = 1.5765
[2023-10-04 13:52:48] iter = 08410, loss = 1.6086
[2023-10-04 13:52:49] iter = 08420, loss = 1.7953
[2023-10-04 13:52:49] iter = 08430, loss = 1.4420
[2023-10-04 13:52:51] iter = 08440, loss = 1.5167
[2023-10-04 13:52:51] iter = 08450, loss = 1.6072
[2023-10-04 13:52:52] iter = 08460, loss = 1.5733
[2023-10-04 13:52:53] iter = 08470, loss = 1.7485
[2023-10-04 13:52:54] iter = 08480, loss = 1.5348
[2023-10-04 13:52:55] iter = 08490, loss = 1.4741
[2023-10-04 13:52:56] iter = 08500, loss = 1.6546
[2023-10-04 13:52:57] iter = 08510, loss = 1.4832
[2023-10-04 13:52:58] iter = 08520, loss = 1.6044
[2023-10-04 13:52:59] iter = 08530, loss = 1.5916
[2023-10-04 13:53:00] iter = 08540, loss = 1.5765
[2023-10-04 13:53:01] iter = 08550, loss = 1.6004
[2023-10-04 13:53:02] iter = 08560, loss = 1.4377
[2023-10-04 13:53:03] iter = 08570, loss = 1.5328
[2023-10-04 13:53:03] iter = 08580, loss = 1.6683
[2023-10-04 13:53:04] iter = 08590, loss = 1.7078
[2023-10-04 13:53:05] iter = 08600, loss = 1.5983
[2023-10-04 13:53:06] iter = 08610, loss = 1.6653
[2023-10-04 13:53:07] iter = 08620, loss = 1.5392
[2023-10-04 13:53:08] iter = 08630, loss = 1.4641
[2023-10-04 13:53:09] iter = 08640, loss = 1.5077
[2023-10-04 13:53:10] iter = 08650, loss = 1.8580
[2023-10-04 13:53:11] iter = 08660, loss = 1.5996
[2023-10-04 13:53:12] iter = 08670, loss = 1.5830
[2023-10-04 13:53:13] iter = 08680, loss = 1.5761
[2023-10-04 13:53:14] iter = 08690, loss = 1.6017
[2023-10-04 13:53:14] iter = 08700, loss = 1.5031
[2023-10-04 13:53:15] iter = 08710, loss = 1.5580
[2023-10-04 13:53:16] iter = 08720, loss = 1.6332
[2023-10-04 13:53:17] iter = 08730, loss = 1.6562
[2023-10-04 13:53:18] iter = 08740, loss = 1.6763
[2023-10-04 13:53:19] iter = 08750, loss = 1.5188
[2023-10-04 13:53:20] iter = 08760, loss = 1.5041
[2023-10-04 13:53:21] iter = 08770, loss = 1.5052
[2023-10-04 13:53:22] iter = 08780, loss = 1.7056
[2023-10-04 13:53:23] iter = 08790, loss = 1.7355
[2023-10-04 13:53:23] iter = 08800, loss = 1.5238
[2023-10-04 13:53:24] iter = 08810, loss = 1.5710
[2023-10-04 13:53:25] iter = 08820, loss = 1.7057
[2023-10-04 13:53:26] iter = 08830, loss = 1.4826
[2023-10-04 13:53:27] iter = 08840, loss = 1.7048
[2023-10-04 13:53:28] iter = 08850, loss = 1.5130
[2023-10-04 13:53:29] iter = 08860, loss = 1.4212
[2023-10-04 13:53:30] iter = 08870, loss = 1.7107
[2023-10-04 13:53:31] iter = 08880, loss = 1.5381
[2023-10-04 13:53:32] iter = 08890, loss = 1.4639
[2023-10-04 13:53:33] iter = 08900, loss = 1.6371
[2023-10-04 13:53:34] iter = 08910, loss = 1.5817
[2023-10-04 13:53:35] iter = 08920, loss = 1.4450
[2023-10-04 13:53:36] iter = 08930, loss = 1.6391
[2023-10-04 13:53:37] iter = 08940, loss = 1.5492
[2023-10-04 13:53:37] iter = 08950, loss = 1.4892
[2023-10-04 13:53:38] iter = 08960, loss = 1.6068
[2023-10-04 13:53:39] iter = 08970, loss = 1.7223
[2023-10-04 13:53:40] iter = 08980, loss = 1.5158
[2023-10-04 13:53:41] iter = 08990, loss = 1.4593
[2023-10-04 13:53:42] iter = 09000, loss = 1.4939
[2023-10-04 13:53:43] iter = 09010, loss = 1.5603
[2023-10-04 13:53:44] iter = 09020, loss = 1.5188
[2023-10-04 13:53:45] iter = 09030, loss = 1.6908
[2023-10-04 13:53:46] iter = 09040, loss = 1.6921
[2023-10-04 13:53:47] iter = 09050, loss = 1.5611
[2023-10-04 13:53:48] iter = 09060, loss = 1.4588
[2023-10-04 13:53:49] iter = 09070, loss = 1.7583
[2023-10-04 13:53:49] iter = 09080, loss = 1.5377
[2023-10-04 13:53:50] iter = 09090, loss = 1.4940
[2023-10-04 13:53:51] iter = 09100, loss = 1.5883
[2023-10-04 13:53:52] iter = 09110, loss = 1.5514
[2023-10-04 13:53:53] iter = 09120, loss = 1.6017
[2023-10-04 13:53:54] iter = 09130, loss = 1.4718
[2023-10-04 13:53:55] iter = 09140, loss = 1.5457
[2023-10-04 13:53:56] iter = 09150, loss = 1.4710
[2023-10-04 13:53:57] iter = 09160, loss = 1.6218
[2023-10-04 13:53:58] iter = 09170, loss = 1.4272
[2023-10-04 13:53:59] iter = 09180, loss = 1.5789
[2023-10-04 13:54:00] iter = 09190, loss = 1.5964
[2023-10-04 13:54:00] iter = 09200, loss = 1.5102
[2023-10-04 13:54:01] iter = 09210, loss = 1.6397
[2023-10-04 13:54:02] iter = 09220, loss = 1.4899
[2023-10-04 13:54:03] iter = 09230, loss = 1.5941
[2023-10-04 13:54:04] iter = 09240, loss = 1.4833
[2023-10-04 13:54:05] iter = 09250, loss = 1.4451
[2023-10-04 13:54:06] iter = 09260, loss = 1.4324
[2023-10-04 13:54:07] iter = 09270, loss = 1.5867
[2023-10-04 13:54:08] iter = 09280, loss = 1.4795
[2023-10-04 13:54:09] iter = 09290, loss = 1.5302
[2023-10-04 13:54:09] iter = 09300, loss = 1.4817
[2023-10-04 13:54:10] iter = 09310, loss = 1.6400
[2023-10-04 13:54:11] iter = 09320, loss = 1.5304
[2023-10-04 13:54:12] iter = 09330, loss = 1.5320
[2023-10-04 13:54:13] iter = 09340, loss = 1.6130
[2023-10-04 13:54:14] iter = 09350, loss = 1.5493
[2023-10-04 13:54:15] iter = 09360, loss = 1.5178
[2023-10-04 13:54:16] iter = 09370, loss = 1.6376
[2023-10-04 13:54:17] iter = 09380, loss = 1.7768
[2023-10-04 13:54:18] iter = 09390, loss = 1.6799
[2023-10-04 13:54:19] iter = 09400, loss = 1.4885
[2023-10-04 13:54:20] iter = 09410, loss = 1.5858
[2023-10-04 13:54:20] iter = 09420, loss = 1.3967
[2023-10-04 13:54:21] iter = 09430, loss = 1.5904
[2023-10-04 13:54:22] iter = 09440, loss = 1.4521
[2023-10-04 13:54:23] iter = 09450, loss = 1.4668
[2023-10-04 13:54:24] iter = 09460, loss = 1.6350
[2023-10-04 13:54:25] iter = 09470, loss = 1.5779
[2023-10-04 13:54:26] iter = 09480, loss = 1.5980
[2023-10-04 13:54:27] iter = 09490, loss = 1.4412
[2023-10-04 13:54:28] iter = 09500, loss = 1.6251
[2023-10-04 13:54:29] iter = 09510, loss = 1.6274
[2023-10-04 13:54:30] iter = 09520, loss = 1.4535
[2023-10-04 13:54:31] iter = 09530, loss = 1.4856
[2023-10-04 13:54:32] iter = 09540, loss = 1.4641
[2023-10-04 13:54:33] iter = 09550, loss = 1.5627
[2023-10-04 13:54:34] iter = 09560, loss = 1.5066
[2023-10-04 13:54:35] iter = 09570, loss = 1.5479
[2023-10-04 13:54:35] iter = 09580, loss = 1.5168
[2023-10-04 13:54:36] iter = 09590, loss = 1.5498
[2023-10-04 13:54:37] iter = 09600, loss = 1.4709
[2023-10-04 13:54:38] iter = 09610, loss = 1.6357
[2023-10-04 13:54:39] iter = 09620, loss = 1.4951
[2023-10-04 13:54:40] iter = 09630, loss = 1.5161
[2023-10-04 13:54:41] iter = 09640, loss = 1.4538
[2023-10-04 13:54:42] iter = 09650, loss = 1.4723
[2023-10-04 13:54:43] iter = 09660, loss = 1.6005
[2023-10-04 13:54:44] iter = 09670, loss = 1.6342
[2023-10-04 13:54:45] iter = 09680, loss = 1.3697
[2023-10-04 13:54:46] iter = 09690, loss = 1.6821
[2023-10-04 13:54:47] iter = 09700, loss = 1.4897
[2023-10-04 13:54:48] iter = 09710, loss = 1.6815
[2023-10-04 13:54:49] iter = 09720, loss = 1.5048
[2023-10-04 13:54:50] iter = 09730, loss = 1.5126
[2023-10-04 13:54:51] iter = 09740, loss = 1.5630
[2023-10-04 13:54:51] iter = 09750, loss = 1.6421
[2023-10-04 13:54:52] iter = 09760, loss = 1.5190
[2023-10-04 13:54:53] iter = 09770, loss = 1.6215
[2023-10-04 13:54:54] iter = 09780, loss = 1.6054
[2023-10-04 13:54:55] iter = 09790, loss = 1.6466
[2023-10-04 13:54:56] iter = 09800, loss = 1.4802
[2023-10-04 13:54:57] iter = 09810, loss = 1.6615
[2023-10-04 13:54:58] iter = 09820, loss = 1.5638
[2023-10-04 13:54:59] iter = 09830, loss = 1.6503
[2023-10-04 13:55:00] iter = 09840, loss = 1.4913
[2023-10-04 13:55:00] iter = 09850, loss = 1.7349
[2023-10-04 13:55:01] iter = 09860, loss = 1.5401
[2023-10-04 13:55:02] iter = 09870, loss = 1.6058
[2023-10-04 13:55:03] iter = 09880, loss = 1.4355
[2023-10-04 13:55:04] iter = 09890, loss = 1.5774
[2023-10-04 13:55:05] iter = 09900, loss = 1.6468
[2023-10-04 13:55:06] iter = 09910, loss = 1.5943
[2023-10-04 13:55:07] iter = 09920, loss = 1.5347
[2023-10-04 13:55:08] iter = 09930, loss = 1.6027
[2023-10-04 13:55:09] iter = 09940, loss = 1.5909
[2023-10-04 13:55:10] iter = 09950, loss = 1.6764
[2023-10-04 13:55:11] iter = 09960, loss = 1.5602
[2023-10-04 13:55:12] iter = 09970, loss = 1.6256
[2023-10-04 13:55:13] iter = 09980, loss = 1.4778
[2023-10-04 13:55:14] iter = 09990, loss = 1.6502
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 14852}
[2023-10-04 13:55:39] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.023860 train acc = 1.0000, test acc = 0.6151
[2023-10-04 13:56:04] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.011262 train acc = 1.0000, test acc = 0.6114
[2023-10-04 13:56:28] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.009628 train acc = 1.0000, test acc = 0.6162
[2023-10-04 13:56:53] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.009833 train acc = 0.9980, test acc = 0.6117
[2023-10-04 13:57:18] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011110 train acc = 1.0000, test acc = 0.6127
[2023-10-04 13:57:42] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.009642 train acc = 1.0000, test acc = 0.6133
[2023-10-04 13:58:07] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.013838 train acc = 1.0000, test acc = 0.6154
[2023-10-04 13:58:31] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003127 train acc = 1.0000, test acc = 0.6139
[2023-10-04 13:58:56] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003884 train acc = 1.0000, test acc = 0.6132
[2023-10-04 13:59:21] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.004436 train acc = 1.0000, test acc = 0.6200
[2023-10-04 13:59:45] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.009575 train acc = 1.0000, test acc = 0.6059
[2023-10-04 14:00:10] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.013752 train acc = 1.0000, test acc = 0.6145
[2023-10-04 14:00:34] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001856 train acc = 1.0000, test acc = 0.6207
[2023-10-04 14:00:59] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004642 train acc = 1.0000, test acc = 0.6153
[2023-10-04 14:01:23] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.022950 train acc = 1.0000, test acc = 0.6198
[2023-10-04 14:01:48] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.021526 train acc = 0.9980, test acc = 0.6154
[2023-10-04 14:02:13] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.009898 train acc = 1.0000, test acc = 0.6193
[2023-10-04 14:02:37] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.010056 train acc = 1.0000, test acc = 0.6170
[2023-10-04 14:03:02] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002087 train acc = 1.0000, test acc = 0.6134
[2023-10-04 14:03:26] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003212 train acc = 1.0000, test acc = 0.6177
Evaluate 20 random ConvNet, mean = 0.6151 std = 0.0034
-------------------------
[2023-10-04 14:03:27] iter = 10000, loss = 1.4968
[2023-10-04 14:03:28] iter = 10010, loss = 1.4876
[2023-10-04 14:03:29] iter = 10020, loss = 1.6508
[2023-10-04 14:03:29] iter = 10030, loss = 1.5022
[2023-10-04 14:03:30] iter = 10040, loss = 1.5543
[2023-10-04 14:03:31] iter = 10050, loss = 1.6849
[2023-10-04 14:03:32] iter = 10060, loss = 1.4404
[2023-10-04 14:03:33] iter = 10070, loss = 1.3826
[2023-10-04 14:03:34] iter = 10080, loss = 1.5746
[2023-10-04 14:03:35] iter = 10090, loss = 1.4050
[2023-10-04 14:03:36] iter = 10100, loss = 1.4992
[2023-10-04 14:03:37] iter = 10110, loss = 1.4682
[2023-10-04 14:03:38] iter = 10120, loss = 1.5108
[2023-10-04 14:03:39] iter = 10130, loss = 1.4971
[2023-10-04 14:03:40] iter = 10140, loss = 1.5002
[2023-10-04 14:03:40] iter = 10150, loss = 1.5509
[2023-10-04 14:03:41] iter = 10160, loss = 1.5453
[2023-10-04 14:03:42] iter = 10170, loss = 1.4996
[2023-10-04 14:03:43] iter = 10180, loss = 1.6729
[2023-10-04 14:03:44] iter = 10190, loss = 1.4821
[2023-10-04 14:03:45] iter = 10200, loss = 1.5136
[2023-10-04 14:03:46] iter = 10210, loss = 1.5074
[2023-10-04 14:03:47] iter = 10220, loss = 1.5553
[2023-10-04 14:03:48] iter = 10230, loss = 1.5078
[2023-10-04 14:03:49] iter = 10240, loss = 1.5375
[2023-10-04 14:03:50] iter = 10250, loss = 1.4934
[2023-10-04 14:03:51] iter = 10260, loss = 1.6725
[2023-10-04 14:03:52] iter = 10270, loss = 1.5609
[2023-10-04 14:03:52] iter = 10280, loss = 1.6821
[2023-10-04 14:03:53] iter = 10290, loss = 1.6017
[2023-10-04 14:03:54] iter = 10300, loss = 1.5315
[2023-10-04 14:03:55] iter = 10310, loss = 1.5327
[2023-10-04 14:03:56] iter = 10320, loss = 1.5084
[2023-10-04 14:03:57] iter = 10330, loss = 1.3355
[2023-10-04 14:03:58] iter = 10340, loss = 1.5357
[2023-10-04 14:03:59] iter = 10350, loss = 1.5025
[2023-10-04 14:04:00] iter = 10360, loss = 1.5756
[2023-10-04 14:04:01] iter = 10370, loss = 1.5296
[2023-10-04 14:04:02] iter = 10380, loss = 1.6007
[2023-10-04 14:04:03] iter = 10390, loss = 1.6507
[2023-10-04 14:04:03] iter = 10400, loss = 1.5569
[2023-10-04 14:04:04] iter = 10410, loss = 1.6349
[2023-10-04 14:04:05] iter = 10420, loss = 1.5545
[2023-10-04 14:04:06] iter = 10430, loss = 1.6111
[2023-10-04 14:04:07] iter = 10440, loss = 1.4336
[2023-10-04 14:04:08] iter = 10450, loss = 1.6242
[2023-10-04 14:04:09] iter = 10460, loss = 1.5373
[2023-10-04 14:04:10] iter = 10470, loss = 1.4628
[2023-10-04 14:04:11] iter = 10480, loss = 1.5567
[2023-10-04 14:04:12] iter = 10490, loss = 1.4029
[2023-10-04 14:04:13] iter = 10500, loss = 1.4749
[2023-10-04 14:04:13] iter = 10510, loss = 1.5134
[2023-10-04 14:04:14] iter = 10520, loss = 1.5260
[2023-10-04 14:04:15] iter = 10530, loss = 1.6152
[2023-10-04 14:04:16] iter = 10540, loss = 1.5145
[2023-10-04 14:04:17] iter = 10550, loss = 1.5277
[2023-10-04 14:04:18] iter = 10560, loss = 1.5574
[2023-10-04 14:04:19] iter = 10570, loss = 1.5547
[2023-10-04 14:04:20] iter = 10580, loss = 1.4812
[2023-10-04 14:04:21] iter = 10590, loss = 1.4799
[2023-10-04 14:04:22] iter = 10600, loss = 1.5270
[2023-10-04 14:04:23] iter = 10610, loss = 1.3809
[2023-10-04 14:04:24] iter = 10620, loss = 1.6634
[2023-10-04 14:04:25] iter = 10630, loss = 1.5340
[2023-10-04 14:04:25] iter = 10640, loss = 1.5286
[2023-10-04 14:04:26] iter = 10650, loss = 1.5277
[2023-10-04 14:04:27] iter = 10660, loss = 1.6353
[2023-10-04 14:04:28] iter = 10670, loss = 1.8200
[2023-10-04 14:04:29] iter = 10680, loss = 1.5876
[2023-10-04 14:04:30] iter = 10690, loss = 1.7717
[2023-10-04 14:04:31] iter = 10700, loss = 1.5021
[2023-10-04 14:04:32] iter = 10710, loss = 1.5430
[2023-10-04 14:04:33] iter = 10720, loss = 1.4232
[2023-10-04 14:04:34] iter = 10730, loss = 1.5938
[2023-10-04 14:04:35] iter = 10740, loss = 1.4362
[2023-10-04 14:04:36] iter = 10750, loss = 1.4509
[2023-10-04 14:04:37] iter = 10760, loss = 1.5044
[2023-10-04 14:04:38] iter = 10770, loss = 1.5497
[2023-10-04 14:04:39] iter = 10780, loss = 1.5374
[2023-10-04 14:04:39] iter = 10790, loss = 1.5030
[2023-10-04 14:04:40] iter = 10800, loss = 1.4500
[2023-10-04 14:04:41] iter = 10810, loss = 1.4332
[2023-10-04 14:04:42] iter = 10820, loss = 1.5382
[2023-10-04 14:04:43] iter = 10830, loss = 1.5802
[2023-10-04 14:04:44] iter = 10840, loss = 1.7421
[2023-10-04 14:04:45] iter = 10850, loss = 1.7584
[2023-10-04 14:04:46] iter = 10860, loss = 1.8112
[2023-10-04 14:04:47] iter = 10870, loss = 1.4824
[2023-10-04 14:04:48] iter = 10880, loss = 1.5640
[2023-10-04 14:04:49] iter = 10890, loss = 1.5404
[2023-10-04 14:04:50] iter = 10900, loss = 1.4788
[2023-10-04 14:04:51] iter = 10910, loss = 1.6194
[2023-10-04 14:04:51] iter = 10920, loss = 1.4813
[2023-10-04 14:04:52] iter = 10930, loss = 1.6295
[2023-10-04 14:04:53] iter = 10940, loss = 1.3855
[2023-10-04 14:04:54] iter = 10950, loss = 1.4043
[2023-10-04 14:04:55] iter = 10960, loss = 1.5892
[2023-10-04 14:04:56] iter = 10970, loss = 1.7454
[2023-10-04 14:04:57] iter = 10980, loss = 1.4921
[2023-10-04 14:04:58] iter = 10990, loss = 1.5629
[2023-10-04 14:04:59] iter = 11000, loss = 1.6377
[2023-10-04 14:05:00] iter = 11010, loss = 1.5286
[2023-10-04 14:05:01] iter = 11020, loss = 1.5951
[2023-10-04 14:05:02] iter = 11030, loss = 1.4972
[2023-10-04 14:05:02] iter = 11040, loss = 1.5206
[2023-10-04 14:05:03] iter = 11050, loss = 1.6613
[2023-10-04 14:05:04] iter = 11060, loss = 1.4609
[2023-10-04 14:05:05] iter = 11070, loss = 1.4520
[2023-10-04 14:05:06] iter = 11080, loss = 1.5924
[2023-10-04 14:05:07] iter = 11090, loss = 1.4386
[2023-10-04 14:05:08] iter = 11100, loss = 1.5106
[2023-10-04 14:05:09] iter = 11110, loss = 1.4469
[2023-10-04 14:05:10] iter = 11120, loss = 1.5940
[2023-10-04 14:05:11] iter = 11130, loss = 1.4449
[2023-10-04 14:05:12] iter = 11140, loss = 1.4918
[2023-10-04 14:05:13] iter = 11150, loss = 1.5645
[2023-10-04 14:05:14] iter = 11160, loss = 1.4702
[2023-10-04 14:05:15] iter = 11170, loss = 1.5641
[2023-10-04 14:05:15] iter = 11180, loss = 1.5218
[2023-10-04 14:05:16] iter = 11190, loss = 1.3813
[2023-10-04 14:05:17] iter = 11200, loss = 1.4468
[2023-10-04 14:05:18] iter = 11210, loss = 1.5438
[2023-10-04 14:05:19] iter = 11220, loss = 1.6518
[2023-10-04 14:05:20] iter = 11230, loss = 1.6960
[2023-10-04 14:05:21] iter = 11240, loss = 1.6429
[2023-10-04 14:05:22] iter = 11250, loss = 1.5499
[2023-10-04 14:05:23] iter = 11260, loss = 1.4965
[2023-10-04 14:05:24] iter = 11270, loss = 1.6385
[2023-10-04 14:05:25] iter = 11280, loss = 1.4056
[2023-10-04 14:05:26] iter = 11290, loss = 1.5131
[2023-10-04 14:05:26] iter = 11300, loss = 1.6204
[2023-10-04 14:05:27] iter = 11310, loss = 1.4995
[2023-10-04 14:05:28] iter = 11320, loss = 1.5102
[2023-10-04 14:05:29] iter = 11330, loss = 1.5505
[2023-10-04 14:05:30] iter = 11340, loss = 1.5357
[2023-10-04 14:05:31] iter = 11350, loss = 1.4190
[2023-10-04 14:05:32] iter = 11360, loss = 1.4695
[2023-10-04 14:05:33] iter = 11370, loss = 1.6475
[2023-10-04 14:05:34] iter = 11380, loss = 1.6305
[2023-10-04 14:05:35] iter = 11390, loss = 1.4400
[2023-10-04 14:05:35] iter = 11400, loss = 1.5140
[2023-10-04 14:05:36] iter = 11410, loss = 1.6159
[2023-10-04 14:05:37] iter = 11420, loss = 1.4962
[2023-10-04 14:05:38] iter = 11430, loss = 1.4200
[2023-10-04 14:05:39] iter = 11440, loss = 1.4744
[2023-10-04 14:05:40] iter = 11450, loss = 1.6327
[2023-10-04 14:05:41] iter = 11460, loss = 1.5318
[2023-10-04 14:05:42] iter = 11470, loss = 1.4498
[2023-10-04 14:05:43] iter = 11480, loss = 1.4556
[2023-10-04 14:05:44] iter = 11490, loss = 1.5672
[2023-10-04 14:05:44] iter = 11500, loss = 1.5431
[2023-10-04 14:05:45] iter = 11510, loss = 1.5227
[2023-10-04 14:05:46] iter = 11520, loss = 1.5758
[2023-10-04 14:05:47] iter = 11530, loss = 1.4558
[2023-10-04 14:05:48] iter = 11540, loss = 1.4572
[2023-10-04 14:05:49] iter = 11550, loss = 1.3604
[2023-10-04 14:05:50] iter = 11560, loss = 1.3881
[2023-10-04 14:05:51] iter = 11570, loss = 1.4743
[2023-10-04 14:05:52] iter = 11580, loss = 1.5912
[2023-10-04 14:05:53] iter = 11590, loss = 1.4837
[2023-10-04 14:05:54] iter = 11600, loss = 1.4980
[2023-10-04 14:05:55] iter = 11610, loss = 1.6293
[2023-10-04 14:05:55] iter = 11620, loss = 1.5015
[2023-10-04 14:05:56] iter = 11630, loss = 1.4796
[2023-10-04 14:05:57] iter = 11640, loss = 1.5487
[2023-10-04 14:05:58] iter = 11650, loss = 1.4702
[2023-10-04 14:05:59] iter = 11660, loss = 1.6128
[2023-10-04 14:06:00] iter = 11670, loss = 1.5723
[2023-10-04 14:06:01] iter = 11680, loss = 1.4363
[2023-10-04 14:06:02] iter = 11690, loss = 1.5217
[2023-10-04 14:06:03] iter = 11700, loss = 1.5615
[2023-10-04 14:06:04] iter = 11710, loss = 1.5391
[2023-10-04 14:06:05] iter = 11720, loss = 1.4082
[2023-10-04 14:06:06] iter = 11730, loss = 1.5341
[2023-10-04 14:06:07] iter = 11740, loss = 1.5996
[2023-10-04 14:06:07] iter = 11750, loss = 1.4648
[2023-10-04 14:06:08] iter = 11760, loss = 1.5658
[2023-10-04 14:06:09] iter = 11770, loss = 1.6472
[2023-10-04 14:06:10] iter = 11780, loss = 1.4984
[2023-10-04 14:06:11] iter = 11790, loss = 1.5730
[2023-10-04 14:06:12] iter = 11800, loss = 1.4518
[2023-10-04 14:06:13] iter = 11810, loss = 1.4885
[2023-10-04 14:06:13] iter = 11820, loss = 1.5885
[2023-10-04 14:06:15] iter = 11830, loss = 1.3982
[2023-10-04 14:06:15] iter = 11840, loss = 1.6143
[2023-10-04 14:06:16] iter = 11850, loss = 1.5900
[2023-10-04 14:06:17] iter = 11860, loss = 1.4787
[2023-10-04 14:06:18] iter = 11870, loss = 1.4286
[2023-10-04 14:06:19] iter = 11880, loss = 1.6443
[2023-10-04 14:06:20] iter = 11890, loss = 1.4070
[2023-10-04 14:06:21] iter = 11900, loss = 1.5224
[2023-10-04 14:06:22] iter = 11910, loss = 1.6171
[2023-10-04 14:06:23] iter = 11920, loss = 1.4993
[2023-10-04 14:06:24] iter = 11930, loss = 1.5898
[2023-10-04 14:06:25] iter = 11940, loss = 1.5555
[2023-10-04 14:06:26] iter = 11950, loss = 1.5053
[2023-10-04 14:06:27] iter = 11960, loss = 1.6924
[2023-10-04 14:06:27] iter = 11970, loss = 1.5852
[2023-10-04 14:06:28] iter = 11980, loss = 1.5119
[2023-10-04 14:06:29] iter = 11990, loss = 1.5878
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 90563}
[2023-10-04 14:06:55] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005277 train acc = 1.0000, test acc = 0.6207
[2023-10-04 14:07:19] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001684 train acc = 1.0000, test acc = 0.6200
[2023-10-04 14:07:43] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.022749 train acc = 1.0000, test acc = 0.6186
[2023-10-04 14:08:08] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.011557 train acc = 1.0000, test acc = 0.6193
[2023-10-04 14:08:33] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.006234 train acc = 0.9980, test acc = 0.6274
[2023-10-04 14:08:57] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.014669 train acc = 1.0000, test acc = 0.6149
[2023-10-04 14:09:22] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.021753 train acc = 1.0000, test acc = 0.6068
[2023-10-04 14:09:46] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.014796 train acc = 1.0000, test acc = 0.6198
[2023-10-04 14:10:11] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.021637 train acc = 1.0000, test acc = 0.6224
[2023-10-04 14:10:35] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.016203 train acc = 1.0000, test acc = 0.6090
[2023-10-04 14:11:00] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013373 train acc = 0.9980, test acc = 0.6134
[2023-10-04 14:11:25] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004344 train acc = 1.0000, test acc = 0.6194
[2023-10-04 14:11:49] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001929 train acc = 1.0000, test acc = 0.6141
[2023-10-04 14:12:14] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.006389 train acc = 1.0000, test acc = 0.6178
[2023-10-04 14:12:38] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.018925 train acc = 1.0000, test acc = 0.6192
[2023-10-04 14:13:03] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001691 train acc = 1.0000, test acc = 0.6204
[2023-10-04 14:13:27] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004004 train acc = 1.0000, test acc = 0.6234
[2023-10-04 14:13:52] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.013438 train acc = 1.0000, test acc = 0.6187
[2023-10-04 14:14:16] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.017414 train acc = 0.9980, test acc = 0.6145
[2023-10-04 14:14:41] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.002124 train acc = 1.0000, test acc = 0.6174
Evaluate 20 random ConvNet, mean = 0.6179 std = 0.0046
-------------------------
[2023-10-04 14:14:41] iter = 12000, loss = 1.5386
[2023-10-04 14:14:42] iter = 12010, loss = 1.5672
[2023-10-04 14:14:43] iter = 12020, loss = 1.5085
[2023-10-04 14:14:44] iter = 12030, loss = 1.3994
[2023-10-04 14:14:45] iter = 12040, loss = 1.4739
[2023-10-04 14:14:46] iter = 12050, loss = 1.4955
[2023-10-04 14:14:47] iter = 12060, loss = 1.4156
[2023-10-04 14:14:47] iter = 12070, loss = 1.5633
[2023-10-04 14:14:48] iter = 12080, loss = 1.5454
[2023-10-04 14:14:50] iter = 12090, loss = 1.6087
[2023-10-04 14:14:51] iter = 12100, loss = 1.6348
[2023-10-04 14:14:51] iter = 12110, loss = 1.5437
[2023-10-04 14:14:52] iter = 12120, loss = 1.4547
[2023-10-04 14:14:53] iter = 12130, loss = 1.4876
[2023-10-04 14:14:54] iter = 12140, loss = 1.5656
[2023-10-04 14:14:55] iter = 12150, loss = 1.4101
[2023-10-04 14:14:56] iter = 12160, loss = 1.3991
[2023-10-04 14:14:57] iter = 12170, loss = 1.5365
[2023-10-04 14:14:58] iter = 12180, loss = 1.6841
[2023-10-04 14:14:59] iter = 12190, loss = 1.5017
[2023-10-04 14:15:00] iter = 12200, loss = 1.4216
[2023-10-04 14:15:01] iter = 12210, loss = 1.5775
[2023-10-04 14:15:02] iter = 12220, loss = 1.5145
[2023-10-04 14:15:03] iter = 12230, loss = 1.3786
[2023-10-04 14:15:03] iter = 12240, loss = 1.4807
[2023-10-04 14:15:04] iter = 12250, loss = 1.4956
[2023-10-04 14:15:05] iter = 12260, loss = 1.5108
[2023-10-04 14:15:06] iter = 12270, loss = 1.6037
[2023-10-04 14:15:07] iter = 12280, loss = 1.5755
[2023-10-04 14:15:08] iter = 12290, loss = 1.5427
[2023-10-04 14:15:09] iter = 12300, loss = 1.4270
[2023-10-04 14:15:10] iter = 12310, loss = 1.4782
[2023-10-04 14:15:11] iter = 12320, loss = 1.5117
[2023-10-04 14:15:12] iter = 12330, loss = 1.5121
[2023-10-04 14:15:13] iter = 12340, loss = 1.6254
[2023-10-04 14:15:14] iter = 12350, loss = 1.5548
[2023-10-04 14:15:15] iter = 12360, loss = 1.4455
[2023-10-04 14:15:16] iter = 12370, loss = 1.5365
[2023-10-04 14:15:17] iter = 12380, loss = 1.4409
[2023-10-04 14:15:18] iter = 12390, loss = 1.5510
[2023-10-04 14:15:19] iter = 12400, loss = 1.5770
[2023-10-04 14:15:19] iter = 12410, loss = 1.4015
[2023-10-04 14:15:20] iter = 12420, loss = 1.5158
[2023-10-04 14:15:21] iter = 12430, loss = 1.5976
[2023-10-04 14:15:22] iter = 12440, loss = 1.5012
[2023-10-04 14:15:23] iter = 12450, loss = 1.6851
[2023-10-04 14:15:24] iter = 12460, loss = 1.4423
[2023-10-04 14:15:25] iter = 12470, loss = 1.5036
[2023-10-04 14:15:26] iter = 12480, loss = 1.4569
[2023-10-04 14:15:27] iter = 12490, loss = 1.5164
[2023-10-04 14:15:28] iter = 12500, loss = 1.5933
[2023-10-04 14:15:28] iter = 12510, loss = 1.4978
[2023-10-04 14:15:29] iter = 12520, loss = 1.5948
[2023-10-04 14:15:30] iter = 12530, loss = 1.7767
[2023-10-04 14:15:31] iter = 12540, loss = 1.3995
[2023-10-04 14:15:32] iter = 12550, loss = 1.5499
[2023-10-04 14:15:33] iter = 12560, loss = 1.4550
[2023-10-04 14:15:34] iter = 12570, loss = 1.5928
[2023-10-04 14:15:35] iter = 12580, loss = 1.5832
[2023-10-04 14:15:36] iter = 12590, loss = 1.6065
[2023-10-04 14:15:37] iter = 12600, loss = 1.4625
[2023-10-04 14:15:38] iter = 12610, loss = 1.4743
[2023-10-04 14:15:39] iter = 12620, loss = 1.5647
[2023-10-04 14:15:39] iter = 12630, loss = 1.5404
[2023-10-04 14:15:40] iter = 12640, loss = 1.3430
[2023-10-04 14:15:41] iter = 12650, loss = 1.4791
[2023-10-04 14:15:42] iter = 12660, loss = 1.4802
[2023-10-04 14:15:43] iter = 12670, loss = 1.5314
[2023-10-04 14:15:44] iter = 12680, loss = 1.4438
[2023-10-04 14:15:45] iter = 12690, loss = 1.6045
[2023-10-04 14:15:46] iter = 12700, loss = 1.3910
[2023-10-04 14:15:47] iter = 12710, loss = 1.5009
[2023-10-04 14:15:47] iter = 12720, loss = 1.4524
[2023-10-04 14:15:48] iter = 12730, loss = 1.4691
[2023-10-04 14:15:49] iter = 12740, loss = 1.5240
[2023-10-04 14:15:50] iter = 12750, loss = 1.4809
[2023-10-04 14:15:51] iter = 12760, loss = 1.5513
[2023-10-04 14:15:52] iter = 12770, loss = 1.4967
[2023-10-04 14:15:53] iter = 12780, loss = 1.5130
[2023-10-04 14:15:54] iter = 12790, loss = 1.4005
[2023-10-04 14:15:55] iter = 12800, loss = 1.5480
[2023-10-04 14:15:56] iter = 12810, loss = 1.4276
[2023-10-04 14:15:57] iter = 12820, loss = 1.3966
[2023-10-04 14:15:57] iter = 12830, loss = 1.4885
[2023-10-04 14:15:58] iter = 12840, loss = 1.6457
[2023-10-04 14:15:59] iter = 12850, loss = 1.4586
[2023-10-04 14:16:00] iter = 12860, loss = 1.5977
[2023-10-04 14:16:01] iter = 12870, loss = 1.4387
[2023-10-04 14:16:02] iter = 12880, loss = 1.5972
[2023-10-04 14:16:03] iter = 12890, loss = 1.5708
[2023-10-04 14:16:04] iter = 12900, loss = 1.4029
[2023-10-04 14:16:05] iter = 12910, loss = 1.4460
[2023-10-04 14:16:06] iter = 12920, loss = 1.4431
[2023-10-04 14:16:06] iter = 12930, loss = 1.6586
[2023-10-04 14:16:07] iter = 12940, loss = 1.6375
[2023-10-04 14:16:08] iter = 12950, loss = 1.4624
[2023-10-04 14:16:09] iter = 12960, loss = 1.5394
[2023-10-04 14:16:10] iter = 12970, loss = 1.4529
[2023-10-04 14:16:11] iter = 12980, loss = 1.7653
[2023-10-04 14:16:12] iter = 12990, loss = 1.3288
[2023-10-04 14:16:13] iter = 13000, loss = 1.4283
[2023-10-04 14:16:14] iter = 13010, loss = 1.4917
[2023-10-04 14:16:15] iter = 13020, loss = 1.5759
[2023-10-04 14:16:16] iter = 13030, loss = 1.4157
[2023-10-04 14:16:17] iter = 13040, loss = 1.4748
[2023-10-04 14:16:18] iter = 13050, loss = 1.3636
[2023-10-04 14:16:19] iter = 13060, loss = 1.6405
[2023-10-04 14:16:20] iter = 13070, loss = 1.4594
[2023-10-04 14:16:21] iter = 13080, loss = 1.4752
[2023-10-04 14:16:21] iter = 13090, loss = 1.4210
[2023-10-04 14:16:22] iter = 13100, loss = 1.3332
[2023-10-04 14:16:23] iter = 13110, loss = 1.4839
[2023-10-04 14:16:24] iter = 13120, loss = 1.5085
[2023-10-04 14:16:25] iter = 13130, loss = 1.4252
[2023-10-04 14:16:26] iter = 13140, loss = 1.6005
[2023-10-04 14:16:27] iter = 13150, loss = 1.4295
[2023-10-04 14:16:28] iter = 13160, loss = 1.6649
[2023-10-04 14:16:29] iter = 13170, loss = 1.5279
[2023-10-04 14:16:30] iter = 13180, loss = 1.5365
[2023-10-04 14:16:30] iter = 13190, loss = 1.5566
[2023-10-04 14:16:31] iter = 13200, loss = 1.6488
[2023-10-04 14:16:32] iter = 13210, loss = 1.4164
[2023-10-04 14:16:33] iter = 13220, loss = 1.5262
[2023-10-04 14:16:34] iter = 13230, loss = 1.5511
[2023-10-04 14:16:35] iter = 13240, loss = 1.5243
[2023-10-04 14:16:36] iter = 13250, loss = 1.6017
[2023-10-04 14:16:37] iter = 13260, loss = 1.4622
[2023-10-04 14:16:38] iter = 13270, loss = 1.5145
[2023-10-04 14:16:39] iter = 13280, loss = 1.4492
[2023-10-04 14:16:40] iter = 13290, loss = 1.4280
[2023-10-04 14:16:40] iter = 13300, loss = 1.6365
[2023-10-04 14:16:41] iter = 13310, loss = 1.5699
[2023-10-04 14:16:42] iter = 13320, loss = 1.4698
[2023-10-04 14:16:43] iter = 13330, loss = 1.4549
[2023-10-04 14:16:44] iter = 13340, loss = 1.4124
[2023-10-04 14:16:45] iter = 13350, loss = 1.6477
[2023-10-04 14:16:46] iter = 13360, loss = 1.6637
[2023-10-04 14:16:47] iter = 13370, loss = 1.4811
[2023-10-04 14:16:48] iter = 13380, loss = 1.5112
[2023-10-04 14:16:49] iter = 13390, loss = 1.6559
[2023-10-04 14:16:50] iter = 13400, loss = 1.5562
[2023-10-04 14:16:51] iter = 13410, loss = 1.4571
[2023-10-04 14:16:52] iter = 13420, loss = 1.4764
[2023-10-04 14:16:52] iter = 13430, loss = 1.7724
[2023-10-04 14:16:53] iter = 13440, loss = 1.4758
[2023-10-04 14:16:54] iter = 13450, loss = 1.4543
[2023-10-04 14:16:55] iter = 13460, loss = 1.6161
[2023-10-04 14:16:56] iter = 13470, loss = 1.5243
[2023-10-04 14:16:57] iter = 13480, loss = 1.4736
[2023-10-04 14:16:58] iter = 13490, loss = 1.4295
[2023-10-04 14:16:59] iter = 13500, loss = 1.4124
[2023-10-04 14:17:00] iter = 13510, loss = 1.5609
[2023-10-04 14:17:01] iter = 13520, loss = 1.5292
[2023-10-04 14:17:02] iter = 13530, loss = 1.6260
[2023-10-04 14:17:03] iter = 13540, loss = 1.3830
[2023-10-04 14:17:04] iter = 13550, loss = 1.5761
[2023-10-04 14:17:05] iter = 13560, loss = 1.5312
[2023-10-04 14:17:05] iter = 13570, loss = 1.5326
[2023-10-04 14:17:06] iter = 13580, loss = 1.7487
[2023-10-04 14:17:07] iter = 13590, loss = 1.5461
[2023-10-04 14:17:08] iter = 13600, loss = 1.5957
[2023-10-04 14:17:09] iter = 13610, loss = 1.4713
[2023-10-04 14:17:10] iter = 13620, loss = 1.4811
[2023-10-04 14:17:11] iter = 13630, loss = 1.5264
[2023-10-04 14:17:12] iter = 13640, loss = 1.3937
[2023-10-04 14:17:13] iter = 13650, loss = 1.5149
[2023-10-04 14:17:14] iter = 13660, loss = 1.5236
[2023-10-04 14:17:15] iter = 13670, loss = 1.6469
[2023-10-04 14:17:16] iter = 13680, loss = 1.4095
[2023-10-04 14:17:16] iter = 13690, loss = 1.3527
[2023-10-04 14:17:17] iter = 13700, loss = 1.5219
[2023-10-04 14:17:18] iter = 13710, loss = 1.4379
[2023-10-04 14:17:19] iter = 13720, loss = 1.4691
[2023-10-04 14:17:20] iter = 13730, loss = 1.6388
[2023-10-04 14:17:21] iter = 13740, loss = 1.4882
[2023-10-04 14:17:22] iter = 13750, loss = 1.4985
[2023-10-04 14:17:23] iter = 13760, loss = 1.6496
[2023-10-04 14:17:24] iter = 13770, loss = 1.4799
[2023-10-04 14:17:25] iter = 13780, loss = 1.5371
[2023-10-04 14:17:26] iter = 13790, loss = 1.6132
[2023-10-04 14:17:27] iter = 13800, loss = 1.4724
[2023-10-04 14:17:28] iter = 13810, loss = 1.5731
[2023-10-04 14:17:28] iter = 13820, loss = 1.5039
[2023-10-04 14:17:29] iter = 13830, loss = 1.3802
[2023-10-04 14:17:30] iter = 13840, loss = 1.5623
[2023-10-04 14:17:31] iter = 13850, loss = 1.4975
[2023-10-04 14:17:32] iter = 13860, loss = 1.5563
[2023-10-04 14:17:33] iter = 13870, loss = 1.5851
[2023-10-04 14:17:34] iter = 13880, loss = 1.6598
[2023-10-04 14:17:35] iter = 13890, loss = 1.4904
[2023-10-04 14:17:36] iter = 13900, loss = 1.5404
[2023-10-04 14:17:37] iter = 13910, loss = 1.5225
[2023-10-04 14:17:38] iter = 13920, loss = 1.4861
[2023-10-04 14:17:38] iter = 13930, loss = 1.5874
[2023-10-04 14:17:39] iter = 13940, loss = 1.4501
[2023-10-04 14:17:40] iter = 13950, loss = 1.4762
[2023-10-04 14:17:41] iter = 13960, loss = 1.3562
[2023-10-04 14:17:42] iter = 13970, loss = 1.4093
[2023-10-04 14:17:43] iter = 13980, loss = 1.4968
[2023-10-04 14:17:44] iter = 13990, loss = 1.5018
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 65027}
[2023-10-04 14:18:09] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.021382 train acc = 0.9980, test acc = 0.6217
[2023-10-04 14:18:34] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001707 train acc = 1.0000, test acc = 0.6149
[2023-10-04 14:18:59] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.004473 train acc = 1.0000, test acc = 0.6153
[2023-10-04 14:19:23] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.005720 train acc = 1.0000, test acc = 0.6265
[2023-10-04 14:19:48] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.005190 train acc = 1.0000, test acc = 0.6249
[2023-10-04 14:20:12] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.029083 train acc = 0.9960, test acc = 0.6210
[2023-10-04 14:20:37] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002235 train acc = 1.0000, test acc = 0.6157
[2023-10-04 14:21:01] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002203 train acc = 1.0000, test acc = 0.6243
[2023-10-04 14:21:26] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.021168 train acc = 1.0000, test acc = 0.6176
[2023-10-04 14:21:50] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003533 train acc = 1.0000, test acc = 0.6264
[2023-10-04 14:22:14] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013535 train acc = 1.0000, test acc = 0.6313
[2023-10-04 14:22:38] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004026 train acc = 1.0000, test acc = 0.6219
[2023-10-04 14:23:03] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001983 train acc = 1.0000, test acc = 0.6195
[2023-10-04 14:23:27] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002071 train acc = 1.0000, test acc = 0.6221
[2023-10-04 14:23:52] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.015424 train acc = 1.0000, test acc = 0.6235
[2023-10-04 14:24:16] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.011780 train acc = 1.0000, test acc = 0.6266
[2023-10-04 14:24:40] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.012325 train acc = 1.0000, test acc = 0.6120
[2023-10-04 14:25:04] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.007040 train acc = 1.0000, test acc = 0.6178
[2023-10-04 14:25:29] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002522 train acc = 1.0000, test acc = 0.6263
[2023-10-04 14:25:53] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.009901 train acc = 0.9980, test acc = 0.6165
Evaluate 20 random ConvNet, mean = 0.6213 std = 0.0049
-------------------------
[2023-10-04 14:25:54] iter = 14000, loss = 1.5003
[2023-10-04 14:25:55] iter = 14010, loss = 1.5187
[2023-10-04 14:25:55] iter = 14020, loss = 1.5376
[2023-10-04 14:25:56] iter = 14030, loss = 1.4607
[2023-10-04 14:25:57] iter = 14040, loss = 1.5924
[2023-10-04 14:25:58] iter = 14050, loss = 1.5147
[2023-10-04 14:25:59] iter = 14060, loss = 1.4705
[2023-10-04 14:26:00] iter = 14070, loss = 1.3916
[2023-10-04 14:26:01] iter = 14080, loss = 1.4697
[2023-10-04 14:26:02] iter = 14090, loss = 1.3438
[2023-10-04 14:26:03] iter = 14100, loss = 1.3801
[2023-10-04 14:26:04] iter = 14110, loss = 1.6313
[2023-10-04 14:26:05] iter = 14120, loss = 1.6951
[2023-10-04 14:26:05] iter = 14130, loss = 1.6116
[2023-10-04 14:26:07] iter = 14140, loss = 1.5187
[2023-10-04 14:26:07] iter = 14150, loss = 1.5468
[2023-10-04 14:26:08] iter = 14160, loss = 1.4681
[2023-10-04 14:26:09] iter = 14170, loss = 1.5811
[2023-10-04 14:26:10] iter = 14180, loss = 1.5135
[2023-10-04 14:26:11] iter = 14190, loss = 1.4977
[2023-10-04 14:26:12] iter = 14200, loss = 1.5331
[2023-10-04 14:26:13] iter = 14210, loss = 1.5026
[2023-10-04 14:26:14] iter = 14220, loss = 1.4493
[2023-10-04 14:26:15] iter = 14230, loss = 1.5246
[2023-10-04 14:26:16] iter = 14240, loss = 1.3256
[2023-10-04 14:26:17] iter = 14250, loss = 1.4951
[2023-10-04 14:26:18] iter = 14260, loss = 1.3915
[2023-10-04 14:26:19] iter = 14270, loss = 1.4649
[2023-10-04 14:26:19] iter = 14280, loss = 1.5753
[2023-10-04 14:26:20] iter = 14290, loss = 1.5076
[2023-10-04 14:26:21] iter = 14300, loss = 1.6872
[2023-10-04 14:26:22] iter = 14310, loss = 1.4582
[2023-10-04 14:26:23] iter = 14320, loss = 1.5955
[2023-10-04 14:26:24] iter = 14330, loss = 1.4253
[2023-10-04 14:26:25] iter = 14340, loss = 1.5716
[2023-10-04 14:26:26] iter = 14350, loss = 1.5633
[2023-10-04 14:26:27] iter = 14360, loss = 1.4707
[2023-10-04 14:26:28] iter = 14370, loss = 1.5012
[2023-10-04 14:26:29] iter = 14380, loss = 1.5002
[2023-10-04 14:26:30] iter = 14390, loss = 1.4824
[2023-10-04 14:26:31] iter = 14400, loss = 1.4170
[2023-10-04 14:26:32] iter = 14410, loss = 1.4432
[2023-10-04 14:26:33] iter = 14420, loss = 1.5953
[2023-10-04 14:26:33] iter = 14430, loss = 1.4102
[2023-10-04 14:26:34] iter = 14440, loss = 1.6083
[2023-10-04 14:26:35] iter = 14450, loss = 1.4362
[2023-10-04 14:26:36] iter = 14460, loss = 1.4045
[2023-10-04 14:26:37] iter = 14470, loss = 1.4206
[2023-10-04 14:26:38] iter = 14480, loss = 1.5251
[2023-10-04 14:26:39] iter = 14490, loss = 1.4458
[2023-10-04 14:26:40] iter = 14500, loss = 1.6883
[2023-10-04 14:26:41] iter = 14510, loss = 1.6609
[2023-10-04 14:26:42] iter = 14520, loss = 1.4528
[2023-10-04 14:26:43] iter = 14530, loss = 1.4477
[2023-10-04 14:26:43] iter = 14540, loss = 1.5293
[2023-10-04 14:26:44] iter = 14550, loss = 1.6501
[2023-10-04 14:26:45] iter = 14560, loss = 1.4199
[2023-10-04 14:26:46] iter = 14570, loss = 1.5115
[2023-10-04 14:26:47] iter = 14580, loss = 1.3957
[2023-10-04 14:26:48] iter = 14590, loss = 1.4909
[2023-10-04 14:26:49] iter = 14600, loss = 1.5202
[2023-10-04 14:26:50] iter = 14610, loss = 1.4357
[2023-10-04 14:26:51] iter = 14620, loss = 1.5184
[2023-10-04 14:26:52] iter = 14630, loss = 1.4731
[2023-10-04 14:26:53] iter = 14640, loss = 1.3899
[2023-10-04 14:26:53] iter = 14650, loss = 1.5336
[2023-10-04 14:26:54] iter = 14660, loss = 1.4108
[2023-10-04 14:26:55] iter = 14670, loss = 1.5951
[2023-10-04 14:26:56] iter = 14680, loss = 1.3644
[2023-10-04 14:26:57] iter = 14690, loss = 1.4723
[2023-10-04 14:26:58] iter = 14700, loss = 1.5763
[2023-10-04 14:26:59] iter = 14710, loss = 1.5188
[2023-10-04 14:27:00] iter = 14720, loss = 1.5236
[2023-10-04 14:27:01] iter = 14730, loss = 1.4964
[2023-10-04 14:27:02] iter = 14740, loss = 1.4421
[2023-10-04 14:27:02] iter = 14750, loss = 1.4541
[2023-10-04 14:27:03] iter = 14760, loss = 1.5137
[2023-10-04 14:27:04] iter = 14770, loss = 1.4126
[2023-10-04 14:27:05] iter = 14780, loss = 1.5196
[2023-10-04 14:27:06] iter = 14790, loss = 1.5497
[2023-10-04 14:27:07] iter = 14800, loss = 1.4663
[2023-10-04 14:27:08] iter = 14810, loss = 1.5029
[2023-10-04 14:27:09] iter = 14820, loss = 1.5411
[2023-10-04 14:27:10] iter = 14830, loss = 1.4928
[2023-10-04 14:27:11] iter = 14840, loss = 1.4728
[2023-10-04 14:27:12] iter = 14850, loss = 1.4528
[2023-10-04 14:27:12] iter = 14860, loss = 1.4715
[2023-10-04 14:27:13] iter = 14870, loss = 1.4219
[2023-10-04 14:27:14] iter = 14880, loss = 1.4191
[2023-10-04 14:27:15] iter = 14890, loss = 1.4732
[2023-10-04 14:27:16] iter = 14900, loss = 1.4758
[2023-10-04 14:27:17] iter = 14910, loss = 1.5225
[2023-10-04 14:27:18] iter = 14920, loss = 1.3727
[2023-10-04 14:27:19] iter = 14930, loss = 1.5475
[2023-10-04 14:27:20] iter = 14940, loss = 1.4801
[2023-10-04 14:27:21] iter = 14950, loss = 1.6079
[2023-10-04 14:27:22] iter = 14960, loss = 1.4051
[2023-10-04 14:27:23] iter = 14970, loss = 1.6435
[2023-10-04 14:27:23] iter = 14980, loss = 1.5185
[2023-10-04 14:27:24] iter = 14990, loss = 1.4752
[2023-10-04 14:27:25] iter = 15000, loss = 1.4657
[2023-10-04 14:27:26] iter = 15010, loss = 1.4861
[2023-10-04 14:27:27] iter = 15020, loss = 1.3596
[2023-10-04 14:27:28] iter = 15030, loss = 1.5976
[2023-10-04 14:27:29] iter = 15040, loss = 1.3921
[2023-10-04 14:27:30] iter = 15050, loss = 1.5169
[2023-10-04 14:27:31] iter = 15060, loss = 1.6472
[2023-10-04 14:27:32] iter = 15070, loss = 1.3613
[2023-10-04 14:27:32] iter = 15080, loss = 1.5213
[2023-10-04 14:27:34] iter = 15090, loss = 1.5016
[2023-10-04 14:27:34] iter = 15100, loss = 1.6060
[2023-10-04 14:27:35] iter = 15110, loss = 1.5721
[2023-10-04 14:27:36] iter = 15120, loss = 1.4457
[2023-10-04 14:27:37] iter = 15130, loss = 1.5505
[2023-10-04 14:27:38] iter = 15140, loss = 1.6258
[2023-10-04 14:27:39] iter = 15150, loss = 1.5473
[2023-10-04 14:27:40] iter = 15160, loss = 1.5834
[2023-10-04 14:27:41] iter = 15170, loss = 1.4701
[2023-10-04 14:27:42] iter = 15180, loss = 1.5075
[2023-10-04 14:27:43] iter = 15190, loss = 1.4567
[2023-10-04 14:27:43] iter = 15200, loss = 1.4035
[2023-10-04 14:27:44] iter = 15210, loss = 1.5057
[2023-10-04 14:27:45] iter = 15220, loss = 1.5125
[2023-10-04 14:27:46] iter = 15230, loss = 1.4910
[2023-10-04 14:27:47] iter = 15240, loss = 1.4984
[2023-10-04 14:27:48] iter = 15250, loss = 1.5132
[2023-10-04 14:27:49] iter = 15260, loss = 1.4201
[2023-10-04 14:27:50] iter = 15270, loss = 1.5258
[2023-10-04 14:27:51] iter = 15280, loss = 1.4591
[2023-10-04 14:27:52] iter = 15290, loss = 1.6150
[2023-10-04 14:27:53] iter = 15300, loss = 1.5310
[2023-10-04 14:27:54] iter = 15310, loss = 1.5357
[2023-10-04 14:27:54] iter = 15320, loss = 1.6257
[2023-10-04 14:27:55] iter = 15330, loss = 1.5867
[2023-10-04 14:27:56] iter = 15340, loss = 1.4745
[2023-10-04 14:27:57] iter = 15350, loss = 1.5307
[2023-10-04 14:27:58] iter = 15360, loss = 1.5898
[2023-10-04 14:27:59] iter = 15370, loss = 1.4973
[2023-10-04 14:28:00] iter = 15380, loss = 1.5415
[2023-10-04 14:28:01] iter = 15390, loss = 1.4326
[2023-10-04 14:28:02] iter = 15400, loss = 1.4801
[2023-10-04 14:28:03] iter = 15410, loss = 1.6519
[2023-10-04 14:28:04] iter = 15420, loss = 1.5150
[2023-10-04 14:28:05] iter = 15430, loss = 1.4011
[2023-10-04 14:28:06] iter = 15440, loss = 1.4751
[2023-10-04 14:28:07] iter = 15450, loss = 1.4869
[2023-10-04 14:28:08] iter = 15460, loss = 1.4746
[2023-10-04 14:28:09] iter = 15470, loss = 1.4901
[2023-10-04 14:28:09] iter = 15480, loss = 1.4808
[2023-10-04 14:28:10] iter = 15490, loss = 1.5350
[2023-10-04 14:28:11] iter = 15500, loss = 1.5409
[2023-10-04 14:28:12] iter = 15510, loss = 1.5412
[2023-10-04 14:28:13] iter = 15520, loss = 1.7309
[2023-10-04 14:28:14] iter = 15530, loss = 1.7400
[2023-10-04 14:28:15] iter = 15540, loss = 1.6141
[2023-10-04 14:28:16] iter = 15550, loss = 1.4756
[2023-10-04 14:28:16] iter = 15560, loss = 1.3714
[2023-10-04 14:28:18] iter = 15570, loss = 1.4177
[2023-10-04 14:28:18] iter = 15580, loss = 1.5869
[2023-10-04 14:28:19] iter = 15590, loss = 1.4648
[2023-10-04 14:28:20] iter = 15600, loss = 1.4678
[2023-10-04 14:28:21] iter = 15610, loss = 1.5705
[2023-10-04 14:28:22] iter = 15620, loss = 1.4849
[2023-10-04 14:28:23] iter = 15630, loss = 1.5619
[2023-10-04 14:28:24] iter = 15640, loss = 1.6220
[2023-10-04 14:28:25] iter = 15650, loss = 1.5129
[2023-10-04 14:28:25] iter = 15660, loss = 1.4242
[2023-10-04 14:28:26] iter = 15670, loss = 1.4260
[2023-10-04 14:28:27] iter = 15680, loss = 1.3982
[2023-10-04 14:28:28] iter = 15690, loss = 1.4884
[2023-10-04 14:28:29] iter = 15700, loss = 1.3893
[2023-10-04 14:28:30] iter = 15710, loss = 1.6542
[2023-10-04 14:28:31] iter = 15720, loss = 1.5021
[2023-10-04 14:28:32] iter = 15730, loss = 1.4795
[2023-10-04 14:28:33] iter = 15740, loss = 1.5674
[2023-10-04 14:28:34] iter = 15750, loss = 1.6071
[2023-10-04 14:28:34] iter = 15760, loss = 1.5523
[2023-10-04 14:28:36] iter = 15770, loss = 1.5289
[2023-10-04 14:28:36] iter = 15780, loss = 1.3859
[2023-10-04 14:28:37] iter = 15790, loss = 1.5607
[2023-10-04 14:28:38] iter = 15800, loss = 1.5335
[2023-10-04 14:28:39] iter = 15810, loss = 1.6929
[2023-10-04 14:28:40] iter = 15820, loss = 1.4079
[2023-10-04 14:28:41] iter = 15830, loss = 1.6686
[2023-10-04 14:28:42] iter = 15840, loss = 1.7356
[2023-10-04 14:28:43] iter = 15850, loss = 1.3351
[2023-10-04 14:28:44] iter = 15860, loss = 1.4603
[2023-10-04 14:28:45] iter = 15870, loss = 1.5014
[2023-10-04 14:28:45] iter = 15880, loss = 1.5613
[2023-10-04 14:28:46] iter = 15890, loss = 1.5659
[2023-10-04 14:28:47] iter = 15900, loss = 1.4630
[2023-10-04 14:28:48] iter = 15910, loss = 1.4825
[2023-10-04 14:28:49] iter = 15920, loss = 1.4982
[2023-10-04 14:28:50] iter = 15930, loss = 1.4151
[2023-10-04 14:28:51] iter = 15940, loss = 1.4281
[2023-10-04 14:28:52] iter = 15950, loss = 1.4349
[2023-10-04 14:28:53] iter = 15960, loss = 1.4350
[2023-10-04 14:28:54] iter = 15970, loss = 1.5006
[2023-10-04 14:28:55] iter = 15980, loss = 1.4720
[2023-10-04 14:28:56] iter = 15990, loss = 1.5201
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 36825}
[2023-10-04 14:29:21] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.015952 train acc = 0.9960, test acc = 0.6268
[2023-10-04 14:29:45] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.016815 train acc = 1.0000, test acc = 0.6199
[2023-10-04 14:30:09] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.015461 train acc = 1.0000, test acc = 0.6167
[2023-10-04 14:30:34] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.013903 train acc = 1.0000, test acc = 0.6199
[2023-10-04 14:30:58] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.010407 train acc = 1.0000, test acc = 0.6286
[2023-10-04 14:31:22] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.008606 train acc = 1.0000, test acc = 0.6239
[2023-10-04 14:31:47] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.005292 train acc = 1.0000, test acc = 0.6240
[2023-10-04 14:32:11] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.007000 train acc = 1.0000, test acc = 0.6235
[2023-10-04 14:32:36] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.004119 train acc = 1.0000, test acc = 0.6244
[2023-10-04 14:33:00] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.012903 train acc = 1.0000, test acc = 0.6245
[2023-10-04 14:33:24] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.018788 train acc = 0.9980, test acc = 0.6199
[2023-10-04 14:33:49] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003637 train acc = 1.0000, test acc = 0.6263
[2023-10-04 14:34:13] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.013043 train acc = 1.0000, test acc = 0.6272
[2023-10-04 14:34:37] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.019767 train acc = 1.0000, test acc = 0.6270
[2023-10-04 14:35:02] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.015675 train acc = 1.0000, test acc = 0.6204
[2023-10-04 14:35:26] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.006115 train acc = 1.0000, test acc = 0.6274
[2023-10-04 14:35:50] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.016552 train acc = 1.0000, test acc = 0.6194
[2023-10-04 14:36:14] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.016547 train acc = 1.0000, test acc = 0.6297
[2023-10-04 14:36:39] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002446 train acc = 1.0000, test acc = 0.6237
[2023-10-04 14:37:03] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.005967 train acc = 1.0000, test acc = 0.6229
Evaluate 20 random ConvNet, mean = 0.6238 std = 0.0034
-------------------------
[2023-10-04 14:37:03] iter = 16000, loss = 1.3595
[2023-10-04 14:37:04] iter = 16010, loss = 1.6199
[2023-10-04 14:37:05] iter = 16020, loss = 1.4476
[2023-10-04 14:37:06] iter = 16030, loss = 1.5067
[2023-10-04 14:37:07] iter = 16040, loss = 1.4085
[2023-10-04 14:37:08] iter = 16050, loss = 1.5512
[2023-10-04 14:37:09] iter = 16060, loss = 1.4985
[2023-10-04 14:37:10] iter = 16070, loss = 1.5933
[2023-10-04 14:37:11] iter = 16080, loss = 1.6861
[2023-10-04 14:37:12] iter = 16090, loss = 1.3481
[2023-10-04 14:37:13] iter = 16100, loss = 1.3630
[2023-10-04 14:37:14] iter = 16110, loss = 1.4728
[2023-10-04 14:37:15] iter = 16120, loss = 1.4848
[2023-10-04 14:37:15] iter = 16130, loss = 1.4316
[2023-10-04 14:37:16] iter = 16140, loss = 1.4887
[2023-10-04 14:37:17] iter = 16150, loss = 1.3191
[2023-10-04 14:37:18] iter = 16160, loss = 1.5232
[2023-10-04 14:37:19] iter = 16170, loss = 1.3939
[2023-10-04 14:37:20] iter = 16180, loss = 1.5291
[2023-10-04 14:37:21] iter = 16190, loss = 1.6171
[2023-10-04 14:37:22] iter = 16200, loss = 1.4852
[2023-10-04 14:37:23] iter = 16210, loss = 1.4298
[2023-10-04 14:37:24] iter = 16220, loss = 1.3966
[2023-10-04 14:37:24] iter = 16230, loss = 1.4585
[2023-10-04 14:37:25] iter = 16240, loss = 1.4526
[2023-10-04 14:37:26] iter = 16250, loss = 1.6785
[2023-10-04 14:37:27] iter = 16260, loss = 1.3609
[2023-10-04 14:37:28] iter = 16270, loss = 1.6238
[2023-10-04 14:37:29] iter = 16280, loss = 1.4375
[2023-10-04 14:37:30] iter = 16290, loss = 1.5297
[2023-10-04 14:37:31] iter = 16300, loss = 1.4296
[2023-10-04 14:37:32] iter = 16310, loss = 1.4478
[2023-10-04 14:37:33] iter = 16320, loss = 1.5575
[2023-10-04 14:37:34] iter = 16330, loss = 1.5051
[2023-10-04 14:37:35] iter = 16340, loss = 1.5432
[2023-10-04 14:37:35] iter = 16350, loss = 1.6746
[2023-10-04 14:37:36] iter = 16360, loss = 1.4475
[2023-10-04 14:37:37] iter = 16370, loss = 1.4224
[2023-10-04 14:37:38] iter = 16380, loss = 1.4776
[2023-10-04 14:37:39] iter = 16390, loss = 1.4200
[2023-10-04 14:37:40] iter = 16400, loss = 1.4053
[2023-10-04 14:37:41] iter = 16410, loss = 1.4953
[2023-10-04 14:37:42] iter = 16420, loss = 1.4341
[2023-10-04 14:37:43] iter = 16430, loss = 1.6134
[2023-10-04 14:37:44] iter = 16440, loss = 1.4299
[2023-10-04 14:37:44] iter = 16450, loss = 1.4540
[2023-10-04 14:37:45] iter = 16460, loss = 1.6111
[2023-10-04 14:37:46] iter = 16470, loss = 1.6064
[2023-10-04 14:37:47] iter = 16480, loss = 1.4825
[2023-10-04 14:37:48] iter = 16490, loss = 1.4428
[2023-10-04 14:37:49] iter = 16500, loss = 1.6276
[2023-10-04 14:37:50] iter = 16510, loss = 1.5293
[2023-10-04 14:37:51] iter = 16520, loss = 1.5665
[2023-10-04 14:37:52] iter = 16530, loss = 1.4291
[2023-10-04 14:37:53] iter = 16540, loss = 1.3528
[2023-10-04 14:37:54] iter = 16550, loss = 1.4985
[2023-10-04 14:37:54] iter = 16560, loss = 1.3839
[2023-10-04 14:37:55] iter = 16570, loss = 1.4972
[2023-10-04 14:37:56] iter = 16580, loss = 1.4914
[2023-10-04 14:37:57] iter = 16590, loss = 1.4549
[2023-10-04 14:37:58] iter = 16600, loss = 1.5287
[2023-10-04 14:37:59] iter = 16610, loss = 1.4833
[2023-10-04 14:38:00] iter = 16620, loss = 1.4601
[2023-10-04 14:38:01] iter = 16630, loss = 1.4009
[2023-10-04 14:38:02] iter = 16640, loss = 1.5377
[2023-10-04 14:38:03] iter = 16650, loss = 1.3434
[2023-10-04 14:38:03] iter = 16660, loss = 1.4508
[2023-10-04 14:38:04] iter = 16670, loss = 1.4374
[2023-10-04 14:38:05] iter = 16680, loss = 1.4578
[2023-10-04 14:38:06] iter = 16690, loss = 1.5065
[2023-10-04 14:38:07] iter = 16700, loss = 1.5707
[2023-10-04 14:38:08] iter = 16710, loss = 1.4486
[2023-10-04 14:38:09] iter = 16720, loss = 1.3478
[2023-10-04 14:38:10] iter = 16730, loss = 1.4647
[2023-10-04 14:38:11] iter = 16740, loss = 1.7411
[2023-10-04 14:38:12] iter = 16750, loss = 1.4639
[2023-10-04 14:38:13] iter = 16760, loss = 1.2896
[2023-10-04 14:38:14] iter = 16770, loss = 1.4503
[2023-10-04 14:38:15] iter = 16780, loss = 1.5419
[2023-10-04 14:38:16] iter = 16790, loss = 1.4494
[2023-10-04 14:38:16] iter = 16800, loss = 1.5199
[2023-10-04 14:38:17] iter = 16810, loss = 1.4610
[2023-10-04 14:38:18] iter = 16820, loss = 1.4965
[2023-10-04 14:38:19] iter = 16830, loss = 1.5794
[2023-10-04 14:38:20] iter = 16840, loss = 1.4453
[2023-10-04 14:38:21] iter = 16850, loss = 1.5333
[2023-10-04 14:38:22] iter = 16860, loss = 1.4133
[2023-10-04 14:38:23] iter = 16870, loss = 1.3876
[2023-10-04 14:38:24] iter = 16880, loss = 1.6449
[2023-10-04 14:38:25] iter = 16890, loss = 1.4401
[2023-10-04 14:38:26] iter = 16900, loss = 1.5536
[2023-10-04 14:38:27] iter = 16910, loss = 1.4308
[2023-10-04 14:38:27] iter = 16920, loss = 1.4603
[2023-10-04 14:38:28] iter = 16930, loss = 1.3323
[2023-10-04 14:38:29] iter = 16940, loss = 1.5356
[2023-10-04 14:38:30] iter = 16950, loss = 1.5099
[2023-10-04 14:38:31] iter = 16960, loss = 1.6420
[2023-10-04 14:38:32] iter = 16970, loss = 1.5171
[2023-10-04 14:38:33] iter = 16980, loss = 1.3956
[2023-10-04 14:38:34] iter = 16990, loss = 1.5459
[2023-10-04 14:38:35] iter = 17000, loss = 1.4852
[2023-10-04 14:38:36] iter = 17010, loss = 1.4556
[2023-10-04 14:38:37] iter = 17020, loss = 1.4334
[2023-10-04 14:38:38] iter = 17030, loss = 1.4214
[2023-10-04 14:38:39] iter = 17040, loss = 1.4885
[2023-10-04 14:38:40] iter = 17050, loss = 1.3020
[2023-10-04 14:38:41] iter = 17060, loss = 1.4302
[2023-10-04 14:38:41] iter = 17070, loss = 1.4781
[2023-10-04 14:38:42] iter = 17080, loss = 1.4336
[2023-10-04 14:38:43] iter = 17090, loss = 1.4420
[2023-10-04 14:38:44] iter = 17100, loss = 1.4209
[2023-10-04 14:38:45] iter = 17110, loss = 1.4531
[2023-10-04 14:38:46] iter = 17120, loss = 1.6161
[2023-10-04 14:38:47] iter = 17130, loss = 1.4880
[2023-10-04 14:38:48] iter = 17140, loss = 1.4928
[2023-10-04 14:38:49] iter = 17150, loss = 1.6951
[2023-10-04 14:38:50] iter = 17160, loss = 1.4114
[2023-10-04 14:38:51] iter = 17170, loss = 1.6443
[2023-10-04 14:38:51] iter = 17180, loss = 1.5064
[2023-10-04 14:38:52] iter = 17190, loss = 1.6269
[2023-10-04 14:38:53] iter = 17200, loss = 1.4290
[2023-10-04 14:38:54] iter = 17210, loss = 1.6123
[2023-10-04 14:38:55] iter = 17220, loss = 1.5445
[2023-10-04 14:38:56] iter = 17230, loss = 1.5294
[2023-10-04 14:38:57] iter = 17240, loss = 1.6110
[2023-10-04 14:38:58] iter = 17250, loss = 1.5739
[2023-10-04 14:38:59] iter = 17260, loss = 1.5227
[2023-10-04 14:39:00] iter = 17270, loss = 1.4336
[2023-10-04 14:39:01] iter = 17280, loss = 1.5089
[2023-10-04 14:39:02] iter = 17290, loss = 1.5590
[2023-10-04 14:39:02] iter = 17300, loss = 1.3813
[2023-10-04 14:39:03] iter = 17310, loss = 1.4893
[2023-10-04 14:39:04] iter = 17320, loss = 1.5941
[2023-10-04 14:39:05] iter = 17330, loss = 1.5138
[2023-10-04 14:39:06] iter = 17340, loss = 1.6232
[2023-10-04 14:39:07] iter = 17350, loss = 1.4453
[2023-10-04 14:39:08] iter = 17360, loss = 1.7381
[2023-10-04 14:39:09] iter = 17370, loss = 1.4306
[2023-10-04 14:39:10] iter = 17380, loss = 1.5202
[2023-10-04 14:39:11] iter = 17390, loss = 1.3880
[2023-10-04 14:39:12] iter = 17400, loss = 1.5573
[2023-10-04 14:39:13] iter = 17410, loss = 1.5409
[2023-10-04 14:39:14] iter = 17420, loss = 1.4036
[2023-10-04 14:39:15] iter = 17430, loss = 1.4088
[2023-10-04 14:39:16] iter = 17440, loss = 1.4994
[2023-10-04 14:39:17] iter = 17450, loss = 1.4675
[2023-10-04 14:39:17] iter = 17460, loss = 1.4705
[2023-10-04 14:39:18] iter = 17470, loss = 1.5029
[2023-10-04 14:39:19] iter = 17480, loss = 1.5878
[2023-10-04 14:39:20] iter = 17490, loss = 1.4161
[2023-10-04 14:39:21] iter = 17500, loss = 1.5249
[2023-10-04 14:39:22] iter = 17510, loss = 1.4155
[2023-10-04 14:39:23] iter = 17520, loss = 1.5672
[2023-10-04 14:39:24] iter = 17530, loss = 1.5481
[2023-10-04 14:39:25] iter = 17540, loss = 1.4372
[2023-10-04 14:39:26] iter = 17550, loss = 1.4312
[2023-10-04 14:39:26] iter = 17560, loss = 1.5337
[2023-10-04 14:39:27] iter = 17570, loss = 1.3871
[2023-10-04 14:39:28] iter = 17580, loss = 1.5868
[2023-10-04 14:39:29] iter = 17590, loss = 1.4445
[2023-10-04 14:39:30] iter = 17600, loss = 1.4876
[2023-10-04 14:39:31] iter = 17610, loss = 1.5534
[2023-10-04 14:39:32] iter = 17620, loss = 1.5045
[2023-10-04 14:39:33] iter = 17630, loss = 1.3197
[2023-10-04 14:39:34] iter = 17640, loss = 1.4900
[2023-10-04 14:39:35] iter = 17650, loss = 1.4665
[2023-10-04 14:39:36] iter = 17660, loss = 1.5278
[2023-10-04 14:39:36] iter = 17670, loss = 1.5573
[2023-10-04 14:39:37] iter = 17680, loss = 1.5106
[2023-10-04 14:39:38] iter = 17690, loss = 1.4318
[2023-10-04 14:39:39] iter = 17700, loss = 1.8209
[2023-10-04 14:39:40] iter = 17710, loss = 1.5541
[2023-10-04 14:39:41] iter = 17720, loss = 1.5229
[2023-10-04 14:39:42] iter = 17730, loss = 1.5599
[2023-10-04 14:39:43] iter = 17740, loss = 1.3975
[2023-10-04 14:39:44] iter = 17750, loss = 1.5116
[2023-10-04 14:39:45] iter = 17760, loss = 1.3522
[2023-10-04 14:39:46] iter = 17770, loss = 1.5677
[2023-10-04 14:39:47] iter = 17780, loss = 1.5027
[2023-10-04 14:39:48] iter = 17790, loss = 1.5136
[2023-10-04 14:39:48] iter = 17800, loss = 1.3305
[2023-10-04 14:39:49] iter = 17810, loss = 1.4700
[2023-10-04 14:39:50] iter = 17820, loss = 1.3765
[2023-10-04 14:39:51] iter = 17830, loss = 1.3401
[2023-10-04 14:39:52] iter = 17840, loss = 1.4353
[2023-10-04 14:39:53] iter = 17850, loss = 1.5015
[2023-10-04 14:39:54] iter = 17860, loss = 1.4431
[2023-10-04 14:39:55] iter = 17870, loss = 1.4708
[2023-10-04 14:39:56] iter = 17880, loss = 1.6166
[2023-10-04 14:39:57] iter = 17890, loss = 1.5149
[2023-10-04 14:39:58] iter = 17900, loss = 1.4234
[2023-10-04 14:39:59] iter = 17910, loss = 1.4598
[2023-10-04 14:40:00] iter = 17920, loss = 1.4520
[2023-10-04 14:40:00] iter = 17930, loss = 1.5120
[2023-10-04 14:40:01] iter = 17940, loss = 1.4370
[2023-10-04 14:40:02] iter = 17950, loss = 1.5205
[2023-10-04 14:40:03] iter = 17960, loss = 1.5101
[2023-10-04 14:40:04] iter = 17970, loss = 1.4582
[2023-10-04 14:40:05] iter = 17980, loss = 1.4930
[2023-10-04 14:40:06] iter = 17990, loss = 1.4914
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 7292}
[2023-10-04 14:40:31] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.003511 train acc = 1.0000, test acc = 0.6246
[2023-10-04 14:40:56] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.007564 train acc = 1.0000, test acc = 0.6263
[2023-10-04 14:41:20] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.013796 train acc = 1.0000, test acc = 0.6298
[2023-10-04 14:41:44] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.022070 train acc = 0.9940, test acc = 0.6234
[2023-10-04 14:42:09] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004699 train acc = 1.0000, test acc = 0.6306
[2023-10-04 14:42:33] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.015783 train acc = 1.0000, test acc = 0.6192
[2023-10-04 14:42:58] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.015981 train acc = 1.0000, test acc = 0.6212
[2023-10-04 14:43:22] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.013387 train acc = 1.0000, test acc = 0.6241
[2023-10-04 14:43:46] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.028528 train acc = 1.0000, test acc = 0.6230
[2023-10-04 14:44:11] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002579 train acc = 1.0000, test acc = 0.6258
[2023-10-04 14:44:35] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.016927 train acc = 1.0000, test acc = 0.6159
[2023-10-04 14:45:00] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.020129 train acc = 0.9920, test acc = 0.6231
[2023-10-04 14:45:24] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011827 train acc = 1.0000, test acc = 0.6210
[2023-10-04 14:45:49] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.005862 train acc = 1.0000, test acc = 0.6266
[2023-10-04 14:46:13] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001832 train acc = 1.0000, test acc = 0.6213
[2023-10-04 14:46:38] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.019856 train acc = 0.9980, test acc = 0.6301
[2023-10-04 14:47:02] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.013749 train acc = 1.0000, test acc = 0.6268
[2023-10-04 14:47:26] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.014765 train acc = 1.0000, test acc = 0.6177
[2023-10-04 14:47:50] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013535 train acc = 1.0000, test acc = 0.6245
[2023-10-04 14:48:15] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.016050 train acc = 1.0000, test acc = 0.6139
Evaluate 20 random ConvNet, mean = 0.6234 std = 0.0044
-------------------------
[2023-10-04 14:48:15] iter = 18000, loss = 1.6161
[2023-10-04 14:48:16] iter = 18010, loss = 1.5574
[2023-10-04 14:48:17] iter = 18020, loss = 1.4718
[2023-10-04 14:48:18] iter = 18030, loss = 1.3938
[2023-10-04 14:48:19] iter = 18040, loss = 1.4034
[2023-10-04 14:48:19] iter = 18050, loss = 1.4310
[2023-10-04 14:48:20] iter = 18060, loss = 1.3559
[2023-10-04 14:48:21] iter = 18070, loss = 1.4868
[2023-10-04 14:48:22] iter = 18080, loss = 1.4966
[2023-10-04 14:48:23] iter = 18090, loss = 1.4854
[2023-10-04 14:48:24] iter = 18100, loss = 1.3113
[2023-10-04 14:48:25] iter = 18110, loss = 1.3644
[2023-10-04 14:48:26] iter = 18120, loss = 1.5740
[2023-10-04 14:48:27] iter = 18130, loss = 1.4350
[2023-10-04 14:48:28] iter = 18140, loss = 1.5629
[2023-10-04 14:48:29] iter = 18150, loss = 1.4427
[2023-10-04 14:48:30] iter = 18160, loss = 1.5264
[2023-10-04 14:48:30] iter = 18170, loss = 1.4869
[2023-10-04 14:48:31] iter = 18180, loss = 1.4056
[2023-10-04 14:48:32] iter = 18190, loss = 1.3659
[2023-10-04 14:48:33] iter = 18200, loss = 1.4504
[2023-10-04 14:48:34] iter = 18210, loss = 1.4511
[2023-10-04 14:48:35] iter = 18220, loss = 1.5421
[2023-10-04 14:48:36] iter = 18230, loss = 1.4946
[2023-10-04 14:48:37] iter = 18240, loss = 1.4323
[2023-10-04 14:48:38] iter = 18250, loss = 1.4926
[2023-10-04 14:48:39] iter = 18260, loss = 1.4363
[2023-10-04 14:48:39] iter = 18270, loss = 1.4981
[2023-10-04 14:48:40] iter = 18280, loss = 1.5290
[2023-10-04 14:48:41] iter = 18290, loss = 1.5430
[2023-10-04 14:48:42] iter = 18300, loss = 1.3955
[2023-10-04 14:48:43] iter = 18310, loss = 1.4402
[2023-10-04 14:48:44] iter = 18320, loss = 1.4910
[2023-10-04 14:48:45] iter = 18330, loss = 1.4408
[2023-10-04 14:48:46] iter = 18340, loss = 1.4851
[2023-10-04 14:48:47] iter = 18350, loss = 1.5382
[2023-10-04 14:48:47] iter = 18360, loss = 1.5056
[2023-10-04 14:48:48] iter = 18370, loss = 1.4375
[2023-10-04 14:48:49] iter = 18380, loss = 1.6143
[2023-10-04 14:48:50] iter = 18390, loss = 1.3537
[2023-10-04 14:48:51] iter = 18400, loss = 1.4768
[2023-10-04 14:48:52] iter = 18410, loss = 1.4570
[2023-10-04 14:48:53] iter = 18420, loss = 1.6932
[2023-10-04 14:48:54] iter = 18430, loss = 1.6086
[2023-10-04 14:48:55] iter = 18440, loss = 1.4998
[2023-10-04 14:48:55] iter = 18450, loss = 1.4962
[2023-10-04 14:48:56] iter = 18460, loss = 1.5306
[2023-10-04 14:48:57] iter = 18470, loss = 1.4258
[2023-10-04 14:48:58] iter = 18480, loss = 1.4789
[2023-10-04 14:48:59] iter = 18490, loss = 1.3939
[2023-10-04 14:49:00] iter = 18500, loss = 1.4828
[2023-10-04 14:49:01] iter = 18510, loss = 1.4037
[2023-10-04 14:49:02] iter = 18520, loss = 1.5960
[2023-10-04 14:49:03] iter = 18530, loss = 1.4480
[2023-10-04 14:49:04] iter = 18540, loss = 1.4243
[2023-10-04 14:49:05] iter = 18550, loss = 1.5352
[2023-10-04 14:49:05] iter = 18560, loss = 1.6313
[2023-10-04 14:49:06] iter = 18570, loss = 1.5288
[2023-10-04 14:49:07] iter = 18580, loss = 1.5721
[2023-10-04 14:49:08] iter = 18590, loss = 1.6291
[2023-10-04 14:49:09] iter = 18600, loss = 1.5614
[2023-10-04 14:49:10] iter = 18610, loss = 1.5455
[2023-10-04 14:49:11] iter = 18620, loss = 1.5088
[2023-10-04 14:49:12] iter = 18630, loss = 1.4200
[2023-10-04 14:49:13] iter = 18640, loss = 1.4635
[2023-10-04 14:49:14] iter = 18650, loss = 1.3827
[2023-10-04 14:49:15] iter = 18660, loss = 1.5854
[2023-10-04 14:49:16] iter = 18670, loss = 1.6927
[2023-10-04 14:49:17] iter = 18680, loss = 1.4427
[2023-10-04 14:49:17] iter = 18690, loss = 1.5055
[2023-10-04 14:49:18] iter = 18700, loss = 1.3812
[2023-10-04 14:49:19] iter = 18710, loss = 1.5972
[2023-10-04 14:49:20] iter = 18720, loss = 1.5537
[2023-10-04 14:49:21] iter = 18730, loss = 1.5106
[2023-10-04 14:49:22] iter = 18740, loss = 1.4300
[2023-10-04 14:49:23] iter = 18750, loss = 1.5234
[2023-10-04 14:49:24] iter = 18760, loss = 1.5926
[2023-10-04 14:49:25] iter = 18770, loss = 1.5299
[2023-10-04 14:49:26] iter = 18780, loss = 1.5274
[2023-10-04 14:49:26] iter = 18790, loss = 1.6053
[2023-10-04 14:49:27] iter = 18800, loss = 1.5352
[2023-10-04 14:49:28] iter = 18810, loss = 1.6257
[2023-10-04 14:49:29] iter = 18820, loss = 1.4827
[2023-10-04 14:49:30] iter = 18830, loss = 1.3988
[2023-10-04 14:49:31] iter = 18840, loss = 1.5235
[2023-10-04 14:49:32] iter = 18850, loss = 1.3565
[2023-10-04 14:49:33] iter = 18860, loss = 1.5090
[2023-10-04 14:49:34] iter = 18870, loss = 1.3330
[2023-10-04 14:49:35] iter = 18880, loss = 1.4968
[2023-10-04 14:49:36] iter = 18890, loss = 1.5310
[2023-10-04 14:49:37] iter = 18900, loss = 1.4740
[2023-10-04 14:49:38] iter = 18910, loss = 1.5027
[2023-10-04 14:49:38] iter = 18920, loss = 1.3485
[2023-10-04 14:49:39] iter = 18930, loss = 1.4635
[2023-10-04 14:49:40] iter = 18940, loss = 1.4199
[2023-10-04 14:49:41] iter = 18950, loss = 1.5007
[2023-10-04 14:49:42] iter = 18960, loss = 1.4974
[2023-10-04 14:49:43] iter = 18970, loss = 1.6065
[2023-10-04 14:49:44] iter = 18980, loss = 1.4216
[2023-10-04 14:49:45] iter = 18990, loss = 1.5223
[2023-10-04 14:49:46] iter = 19000, loss = 1.4425
[2023-10-04 14:49:47] iter = 19010, loss = 1.5368
[2023-10-04 14:49:48] iter = 19020, loss = 1.5546
[2023-10-04 14:49:49] iter = 19030, loss = 1.4785
[2023-10-04 14:49:49] iter = 19040, loss = 1.3412
[2023-10-04 14:49:51] iter = 19050, loss = 1.3861
[2023-10-04 14:49:51] iter = 19060, loss = 1.3657
[2023-10-04 14:49:52] iter = 19070, loss = 1.6726
[2023-10-04 14:49:53] iter = 19080, loss = 1.7498
[2023-10-04 14:49:54] iter = 19090, loss = 1.4885
[2023-10-04 14:49:55] iter = 19100, loss = 1.5037
[2023-10-04 14:49:56] iter = 19110, loss = 1.5671
[2023-10-04 14:49:57] iter = 19120, loss = 1.3816
[2023-10-04 14:49:58] iter = 19130, loss = 1.5376
[2023-10-04 14:49:59] iter = 19140, loss = 1.4001
[2023-10-04 14:50:00] iter = 19150, loss = 1.4969
[2023-10-04 14:50:00] iter = 19160, loss = 1.4802
[2023-10-04 14:50:01] iter = 19170, loss = 1.4330
[2023-10-04 14:50:02] iter = 19180, loss = 1.6812
[2023-10-04 14:50:03] iter = 19190, loss = 1.5124
[2023-10-04 14:50:04] iter = 19200, loss = 1.4614
[2023-10-04 14:50:05] iter = 19210, loss = 1.5791
[2023-10-04 14:50:06] iter = 19220, loss = 1.4548
[2023-10-04 14:50:07] iter = 19230, loss = 1.6913
[2023-10-04 14:50:08] iter = 19240, loss = 1.4215
[2023-10-04 14:50:09] iter = 19250, loss = 1.5269
[2023-10-04 14:50:10] iter = 19260, loss = 1.5188
[2023-10-04 14:50:11] iter = 19270, loss = 1.4265
[2023-10-04 14:50:11] iter = 19280, loss = 1.4694
[2023-10-04 14:50:12] iter = 19290, loss = 1.4418
[2023-10-04 14:50:13] iter = 19300, loss = 1.3421
[2023-10-04 14:50:14] iter = 19310, loss = 1.5715
[2023-10-04 14:50:15] iter = 19320, loss = 1.3169
[2023-10-04 14:50:16] iter = 19330, loss = 1.3439
[2023-10-04 14:50:17] iter = 19340, loss = 1.4797
[2023-10-04 14:50:18] iter = 19350, loss = 1.4909
[2023-10-04 14:50:19] iter = 19360, loss = 1.4447
[2023-10-04 14:50:20] iter = 19370, loss = 1.4654
[2023-10-04 14:50:20] iter = 19380, loss = 1.3831
[2023-10-04 14:50:21] iter = 19390, loss = 1.3855
[2023-10-04 14:50:22] iter = 19400, loss = 1.2961
[2023-10-04 14:50:23] iter = 19410, loss = 1.4148
[2023-10-04 14:50:24] iter = 19420, loss = 1.4890
[2023-10-04 14:50:25] iter = 19430, loss = 1.4737
[2023-10-04 14:50:26] iter = 19440, loss = 1.4744
[2023-10-04 14:50:27] iter = 19450, loss = 1.4428
[2023-10-04 14:50:28] iter = 19460, loss = 1.3695
[2023-10-04 14:50:29] iter = 19470, loss = 1.3856
[2023-10-04 14:50:30] iter = 19480, loss = 1.4867
[2023-10-04 14:50:31] iter = 19490, loss = 1.4821
[2023-10-04 14:50:32] iter = 19500, loss = 1.5586
[2023-10-04 14:50:32] iter = 19510, loss = 1.4490
[2023-10-04 14:50:33] iter = 19520, loss = 1.3655
[2023-10-04 14:50:34] iter = 19530, loss = 1.4661
[2023-10-04 14:50:35] iter = 19540, loss = 1.5063
[2023-10-04 14:50:36] iter = 19550, loss = 1.4318
[2023-10-04 14:50:37] iter = 19560, loss = 1.3949
[2023-10-04 14:50:38] iter = 19570, loss = 1.6047
[2023-10-04 14:50:39] iter = 19580, loss = 1.5411
[2023-10-04 14:50:40] iter = 19590, loss = 1.4389
[2023-10-04 14:50:41] iter = 19600, loss = 1.5071
[2023-10-04 14:50:42] iter = 19610, loss = 1.4772
[2023-10-04 14:50:43] iter = 19620, loss = 1.3654
[2023-10-04 14:50:44] iter = 19630, loss = 1.4802
[2023-10-04 14:50:44] iter = 19640, loss = 1.5489
[2023-10-04 14:50:45] iter = 19650, loss = 1.4917
[2023-10-04 14:50:46] iter = 19660, loss = 1.5336
[2023-10-04 14:50:47] iter = 19670, loss = 1.4941
[2023-10-04 14:50:48] iter = 19680, loss = 1.4704
[2023-10-04 14:50:49] iter = 19690, loss = 1.5609
[2023-10-04 14:50:50] iter = 19700, loss = 1.5369
[2023-10-04 14:50:51] iter = 19710, loss = 1.5605
[2023-10-04 14:50:52] iter = 19720, loss = 1.4951
[2023-10-04 14:50:53] iter = 19730, loss = 1.4189
[2023-10-04 14:50:53] iter = 19740, loss = 1.4959
[2023-10-04 14:50:54] iter = 19750, loss = 1.3455
[2023-10-04 14:50:55] iter = 19760, loss = 1.6126
[2023-10-04 14:50:56] iter = 19770, loss = 1.5188
[2023-10-04 14:50:57] iter = 19780, loss = 1.5192
[2023-10-04 14:50:58] iter = 19790, loss = 1.4014
[2023-10-04 14:50:59] iter = 19800, loss = 1.5953
[2023-10-04 14:51:00] iter = 19810, loss = 1.4926
[2023-10-04 14:51:01] iter = 19820, loss = 1.5566
[2023-10-04 14:51:01] iter = 19830, loss = 1.5467
[2023-10-04 14:51:02] iter = 19840, loss = 1.4458
[2023-10-04 14:51:03] iter = 19850, loss = 1.3562
[2023-10-04 14:51:04] iter = 19860, loss = 1.4051
[2023-10-04 14:51:05] iter = 19870, loss = 1.3293
[2023-10-04 14:51:06] iter = 19880, loss = 1.4790
[2023-10-04 14:51:07] iter = 19890, loss = 1.6061
[2023-10-04 14:51:08] iter = 19900, loss = 1.4984
[2023-10-04 14:51:09] iter = 19910, loss = 1.5985
[2023-10-04 14:51:10] iter = 19920, loss = 1.3926
[2023-10-04 14:51:10] iter = 19930, loss = 1.3523
[2023-10-04 14:51:11] iter = 19940, loss = 1.3877
[2023-10-04 14:51:12] iter = 19950, loss = 1.5117
[2023-10-04 14:51:13] iter = 19960, loss = 1.2758
[2023-10-04 14:51:14] iter = 19970, loss = 1.5049
[2023-10-04 14:51:15] iter = 19980, loss = 1.5175
[2023-10-04 14:51:16] iter = 19990, loss = 1.4970
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 77219}
[2023-10-04 14:51:41] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.010656 train acc = 1.0000, test acc = 0.6337
[2023-10-04 14:52:05] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003543 train acc = 1.0000, test acc = 0.6264
[2023-10-04 14:52:30] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.037978 train acc = 0.9920, test acc = 0.6274
[2023-10-04 14:52:54] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.015256 train acc = 1.0000, test acc = 0.6252
[2023-10-04 14:53:19] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.016784 train acc = 1.0000, test acc = 0.6293
[2023-10-04 14:53:43] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004620 train acc = 1.0000, test acc = 0.6261
[2023-10-04 14:54:07] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.011921 train acc = 1.0000, test acc = 0.6260
[2023-10-04 14:54:31] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.015554 train acc = 0.9980, test acc = 0.6287
[2023-10-04 14:54:56] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.006057 train acc = 1.0000, test acc = 0.6218
[2023-10-04 14:55:20] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002794 train acc = 1.0000, test acc = 0.6288
[2023-10-04 14:55:45] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013067 train acc = 1.0000, test acc = 0.6197
[2023-10-04 14:56:09] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002643 train acc = 1.0000, test acc = 0.6290
[2023-10-04 14:56:33] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.005162 train acc = 1.0000, test acc = 0.6281
[2023-10-04 14:56:58] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.011741 train acc = 1.0000, test acc = 0.6266
[2023-10-04 14:57:22] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.011145 train acc = 1.0000, test acc = 0.6158
[2023-10-04 14:57:46] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.017864 train acc = 1.0000, test acc = 0.6266
[2023-10-04 14:58:11] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004388 train acc = 1.0000, test acc = 0.6246
[2023-10-04 14:58:35] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.030780 train acc = 0.9980, test acc = 0.6275
[2023-10-04 14:59:00] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.015700 train acc = 0.9980, test acc = 0.6318
[2023-10-04 14:59:24] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.010652 train acc = 1.0000, test acc = 0.6272
Evaluate 20 random ConvNet, mean = 0.6265 std = 0.0039
-------------------------
[2023-10-04 14:59:24] iter = 20000, loss = 1.5709

================== Exp 1 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5173a57f40>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-04 14:59:42] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 64668}
[2023-10-04 15:00:06] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.009948 train acc = 1.0000, test acc = 0.4770
[2023-10-04 15:00:31] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.012804 train acc = 1.0000, test acc = 0.4845
[2023-10-04 15:00:55] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.000871 train acc = 1.0000, test acc = 0.4804
[2023-10-04 15:01:20] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.000738 train acc = 1.0000, test acc = 0.4834
[2023-10-04 15:01:44] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.012926 train acc = 1.0000, test acc = 0.4844
[2023-10-04 15:02:09] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013690 train acc = 1.0000, test acc = 0.4824
[2023-10-04 15:02:33] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002554 train acc = 1.0000, test acc = 0.4874
[2023-10-04 15:02:58] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004646 train acc = 1.0000, test acc = 0.4872
[2023-10-04 15:03:22] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.008634 train acc = 1.0000, test acc = 0.4749
[2023-10-04 15:03:46] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.011744 train acc = 1.0000, test acc = 0.4822
[2023-10-04 15:04:11] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.000850 train acc = 1.0000, test acc = 0.4816
[2023-10-04 15:04:35] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.005421 train acc = 1.0000, test acc = 0.4833
[2023-10-04 15:05:00] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003338 train acc = 1.0000, test acc = 0.4809
[2023-10-04 15:05:24] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002944 train acc = 1.0000, test acc = 0.4869
[2023-10-04 15:05:49] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.006691 train acc = 1.0000, test acc = 0.4814
[2023-10-04 15:06:13] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001088 train acc = 1.0000, test acc = 0.4906
[2023-10-04 15:06:38] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.010244 train acc = 1.0000, test acc = 0.4798
[2023-10-04 15:07:02] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.002905 train acc = 1.0000, test acc = 0.4872
[2023-10-04 15:07:27] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013732 train acc = 1.0000, test acc = 0.4809
[2023-10-04 15:07:51] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.002855 train acc = 1.0000, test acc = 0.4782
Evaluate 20 random ConvNet, mean = 0.4827 std = 0.0038
-------------------------
[2023-10-04 15:07:52] iter = 00000, loss = 7.9321
[2023-10-04 15:07:53] iter = 00010, loss = 6.2709
[2023-10-04 15:07:54] iter = 00020, loss = 5.4418
[2023-10-04 15:07:54] iter = 00030, loss = 4.8727
[2023-10-04 15:07:55] iter = 00040, loss = 4.7089
[2023-10-04 15:07:56] iter = 00050, loss = 3.9441
[2023-10-04 15:07:57] iter = 00060, loss = 4.3882
[2023-10-04 15:07:58] iter = 00070, loss = 3.9490
[2023-10-04 15:07:59] iter = 00080, loss = 3.7200
[2023-10-04 15:08:00] iter = 00090, loss = 3.3897
[2023-10-04 15:08:01] iter = 00100, loss = 3.9109
[2023-10-04 15:08:02] iter = 00110, loss = 3.2042
[2023-10-04 15:08:03] iter = 00120, loss = 3.4872
[2023-10-04 15:08:04] iter = 00130, loss = 3.0012
[2023-10-04 15:08:05] iter = 00140, loss = 3.3869
[2023-10-04 15:08:06] iter = 00150, loss = 2.9590
[2023-10-04 15:08:07] iter = 00160, loss = 3.2446
[2023-10-04 15:08:08] iter = 00170, loss = 3.1894
[2023-10-04 15:08:09] iter = 00180, loss = 2.9643
[2023-10-04 15:08:10] iter = 00190, loss = 2.9460
[2023-10-04 15:08:10] iter = 00200, loss = 3.1037
[2023-10-04 15:08:11] iter = 00210, loss = 2.9303
[2023-10-04 15:08:12] iter = 00220, loss = 3.0748
[2023-10-04 15:08:13] iter = 00230, loss = 2.7785
[2023-10-04 15:08:14] iter = 00240, loss = 2.7540
[2023-10-04 15:08:15] iter = 00250, loss = 2.7815
[2023-10-04 15:08:16] iter = 00260, loss = 2.8167
[2023-10-04 15:08:17] iter = 00270, loss = 2.7194
[2023-10-04 15:08:18] iter = 00280, loss = 2.6769
[2023-10-04 15:08:19] iter = 00290, loss = 2.7148
[2023-10-04 15:08:20] iter = 00300, loss = 2.7383
[2023-10-04 15:08:21] iter = 00310, loss = 2.6570
[2023-10-04 15:08:21] iter = 00320, loss = 2.3535
[2023-10-04 15:08:22] iter = 00330, loss = 2.5264
[2023-10-04 15:08:24] iter = 00340, loss = 2.3933
[2023-10-04 15:08:24] iter = 00350, loss = 2.5050
[2023-10-04 15:08:25] iter = 00360, loss = 2.2971
[2023-10-04 15:08:26] iter = 00370, loss = 2.2597
[2023-10-04 15:08:27] iter = 00380, loss = 2.3047
[2023-10-04 15:08:28] iter = 00390, loss = 2.2287
[2023-10-04 15:08:29] iter = 00400, loss = 2.6124
[2023-10-04 15:08:30] iter = 00410, loss = 2.4689
[2023-10-04 15:08:31] iter = 00420, loss = 2.3106
[2023-10-04 15:08:32] iter = 00430, loss = 2.2280
[2023-10-04 15:08:33] iter = 00440, loss = 2.3147
[2023-10-04 15:08:34] iter = 00450, loss = 2.4709
[2023-10-04 15:08:34] iter = 00460, loss = 2.4517
[2023-10-04 15:08:35] iter = 00470, loss = 2.3509
[2023-10-04 15:08:36] iter = 00480, loss = 2.3036
[2023-10-04 15:08:37] iter = 00490, loss = 2.3421
[2023-10-04 15:08:38] iter = 00500, loss = 2.2372
[2023-10-04 15:08:39] iter = 00510, loss = 2.3223
[2023-10-04 15:08:40] iter = 00520, loss = 2.3735
[2023-10-04 15:08:41] iter = 00530, loss = 2.1989
[2023-10-04 15:08:42] iter = 00540, loss = 2.2744
[2023-10-04 15:08:43] iter = 00550, loss = 2.3564
[2023-10-04 15:08:43] iter = 00560, loss = 2.1824
[2023-10-04 15:08:44] iter = 00570, loss = 2.1276
[2023-10-04 15:08:45] iter = 00580, loss = 2.2230
[2023-10-04 15:08:46] iter = 00590, loss = 2.2784
[2023-10-04 15:08:47] iter = 00600, loss = 2.1446
[2023-10-04 15:08:48] iter = 00610, loss = 2.3588
[2023-10-04 15:08:49] iter = 00620, loss = 2.2591
[2023-10-04 15:08:50] iter = 00630, loss = 1.9788
[2023-10-04 15:08:51] iter = 00640, loss = 2.2680
[2023-10-04 15:08:52] iter = 00650, loss = 2.0961
[2023-10-04 15:08:53] iter = 00660, loss = 2.3247
[2023-10-04 15:08:54] iter = 00670, loss = 2.2775
[2023-10-04 15:08:54] iter = 00680, loss = 2.4352
[2023-10-04 15:08:55] iter = 00690, loss = 2.3322
[2023-10-04 15:08:56] iter = 00700, loss = 2.0465
[2023-10-04 15:08:57] iter = 00710, loss = 2.1905
[2023-10-04 15:08:58] iter = 00720, loss = 2.1828
[2023-10-04 15:08:59] iter = 00730, loss = 1.9782
[2023-10-04 15:09:00] iter = 00740, loss = 2.0751
[2023-10-04 15:09:01] iter = 00750, loss = 2.1195
[2023-10-04 15:09:02] iter = 00760, loss = 2.2947
[2023-10-04 15:09:02] iter = 00770, loss = 2.2381
[2023-10-04 15:09:03] iter = 00780, loss = 2.1632
[2023-10-04 15:09:04] iter = 00790, loss = 2.1504
[2023-10-04 15:09:05] iter = 00800, loss = 2.2110
[2023-10-04 15:09:06] iter = 00810, loss = 2.2031
[2023-10-04 15:09:07] iter = 00820, loss = 2.1058
[2023-10-04 15:09:08] iter = 00830, loss = 2.0874
[2023-10-04 15:09:09] iter = 00840, loss = 2.0199
[2023-10-04 15:09:10] iter = 00850, loss = 2.2728
[2023-10-04 15:09:11] iter = 00860, loss = 2.0930
[2023-10-04 15:09:11] iter = 00870, loss = 2.1561
[2023-10-04 15:09:12] iter = 00880, loss = 2.1364
[2023-10-04 15:09:13] iter = 00890, loss = 1.9933
[2023-10-04 15:09:14] iter = 00900, loss = 2.0470
[2023-10-04 15:09:15] iter = 00910, loss = 1.9117
[2023-10-04 15:09:16] iter = 00920, loss = 2.1038
[2023-10-04 15:09:17] iter = 00930, loss = 2.0132
[2023-10-04 15:09:18] iter = 00940, loss = 2.0091
[2023-10-04 15:09:19] iter = 00950, loss = 1.9256
[2023-10-04 15:09:20] iter = 00960, loss = 2.1099
[2023-10-04 15:09:21] iter = 00970, loss = 2.0259
[2023-10-04 15:09:21] iter = 00980, loss = 2.0926
[2023-10-04 15:09:22] iter = 00990, loss = 2.0224
[2023-10-04 15:09:23] iter = 01000, loss = 2.2595
[2023-10-04 15:09:24] iter = 01010, loss = 1.9463
[2023-10-04 15:09:25] iter = 01020, loss = 2.0299
[2023-10-04 15:09:26] iter = 01030, loss = 2.0822
[2023-10-04 15:09:27] iter = 01040, loss = 2.1375
[2023-10-04 15:09:28] iter = 01050, loss = 1.9833
[2023-10-04 15:09:29] iter = 01060, loss = 1.9743
[2023-10-04 15:09:30] iter = 01070, loss = 2.1502
[2023-10-04 15:09:31] iter = 01080, loss = 2.1562
[2023-10-04 15:09:31] iter = 01090, loss = 2.0527
[2023-10-04 15:09:32] iter = 01100, loss = 1.9864
[2023-10-04 15:09:33] iter = 01110, loss = 2.1777
[2023-10-04 15:09:34] iter = 01120, loss = 2.0841
[2023-10-04 15:09:35] iter = 01130, loss = 1.9633
[2023-10-04 15:09:36] iter = 01140, loss = 1.8801
[2023-10-04 15:09:37] iter = 01150, loss = 2.0311
[2023-10-04 15:09:38] iter = 01160, loss = 2.0874
[2023-10-04 15:09:39] iter = 01170, loss = 2.0475
[2023-10-04 15:09:40] iter = 01180, loss = 1.8918
[2023-10-04 15:09:41] iter = 01190, loss = 1.9152
[2023-10-04 15:09:42] iter = 01200, loss = 2.0851
[2023-10-04 15:09:43] iter = 01210, loss = 2.1233
[2023-10-04 15:09:44] iter = 01220, loss = 2.1888
[2023-10-04 15:09:44] iter = 01230, loss = 1.9474
[2023-10-04 15:09:45] iter = 01240, loss = 1.8614
[2023-10-04 15:09:46] iter = 01250, loss = 1.8307
[2023-10-04 15:09:47] iter = 01260, loss = 1.9297
[2023-10-04 15:09:48] iter = 01270, loss = 2.0764
[2023-10-04 15:09:49] iter = 01280, loss = 2.0425
[2023-10-04 15:09:50] iter = 01290, loss = 2.1638
[2023-10-04 15:09:51] iter = 01300, loss = 2.0167
[2023-10-04 15:09:52] iter = 01310, loss = 1.9745
[2023-10-04 15:09:53] iter = 01320, loss = 2.1081
[2023-10-04 15:09:54] iter = 01330, loss = 1.9566
[2023-10-04 15:09:55] iter = 01340, loss = 1.9475
[2023-10-04 15:09:56] iter = 01350, loss = 1.9589
[2023-10-04 15:09:57] iter = 01360, loss = 1.9684
[2023-10-04 15:09:57] iter = 01370, loss = 1.9497
[2023-10-04 15:09:58] iter = 01380, loss = 1.9307
[2023-10-04 15:09:59] iter = 01390, loss = 1.9769
[2023-10-04 15:10:00] iter = 01400, loss = 1.9212
[2023-10-04 15:10:01] iter = 01410, loss = 1.9834
[2023-10-04 15:10:02] iter = 01420, loss = 1.9914
[2023-10-04 15:10:03] iter = 01430, loss = 2.0199
[2023-10-04 15:10:04] iter = 01440, loss = 1.8811
[2023-10-04 15:10:05] iter = 01450, loss = 2.0029
[2023-10-04 15:10:06] iter = 01460, loss = 2.2537
[2023-10-04 15:10:07] iter = 01470, loss = 1.9054
[2023-10-04 15:10:07] iter = 01480, loss = 2.0537
[2023-10-04 15:10:08] iter = 01490, loss = 1.8931
[2023-10-04 15:10:09] iter = 01500, loss = 1.9023
[2023-10-04 15:10:10] iter = 01510, loss = 1.8498
[2023-10-04 15:10:11] iter = 01520, loss = 1.8908
[2023-10-04 15:10:12] iter = 01530, loss = 1.9127
[2023-10-04 15:10:13] iter = 01540, loss = 1.8884
[2023-10-04 15:10:14] iter = 01550, loss = 1.9573
[2023-10-04 15:10:15] iter = 01560, loss = 1.8251
[2023-10-04 15:10:16] iter = 01570, loss = 1.8740
[2023-10-04 15:10:17] iter = 01580, loss = 1.9533
[2023-10-04 15:10:18] iter = 01590, loss = 2.1065
[2023-10-04 15:10:19] iter = 01600, loss = 1.7721
[2023-10-04 15:10:20] iter = 01610, loss = 2.0402
[2023-10-04 15:10:20] iter = 01620, loss = 1.9821
[2023-10-04 15:10:21] iter = 01630, loss = 1.8305
[2023-10-04 15:10:22] iter = 01640, loss = 1.8438
[2023-10-04 15:10:23] iter = 01650, loss = 1.9539
[2023-10-04 15:10:24] iter = 01660, loss = 1.7553
[2023-10-04 15:10:25] iter = 01670, loss = 1.9589
[2023-10-04 15:10:26] iter = 01680, loss = 1.8223
[2023-10-04 15:10:27] iter = 01690, loss = 1.8940
[2023-10-04 15:10:28] iter = 01700, loss = 1.9068
[2023-10-04 15:10:29] iter = 01710, loss = 1.9294
[2023-10-04 15:10:30] iter = 01720, loss = 1.9024
[2023-10-04 15:10:31] iter = 01730, loss = 1.9102
[2023-10-04 15:10:31] iter = 01740, loss = 1.8390
[2023-10-04 15:10:32] iter = 01750, loss = 1.7402
[2023-10-04 15:10:33] iter = 01760, loss = 2.0151
[2023-10-04 15:10:34] iter = 01770, loss = 1.8350
[2023-10-04 15:10:35] iter = 01780, loss = 1.9179
[2023-10-04 15:10:36] iter = 01790, loss = 1.9390
[2023-10-04 15:10:37] iter = 01800, loss = 1.8064
[2023-10-04 15:10:38] iter = 01810, loss = 2.0639
[2023-10-04 15:10:39] iter = 01820, loss = 1.9691
[2023-10-04 15:10:40] iter = 01830, loss = 1.7169
[2023-10-04 15:10:41] iter = 01840, loss = 1.8863
[2023-10-04 15:10:41] iter = 01850, loss = 1.9679
[2023-10-04 15:10:42] iter = 01860, loss = 1.8477
[2023-10-04 15:10:43] iter = 01870, loss = 1.8837
[2023-10-04 15:10:44] iter = 01880, loss = 1.8015
[2023-10-04 15:10:45] iter = 01890, loss = 1.8513
[2023-10-04 15:10:46] iter = 01900, loss = 1.8495
[2023-10-04 15:10:47] iter = 01910, loss = 1.8403
[2023-10-04 15:10:48] iter = 01920, loss = 1.8382
[2023-10-04 15:10:49] iter = 01930, loss = 1.7330
[2023-10-04 15:10:50] iter = 01940, loss = 1.8314
[2023-10-04 15:10:51] iter = 01950, loss = 1.7888
[2023-10-04 15:10:52] iter = 01960, loss = 1.8012
[2023-10-04 15:10:52] iter = 01970, loss = 2.0073
[2023-10-04 15:10:53] iter = 01980, loss = 1.8244
[2023-10-04 15:10:54] iter = 01990, loss = 1.9674
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 55699}
[2023-10-04 15:11:20] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.001192 train acc = 1.0000, test acc = 0.5817
[2023-10-04 15:11:44] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006861 train acc = 1.0000, test acc = 0.5898
[2023-10-04 15:12:08] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.012871 train acc = 1.0000, test acc = 0.5910
[2023-10-04 15:12:33] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001584 train acc = 1.0000, test acc = 0.5800
[2023-10-04 15:12:57] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.009648 train acc = 1.0000, test acc = 0.5832
[2023-10-04 15:13:22] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002690 train acc = 1.0000, test acc = 0.5842
[2023-10-04 15:13:46] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004071 train acc = 1.0000, test acc = 0.5831
[2023-10-04 15:14:10] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.006650 train acc = 1.0000, test acc = 0.5768
[2023-10-04 15:14:35] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.007269 train acc = 1.0000, test acc = 0.5904
[2023-10-04 15:14:59] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.015255 train acc = 0.9980, test acc = 0.5829
[2023-10-04 15:15:23] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002829 train acc = 1.0000, test acc = 0.5806
[2023-10-04 15:15:47] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003912 train acc = 1.0000, test acc = 0.5870
[2023-10-04 15:16:12] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004521 train acc = 1.0000, test acc = 0.5830
[2023-10-04 15:16:36] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.007117 train acc = 1.0000, test acc = 0.5793
[2023-10-04 15:17:01] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001410 train acc = 1.0000, test acc = 0.5724
[2023-10-04 15:17:25] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002728 train acc = 1.0000, test acc = 0.5778
[2023-10-04 15:17:50] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002082 train acc = 1.0000, test acc = 0.5816
[2023-10-04 15:18:14] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.013105 train acc = 1.0000, test acc = 0.5846
[2023-10-04 15:18:38] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004602 train acc = 1.0000, test acc = 0.5764
[2023-10-04 15:19:03] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.000974 train acc = 1.0000, test acc = 0.5890
Evaluate 20 random ConvNet, mean = 0.5827 std = 0.0049
-------------------------
[2023-10-04 15:19:03] iter = 02000, loss = 1.7967
[2023-10-04 15:19:04] iter = 02010, loss = 1.8922
[2023-10-04 15:19:05] iter = 02020, loss = 1.9635
[2023-10-04 15:19:06] iter = 02030, loss = 1.7373
[2023-10-04 15:19:07] iter = 02040, loss = 1.7558
[2023-10-04 15:19:08] iter = 02050, loss = 1.8236
[2023-10-04 15:19:09] iter = 02060, loss = 1.7633
[2023-10-04 15:19:09] iter = 02070, loss = 1.8126
[2023-10-04 15:19:10] iter = 02080, loss = 1.8306
[2023-10-04 15:19:11] iter = 02090, loss = 1.8227
[2023-10-04 15:19:12] iter = 02100, loss = 1.7678
[2023-10-04 15:19:13] iter = 02110, loss = 1.7463
[2023-10-04 15:19:14] iter = 02120, loss = 1.9192
[2023-10-04 15:19:15] iter = 02130, loss = 1.8626
[2023-10-04 15:19:16] iter = 02140, loss = 1.9460
[2023-10-04 15:19:17] iter = 02150, loss = 1.7662
[2023-10-04 15:19:18] iter = 02160, loss = 1.6951
[2023-10-04 15:19:19] iter = 02170, loss = 1.7372
[2023-10-04 15:19:19] iter = 02180, loss = 2.0101
[2023-10-04 15:19:20] iter = 02190, loss = 1.7355
[2023-10-04 15:19:21] iter = 02200, loss = 1.8751
[2023-10-04 15:19:22] iter = 02210, loss = 2.0194
[2023-10-04 15:19:23] iter = 02220, loss = 1.8049
[2023-10-04 15:19:24] iter = 02230, loss = 1.9818
[2023-10-04 15:19:25] iter = 02240, loss = 1.7700
[2023-10-04 15:19:26] iter = 02250, loss = 1.6977
[2023-10-04 15:19:27] iter = 02260, loss = 1.8471
[2023-10-04 15:19:28] iter = 02270, loss = 1.8132
[2023-10-04 15:19:29] iter = 02280, loss = 1.9090
[2023-10-04 15:19:29] iter = 02290, loss = 1.7219
[2023-10-04 15:19:30] iter = 02300, loss = 1.7330
[2023-10-04 15:19:31] iter = 02310, loss = 1.6505
[2023-10-04 15:19:32] iter = 02320, loss = 1.7964
[2023-10-04 15:19:33] iter = 02330, loss = 1.9023
[2023-10-04 15:19:34] iter = 02340, loss = 1.8620
[2023-10-04 15:19:35] iter = 02350, loss = 1.7692
[2023-10-04 15:19:36] iter = 02360, loss = 1.8431
[2023-10-04 15:19:37] iter = 02370, loss = 1.7995
[2023-10-04 15:19:38] iter = 02380, loss = 1.8958
[2023-10-04 15:19:39] iter = 02390, loss = 1.8249
[2023-10-04 15:19:39] iter = 02400, loss = 1.6997
[2023-10-04 15:19:40] iter = 02410, loss = 1.8072
[2023-10-04 15:19:41] iter = 02420, loss = 1.7644
[2023-10-04 15:19:42] iter = 02430, loss = 1.7401
[2023-10-04 15:19:43] iter = 02440, loss = 1.7989
[2023-10-04 15:19:44] iter = 02450, loss = 1.7585
[2023-10-04 15:19:45] iter = 02460, loss = 1.7450
[2023-10-04 15:19:46] iter = 02470, loss = 1.9582
[2023-10-04 15:19:47] iter = 02480, loss = 1.9240
[2023-10-04 15:19:48] iter = 02490, loss = 1.8374
[2023-10-04 15:19:49] iter = 02500, loss = 1.7068
[2023-10-04 15:19:50] iter = 02510, loss = 1.8900
[2023-10-04 15:19:51] iter = 02520, loss = 1.8776
[2023-10-04 15:19:52] iter = 02530, loss = 1.9158
[2023-10-04 15:19:52] iter = 02540, loss = 2.0301
[2023-10-04 15:19:53] iter = 02550, loss = 1.7130
[2023-10-04 15:19:54] iter = 02560, loss = 1.7225
[2023-10-04 15:19:55] iter = 02570, loss = 1.8455
[2023-10-04 15:19:56] iter = 02580, loss = 1.8048
[2023-10-04 15:19:57] iter = 02590, loss = 1.7592
[2023-10-04 15:19:58] iter = 02600, loss = 1.7272
[2023-10-04 15:19:59] iter = 02610, loss = 1.7458
[2023-10-04 15:20:00] iter = 02620, loss = 1.7550
[2023-10-04 15:20:01] iter = 02630, loss = 1.7683
[2023-10-04 15:20:02] iter = 02640, loss = 1.7725
[2023-10-04 15:20:03] iter = 02650, loss = 1.6703
[2023-10-04 15:20:04] iter = 02660, loss = 1.8645
[2023-10-04 15:20:04] iter = 02670, loss = 1.7844
[2023-10-04 15:20:05] iter = 02680, loss = 1.8232
[2023-10-04 15:20:06] iter = 02690, loss = 1.7734
[2023-10-04 15:20:07] iter = 02700, loss = 1.7893
[2023-10-04 15:20:08] iter = 02710, loss = 1.8307
[2023-10-04 15:20:09] iter = 02720, loss = 1.8797
[2023-10-04 15:20:10] iter = 02730, loss = 1.7173
[2023-10-04 15:20:11] iter = 02740, loss = 1.7744
[2023-10-04 15:20:12] iter = 02750, loss = 1.6986
[2023-10-04 15:20:13] iter = 02760, loss = 1.7798
[2023-10-04 15:20:14] iter = 02770, loss = 1.6704
[2023-10-04 15:20:15] iter = 02780, loss = 1.6925
[2023-10-04 15:20:16] iter = 02790, loss = 1.8122
[2023-10-04 15:20:16] iter = 02800, loss = 1.6272
[2023-10-04 15:20:17] iter = 02810, loss = 1.9001
[2023-10-04 15:20:18] iter = 02820, loss = 1.7819
[2023-10-04 15:20:19] iter = 02830, loss = 1.6749
[2023-10-04 15:20:20] iter = 02840, loss = 1.7981
[2023-10-04 15:20:21] iter = 02850, loss = 1.6450
[2023-10-04 15:20:22] iter = 02860, loss = 1.8928
[2023-10-04 15:20:23] iter = 02870, loss = 1.6454
[2023-10-04 15:20:24] iter = 02880, loss = 1.6180
[2023-10-04 15:20:25] iter = 02890, loss = 1.8287
[2023-10-04 15:20:26] iter = 02900, loss = 1.8339
[2023-10-04 15:20:26] iter = 02910, loss = 1.6698
[2023-10-04 15:20:27] iter = 02920, loss = 1.8573
[2023-10-04 15:20:28] iter = 02930, loss = 1.8768
[2023-10-04 15:20:29] iter = 02940, loss = 1.7550
[2023-10-04 15:20:30] iter = 02950, loss = 1.8660
[2023-10-04 15:20:31] iter = 02960, loss = 1.8873
[2023-10-04 15:20:32] iter = 02970, loss = 1.8204
[2023-10-04 15:20:33] iter = 02980, loss = 1.6517
[2023-10-04 15:20:34] iter = 02990, loss = 1.7915
[2023-10-04 15:20:35] iter = 03000, loss = 1.7368
[2023-10-04 15:20:36] iter = 03010, loss = 1.7182
[2023-10-04 15:20:37] iter = 03020, loss = 1.6893
[2023-10-04 15:20:38] iter = 03030, loss = 1.8225
[2023-10-04 15:20:39] iter = 03040, loss = 1.7786
[2023-10-04 15:20:40] iter = 03050, loss = 2.0213
[2023-10-04 15:20:40] iter = 03060, loss = 1.8026
[2023-10-04 15:20:41] iter = 03070, loss = 1.7857
[2023-10-04 15:20:42] iter = 03080, loss = 1.7753
[2023-10-04 15:20:43] iter = 03090, loss = 1.6119
[2023-10-04 15:20:44] iter = 03100, loss = 1.6965
[2023-10-04 15:20:45] iter = 03110, loss = 1.7546
[2023-10-04 15:20:46] iter = 03120, loss = 1.8946
[2023-10-04 15:20:47] iter = 03130, loss = 1.6969
[2023-10-04 15:20:48] iter = 03140, loss = 1.7792
[2023-10-04 15:20:49] iter = 03150, loss = 1.7307
[2023-10-04 15:20:50] iter = 03160, loss = 1.7883
[2023-10-04 15:20:50] iter = 03170, loss = 1.9061
[2023-10-04 15:20:51] iter = 03180, loss = 1.6157
[2023-10-04 15:20:52] iter = 03190, loss = 1.8214
[2023-10-04 15:20:53] iter = 03200, loss = 1.6386
[2023-10-04 15:20:54] iter = 03210, loss = 1.9665
[2023-10-04 15:20:55] iter = 03220, loss = 1.7588
[2023-10-04 15:20:56] iter = 03230, loss = 1.8495
[2023-10-04 15:20:57] iter = 03240, loss = 1.6814
[2023-10-04 15:20:58] iter = 03250, loss = 1.7122
[2023-10-04 15:20:59] iter = 03260, loss = 1.6880
[2023-10-04 15:20:59] iter = 03270, loss = 1.8903
[2023-10-04 15:21:00] iter = 03280, loss = 1.7051
[2023-10-04 15:21:01] iter = 03290, loss = 1.7199
[2023-10-04 15:21:02] iter = 03300, loss = 1.6922
[2023-10-04 15:21:03] iter = 03310, loss = 1.7488
[2023-10-04 15:21:04] iter = 03320, loss = 1.6299
[2023-10-04 15:21:05] iter = 03330, loss = 1.5758
[2023-10-04 15:21:06] iter = 03340, loss = 1.7801
[2023-10-04 15:21:06] iter = 03350, loss = 1.7680
[2023-10-04 15:21:08] iter = 03360, loss = 1.6589
[2023-10-04 15:21:08] iter = 03370, loss = 1.7384
[2023-10-04 15:21:09] iter = 03380, loss = 1.7152
[2023-10-04 15:21:10] iter = 03390, loss = 1.6895
[2023-10-04 15:21:11] iter = 03400, loss = 1.6832
[2023-10-04 15:21:12] iter = 03410, loss = 1.7789
[2023-10-04 15:21:13] iter = 03420, loss = 1.6661
[2023-10-04 15:21:14] iter = 03430, loss = 1.6329
[2023-10-04 15:21:15] iter = 03440, loss = 1.7839
[2023-10-04 15:21:16] iter = 03450, loss = 1.6032
[2023-10-04 15:21:17] iter = 03460, loss = 1.7472
[2023-10-04 15:21:18] iter = 03470, loss = 1.6294
[2023-10-04 15:21:19] iter = 03480, loss = 1.8227
[2023-10-04 15:21:19] iter = 03490, loss = 1.7586
[2023-10-04 15:21:20] iter = 03500, loss = 1.7140
[2023-10-04 15:21:21] iter = 03510, loss = 1.6962
[2023-10-04 15:21:22] iter = 03520, loss = 1.8357
[2023-10-04 15:21:23] iter = 03530, loss = 1.6617
[2023-10-04 15:21:24] iter = 03540, loss = 1.7338
[2023-10-04 15:21:25] iter = 03550, loss = 1.6548
[2023-10-04 15:21:26] iter = 03560, loss = 1.7425
[2023-10-04 15:21:27] iter = 03570, loss = 1.6423
[2023-10-04 15:21:28] iter = 03580, loss = 1.8065
[2023-10-04 15:21:29] iter = 03590, loss = 1.8029
[2023-10-04 15:21:30] iter = 03600, loss = 1.7754
[2023-10-04 15:21:31] iter = 03610, loss = 1.7147
[2023-10-04 15:21:31] iter = 03620, loss = 1.8205
[2023-10-04 15:21:32] iter = 03630, loss = 1.5905
[2023-10-04 15:21:33] iter = 03640, loss = 1.7329
[2023-10-04 15:21:34] iter = 03650, loss = 1.5760
[2023-10-04 15:21:35] iter = 03660, loss = 1.8119
[2023-10-04 15:21:36] iter = 03670, loss = 1.7834
[2023-10-04 15:21:37] iter = 03680, loss = 1.7938
[2023-10-04 15:21:38] iter = 03690, loss = 1.7043
[2023-10-04 15:21:39] iter = 03700, loss = 1.7418
[2023-10-04 15:21:39] iter = 03710, loss = 1.6096
[2023-10-04 15:21:40] iter = 03720, loss = 1.5483
[2023-10-04 15:21:41] iter = 03730, loss = 1.6423
[2023-10-04 15:21:42] iter = 03740, loss = 1.7912
[2023-10-04 15:21:43] iter = 03750, loss = 1.7067
[2023-10-04 15:21:44] iter = 03760, loss = 1.6828
[2023-10-04 15:21:45] iter = 03770, loss = 1.7777
[2023-10-04 15:21:46] iter = 03780, loss = 1.6066
[2023-10-04 15:21:47] iter = 03790, loss = 1.7466
[2023-10-04 15:21:48] iter = 03800, loss = 1.6963
[2023-10-04 15:21:49] iter = 03810, loss = 1.6581
[2023-10-04 15:21:50] iter = 03820, loss = 1.7436
[2023-10-04 15:21:51] iter = 03830, loss = 1.7102
[2023-10-04 15:21:52] iter = 03840, loss = 1.6379
[2023-10-04 15:21:53] iter = 03850, loss = 1.7342
[2023-10-04 15:21:54] iter = 03860, loss = 1.6691
[2023-10-04 15:21:54] iter = 03870, loss = 1.6665
[2023-10-04 15:21:55] iter = 03880, loss = 1.7067
[2023-10-04 15:21:56] iter = 03890, loss = 1.5648
[2023-10-04 15:21:57] iter = 03900, loss = 1.6882
[2023-10-04 15:21:58] iter = 03910, loss = 1.7331
[2023-10-04 15:21:59] iter = 03920, loss = 1.7386
[2023-10-04 15:22:00] iter = 03930, loss = 1.6818
[2023-10-04 15:22:01] iter = 03940, loss = 1.7547
[2023-10-04 15:22:02] iter = 03950, loss = 1.7137
[2023-10-04 15:22:03] iter = 03960, loss = 1.6886
[2023-10-04 15:22:04] iter = 03970, loss = 1.7506
[2023-10-04 15:22:05] iter = 03980, loss = 1.7786
[2023-10-04 15:22:05] iter = 03990, loss = 1.7737
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 26686}
[2023-10-04 15:22:31] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.003498 train acc = 1.0000, test acc = 0.6028
[2023-10-04 15:22:55] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003588 train acc = 1.0000, test acc = 0.5974
[2023-10-04 15:23:20] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.020076 train acc = 0.9980, test acc = 0.6018
[2023-10-04 15:23:44] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002225 train acc = 1.0000, test acc = 0.6030
[2023-10-04 15:24:09] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.005873 train acc = 1.0000, test acc = 0.6081
[2023-10-04 15:24:33] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001188 train acc = 1.0000, test acc = 0.6016
[2023-10-04 15:24:58] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016112 train acc = 1.0000, test acc = 0.6033
[2023-10-04 15:25:22] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.009922 train acc = 1.0000, test acc = 0.6098
[2023-10-04 15:25:46] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003142 train acc = 1.0000, test acc = 0.6122
[2023-10-04 15:26:11] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.001642 train acc = 1.0000, test acc = 0.5994
[2023-10-04 15:26:35] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.011930 train acc = 1.0000, test acc = 0.5912
[2023-10-04 15:27:00] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.011284 train acc = 1.0000, test acc = 0.6040
[2023-10-04 15:27:24] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.010919 train acc = 1.0000, test acc = 0.6088
[2023-10-04 15:27:48] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.010038 train acc = 1.0000, test acc = 0.6052
[2023-10-04 15:28:13] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003972 train acc = 1.0000, test acc = 0.6037
[2023-10-04 15:28:38] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.009735 train acc = 1.0000, test acc = 0.5990
[2023-10-04 15:29:02] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.001422 train acc = 1.0000, test acc = 0.6066
[2023-10-04 15:29:26] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.016059 train acc = 1.0000, test acc = 0.5973
[2023-10-04 15:29:51] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.006527 train acc = 1.0000, test acc = 0.6070
[2023-10-04 15:30:15] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.018731 train acc = 1.0000, test acc = 0.6022
Evaluate 20 random ConvNet, mean = 0.6032 std = 0.0048
-------------------------
[2023-10-04 15:30:15] iter = 04000, loss = 1.6213
[2023-10-04 15:30:16] iter = 04010, loss = 1.7248
[2023-10-04 15:30:17] iter = 04020, loss = 1.7440
[2023-10-04 15:30:18] iter = 04030, loss = 1.7034
[2023-10-04 15:30:19] iter = 04040, loss = 1.7701
[2023-10-04 15:30:20] iter = 04050, loss = 1.7459
[2023-10-04 15:30:21] iter = 04060, loss = 1.6261
[2023-10-04 15:30:22] iter = 04070, loss = 1.6423
[2023-10-04 15:30:23] iter = 04080, loss = 1.7766
[2023-10-04 15:30:24] iter = 04090, loss = 1.6865
[2023-10-04 15:30:25] iter = 04100, loss = 1.9523
[2023-10-04 15:30:25] iter = 04110, loss = 1.8217
[2023-10-04 15:30:26] iter = 04120, loss = 1.7847
[2023-10-04 15:30:27] iter = 04130, loss = 1.6562
[2023-10-04 15:30:28] iter = 04140, loss = 1.8527
[2023-10-04 15:30:29] iter = 04150, loss = 1.6132
[2023-10-04 15:30:30] iter = 04160, loss = 1.8373
[2023-10-04 15:30:31] iter = 04170, loss = 1.6927
[2023-10-04 15:30:32] iter = 04180, loss = 1.7874
[2023-10-04 15:30:33] iter = 04190, loss = 1.6424
[2023-10-04 15:30:34] iter = 04200, loss = 1.6913
[2023-10-04 15:30:35] iter = 04210, loss = 1.7370
[2023-10-04 15:30:35] iter = 04220, loss = 1.6129
[2023-10-04 15:30:36] iter = 04230, loss = 1.6756
[2023-10-04 15:30:37] iter = 04240, loss = 1.7483
[2023-10-04 15:30:38] iter = 04250, loss = 1.5185
[2023-10-04 15:30:39] iter = 04260, loss = 1.6949
[2023-10-04 15:30:40] iter = 04270, loss = 1.7411
[2023-10-04 15:30:41] iter = 04280, loss = 1.6869
[2023-10-04 15:30:42] iter = 04290, loss = 1.6827
[2023-10-04 15:30:43] iter = 04300, loss = 1.6205
[2023-10-04 15:30:44] iter = 04310, loss = 1.7181
[2023-10-04 15:30:45] iter = 04320, loss = 1.6056
[2023-10-04 15:30:46] iter = 04330, loss = 1.8471
[2023-10-04 15:30:47] iter = 04340, loss = 1.8033
[2023-10-04 15:30:47] iter = 04350, loss = 1.6042
[2023-10-04 15:30:48] iter = 04360, loss = 1.7157
[2023-10-04 15:30:49] iter = 04370, loss = 1.5476
[2023-10-04 15:30:50] iter = 04380, loss = 1.6215
[2023-10-04 15:30:51] iter = 04390, loss = 1.6806
[2023-10-04 15:30:52] iter = 04400, loss = 1.7275
[2023-10-04 15:30:53] iter = 04410, loss = 1.7807
[2023-10-04 15:30:54] iter = 04420, loss = 1.5787
[2023-10-04 15:30:55] iter = 04430, loss = 1.6107
[2023-10-04 15:30:56] iter = 04440, loss = 1.7625
[2023-10-04 15:30:57] iter = 04450, loss = 1.5915
[2023-10-04 15:30:57] iter = 04460, loss = 1.6783
[2023-10-04 15:30:58] iter = 04470, loss = 1.7708
[2023-10-04 15:30:59] iter = 04480, loss = 1.5502
[2023-10-04 15:31:00] iter = 04490, loss = 1.7209
[2023-10-04 15:31:01] iter = 04500, loss = 1.6604
[2023-10-04 15:31:02] iter = 04510, loss = 1.8512
[2023-10-04 15:31:03] iter = 04520, loss = 1.7032
[2023-10-04 15:31:04] iter = 04530, loss = 1.8871
[2023-10-04 15:31:04] iter = 04540, loss = 1.7155
[2023-10-04 15:31:05] iter = 04550, loss = 1.5798
[2023-10-04 15:31:06] iter = 04560, loss = 1.7382
[2023-10-04 15:31:07] iter = 04570, loss = 1.5347
[2023-10-04 15:31:08] iter = 04580, loss = 1.7088
[2023-10-04 15:31:09] iter = 04590, loss = 1.6775
[2023-10-04 15:31:10] iter = 04600, loss = 1.6410
[2023-10-04 15:31:11] iter = 04610, loss = 1.7049
[2023-10-04 15:31:12] iter = 04620, loss = 1.6753
[2023-10-04 15:31:13] iter = 04630, loss = 1.6441
[2023-10-04 15:31:14] iter = 04640, loss = 1.6933
[2023-10-04 15:31:15] iter = 04650, loss = 1.7389
[2023-10-04 15:31:15] iter = 04660, loss = 1.6458
[2023-10-04 15:31:16] iter = 04670, loss = 1.7601
[2023-10-04 15:31:17] iter = 04680, loss = 1.7142
[2023-10-04 15:31:18] iter = 04690, loss = 1.5979
[2023-10-04 15:31:19] iter = 04700, loss = 1.7401
[2023-10-04 15:31:20] iter = 04710, loss = 1.7737
[2023-10-04 15:31:21] iter = 04720, loss = 1.6652
[2023-10-04 15:31:22] iter = 04730, loss = 1.5405
[2023-10-04 15:31:23] iter = 04740, loss = 1.6581
[2023-10-04 15:31:24] iter = 04750, loss = 1.6050
[2023-10-04 15:31:25] iter = 04760, loss = 1.6018
[2023-10-04 15:31:26] iter = 04770, loss = 1.8273
[2023-10-04 15:31:26] iter = 04780, loss = 1.6505
[2023-10-04 15:31:27] iter = 04790, loss = 1.7167
[2023-10-04 15:31:28] iter = 04800, loss = 1.7780
[2023-10-04 15:31:29] iter = 04810, loss = 1.7297
[2023-10-04 15:31:30] iter = 04820, loss = 1.7904
[2023-10-04 15:31:31] iter = 04830, loss = 1.6156
[2023-10-04 15:31:32] iter = 04840, loss = 1.8548
[2023-10-04 15:31:33] iter = 04850, loss = 1.7338
[2023-10-04 15:31:34] iter = 04860, loss = 1.5804
[2023-10-04 15:31:35] iter = 04870, loss = 1.7514
[2023-10-04 15:31:36] iter = 04880, loss = 1.6481
[2023-10-04 15:31:37] iter = 04890, loss = 1.7191
[2023-10-04 15:31:38] iter = 04900, loss = 1.8014
[2023-10-04 15:31:39] iter = 04910, loss = 1.4877
[2023-10-04 15:31:39] iter = 04920, loss = 1.7300
[2023-10-04 15:31:40] iter = 04930, loss = 1.5190
[2023-10-04 15:31:41] iter = 04940, loss = 1.8066
[2023-10-04 15:31:42] iter = 04950, loss = 1.7301
[2023-10-04 15:31:43] iter = 04960, loss = 1.6945
[2023-10-04 15:31:44] iter = 04970, loss = 1.6156
[2023-10-04 15:31:45] iter = 04980, loss = 1.7463
[2023-10-04 15:31:46] iter = 04990, loss = 1.7087
[2023-10-04 15:31:47] iter = 05000, loss = 1.6092
[2023-10-04 15:31:48] iter = 05010, loss = 1.6658
[2023-10-04 15:31:49] iter = 05020, loss = 1.5930
[2023-10-04 15:31:49] iter = 05030, loss = 1.8442
[2023-10-04 15:31:50] iter = 05040, loss = 1.6279
[2023-10-04 15:31:51] iter = 05050, loss = 1.6348
[2023-10-04 15:31:52] iter = 05060, loss = 1.7820
[2023-10-04 15:31:53] iter = 05070, loss = 1.5527
[2023-10-04 15:31:54] iter = 05080, loss = 1.6217
[2023-10-04 15:31:55] iter = 05090, loss = 1.6143
[2023-10-04 15:31:56] iter = 05100, loss = 1.5855
[2023-10-04 15:31:57] iter = 05110, loss = 1.7226
[2023-10-04 15:31:58] iter = 05120, loss = 1.5322
[2023-10-04 15:31:59] iter = 05130, loss = 1.5664
[2023-10-04 15:32:00] iter = 05140, loss = 1.7539
[2023-10-04 15:32:01] iter = 05150, loss = 1.7446
[2023-10-04 15:32:01] iter = 05160, loss = 1.9358
[2023-10-04 15:32:02] iter = 05170, loss = 1.5731
[2023-10-04 15:32:03] iter = 05180, loss = 1.7052
[2023-10-04 15:32:04] iter = 05190, loss = 1.6243
[2023-10-04 15:32:05] iter = 05200, loss = 1.7358
[2023-10-04 15:32:06] iter = 05210, loss = 1.5577
[2023-10-04 15:32:07] iter = 05220, loss = 1.7072
[2023-10-04 15:32:08] iter = 05230, loss = 1.5993
[2023-10-04 15:32:09] iter = 05240, loss = 1.6489
[2023-10-04 15:32:09] iter = 05250, loss = 1.7017
[2023-10-04 15:32:10] iter = 05260, loss = 1.6407
[2023-10-04 15:32:11] iter = 05270, loss = 1.6701
[2023-10-04 15:32:12] iter = 05280, loss = 1.6272
[2023-10-04 15:32:13] iter = 05290, loss = 1.7563
[2023-10-04 15:32:14] iter = 05300, loss = 1.6568
[2023-10-04 15:32:15] iter = 05310, loss = 1.6508
[2023-10-04 15:32:16] iter = 05320, loss = 1.7686
[2023-10-04 15:32:17] iter = 05330, loss = 1.7147
[2023-10-04 15:32:18] iter = 05340, loss = 1.5990
[2023-10-04 15:32:19] iter = 05350, loss = 1.5835
[2023-10-04 15:32:20] iter = 05360, loss = 1.6120
[2023-10-04 15:32:21] iter = 05370, loss = 1.6845
[2023-10-04 15:32:22] iter = 05380, loss = 1.6329
[2023-10-04 15:32:22] iter = 05390, loss = 1.6938
[2023-10-04 15:32:23] iter = 05400, loss = 1.6432
[2023-10-04 15:32:24] iter = 05410, loss = 1.7343
[2023-10-04 15:32:25] iter = 05420, loss = 1.6266
[2023-10-04 15:32:26] iter = 05430, loss = 1.6438
[2023-10-04 15:32:27] iter = 05440, loss = 1.6920
[2023-10-04 15:32:28] iter = 05450, loss = 1.6156
[2023-10-04 15:32:29] iter = 05460, loss = 1.6408
[2023-10-04 15:32:29] iter = 05470, loss = 1.5963
[2023-10-04 15:32:30] iter = 05480, loss = 1.8024
[2023-10-04 15:32:31] iter = 05490, loss = 1.5514
[2023-10-04 15:32:32] iter = 05500, loss = 1.6198
[2023-10-04 15:32:33] iter = 05510, loss = 1.5632
[2023-10-04 15:32:34] iter = 05520, loss = 1.6754
[2023-10-04 15:32:35] iter = 05530, loss = 1.5672
[2023-10-04 15:32:36] iter = 05540, loss = 1.4096
[2023-10-04 15:32:37] iter = 05550, loss = 1.6565
[2023-10-04 15:32:37] iter = 05560, loss = 1.6509
[2023-10-04 15:32:38] iter = 05570, loss = 1.7329
[2023-10-04 15:32:39] iter = 05580, loss = 1.6313
[2023-10-04 15:32:40] iter = 05590, loss = 1.5330
[2023-10-04 15:32:41] iter = 05600, loss = 1.6553
[2023-10-04 15:32:42] iter = 05610, loss = 1.6197
[2023-10-04 15:32:43] iter = 05620, loss = 1.6523
[2023-10-04 15:32:44] iter = 05630, loss = 1.6174
[2023-10-04 15:32:45] iter = 05640, loss = 1.6782
[2023-10-04 15:32:46] iter = 05650, loss = 1.5462
[2023-10-04 15:32:46] iter = 05660, loss = 1.7151
[2023-10-04 15:32:47] iter = 05670, loss = 1.5881
[2023-10-04 15:32:48] iter = 05680, loss = 1.6427
[2023-10-04 15:32:49] iter = 05690, loss = 1.6479
[2023-10-04 15:32:50] iter = 05700, loss = 1.6915
[2023-10-04 15:32:51] iter = 05710, loss = 1.6108
[2023-10-04 15:32:52] iter = 05720, loss = 1.4524
[2023-10-04 15:32:53] iter = 05730, loss = 1.7304
[2023-10-04 15:32:54] iter = 05740, loss = 1.8310
[2023-10-04 15:32:55] iter = 05750, loss = 1.6300
[2023-10-04 15:32:56] iter = 05760, loss = 1.7113
[2023-10-04 15:32:57] iter = 05770, loss = 1.6701
[2023-10-04 15:32:58] iter = 05780, loss = 1.5096
[2023-10-04 15:32:58] iter = 05790, loss = 1.7790
[2023-10-04 15:33:00] iter = 05800, loss = 1.6272
[2023-10-04 15:33:00] iter = 05810, loss = 1.4855
[2023-10-04 15:33:01] iter = 05820, loss = 1.5923
[2023-10-04 15:33:02] iter = 05830, loss = 1.6666
[2023-10-04 15:33:03] iter = 05840, loss = 1.5374
[2023-10-04 15:33:04] iter = 05850, loss = 1.6783
[2023-10-04 15:33:05] iter = 05860, loss = 1.6812
[2023-10-04 15:33:06] iter = 05870, loss = 1.5876
[2023-10-04 15:33:07] iter = 05880, loss = 1.5369
[2023-10-04 15:33:08] iter = 05890, loss = 1.5799
[2023-10-04 15:33:08] iter = 05900, loss = 1.5903
[2023-10-04 15:33:09] iter = 05910, loss = 1.8201
[2023-10-04 15:33:10] iter = 05920, loss = 1.5939
[2023-10-04 15:33:11] iter = 05930, loss = 1.5630
[2023-10-04 15:33:12] iter = 05940, loss = 1.6119
[2023-10-04 15:33:13] iter = 05950, loss = 1.6922
[2023-10-04 15:33:14] iter = 05960, loss = 1.6016
[2023-10-04 15:33:15] iter = 05970, loss = 1.5887
[2023-10-04 15:33:16] iter = 05980, loss = 1.6091
[2023-10-04 15:33:17] iter = 05990, loss = 1.5980
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 97940}
[2023-10-04 15:33:42] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005698 train acc = 1.0000, test acc = 0.6125
[2023-10-04 15:34:06] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.009860 train acc = 1.0000, test acc = 0.6087
[2023-10-04 15:34:31] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010743 train acc = 1.0000, test acc = 0.6014
[2023-10-04 15:34:55] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003527 train acc = 1.0000, test acc = 0.6097
[2023-10-04 15:35:20] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.010401 train acc = 1.0000, test acc = 0.6093
[2023-10-04 15:35:44] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004040 train acc = 1.0000, test acc = 0.6075
[2023-10-04 15:36:08] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.024056 train acc = 0.9980, test acc = 0.6114
[2023-10-04 15:36:33] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001743 train acc = 1.0000, test acc = 0.6198
[2023-10-04 15:36:57] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.019198 train acc = 0.9980, test acc = 0.6128
[2023-10-04 15:37:21] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005326 train acc = 1.0000, test acc = 0.6140
[2023-10-04 15:37:46] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.001654 train acc = 1.0000, test acc = 0.6095
[2023-10-04 15:38:10] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.014008 train acc = 0.9980, test acc = 0.6094
[2023-10-04 15:38:35] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.020090 train acc = 1.0000, test acc = 0.6127
[2023-10-04 15:38:59] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.013534 train acc = 1.0000, test acc = 0.6107
[2023-10-04 15:39:23] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002895 train acc = 1.0000, test acc = 0.6055
[2023-10-04 15:39:48] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.010950 train acc = 1.0000, test acc = 0.6100
[2023-10-04 15:40:13] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.008538 train acc = 1.0000, test acc = 0.6066
[2023-10-04 15:40:37] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.007606 train acc = 1.0000, test acc = 0.6071
[2023-10-04 15:41:01] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.014649 train acc = 1.0000, test acc = 0.6124
[2023-10-04 15:41:26] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004049 train acc = 1.0000, test acc = 0.6143
Evaluate 20 random ConvNet, mean = 0.6103 std = 0.0038
-------------------------
[2023-10-04 15:41:26] iter = 06000, loss = 1.6196
[2023-10-04 15:41:27] iter = 06010, loss = 1.6646
[2023-10-04 15:41:28] iter = 06020, loss = 1.6792
[2023-10-04 15:41:29] iter = 06030, loss = 1.7543
[2023-10-04 15:41:30] iter = 06040, loss = 1.5969
[2023-10-04 15:41:31] iter = 06050, loss = 1.5609
[2023-10-04 15:41:32] iter = 06060, loss = 1.6571
[2023-10-04 15:41:32] iter = 06070, loss = 1.6442
[2023-10-04 15:41:33] iter = 06080, loss = 1.6227
[2023-10-04 15:41:34] iter = 06090, loss = 1.6439
[2023-10-04 15:41:35] iter = 06100, loss = 1.6526
[2023-10-04 15:41:36] iter = 06110, loss = 1.7353
[2023-10-04 15:41:37] iter = 06120, loss = 1.6040
[2023-10-04 15:41:38] iter = 06130, loss = 1.6200
[2023-10-04 15:41:39] iter = 06140, loss = 1.6701
[2023-10-04 15:41:40] iter = 06150, loss = 1.5713
[2023-10-04 15:41:41] iter = 06160, loss = 1.5324
[2023-10-04 15:41:42] iter = 06170, loss = 1.6609
[2023-10-04 15:41:43] iter = 06180, loss = 1.7125
[2023-10-04 15:41:43] iter = 06190, loss = 1.5786
[2023-10-04 15:41:44] iter = 06200, loss = 1.6008
[2023-10-04 15:41:45] iter = 06210, loss = 1.6603
[2023-10-04 15:41:46] iter = 06220, loss = 1.5959
[2023-10-04 15:41:47] iter = 06230, loss = 1.7640
[2023-10-04 15:41:48] iter = 06240, loss = 1.5528
[2023-10-04 15:41:49] iter = 06250, loss = 1.7143
[2023-10-04 15:41:50] iter = 06260, loss = 1.5622
[2023-10-04 15:41:51] iter = 06270, loss = 1.8475
[2023-10-04 15:41:52] iter = 06280, loss = 1.6625
[2023-10-04 15:41:53] iter = 06290, loss = 1.6595
[2023-10-04 15:41:54] iter = 06300, loss = 1.6331
[2023-10-04 15:41:55] iter = 06310, loss = 1.5460
[2023-10-04 15:41:56] iter = 06320, loss = 1.6049
[2023-10-04 15:41:57] iter = 06330, loss = 1.5300
[2023-10-04 15:41:57] iter = 06340, loss = 1.6289
[2023-10-04 15:41:58] iter = 06350, loss = 1.5480
[2023-10-04 15:41:59] iter = 06360, loss = 1.4752
[2023-10-04 15:42:00] iter = 06370, loss = 1.6919
[2023-10-04 15:42:01] iter = 06380, loss = 1.7372
[2023-10-04 15:42:02] iter = 06390, loss = 1.6089
[2023-10-04 15:42:03] iter = 06400, loss = 1.5558
[2023-10-04 15:42:04] iter = 06410, loss = 1.6249
[2023-10-04 15:42:05] iter = 06420, loss = 1.7131
[2023-10-04 15:42:06] iter = 06430, loss = 1.5848
[2023-10-04 15:42:07] iter = 06440, loss = 1.6127
[2023-10-04 15:42:07] iter = 06450, loss = 1.6496
[2023-10-04 15:42:08] iter = 06460, loss = 1.6553
[2023-10-04 15:42:09] iter = 06470, loss = 1.4255
[2023-10-04 15:42:10] iter = 06480, loss = 1.7408
[2023-10-04 15:42:11] iter = 06490, loss = 1.6086
[2023-10-04 15:42:12] iter = 06500, loss = 1.5873
[2023-10-04 15:42:13] iter = 06510, loss = 1.6151
[2023-10-04 15:42:14] iter = 06520, loss = 1.5533
[2023-10-04 15:42:15] iter = 06530, loss = 1.5292
[2023-10-04 15:42:16] iter = 06540, loss = 1.6793
[2023-10-04 15:42:17] iter = 06550, loss = 1.5976
[2023-10-04 15:42:18] iter = 06560, loss = 1.4921
[2023-10-04 15:42:19] iter = 06570, loss = 1.6202
[2023-10-04 15:42:20] iter = 06580, loss = 1.7530
[2023-10-04 15:42:20] iter = 06590, loss = 1.6761
[2023-10-04 15:42:21] iter = 06600, loss = 1.5849
[2023-10-04 15:42:22] iter = 06610, loss = 1.6364
[2023-10-04 15:42:23] iter = 06620, loss = 1.7772
[2023-10-04 15:42:24] iter = 06630, loss = 1.7458
[2023-10-04 15:42:25] iter = 06640, loss = 1.6616
[2023-10-04 15:42:26] iter = 06650, loss = 1.8088
[2023-10-04 15:42:27] iter = 06660, loss = 1.6721
[2023-10-04 15:42:28] iter = 06670, loss = 1.5535
[2023-10-04 15:42:29] iter = 06680, loss = 1.5963
[2023-10-04 15:42:29] iter = 06690, loss = 1.7094
[2023-10-04 15:42:30] iter = 06700, loss = 1.4832
[2023-10-04 15:42:31] iter = 06710, loss = 1.4337
[2023-10-04 15:42:32] iter = 06720, loss = 1.6575
[2023-10-04 15:42:33] iter = 06730, loss = 1.5994
[2023-10-04 15:42:34] iter = 06740, loss = 1.7619
[2023-10-04 15:42:35] iter = 06750, loss = 1.6525
[2023-10-04 15:42:36] iter = 06760, loss = 1.5796
[2023-10-04 15:42:37] iter = 06770, loss = 1.6689
[2023-10-04 15:42:38] iter = 06780, loss = 1.6436
[2023-10-04 15:42:38] iter = 06790, loss = 1.4923
[2023-10-04 15:42:39] iter = 06800, loss = 1.6716
[2023-10-04 15:42:40] iter = 06810, loss = 1.6176
[2023-10-04 15:42:41] iter = 06820, loss = 1.4642
[2023-10-04 15:42:42] iter = 06830, loss = 1.7171
[2023-10-04 15:42:43] iter = 06840, loss = 1.5271
[2023-10-04 15:42:44] iter = 06850, loss = 1.5338
[2023-10-04 15:42:45] iter = 06860, loss = 1.6242
[2023-10-04 15:42:46] iter = 06870, loss = 1.5119
[2023-10-04 15:42:47] iter = 06880, loss = 1.5914
[2023-10-04 15:42:48] iter = 06890, loss = 1.5365
[2023-10-04 15:42:49] iter = 06900, loss = 1.5562
[2023-10-04 15:42:50] iter = 06910, loss = 1.6429
[2023-10-04 15:42:50] iter = 06920, loss = 1.4968
[2023-10-04 15:42:51] iter = 06930, loss = 1.6317
[2023-10-04 15:42:52] iter = 06940, loss = 1.7222
[2023-10-04 15:42:53] iter = 06950, loss = 1.7173
[2023-10-04 15:42:54] iter = 06960, loss = 1.5907
[2023-10-04 15:42:55] iter = 06970, loss = 1.5991
[2023-10-04 15:42:56] iter = 06980, loss = 1.6434
[2023-10-04 15:42:57] iter = 06990, loss = 1.5883
[2023-10-04 15:42:58] iter = 07000, loss = 1.6810
[2023-10-04 15:42:59] iter = 07010, loss = 1.5642
[2023-10-04 15:43:00] iter = 07020, loss = 1.5991
[2023-10-04 15:43:01] iter = 07030, loss = 1.6633
[2023-10-04 15:43:02] iter = 07040, loss = 1.6654
[2023-10-04 15:43:02] iter = 07050, loss = 1.5144
[2023-10-04 15:43:03] iter = 07060, loss = 1.6808
[2023-10-04 15:43:04] iter = 07070, loss = 1.7192
[2023-10-04 15:43:05] iter = 07080, loss = 1.8991
[2023-10-04 15:43:06] iter = 07090, loss = 1.7372
[2023-10-04 15:43:07] iter = 07100, loss = 1.5384
[2023-10-04 15:43:08] iter = 07110, loss = 1.6636
[2023-10-04 15:43:09] iter = 07120, loss = 1.5898
[2023-10-04 15:43:10] iter = 07130, loss = 1.5830
[2023-10-04 15:43:11] iter = 07140, loss = 1.6155
[2023-10-04 15:43:11] iter = 07150, loss = 1.7799
[2023-10-04 15:43:12] iter = 07160, loss = 1.5593
[2023-10-04 15:43:13] iter = 07170, loss = 1.5868
[2023-10-04 15:43:14] iter = 07180, loss = 1.5217
[2023-10-04 15:43:15] iter = 07190, loss = 1.5583
[2023-10-04 15:43:16] iter = 07200, loss = 1.5797
[2023-10-04 15:43:17] iter = 07210, loss = 1.7413
[2023-10-04 15:43:18] iter = 07220, loss = 1.7732
[2023-10-04 15:43:19] iter = 07230, loss = 1.5432
[2023-10-04 15:43:20] iter = 07240, loss = 1.6917
[2023-10-04 15:43:21] iter = 07250, loss = 1.5332
[2023-10-04 15:43:22] iter = 07260, loss = 1.5985
[2023-10-04 15:43:23] iter = 07270, loss = 1.5698
[2023-10-04 15:43:23] iter = 07280, loss = 1.5249
[2023-10-04 15:43:24] iter = 07290, loss = 1.5988
[2023-10-04 15:43:25] iter = 07300, loss = 1.7017
[2023-10-04 15:43:26] iter = 07310, loss = 1.6483
[2023-10-04 15:43:27] iter = 07320, loss = 1.5618
[2023-10-04 15:43:28] iter = 07330, loss = 1.5235
[2023-10-04 15:43:29] iter = 07340, loss = 1.6038
[2023-10-04 15:43:30] iter = 07350, loss = 1.6310
[2023-10-04 15:43:31] iter = 07360, loss = 1.6053
[2023-10-04 15:43:32] iter = 07370, loss = 1.4048
[2023-10-04 15:43:33] iter = 07380, loss = 1.6413
[2023-10-04 15:43:34] iter = 07390, loss = 1.5538
[2023-10-04 15:43:35] iter = 07400, loss = 1.5117
[2023-10-04 15:43:36] iter = 07410, loss = 1.5673
[2023-10-04 15:43:36] iter = 07420, loss = 1.5040
[2023-10-04 15:43:37] iter = 07430, loss = 1.4907
[2023-10-04 15:43:38] iter = 07440, loss = 1.5420
[2023-10-04 15:43:39] iter = 07450, loss = 1.7434
[2023-10-04 15:43:40] iter = 07460, loss = 1.6649
[2023-10-04 15:43:41] iter = 07470, loss = 1.6513
[2023-10-04 15:43:42] iter = 07480, loss = 1.6849
[2023-10-04 15:43:43] iter = 07490, loss = 1.6412
[2023-10-04 15:43:44] iter = 07500, loss = 1.5586
[2023-10-04 15:43:45] iter = 07510, loss = 1.6772
[2023-10-04 15:43:45] iter = 07520, loss = 1.5636
[2023-10-04 15:43:46] iter = 07530, loss = 1.6714
[2023-10-04 15:43:47] iter = 07540, loss = 1.5252
[2023-10-04 15:43:48] iter = 07550, loss = 1.7125
[2023-10-04 15:43:49] iter = 07560, loss = 1.5268
[2023-10-04 15:43:50] iter = 07570, loss = 1.6571
[2023-10-04 15:43:51] iter = 07580, loss = 1.6273
[2023-10-04 15:43:52] iter = 07590, loss = 1.4209
[2023-10-04 15:43:53] iter = 07600, loss = 1.4753
[2023-10-04 15:43:54] iter = 07610, loss = 1.6824
[2023-10-04 15:43:55] iter = 07620, loss = 1.5857
[2023-10-04 15:43:56] iter = 07630, loss = 1.5246
[2023-10-04 15:43:56] iter = 07640, loss = 1.6209
[2023-10-04 15:43:57] iter = 07650, loss = 1.5167
[2023-10-04 15:43:58] iter = 07660, loss = 1.7120
[2023-10-04 15:43:59] iter = 07670, loss = 1.6948
[2023-10-04 15:44:00] iter = 07680, loss = 1.5492
[2023-10-04 15:44:01] iter = 07690, loss = 1.7810
[2023-10-04 15:44:02] iter = 07700, loss = 1.7901
[2023-10-04 15:44:03] iter = 07710, loss = 1.7321
[2023-10-04 15:44:04] iter = 07720, loss = 1.6657
[2023-10-04 15:44:05] iter = 07730, loss = 1.5457
[2023-10-04 15:44:06] iter = 07740, loss = 1.5889
[2023-10-04 15:44:07] iter = 07750, loss = 1.5955
[2023-10-04 15:44:07] iter = 07760, loss = 1.6236
[2023-10-04 15:44:09] iter = 07770, loss = 1.5766
[2023-10-04 15:44:09] iter = 07780, loss = 1.5075
[2023-10-04 15:44:10] iter = 07790, loss = 1.5705
[2023-10-04 15:44:11] iter = 07800, loss = 1.5104
[2023-10-04 15:44:12] iter = 07810, loss = 1.5821
[2023-10-04 15:44:13] iter = 07820, loss = 1.6718
[2023-10-04 15:44:14] iter = 07830, loss = 1.5026
[2023-10-04 15:44:15] iter = 07840, loss = 1.6440
[2023-10-04 15:44:16] iter = 07850, loss = 1.6133
[2023-10-04 15:44:17] iter = 07860, loss = 1.6172
[2023-10-04 15:44:18] iter = 07870, loss = 1.5909
[2023-10-04 15:44:19] iter = 07880, loss = 1.6539
[2023-10-04 15:44:19] iter = 07890, loss = 1.5801
[2023-10-04 15:44:20] iter = 07900, loss = 1.6422
[2023-10-04 15:44:21] iter = 07910, loss = 1.6681
[2023-10-04 15:44:22] iter = 07920, loss = 1.5725
[2023-10-04 15:44:23] iter = 07930, loss = 1.5515
[2023-10-04 15:44:24] iter = 07940, loss = 1.6146
[2023-10-04 15:44:25] iter = 07950, loss = 1.6064
[2023-10-04 15:44:26] iter = 07960, loss = 1.6250
[2023-10-04 15:44:27] iter = 07970, loss = 1.6094
[2023-10-04 15:44:28] iter = 07980, loss = 1.6169
[2023-10-04 15:44:29] iter = 07990, loss = 1.5200
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 69830}
[2023-10-04 15:44:54] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.008567 train acc = 1.0000, test acc = 0.6244
[2023-10-04 15:45:18] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.015222 train acc = 1.0000, test acc = 0.6066
[2023-10-04 15:45:43] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.013685 train acc = 1.0000, test acc = 0.6183
[2023-10-04 15:46:07] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.014852 train acc = 1.0000, test acc = 0.6113
[2023-10-04 15:46:31] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.015659 train acc = 0.9980, test acc = 0.6213
[2023-10-04 15:46:56] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.019753 train acc = 1.0000, test acc = 0.6217
[2023-10-04 15:47:20] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.013005 train acc = 0.9980, test acc = 0.6199
[2023-10-04 15:47:45] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012299 train acc = 1.0000, test acc = 0.6158
[2023-10-04 15:48:10] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.025139 train acc = 0.9980, test acc = 0.6122
[2023-10-04 15:48:34] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.015401 train acc = 0.9980, test acc = 0.6179
[2023-10-04 15:48:59] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013543 train acc = 1.0000, test acc = 0.6214
[2023-10-04 15:49:23] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015743 train acc = 0.9980, test acc = 0.6114
[2023-10-04 15:49:48] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.028129 train acc = 1.0000, test acc = 0.6132
[2023-10-04 15:50:12] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.011781 train acc = 1.0000, test acc = 0.6155
[2023-10-04 15:50:37] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.018877 train acc = 0.9980, test acc = 0.6183
[2023-10-04 15:51:01] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002095 train acc = 1.0000, test acc = 0.6096
[2023-10-04 15:51:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.008926 train acc = 1.0000, test acc = 0.6156
[2023-10-04 15:51:50] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.018313 train acc = 1.0000, test acc = 0.6208
[2023-10-04 15:52:15] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013155 train acc = 0.9980, test acc = 0.6133
[2023-10-04 15:52:39] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.024689 train acc = 1.0000, test acc = 0.6156
Evaluate 20 random ConvNet, mean = 0.6162 std = 0.0046
-------------------------
[2023-10-04 15:52:39] iter = 08000, loss = 1.5618
[2023-10-04 15:52:40] iter = 08010, loss = 1.5049
[2023-10-04 15:52:41] iter = 08020, loss = 1.5383
[2023-10-04 15:52:42] iter = 08030, loss = 1.6410
[2023-10-04 15:52:43] iter = 08040, loss = 1.6001
[2023-10-04 15:52:44] iter = 08050, loss = 1.5089
[2023-10-04 15:52:45] iter = 08060, loss = 1.4971
[2023-10-04 15:52:46] iter = 08070, loss = 1.5650
[2023-10-04 15:52:46] iter = 08080, loss = 1.6178
[2023-10-04 15:52:47] iter = 08090, loss = 1.5030
[2023-10-04 15:52:48] iter = 08100, loss = 1.8716
[2023-10-04 15:52:49] iter = 08110, loss = 1.5446
[2023-10-04 15:52:50] iter = 08120, loss = 1.4623
[2023-10-04 15:52:51] iter = 08130, loss = 1.6002
[2023-10-04 15:52:52] iter = 08140, loss = 1.7426
[2023-10-04 15:52:53] iter = 08150, loss = 1.6345
[2023-10-04 15:52:54] iter = 08160, loss = 1.7661
[2023-10-04 15:52:55] iter = 08170, loss = 1.6650
[2023-10-04 15:52:56] iter = 08180, loss = 1.4910
[2023-10-04 15:52:57] iter = 08190, loss = 1.6074
[2023-10-04 15:52:58] iter = 08200, loss = 1.5477
[2023-10-04 15:52:59] iter = 08210, loss = 1.5753
[2023-10-04 15:53:00] iter = 08220, loss = 1.5849
[2023-10-04 15:53:00] iter = 08230, loss = 1.5036
[2023-10-04 15:53:01] iter = 08240, loss = 1.5890
[2023-10-04 15:53:02] iter = 08250, loss = 1.5472
[2023-10-04 15:53:03] iter = 08260, loss = 1.5487
[2023-10-04 15:53:04] iter = 08270, loss = 1.5015
[2023-10-04 15:53:05] iter = 08280, loss = 1.5463
[2023-10-04 15:53:06] iter = 08290, loss = 1.5142
[2023-10-04 15:53:07] iter = 08300, loss = 1.5966
[2023-10-04 15:53:08] iter = 08310, loss = 1.6613
[2023-10-04 15:53:09] iter = 08320, loss = 1.5208
[2023-10-04 15:53:10] iter = 08330, loss = 1.6163
[2023-10-04 15:53:11] iter = 08340, loss = 1.6303
[2023-10-04 15:53:12] iter = 08350, loss = 1.5691
[2023-10-04 15:53:13] iter = 08360, loss = 1.7372
[2023-10-04 15:53:13] iter = 08370, loss = 1.4838
[2023-10-04 15:53:14] iter = 08380, loss = 1.7149
[2023-10-04 15:53:15] iter = 08390, loss = 1.4398
[2023-10-04 15:53:16] iter = 08400, loss = 1.5212
[2023-10-04 15:53:17] iter = 08410, loss = 1.5079
[2023-10-04 15:53:18] iter = 08420, loss = 1.6150
[2023-10-04 15:53:19] iter = 08430, loss = 1.5758
[2023-10-04 15:53:20] iter = 08440, loss = 1.6032
[2023-10-04 15:53:21] iter = 08450, loss = 1.5436
[2023-10-04 15:53:22] iter = 08460, loss = 1.7237
[2023-10-04 15:53:23] iter = 08470, loss = 1.6066
[2023-10-04 15:53:24] iter = 08480, loss = 1.6344
[2023-10-04 15:53:24] iter = 08490, loss = 1.6037
[2023-10-04 15:53:25] iter = 08500, loss = 1.6380
[2023-10-04 15:53:26] iter = 08510, loss = 1.5537
[2023-10-04 15:53:27] iter = 08520, loss = 1.5275
[2023-10-04 15:53:28] iter = 08530, loss = 1.5184
[2023-10-04 15:53:29] iter = 08540, loss = 1.5693
[2023-10-04 15:53:30] iter = 08550, loss = 1.5955
[2023-10-04 15:53:31] iter = 08560, loss = 1.5249
[2023-10-04 15:53:32] iter = 08570, loss = 1.5948
[2023-10-04 15:53:33] iter = 08580, loss = 1.4946
[2023-10-04 15:53:34] iter = 08590, loss = 1.6600
[2023-10-04 15:53:35] iter = 08600, loss = 1.4902
[2023-10-04 15:53:35] iter = 08610, loss = 1.5660
[2023-10-04 15:53:36] iter = 08620, loss = 1.5872
[2023-10-04 15:53:37] iter = 08630, loss = 1.6403
[2023-10-04 15:53:38] iter = 08640, loss = 1.5436
[2023-10-04 15:53:39] iter = 08650, loss = 1.5825
[2023-10-04 15:53:40] iter = 08660, loss = 1.6823
[2023-10-04 15:53:41] iter = 08670, loss = 1.6075
[2023-10-04 15:53:42] iter = 08680, loss = 1.5007
[2023-10-04 15:53:43] iter = 08690, loss = 1.5867
[2023-10-04 15:53:44] iter = 08700, loss = 1.5515
[2023-10-04 15:53:45] iter = 08710, loss = 1.5356
[2023-10-04 15:53:46] iter = 08720, loss = 1.5464
[2023-10-04 15:53:47] iter = 08730, loss = 1.5348
[2023-10-04 15:53:47] iter = 08740, loss = 1.5797
[2023-10-04 15:53:48] iter = 08750, loss = 1.5875
[2023-10-04 15:53:49] iter = 08760, loss = 1.5619
[2023-10-04 15:53:50] iter = 08770, loss = 1.6809
[2023-10-04 15:53:51] iter = 08780, loss = 1.6768
[2023-10-04 15:53:52] iter = 08790, loss = 1.7194
[2023-10-04 15:53:53] iter = 08800, loss = 1.5080
[2023-10-04 15:53:54] iter = 08810, loss = 1.5339
[2023-10-04 15:53:55] iter = 08820, loss = 1.5732
[2023-10-04 15:53:56] iter = 08830, loss = 1.5844
[2023-10-04 15:53:57] iter = 08840, loss = 1.6028
[2023-10-04 15:53:57] iter = 08850, loss = 1.6281
[2023-10-04 15:53:58] iter = 08860, loss = 1.5523
[2023-10-04 15:53:59] iter = 08870, loss = 1.6777
[2023-10-04 15:54:00] iter = 08880, loss = 1.6815
[2023-10-04 15:54:01] iter = 08890, loss = 1.5202
[2023-10-04 15:54:02] iter = 08900, loss = 1.5212
[2023-10-04 15:54:03] iter = 08910, loss = 1.6103
[2023-10-04 15:54:04] iter = 08920, loss = 1.5183
[2023-10-04 15:54:05] iter = 08930, loss = 1.5497
[2023-10-04 15:54:06] iter = 08940, loss = 1.5494
[2023-10-04 15:54:07] iter = 08950, loss = 1.5332
[2023-10-04 15:54:08] iter = 08960, loss = 1.5452
[2023-10-04 15:54:08] iter = 08970, loss = 1.6372
[2023-10-04 15:54:09] iter = 08980, loss = 1.5419
[2023-10-04 15:54:10] iter = 08990, loss = 1.7216
[2023-10-04 15:54:11] iter = 09000, loss = 1.5513
[2023-10-04 15:54:12] iter = 09010, loss = 1.5812
[2023-10-04 15:54:13] iter = 09020, loss = 1.5385
[2023-10-04 15:54:14] iter = 09030, loss = 1.5789
[2023-10-04 15:54:15] iter = 09040, loss = 1.5551
[2023-10-04 15:54:16] iter = 09050, loss = 1.5938
[2023-10-04 15:54:17] iter = 09060, loss = 1.6743
[2023-10-04 15:54:18] iter = 09070, loss = 1.4088
[2023-10-04 15:54:18] iter = 09080, loss = 1.4960
[2023-10-04 15:54:19] iter = 09090, loss = 1.5849
[2023-10-04 15:54:20] iter = 09100, loss = 1.5161
[2023-10-04 15:54:21] iter = 09110, loss = 1.5809
[2023-10-04 15:54:22] iter = 09120, loss = 1.4998
[2023-10-04 15:54:23] iter = 09130, loss = 1.4966
[2023-10-04 15:54:24] iter = 09140, loss = 1.4982
[2023-10-04 15:54:25] iter = 09150, loss = 1.5715
[2023-10-04 15:54:26] iter = 09160, loss = 1.5629
[2023-10-04 15:54:27] iter = 09170, loss = 1.5018
[2023-10-04 15:54:27] iter = 09180, loss = 1.5598
[2023-10-04 15:54:28] iter = 09190, loss = 1.4765
[2023-10-04 15:54:29] iter = 09200, loss = 1.4756
[2023-10-04 15:54:30] iter = 09210, loss = 1.6270
[2023-10-04 15:54:31] iter = 09220, loss = 1.5859
[2023-10-04 15:54:32] iter = 09230, loss = 1.4844
[2023-10-04 15:54:33] iter = 09240, loss = 1.6768
[2023-10-04 15:54:34] iter = 09250, loss = 1.4268
[2023-10-04 15:54:35] iter = 09260, loss = 1.5880
[2023-10-04 15:54:36] iter = 09270, loss = 1.6141
[2023-10-04 15:54:37] iter = 09280, loss = 1.4960
[2023-10-04 15:54:37] iter = 09290, loss = 1.5999
[2023-10-04 15:54:38] iter = 09300, loss = 1.6581
[2023-10-04 15:54:39] iter = 09310, loss = 1.5752
[2023-10-04 15:54:40] iter = 09320, loss = 1.5450
[2023-10-04 15:54:41] iter = 09330, loss = 1.6554
[2023-10-04 15:54:42] iter = 09340, loss = 1.6768
[2023-10-04 15:54:43] iter = 09350, loss = 1.5078
[2023-10-04 15:54:44] iter = 09360, loss = 1.5036
[2023-10-04 15:54:45] iter = 09370, loss = 1.5689
[2023-10-04 15:54:46] iter = 09380, loss = 1.5597
[2023-10-04 15:54:47] iter = 09390, loss = 1.5394
[2023-10-04 15:54:48] iter = 09400, loss = 1.5265
[2023-10-04 15:54:49] iter = 09410, loss = 1.6889
[2023-10-04 15:54:50] iter = 09420, loss = 1.6760
[2023-10-04 15:54:51] iter = 09430, loss = 1.7191
[2023-10-04 15:54:52] iter = 09440, loss = 1.5971
[2023-10-04 15:54:52] iter = 09450, loss = 1.5807
[2023-10-04 15:54:53] iter = 09460, loss = 1.5521
[2023-10-04 15:54:54] iter = 09470, loss = 1.5627
[2023-10-04 15:54:55] iter = 09480, loss = 1.7372
[2023-10-04 15:54:56] iter = 09490, loss = 1.5450
[2023-10-04 15:54:57] iter = 09500, loss = 1.6373
[2023-10-04 15:54:58] iter = 09510, loss = 1.6719
[2023-10-04 15:54:59] iter = 09520, loss = 1.4882
[2023-10-04 15:55:00] iter = 09530, loss = 1.5147
[2023-10-04 15:55:01] iter = 09540, loss = 1.5509
[2023-10-04 15:55:02] iter = 09550, loss = 1.5510
[2023-10-04 15:55:03] iter = 09560, loss = 1.5406
[2023-10-04 15:55:04] iter = 09570, loss = 1.6844
[2023-10-04 15:55:05] iter = 09580, loss = 1.4953
[2023-10-04 15:55:06] iter = 09590, loss = 1.6987
[2023-10-04 15:55:07] iter = 09600, loss = 1.7710
[2023-10-04 15:55:07] iter = 09610, loss = 1.5621
[2023-10-04 15:55:08] iter = 09620, loss = 1.6504
[2023-10-04 15:55:09] iter = 09630, loss = 1.6253
[2023-10-04 15:55:10] iter = 09640, loss = 1.5216
[2023-10-04 15:55:11] iter = 09650, loss = 1.4049
[2023-10-04 15:55:12] iter = 09660, loss = 1.5606
[2023-10-04 15:55:13] iter = 09670, loss = 1.5122
[2023-10-04 15:55:14] iter = 09680, loss = 1.6698
[2023-10-04 15:55:15] iter = 09690, loss = 1.5886
[2023-10-04 15:55:16] iter = 09700, loss = 1.5318
[2023-10-04 15:55:16] iter = 09710, loss = 1.5583
[2023-10-04 15:55:17] iter = 09720, loss = 1.6855
[2023-10-04 15:55:18] iter = 09730, loss = 1.4972
[2023-10-04 15:55:19] iter = 09740, loss = 1.4688
[2023-10-04 15:55:20] iter = 09750, loss = 1.5236
[2023-10-04 15:55:21] iter = 09760, loss = 1.5717
[2023-10-04 15:55:22] iter = 09770, loss = 1.6537
[2023-10-04 15:55:23] iter = 09780, loss = 1.5818
[2023-10-04 15:55:24] iter = 09790, loss = 1.4123
[2023-10-04 15:55:25] iter = 09800, loss = 1.4811
[2023-10-04 15:55:26] iter = 09810, loss = 1.5953
[2023-10-04 15:55:26] iter = 09820, loss = 1.5426
[2023-10-04 15:55:27] iter = 09830, loss = 1.4387
[2023-10-04 15:55:28] iter = 09840, loss = 1.6389
[2023-10-04 15:55:29] iter = 09850, loss = 1.4659
[2023-10-04 15:55:30] iter = 09860, loss = 1.5586
[2023-10-04 15:55:31] iter = 09870, loss = 1.6485
[2023-10-04 15:55:32] iter = 09880, loss = 1.5225
[2023-10-04 15:55:33] iter = 09890, loss = 1.6876
[2023-10-04 15:55:34] iter = 09900, loss = 1.5905
[2023-10-04 15:55:35] iter = 09910, loss = 1.5397
[2023-10-04 15:55:36] iter = 09920, loss = 1.6075
[2023-10-04 15:55:37] iter = 09930, loss = 1.4920
[2023-10-04 15:55:37] iter = 09940, loss = 1.5282
[2023-10-04 15:55:38] iter = 09950, loss = 1.5315
[2023-10-04 15:55:39] iter = 09960, loss = 1.6107
[2023-10-04 15:55:40] iter = 09970, loss = 1.6659
[2023-10-04 15:55:41] iter = 09980, loss = 1.4060
[2023-10-04 15:55:42] iter = 09990, loss = 1.5688
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 43629}
[2023-10-04 15:56:07] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.013803 train acc = 1.0000, test acc = 0.6153
[2023-10-04 15:56:32] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.017224 train acc = 0.9980, test acc = 0.6209
[2023-10-04 15:56:56] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.018946 train acc = 1.0000, test acc = 0.6184
[2023-10-04 15:57:21] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.005623 train acc = 1.0000, test acc = 0.6234
[2023-10-04 15:57:45] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004104 train acc = 1.0000, test acc = 0.6257
[2023-10-04 15:58:10] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002377 train acc = 1.0000, test acc = 0.6213
[2023-10-04 15:58:35] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.022833 train acc = 1.0000, test acc = 0.6196
[2023-10-04 15:58:59] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012949 train acc = 1.0000, test acc = 0.6188
[2023-10-04 15:59:24] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002021 train acc = 1.0000, test acc = 0.6246
[2023-10-04 15:59:49] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.001992 train acc = 1.0000, test acc = 0.6160
[2023-10-04 16:00:13] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.005203 train acc = 1.0000, test acc = 0.6180
[2023-10-04 16:00:38] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.014207 train acc = 1.0000, test acc = 0.6188
[2023-10-04 16:01:02] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.013301 train acc = 1.0000, test acc = 0.6181
[2023-10-04 16:01:27] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.005792 train acc = 1.0000, test acc = 0.6254
[2023-10-04 16:01:51] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.016264 train acc = 1.0000, test acc = 0.6248
[2023-10-04 16:02:16] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015762 train acc = 1.0000, test acc = 0.6201
[2023-10-04 16:02:41] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.006111 train acc = 1.0000, test acc = 0.6217
[2023-10-04 16:03:05] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.022263 train acc = 0.9980, test acc = 0.6199
[2023-10-04 16:03:30] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003431 train acc = 1.0000, test acc = 0.6222
[2023-10-04 16:03:54] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.023816 train acc = 0.9980, test acc = 0.6214
Evaluate 20 random ConvNet, mean = 0.6207 std = 0.0029
-------------------------
[2023-10-04 16:03:54] iter = 10000, loss = 1.5190
[2023-10-04 16:03:55] iter = 10010, loss = 1.6173
[2023-10-04 16:03:56] iter = 10020, loss = 1.4641
[2023-10-04 16:03:57] iter = 10030, loss = 1.5752
[2023-10-04 16:03:58] iter = 10040, loss = 1.4625
[2023-10-04 16:03:59] iter = 10050, loss = 1.5841
[2023-10-04 16:04:00] iter = 10060, loss = 1.5888
[2023-10-04 16:04:01] iter = 10070, loss = 1.5756
[2023-10-04 16:04:02] iter = 10080, loss = 1.5583
[2023-10-04 16:04:03] iter = 10090, loss = 1.6069
[2023-10-04 16:04:04] iter = 10100, loss = 1.4600
[2023-10-04 16:04:04] iter = 10110, loss = 1.4964
[2023-10-04 16:04:05] iter = 10120, loss = 1.4435
[2023-10-04 16:04:06] iter = 10130, loss = 1.5358
[2023-10-04 16:04:07] iter = 10140, loss = 1.5837
[2023-10-04 16:04:08] iter = 10150, loss = 1.4604
[2023-10-04 16:04:09] iter = 10160, loss = 1.5789
[2023-10-04 16:04:10] iter = 10170, loss = 1.4046
[2023-10-04 16:04:11] iter = 10180, loss = 1.5257
[2023-10-04 16:04:12] iter = 10190, loss = 1.4641
[2023-10-04 16:04:13] iter = 10200, loss = 1.4758
[2023-10-04 16:04:14] iter = 10210, loss = 1.6151
[2023-10-04 16:04:14] iter = 10220, loss = 1.6135
[2023-10-04 16:04:15] iter = 10230, loss = 1.6949
[2023-10-04 16:04:16] iter = 10240, loss = 1.4597
[2023-10-04 16:04:17] iter = 10250, loss = 1.5147
[2023-10-04 16:04:18] iter = 10260, loss = 1.4990
[2023-10-04 16:04:19] iter = 10270, loss = 1.5716
[2023-10-04 16:04:20] iter = 10280, loss = 1.5443
[2023-10-04 16:04:21] iter = 10290, loss = 1.4347
[2023-10-04 16:04:22] iter = 10300, loss = 1.6541
[2023-10-04 16:04:23] iter = 10310, loss = 1.7044
[2023-10-04 16:04:24] iter = 10320, loss = 1.6216
[2023-10-04 16:04:25] iter = 10330, loss = 1.5914
[2023-10-04 16:04:26] iter = 10340, loss = 1.5051
[2023-10-04 16:04:26] iter = 10350, loss = 1.5424
[2023-10-04 16:04:27] iter = 10360, loss = 1.6659
[2023-10-04 16:04:28] iter = 10370, loss = 1.5311
[2023-10-04 16:04:29] iter = 10380, loss = 1.4259
[2023-10-04 16:04:30] iter = 10390, loss = 1.5592
[2023-10-04 16:04:31] iter = 10400, loss = 1.5531
[2023-10-04 16:04:32] iter = 10410, loss = 1.6511
[2023-10-04 16:04:33] iter = 10420, loss = 1.5326
[2023-10-04 16:04:34] iter = 10430, loss = 1.5095
[2023-10-04 16:04:35] iter = 10440, loss = 1.5087
[2023-10-04 16:04:36] iter = 10450, loss = 1.4775
[2023-10-04 16:04:37] iter = 10460, loss = 1.4103
[2023-10-04 16:04:38] iter = 10470, loss = 1.4826
[2023-10-04 16:04:39] iter = 10480, loss = 1.5130
[2023-10-04 16:04:40] iter = 10490, loss = 1.4877
[2023-10-04 16:04:40] iter = 10500, loss = 1.7480
[2023-10-04 16:04:41] iter = 10510, loss = 1.5885
[2023-10-04 16:04:42] iter = 10520, loss = 1.5593
[2023-10-04 16:04:43] iter = 10530, loss = 1.5237
[2023-10-04 16:04:44] iter = 10540, loss = 1.6265
[2023-10-04 16:04:45] iter = 10550, loss = 1.5290
[2023-10-04 16:04:46] iter = 10560, loss = 1.5456
[2023-10-04 16:04:47] iter = 10570, loss = 1.6200
[2023-10-04 16:04:48] iter = 10580, loss = 1.5554
[2023-10-04 16:04:49] iter = 10590, loss = 1.5612
[2023-10-04 16:04:50] iter = 10600, loss = 1.6907
[2023-10-04 16:04:51] iter = 10610, loss = 1.4899
[2023-10-04 16:04:52] iter = 10620, loss = 1.5241
[2023-10-04 16:04:53] iter = 10630, loss = 1.4742
[2023-10-04 16:04:53] iter = 10640, loss = 1.6087
[2023-10-04 16:04:55] iter = 10650, loss = 1.4906
[2023-10-04 16:04:55] iter = 10660, loss = 1.5275
[2023-10-04 16:04:56] iter = 10670, loss = 1.4254
[2023-10-04 16:04:57] iter = 10680, loss = 1.4959
[2023-10-04 16:04:58] iter = 10690, loss = 1.6343
[2023-10-04 16:04:59] iter = 10700, loss = 1.5200
[2023-10-04 16:05:00] iter = 10710, loss = 1.5817
[2023-10-04 16:05:01] iter = 10720, loss = 1.4986
[2023-10-04 16:05:02] iter = 10730, loss = 1.4348
[2023-10-04 16:05:03] iter = 10740, loss = 1.5592
[2023-10-04 16:05:04] iter = 10750, loss = 1.5102
[2023-10-04 16:05:04] iter = 10760, loss = 1.6002
[2023-10-04 16:05:05] iter = 10770, loss = 1.4145
[2023-10-04 16:05:06] iter = 10780, loss = 1.5800
[2023-10-04 16:05:07] iter = 10790, loss = 1.5935
[2023-10-04 16:05:08] iter = 10800, loss = 1.5674
[2023-10-04 16:05:09] iter = 10810, loss = 1.4839
[2023-10-04 16:05:10] iter = 10820, loss = 1.5192
[2023-10-04 16:05:11] iter = 10830, loss = 1.6527
[2023-10-04 16:05:12] iter = 10840, loss = 1.5398
[2023-10-04 16:05:13] iter = 10850, loss = 1.5374
[2023-10-04 16:05:14] iter = 10860, loss = 1.4975
[2023-10-04 16:05:15] iter = 10870, loss = 1.4871
[2023-10-04 16:05:16] iter = 10880, loss = 1.6916
[2023-10-04 16:05:17] iter = 10890, loss = 1.5522
[2023-10-04 16:05:18] iter = 10900, loss = 1.6438
[2023-10-04 16:05:19] iter = 10910, loss = 1.6042
[2023-10-04 16:05:20] iter = 10920, loss = 1.6703
[2023-10-04 16:05:21] iter = 10930, loss = 1.4856
[2023-10-04 16:05:22] iter = 10940, loss = 1.4451
[2023-10-04 16:05:23] iter = 10950, loss = 1.4944
[2023-10-04 16:05:23] iter = 10960, loss = 1.5870
[2023-10-04 16:05:24] iter = 10970, loss = 1.7341
[2023-10-04 16:05:25] iter = 10980, loss = 1.4382
[2023-10-04 16:05:26] iter = 10990, loss = 1.5112
[2023-10-04 16:05:27] iter = 11000, loss = 1.5372
[2023-10-04 16:05:28] iter = 11010, loss = 1.6116
[2023-10-04 16:05:29] iter = 11020, loss = 1.4862
[2023-10-04 16:05:30] iter = 11030, loss = 1.4983
[2023-10-04 16:05:31] iter = 11040, loss = 1.5844
[2023-10-04 16:05:32] iter = 11050, loss = 1.6667
[2023-10-04 16:05:33] iter = 11060, loss = 1.6236
[2023-10-04 16:05:34] iter = 11070, loss = 1.6518
[2023-10-04 16:05:35] iter = 11080, loss = 1.5447
[2023-10-04 16:05:35] iter = 11090, loss = 1.4101
[2023-10-04 16:05:36] iter = 11100, loss = 1.4915
[2023-10-04 16:05:37] iter = 11110, loss = 1.5130
[2023-10-04 16:05:38] iter = 11120, loss = 1.5464
[2023-10-04 16:05:39] iter = 11130, loss = 1.5563
[2023-10-04 16:05:40] iter = 11140, loss = 1.4155
[2023-10-04 16:05:41] iter = 11150, loss = 1.6000
[2023-10-04 16:05:42] iter = 11160, loss = 1.5252
[2023-10-04 16:05:43] iter = 11170, loss = 1.5346
[2023-10-04 16:05:44] iter = 11180, loss = 1.6815
[2023-10-04 16:05:44] iter = 11190, loss = 1.5236
[2023-10-04 16:05:45] iter = 11200, loss = 1.6260
[2023-10-04 16:05:46] iter = 11210, loss = 1.5642
[2023-10-04 16:05:47] iter = 11220, loss = 1.6007
[2023-10-04 16:05:48] iter = 11230, loss = 1.6636
[2023-10-04 16:05:49] iter = 11240, loss = 1.4848
[2023-10-04 16:05:50] iter = 11250, loss = 1.5176
[2023-10-04 16:05:51] iter = 11260, loss = 1.4714
[2023-10-04 16:05:52] iter = 11270, loss = 1.6439
[2023-10-04 16:05:53] iter = 11280, loss = 1.5249
[2023-10-04 16:05:54] iter = 11290, loss = 1.5657
[2023-10-04 16:05:55] iter = 11300, loss = 1.5272
[2023-10-04 16:05:55] iter = 11310, loss = 1.6494
[2023-10-04 16:05:56] iter = 11320, loss = 1.4510
[2023-10-04 16:05:57] iter = 11330, loss = 1.7291
[2023-10-04 16:05:58] iter = 11340, loss = 1.5412
[2023-10-04 16:05:59] iter = 11350, loss = 1.5557
[2023-10-04 16:06:00] iter = 11360, loss = 1.7566
[2023-10-04 16:06:01] iter = 11370, loss = 1.6748
[2023-10-04 16:06:02] iter = 11380, loss = 1.4965
[2023-10-04 16:06:03] iter = 11390, loss = 1.5040
[2023-10-04 16:06:04] iter = 11400, loss = 1.5356
[2023-10-04 16:06:04] iter = 11410, loss = 1.4523
[2023-10-04 16:06:05] iter = 11420, loss = 1.5375
[2023-10-04 16:06:06] iter = 11430, loss = 1.6407
[2023-10-04 16:06:07] iter = 11440, loss = 1.6956
[2023-10-04 16:06:08] iter = 11450, loss = 1.6041
[2023-10-04 16:06:09] iter = 11460, loss = 1.5171
[2023-10-04 16:06:10] iter = 11470, loss = 1.5238
[2023-10-04 16:06:11] iter = 11480, loss = 1.6193
[2023-10-04 16:06:12] iter = 11490, loss = 1.5554
[2023-10-04 16:06:13] iter = 11500, loss = 1.6886
[2023-10-04 16:06:14] iter = 11510, loss = 1.5711
[2023-10-04 16:06:14] iter = 11520, loss = 1.4301
[2023-10-04 16:06:15] iter = 11530, loss = 1.4584
[2023-10-04 16:06:16] iter = 11540, loss = 1.5820
[2023-10-04 16:06:17] iter = 11550, loss = 1.6791
[2023-10-04 16:06:18] iter = 11560, loss = 1.5153
[2023-10-04 16:06:19] iter = 11570, loss = 1.5958
[2023-10-04 16:06:20] iter = 11580, loss = 1.6079
[2023-10-04 16:06:21] iter = 11590, loss = 1.5558
[2023-10-04 16:06:22] iter = 11600, loss = 1.4935
[2023-10-04 16:06:23] iter = 11610, loss = 1.7492
[2023-10-04 16:06:24] iter = 11620, loss = 1.6053
[2023-10-04 16:06:25] iter = 11630, loss = 1.4881
[2023-10-04 16:06:26] iter = 11640, loss = 1.5437
[2023-10-04 16:06:27] iter = 11650, loss = 1.5818
[2023-10-04 16:06:27] iter = 11660, loss = 1.6257
[2023-10-04 16:06:28] iter = 11670, loss = 1.4938
[2023-10-04 16:06:29] iter = 11680, loss = 1.4962
[2023-10-04 16:06:30] iter = 11690, loss = 1.5862
[2023-10-04 16:06:31] iter = 11700, loss = 1.4517
[2023-10-04 16:06:32] iter = 11710, loss = 1.4496
[2023-10-04 16:06:33] iter = 11720, loss = 1.6626
[2023-10-04 16:06:34] iter = 11730, loss = 1.5204
[2023-10-04 16:06:35] iter = 11740, loss = 1.5386
[2023-10-04 16:06:36] iter = 11750, loss = 1.4199
[2023-10-04 16:06:37] iter = 11760, loss = 1.5034
[2023-10-04 16:06:37] iter = 11770, loss = 1.6116
[2023-10-04 16:06:38] iter = 11780, loss = 1.4901
[2023-10-04 16:06:39] iter = 11790, loss = 1.4816
[2023-10-04 16:06:40] iter = 11800, loss = 1.4008
[2023-10-04 16:06:41] iter = 11810, loss = 1.4343
[2023-10-04 16:06:42] iter = 11820, loss = 1.5328
[2023-10-04 16:06:43] iter = 11830, loss = 1.4203
[2023-10-04 16:06:44] iter = 11840, loss = 1.5075
[2023-10-04 16:06:45] iter = 11850, loss = 1.4777
[2023-10-04 16:06:46] iter = 11860, loss = 1.5728
[2023-10-04 16:06:47] iter = 11870, loss = 1.5818
[2023-10-04 16:06:47] iter = 11880, loss = 1.6574
[2023-10-04 16:06:48] iter = 11890, loss = 1.3968
[2023-10-04 16:06:49] iter = 11900, loss = 1.5646
[2023-10-04 16:06:50] iter = 11910, loss = 1.5476
[2023-10-04 16:06:51] iter = 11920, loss = 1.6974
[2023-10-04 16:06:52] iter = 11930, loss = 1.6985
[2023-10-04 16:06:53] iter = 11940, loss = 1.4648
[2023-10-04 16:06:54] iter = 11950, loss = 1.4798
[2023-10-04 16:06:55] iter = 11960, loss = 1.6714
[2023-10-04 16:06:56] iter = 11970, loss = 1.4651
[2023-10-04 16:06:57] iter = 11980, loss = 1.4248
[2023-10-04 16:06:58] iter = 11990, loss = 1.5124
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 18802}
[2023-10-04 16:07:23] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.009121 train acc = 1.0000, test acc = 0.6254
[2023-10-04 16:07:47] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.012834 train acc = 1.0000, test acc = 0.6243
[2023-10-04 16:08:12] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.018416 train acc = 1.0000, test acc = 0.6221
[2023-10-04 16:08:36] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002039 train acc = 1.0000, test acc = 0.6194
[2023-10-04 16:09:01] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.021986 train acc = 1.0000, test acc = 0.6245
[2023-10-04 16:09:25] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013117 train acc = 1.0000, test acc = 0.6269
[2023-10-04 16:09:50] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016264 train acc = 1.0000, test acc = 0.6238
[2023-10-04 16:10:15] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.011011 train acc = 1.0000, test acc = 0.6194
[2023-10-04 16:10:39] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.001960 train acc = 1.0000, test acc = 0.6266
[2023-10-04 16:11:04] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013797 train acc = 1.0000, test acc = 0.6249
[2023-10-04 16:11:28] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.009956 train acc = 1.0000, test acc = 0.6177
[2023-10-04 16:11:53] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.005841 train acc = 1.0000, test acc = 0.6209
[2023-10-04 16:12:17] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.020188 train acc = 0.9960, test acc = 0.6207
[2023-10-04 16:12:42] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004271 train acc = 1.0000, test acc = 0.6308
[2023-10-04 16:13:07] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013424 train acc = 0.9980, test acc = 0.6231
[2023-10-04 16:13:31] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004258 train acc = 1.0000, test acc = 0.6288
[2023-10-04 16:13:56] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.024791 train acc = 1.0000, test acc = 0.6243
[2023-10-04 16:14:20] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.013771 train acc = 1.0000, test acc = 0.6229
[2023-10-04 16:14:45] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013878 train acc = 1.0000, test acc = 0.6203
[2023-10-04 16:15:09] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.014313 train acc = 1.0000, test acc = 0.6222
Evaluate 20 random ConvNet, mean = 0.6234 std = 0.0032
-------------------------
[2023-10-04 16:15:10] iter = 12000, loss = 1.4215
[2023-10-04 16:15:11] iter = 12010, loss = 1.5335
[2023-10-04 16:15:12] iter = 12020, loss = 1.4058
[2023-10-04 16:15:13] iter = 12030, loss = 1.4710
[2023-10-04 16:15:14] iter = 12040, loss = 1.5078
[2023-10-04 16:15:14] iter = 12050, loss = 1.5759
[2023-10-04 16:15:15] iter = 12060, loss = 1.5384
[2023-10-04 16:15:16] iter = 12070, loss = 1.6080
[2023-10-04 16:15:17] iter = 12080, loss = 1.5883
[2023-10-04 16:15:18] iter = 12090, loss = 1.4769
[2023-10-04 16:15:19] iter = 12100, loss = 1.6704
[2023-10-04 16:15:20] iter = 12110, loss = 1.5667
[2023-10-04 16:15:21] iter = 12120, loss = 1.4530
[2023-10-04 16:15:22] iter = 12130, loss = 1.5049
[2023-10-04 16:15:23] iter = 12140, loss = 1.6179
[2023-10-04 16:15:24] iter = 12150, loss = 1.5288
[2023-10-04 16:15:25] iter = 12160, loss = 1.4482
[2023-10-04 16:15:26] iter = 12170, loss = 1.5693
[2023-10-04 16:15:26] iter = 12180, loss = 1.5337
[2023-10-04 16:15:27] iter = 12190, loss = 1.6514
[2023-10-04 16:15:28] iter = 12200, loss = 1.4046
[2023-10-04 16:15:29] iter = 12210, loss = 1.5142
[2023-10-04 16:15:30] iter = 12220, loss = 1.4946
[2023-10-04 16:15:31] iter = 12230, loss = 1.4995
[2023-10-04 16:15:32] iter = 12240, loss = 1.3749
[2023-10-04 16:15:33] iter = 12250, loss = 1.5486
[2023-10-04 16:15:34] iter = 12260, loss = 1.5742
[2023-10-04 16:15:35] iter = 12270, loss = 1.5062
[2023-10-04 16:15:36] iter = 12280, loss = 1.6669
[2023-10-04 16:15:37] iter = 12290, loss = 1.6670
[2023-10-04 16:15:38] iter = 12300, loss = 1.4698
[2023-10-04 16:15:39] iter = 12310, loss = 1.5216
[2023-10-04 16:15:39] iter = 12320, loss = 1.4598
[2023-10-04 16:15:40] iter = 12330, loss = 1.5341
[2023-10-04 16:15:41] iter = 12340, loss = 1.5948
[2023-10-04 16:15:42] iter = 12350, loss = 1.4706
[2023-10-04 16:15:43] iter = 12360, loss = 1.5989
[2023-10-04 16:15:44] iter = 12370, loss = 1.5561
[2023-10-04 16:15:45] iter = 12380, loss = 1.4733
[2023-10-04 16:15:46] iter = 12390, loss = 1.7017
[2023-10-04 16:15:47] iter = 12400, loss = 1.6295
[2023-10-04 16:15:48] iter = 12410, loss = 1.4254
[2023-10-04 16:15:49] iter = 12420, loss = 1.4486
[2023-10-04 16:15:49] iter = 12430, loss = 1.7302
[2023-10-04 16:15:50] iter = 12440, loss = 1.6839
[2023-10-04 16:15:51] iter = 12450, loss = 1.5077
[2023-10-04 16:15:52] iter = 12460, loss = 1.6840
[2023-10-04 16:15:53] iter = 12470, loss = 1.4382
[2023-10-04 16:15:54] iter = 12480, loss = 1.4778
[2023-10-04 16:15:55] iter = 12490, loss = 1.5474
[2023-10-04 16:15:56] iter = 12500, loss = 1.5138
[2023-10-04 16:15:57] iter = 12510, loss = 1.5197
[2023-10-04 16:15:58] iter = 12520, loss = 1.3811
[2023-10-04 16:15:59] iter = 12530, loss = 1.5062
[2023-10-04 16:16:00] iter = 12540, loss = 1.4579
[2023-10-04 16:16:00] iter = 12550, loss = 1.4902
[2023-10-04 16:16:01] iter = 12560, loss = 1.6036
[2023-10-04 16:16:02] iter = 12570, loss = 1.5213
[2023-10-04 16:16:03] iter = 12580, loss = 1.4606
[2023-10-04 16:16:04] iter = 12590, loss = 1.4857
[2023-10-04 16:16:05] iter = 12600, loss = 1.4517
[2023-10-04 16:16:06] iter = 12610, loss = 1.5020
[2023-10-04 16:16:07] iter = 12620, loss = 1.5356
[2023-10-04 16:16:08] iter = 12630, loss = 1.4738
[2023-10-04 16:16:09] iter = 12640, loss = 1.5713
[2023-10-04 16:16:10] iter = 12650, loss = 1.4815
[2023-10-04 16:16:11] iter = 12660, loss = 1.5687
[2023-10-04 16:16:11] iter = 12670, loss = 1.4371
[2023-10-04 16:16:13] iter = 12680, loss = 1.6168
[2023-10-04 16:16:13] iter = 12690, loss = 1.5355
[2023-10-04 16:16:14] iter = 12700, loss = 1.4773
[2023-10-04 16:16:15] iter = 12710, loss = 1.6506
[2023-10-04 16:16:16] iter = 12720, loss = 1.5742
[2023-10-04 16:16:17] iter = 12730, loss = 1.4854
[2023-10-04 16:16:18] iter = 12740, loss = 1.5043
[2023-10-04 16:16:19] iter = 12750, loss = 1.6495
[2023-10-04 16:16:20] iter = 12760, loss = 1.4707
[2023-10-04 16:16:21] iter = 12770, loss = 1.5068
[2023-10-04 16:16:21] iter = 12780, loss = 1.5552
[2023-10-04 16:16:22] iter = 12790, loss = 1.6262
[2023-10-04 16:16:23] iter = 12800, loss = 1.5282
[2023-10-04 16:16:24] iter = 12810, loss = 1.4314
[2023-10-04 16:16:25] iter = 12820, loss = 1.5434
[2023-10-04 16:16:26] iter = 12830, loss = 1.5421
[2023-10-04 16:16:27] iter = 12840, loss = 1.5309
[2023-10-04 16:16:28] iter = 12850, loss = 1.5959
[2023-10-04 16:16:29] iter = 12860, loss = 1.4510
[2023-10-04 16:16:30] iter = 12870, loss = 1.3776
[2023-10-04 16:16:31] iter = 12880, loss = 1.5002
[2023-10-04 16:16:32] iter = 12890, loss = 1.6877
[2023-10-04 16:16:32] iter = 12900, loss = 1.4652
[2023-10-04 16:16:33] iter = 12910, loss = 1.4745
[2023-10-04 16:16:34] iter = 12920, loss = 1.5710
[2023-10-04 16:16:35] iter = 12930, loss = 1.4536
[2023-10-04 16:16:36] iter = 12940, loss = 1.6042
[2023-10-04 16:16:37] iter = 12950, loss = 1.6063
[2023-10-04 16:16:38] iter = 12960, loss = 1.5195
[2023-10-04 16:16:39] iter = 12970, loss = 1.5811
[2023-10-04 16:16:40] iter = 12980, loss = 1.5153
[2023-10-04 16:16:41] iter = 12990, loss = 1.7027
[2023-10-04 16:16:41] iter = 13000, loss = 1.5338
[2023-10-04 16:16:42] iter = 13010, loss = 1.6276
[2023-10-04 16:16:43] iter = 13020, loss = 1.6047
[2023-10-04 16:16:44] iter = 13030, loss = 1.7241
[2023-10-04 16:16:45] iter = 13040, loss = 1.6322
[2023-10-04 16:16:46] iter = 13050, loss = 1.5209
[2023-10-04 16:16:47] iter = 13060, loss = 1.4976
[2023-10-04 16:16:48] iter = 13070, loss = 1.5593
[2023-10-04 16:16:49] iter = 13080, loss = 1.5223
[2023-10-04 16:16:50] iter = 13090, loss = 1.5339
[2023-10-04 16:16:51] iter = 13100, loss = 1.3716
[2023-10-04 16:16:52] iter = 13110, loss = 1.5787
[2023-10-04 16:16:52] iter = 13120, loss = 1.5016
[2023-10-04 16:16:53] iter = 13130, loss = 1.6851
[2023-10-04 16:16:54] iter = 13140, loss = 1.4950
[2023-10-04 16:16:55] iter = 13150, loss = 1.4862
[2023-10-04 16:16:56] iter = 13160, loss = 1.4871
[2023-10-04 16:16:57] iter = 13170, loss = 1.3960
[2023-10-04 16:16:58] iter = 13180, loss = 1.5243
[2023-10-04 16:16:59] iter = 13190, loss = 1.4816
[2023-10-04 16:17:00] iter = 13200, loss = 1.4113
[2023-10-04 16:17:01] iter = 13210, loss = 1.5132
[2023-10-04 16:17:02] iter = 13220, loss = 1.4780
[2023-10-04 16:17:03] iter = 13230, loss = 1.3609
[2023-10-04 16:17:04] iter = 13240, loss = 1.5137
[2023-10-04 16:17:05] iter = 13250, loss = 1.5016
[2023-10-04 16:17:05] iter = 13260, loss = 1.5876
[2023-10-04 16:17:06] iter = 13270, loss = 1.4798
[2023-10-04 16:17:07] iter = 13280, loss = 1.4833
[2023-10-04 16:17:08] iter = 13290, loss = 1.6932
[2023-10-04 16:17:09] iter = 13300, loss = 1.6710
[2023-10-04 16:17:10] iter = 13310, loss = 1.5874
[2023-10-04 16:17:11] iter = 13320, loss = 1.6821
[2023-10-04 16:17:12] iter = 13330, loss = 1.5495
[2023-10-04 16:17:13] iter = 13340, loss = 1.4667
[2023-10-04 16:17:14] iter = 13350, loss = 1.6889
[2023-10-04 16:17:15] iter = 13360, loss = 1.4086
[2023-10-04 16:17:16] iter = 13370, loss = 1.4868
[2023-10-04 16:17:17] iter = 13380, loss = 1.4893
[2023-10-04 16:17:18] iter = 13390, loss = 1.4795
[2023-10-04 16:17:19] iter = 13400, loss = 1.4932
[2023-10-04 16:17:20] iter = 13410, loss = 1.5774
[2023-10-04 16:17:21] iter = 13420, loss = 1.3678
[2023-10-04 16:17:21] iter = 13430, loss = 1.5108
[2023-10-04 16:17:22] iter = 13440, loss = 1.4394
[2023-10-04 16:17:23] iter = 13450, loss = 1.4321
[2023-10-04 16:17:24] iter = 13460, loss = 1.6277
[2023-10-04 16:17:25] iter = 13470, loss = 1.5011
[2023-10-04 16:17:26] iter = 13480, loss = 1.4390
[2023-10-04 16:17:27] iter = 13490, loss = 1.7556
[2023-10-04 16:17:28] iter = 13500, loss = 1.5418
[2023-10-04 16:17:29] iter = 13510, loss = 1.4349
[2023-10-04 16:17:30] iter = 13520, loss = 1.5473
[2023-10-04 16:17:31] iter = 13530, loss = 1.5295
[2023-10-04 16:17:31] iter = 13540, loss = 1.4486
[2023-10-04 16:17:32] iter = 13550, loss = 1.3596
[2023-10-04 16:17:33] iter = 13560, loss = 1.5091
[2023-10-04 16:17:34] iter = 13570, loss = 1.5177
[2023-10-04 16:17:35] iter = 13580, loss = 1.5940
[2023-10-04 16:17:36] iter = 13590, loss = 1.6726
[2023-10-04 16:17:37] iter = 13600, loss = 1.5603
[2023-10-04 16:17:38] iter = 13610, loss = 1.6463
[2023-10-04 16:17:39] iter = 13620, loss = 1.5711
[2023-10-04 16:17:40] iter = 13630, loss = 1.7317
[2023-10-04 16:17:41] iter = 13640, loss = 1.5371
[2023-10-04 16:17:41] iter = 13650, loss = 1.6061
[2023-10-04 16:17:42] iter = 13660, loss = 1.4603
[2023-10-04 16:17:43] iter = 13670, loss = 1.6076
[2023-10-04 16:17:44] iter = 13680, loss = 1.5222
[2023-10-04 16:17:45] iter = 13690, loss = 1.4669
[2023-10-04 16:17:46] iter = 13700, loss = 1.7719
[2023-10-04 16:17:47] iter = 13710, loss = 1.6949
[2023-10-04 16:17:48] iter = 13720, loss = 1.6460
[2023-10-04 16:17:49] iter = 13730, loss = 1.6843
[2023-10-04 16:17:50] iter = 13740, loss = 1.4504
[2023-10-04 16:17:50] iter = 13750, loss = 1.5084
[2023-10-04 16:17:51] iter = 13760, loss = 1.4383
[2023-10-04 16:17:52] iter = 13770, loss = 1.5737
[2023-10-04 16:17:53] iter = 13780, loss = 1.3384
[2023-10-04 16:17:54] iter = 13790, loss = 1.5601
[2023-10-04 16:17:55] iter = 13800, loss = 1.5740
[2023-10-04 16:17:56] iter = 13810, loss = 1.5431
[2023-10-04 16:17:57] iter = 13820, loss = 1.5407
[2023-10-04 16:17:58] iter = 13830, loss = 1.4351
[2023-10-04 16:17:59] iter = 13840, loss = 1.5055
[2023-10-04 16:18:00] iter = 13850, loss = 1.4401
[2023-10-04 16:18:00] iter = 13860, loss = 1.4200
[2023-10-04 16:18:01] iter = 13870, loss = 1.5518
[2023-10-04 16:18:02] iter = 13880, loss = 1.5266
[2023-10-04 16:18:03] iter = 13890, loss = 1.4744
[2023-10-04 16:18:04] iter = 13900, loss = 1.7061
[2023-10-04 16:18:05] iter = 13910, loss = 1.4845
[2023-10-04 16:18:06] iter = 13920, loss = 1.4556
[2023-10-04 16:18:07] iter = 13930, loss = 1.3373
[2023-10-04 16:18:08] iter = 13940, loss = 1.5528
[2023-10-04 16:18:09] iter = 13950, loss = 1.5131
[2023-10-04 16:18:10] iter = 13960, loss = 1.4379
[2023-10-04 16:18:11] iter = 13970, loss = 1.5582
[2023-10-04 16:18:12] iter = 13980, loss = 1.6024
[2023-10-04 16:18:13] iter = 13990, loss = 1.5313
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 94135}
[2023-10-04 16:18:38] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.010105 train acc = 1.0000, test acc = 0.6293
[2023-10-04 16:19:03] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004344 train acc = 1.0000, test acc = 0.6246
[2023-10-04 16:19:27] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002363 train acc = 1.0000, test acc = 0.6300
[2023-10-04 16:19:52] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.017638 train acc = 1.0000, test acc = 0.6232
[2023-10-04 16:20:16] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.016043 train acc = 0.9980, test acc = 0.6315
[2023-10-04 16:20:41] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011513 train acc = 1.0000, test acc = 0.6267
[2023-10-04 16:21:06] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.020424 train acc = 0.9980, test acc = 0.6214
[2023-10-04 16:21:30] Evaluate_07: epoch = 1000 train time = 23 s train loss = 0.014333 train acc = 0.9980, test acc = 0.6229
[2023-10-04 16:21:55] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.012122 train acc = 1.0000, test acc = 0.6249
[2023-10-04 16:22:19] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.020138 train acc = 1.0000, test acc = 0.6287
[2023-10-04 16:22:44] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.012481 train acc = 1.0000, test acc = 0.6234
[2023-10-04 16:23:08] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.014896 train acc = 1.0000, test acc = 0.6217
[2023-10-04 16:23:33] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004578 train acc = 1.0000, test acc = 0.6291
[2023-10-04 16:23:58] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.017380 train acc = 0.9980, test acc = 0.6298
[2023-10-04 16:24:22] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.017451 train acc = 1.0000, test acc = 0.6244
[2023-10-04 16:24:47] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.014659 train acc = 1.0000, test acc = 0.6312
[2023-10-04 16:25:11] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.009378 train acc = 1.0000, test acc = 0.6179
[2023-10-04 16:25:36] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.013165 train acc = 1.0000, test acc = 0.6247
[2023-10-04 16:26:01] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.015036 train acc = 1.0000, test acc = 0.6261
[2023-10-04 16:26:25] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003985 train acc = 1.0000, test acc = 0.6260
Evaluate 20 random ConvNet, mean = 0.6259 std = 0.0036
-------------------------
[2023-10-04 16:26:25] iter = 14000, loss = 1.5811
[2023-10-04 16:26:26] iter = 14010, loss = 1.4770
[2023-10-04 16:26:27] iter = 14020, loss = 1.4331
[2023-10-04 16:26:28] iter = 14030, loss = 1.5195
[2023-10-04 16:26:29] iter = 14040, loss = 1.5043
[2023-10-04 16:26:30] iter = 14050, loss = 1.5339
[2023-10-04 16:26:31] iter = 14060, loss = 1.4189
[2023-10-04 16:26:32] iter = 14070, loss = 1.4686
[2023-10-04 16:26:33] iter = 14080, loss = 1.5088
[2023-10-04 16:26:34] iter = 14090, loss = 1.5411
[2023-10-04 16:26:35] iter = 14100, loss = 1.5770
[2023-10-04 16:26:36] iter = 14110, loss = 1.4429
[2023-10-04 16:26:37] iter = 14120, loss = 1.5861
[2023-10-04 16:26:38] iter = 14130, loss = 1.4117
[2023-10-04 16:26:39] iter = 14140, loss = 1.5782
[2023-10-04 16:26:39] iter = 14150, loss = 1.4848
[2023-10-04 16:26:40] iter = 14160, loss = 1.6379
[2023-10-04 16:26:41] iter = 14170, loss = 1.5739
[2023-10-04 16:26:42] iter = 14180, loss = 1.4234
[2023-10-04 16:26:43] iter = 14190, loss = 1.4184
[2023-10-04 16:26:44] iter = 14200, loss = 1.5679
[2023-10-04 16:26:45] iter = 14210, loss = 1.4943
[2023-10-04 16:26:46] iter = 14220, loss = 1.6061
[2023-10-04 16:26:47] iter = 14230, loss = 1.6321
[2023-10-04 16:26:48] iter = 14240, loss = 1.4765
[2023-10-04 16:26:49] iter = 14250, loss = 1.4686
[2023-10-04 16:26:49] iter = 14260, loss = 1.3673
[2023-10-04 16:26:50] iter = 14270, loss = 1.4780
[2023-10-04 16:26:51] iter = 14280, loss = 1.5042
[2023-10-04 16:26:52] iter = 14290, loss = 1.4705
[2023-10-04 16:26:53] iter = 14300, loss = 1.4371
[2023-10-04 16:26:54] iter = 14310, loss = 1.5265
[2023-10-04 16:26:55] iter = 14320, loss = 1.4670
[2023-10-04 16:26:56] iter = 14330, loss = 1.6066
[2023-10-04 16:26:57] iter = 14340, loss = 1.5688
[2023-10-04 16:26:58] iter = 14350, loss = 1.6495
[2023-10-04 16:26:59] iter = 14360, loss = 1.5480
[2023-10-04 16:26:59] iter = 14370, loss = 1.4324
[2023-10-04 16:27:00] iter = 14380, loss = 1.5354
[2023-10-04 16:27:01] iter = 14390, loss = 1.4847
[2023-10-04 16:27:02] iter = 14400, loss = 1.4670
[2023-10-04 16:27:03] iter = 14410, loss = 1.5283
[2023-10-04 16:27:04] iter = 14420, loss = 1.5209
[2023-10-04 16:27:05] iter = 14430, loss = 1.5234
[2023-10-04 16:27:06] iter = 14440, loss = 1.6036
[2023-10-04 16:27:07] iter = 14450, loss = 1.4970
[2023-10-04 16:27:08] iter = 14460, loss = 1.4884
[2023-10-04 16:27:08] iter = 14470, loss = 1.6078
[2023-10-04 16:27:09] iter = 14480, loss = 1.5293
[2023-10-04 16:27:10] iter = 14490, loss = 1.5032
[2023-10-04 16:27:11] iter = 14500, loss = 1.5118
[2023-10-04 16:27:12] iter = 14510, loss = 1.4557
[2023-10-04 16:27:13] iter = 14520, loss = 1.5010
[2023-10-04 16:27:14] iter = 14530, loss = 1.5561
[2023-10-04 16:27:15] iter = 14540, loss = 1.6835
[2023-10-04 16:27:16] iter = 14550, loss = 1.4197
[2023-10-04 16:27:17] iter = 14560, loss = 1.5513
[2023-10-04 16:27:18] iter = 14570, loss = 1.5635
[2023-10-04 16:27:18] iter = 14580, loss = 1.7141
[2023-10-04 16:27:19] iter = 14590, loss = 1.4196
[2023-10-04 16:27:20] iter = 14600, loss = 1.5566
[2023-10-04 16:27:21] iter = 14610, loss = 1.4820
[2023-10-04 16:27:22] iter = 14620, loss = 1.5489
[2023-10-04 16:27:23] iter = 14630, loss = 1.4357
[2023-10-04 16:27:24] iter = 14640, loss = 1.4559
[2023-10-04 16:27:25] iter = 14650, loss = 1.4778
[2023-10-04 16:27:26] iter = 14660, loss = 1.5283
[2023-10-04 16:27:27] iter = 14670, loss = 1.4333
[2023-10-04 16:27:28] iter = 14680, loss = 1.5600
[2023-10-04 16:27:29] iter = 14690, loss = 1.5759
[2023-10-04 16:27:29] iter = 14700, loss = 1.5037
[2023-10-04 16:27:30] iter = 14710, loss = 1.5144
[2023-10-04 16:27:31] iter = 14720, loss = 1.4943
[2023-10-04 16:27:32] iter = 14730, loss = 1.3647
[2023-10-04 16:27:33] iter = 14740, loss = 1.4867
[2023-10-04 16:27:34] iter = 14750, loss = 1.4793
[2023-10-04 16:27:35] iter = 14760, loss = 1.5362
[2023-10-04 16:27:36] iter = 14770, loss = 1.6852
[2023-10-04 16:27:37] iter = 14780, loss = 1.5265
[2023-10-04 16:27:38] iter = 14790, loss = 1.5802
[2023-10-04 16:27:39] iter = 14800, loss = 1.4450
[2023-10-04 16:27:40] iter = 14810, loss = 1.5342
[2023-10-04 16:27:41] iter = 14820, loss = 1.5154
[2023-10-04 16:27:41] iter = 14830, loss = 1.5535
[2023-10-04 16:27:42] iter = 14840, loss = 1.5807
[2023-10-04 16:27:43] iter = 14850, loss = 1.5361
[2023-10-04 16:27:44] iter = 14860, loss = 1.6035
[2023-10-04 16:27:45] iter = 14870, loss = 1.4253
[2023-10-04 16:27:46] iter = 14880, loss = 1.4272
[2023-10-04 16:27:47] iter = 14890, loss = 1.4238
[2023-10-04 16:27:48] iter = 14900, loss = 1.5280
[2023-10-04 16:27:49] iter = 14910, loss = 1.5929
[2023-10-04 16:27:50] iter = 14920, loss = 1.4439
[2023-10-04 16:27:51] iter = 14930, loss = 1.5403
[2023-10-04 16:27:52] iter = 14940, loss = 1.5384
[2023-10-04 16:27:52] iter = 14950, loss = 1.4985
[2023-10-04 16:27:53] iter = 14960, loss = 1.5254
[2023-10-04 16:27:54] iter = 14970, loss = 1.5046
[2023-10-04 16:27:55] iter = 14980, loss = 1.5712
[2023-10-04 16:27:56] iter = 14990, loss = 1.5497
[2023-10-04 16:27:57] iter = 15000, loss = 1.6364
[2023-10-04 16:27:58] iter = 15010, loss = 1.3986
[2023-10-04 16:27:59] iter = 15020, loss = 1.5841
[2023-10-04 16:28:00] iter = 15030, loss = 1.6325
[2023-10-04 16:28:01] iter = 15040, loss = 1.6226
[2023-10-04 16:28:02] iter = 15050, loss = 1.6383
[2023-10-04 16:28:03] iter = 15060, loss = 1.4186
[2023-10-04 16:28:04] iter = 15070, loss = 1.4395
[2023-10-04 16:28:05] iter = 15080, loss = 1.4724
[2023-10-04 16:28:06] iter = 15090, loss = 1.3968
[2023-10-04 16:28:07] iter = 15100, loss = 1.4506
[2023-10-04 16:28:07] iter = 15110, loss = 1.5422
[2023-10-04 16:28:08] iter = 15120, loss = 1.5961
[2023-10-04 16:28:09] iter = 15130, loss = 1.4548
[2023-10-04 16:28:10] iter = 15140, loss = 1.4064
[2023-10-04 16:28:11] iter = 15150, loss = 1.5554
[2023-10-04 16:28:12] iter = 15160, loss = 1.5242
[2023-10-04 16:28:13] iter = 15170, loss = 1.6501
[2023-10-04 16:28:14] iter = 15180, loss = 1.6149
[2023-10-04 16:28:15] iter = 15190, loss = 1.5895
[2023-10-04 16:28:16] iter = 15200, loss = 1.5445
[2023-10-04 16:28:16] iter = 15210, loss = 1.4324
[2023-10-04 16:28:18] iter = 15220, loss = 1.6593
[2023-10-04 16:28:18] iter = 15230, loss = 1.5812
[2023-10-04 16:28:19] iter = 15240, loss = 1.4966
[2023-10-04 16:28:20] iter = 15250, loss = 1.5611
[2023-10-04 16:28:21] iter = 15260, loss = 1.4570
[2023-10-04 16:28:22] iter = 15270, loss = 1.5212
[2023-10-04 16:28:23] iter = 15280, loss = 1.5455
[2023-10-04 16:28:24] iter = 15290, loss = 1.4189
[2023-10-04 16:28:25] iter = 15300, loss = 1.4237
[2023-10-04 16:28:26] iter = 15310, loss = 1.5573
[2023-10-04 16:28:27] iter = 15320, loss = 1.4624
[2023-10-04 16:28:28] iter = 15330, loss = 1.6329
[2023-10-04 16:28:29] iter = 15340, loss = 1.5629
[2023-10-04 16:28:30] iter = 15350, loss = 1.5283
[2023-10-04 16:28:30] iter = 15360, loss = 1.5603
[2023-10-04 16:28:31] iter = 15370, loss = 1.5827
[2023-10-04 16:28:32] iter = 15380, loss = 1.4959
[2023-10-04 16:28:33] iter = 15390, loss = 1.4706
[2023-10-04 16:28:34] iter = 15400, loss = 1.4629
[2023-10-04 16:28:35] iter = 15410, loss = 1.4886
[2023-10-04 16:28:36] iter = 15420, loss = 1.4145
[2023-10-04 16:28:37] iter = 15430, loss = 1.4692
[2023-10-04 16:28:38] iter = 15440, loss = 1.5441
[2023-10-04 16:28:39] iter = 15450, loss = 1.4641
[2023-10-04 16:28:40] iter = 15460, loss = 1.5953
[2023-10-04 16:28:41] iter = 15470, loss = 1.4012
[2023-10-04 16:28:41] iter = 15480, loss = 1.5083
[2023-10-04 16:28:42] iter = 15490, loss = 1.4105
[2023-10-04 16:28:43] iter = 15500, loss = 1.4130
[2023-10-04 16:28:44] iter = 15510, loss = 1.5382
[2023-10-04 16:28:45] iter = 15520, loss = 1.5688
[2023-10-04 16:28:46] iter = 15530, loss = 1.5786
[2023-10-04 16:28:47] iter = 15540, loss = 1.4107
[2023-10-04 16:28:48] iter = 15550, loss = 1.5519
[2023-10-04 16:28:49] iter = 15560, loss = 1.4783
[2023-10-04 16:28:50] iter = 15570, loss = 1.5172
[2023-10-04 16:28:50] iter = 15580, loss = 1.4149
[2023-10-04 16:28:51] iter = 15590, loss = 1.4773
[2023-10-04 16:28:52] iter = 15600, loss = 1.5096
[2023-10-04 16:28:53] iter = 15610, loss = 1.4332
[2023-10-04 16:28:54] iter = 15620, loss = 1.5931
[2023-10-04 16:28:55] iter = 15630, loss = 1.4347
[2023-10-04 16:28:56] iter = 15640, loss = 1.5440
[2023-10-04 16:28:57] iter = 15650, loss = 1.5301
[2023-10-04 16:28:58] iter = 15660, loss = 1.5234
[2023-10-04 16:28:59] iter = 15670, loss = 1.3524
[2023-10-04 16:29:00] iter = 15680, loss = 1.5896
[2023-10-04 16:29:01] iter = 15690, loss = 1.5215
[2023-10-04 16:29:01] iter = 15700, loss = 1.4585
[2023-10-04 16:29:02] iter = 15710, loss = 1.5119
[2023-10-04 16:29:03] iter = 15720, loss = 1.6546
[2023-10-04 16:29:04] iter = 15730, loss = 1.4318
[2023-10-04 16:29:05] iter = 15740, loss = 1.5590
[2023-10-04 16:29:06] iter = 15750, loss = 1.4333
[2023-10-04 16:29:07] iter = 15760, loss = 1.4281
[2023-10-04 16:29:08] iter = 15770, loss = 1.6447
[2023-10-04 16:29:09] iter = 15780, loss = 1.5813
[2023-10-04 16:29:10] iter = 15790, loss = 1.4619
[2023-10-04 16:29:10] iter = 15800, loss = 1.6046
[2023-10-04 16:29:11] iter = 15810, loss = 1.4262
[2023-10-04 16:29:12] iter = 15820, loss = 1.6249
[2023-10-04 16:29:13] iter = 15830, loss = 1.4694
[2023-10-04 16:29:14] iter = 15840, loss = 1.3973
[2023-10-04 16:29:15] iter = 15850, loss = 1.6332
[2023-10-04 16:29:16] iter = 15860, loss = 1.4424
[2023-10-04 16:29:17] iter = 15870, loss = 1.3747
[2023-10-04 16:29:18] iter = 15880, loss = 1.4825
[2023-10-04 16:29:19] iter = 15890, loss = 1.3728
[2023-10-04 16:29:20] iter = 15900, loss = 1.4994
[2023-10-04 16:29:20] iter = 15910, loss = 1.4548
[2023-10-04 16:29:21] iter = 15920, loss = 1.4136
[2023-10-04 16:29:22] iter = 15930, loss = 1.4488
[2023-10-04 16:29:23] iter = 15940, loss = 1.5314
[2023-10-04 16:29:24] iter = 15950, loss = 1.4861
[2023-10-04 16:29:25] iter = 15960, loss = 1.5443
[2023-10-04 16:29:26] iter = 15970, loss = 1.6389
[2023-10-04 16:29:27] iter = 15980, loss = 1.4063
[2023-10-04 16:29:28] iter = 15990, loss = 1.6224
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 68952}
[2023-10-04 16:29:53] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.022785 train acc = 0.9980, test acc = 0.6340
[2023-10-04 16:30:18] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.014208 train acc = 1.0000, test acc = 0.6305
[2023-10-04 16:30:42] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.013898 train acc = 0.9980, test acc = 0.6300
[2023-10-04 16:31:06] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.019940 train acc = 0.9960, test acc = 0.6249
[2023-10-04 16:31:31] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003436 train acc = 1.0000, test acc = 0.6314
[2023-10-04 16:31:55] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.018413 train acc = 0.9980, test acc = 0.6266
[2023-10-04 16:32:20] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002891 train acc = 1.0000, test acc = 0.6320
[2023-10-04 16:32:44] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004372 train acc = 1.0000, test acc = 0.6245
[2023-10-04 16:33:09] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002567 train acc = 1.0000, test acc = 0.6229
[2023-10-04 16:33:33] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.014417 train acc = 0.9980, test acc = 0.6251
[2023-10-04 16:33:58] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.018093 train acc = 1.0000, test acc = 0.6234
[2023-10-04 16:34:23] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.010326 train acc = 1.0000, test acc = 0.6296
[2023-10-04 16:34:47] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004459 train acc = 1.0000, test acc = 0.6197
[2023-10-04 16:35:12] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.032934 train acc = 1.0000, test acc = 0.6265
[2023-10-04 16:35:36] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.006096 train acc = 1.0000, test acc = 0.6260
[2023-10-04 16:36:01] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003944 train acc = 1.0000, test acc = 0.6263
[2023-10-04 16:36:25] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.012991 train acc = 1.0000, test acc = 0.6230
[2023-10-04 16:36:50] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.005667 train acc = 1.0000, test acc = 0.6310
[2023-10-04 16:37:14] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.028843 train acc = 1.0000, test acc = 0.6246
[2023-10-04 16:37:39] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004016 train acc = 1.0000, test acc = 0.6308
Evaluate 20 random ConvNet, mean = 0.6271 std = 0.0037
-------------------------
[2023-10-04 16:37:39] iter = 16000, loss = 1.4432
[2023-10-04 16:37:40] iter = 16010, loss = 1.3941
[2023-10-04 16:37:41] iter = 16020, loss = 1.5561
[2023-10-04 16:37:42] iter = 16030, loss = 1.5591
[2023-10-04 16:37:43] iter = 16040, loss = 1.6397
[2023-10-04 16:37:44] iter = 16050, loss = 1.5138
[2023-10-04 16:37:45] iter = 16060, loss = 1.5207
[2023-10-04 16:37:46] iter = 16070, loss = 1.5396
[2023-10-04 16:37:46] iter = 16080, loss = 1.4981
[2023-10-04 16:37:47] iter = 16090, loss = 1.5350
[2023-10-04 16:37:48] iter = 16100, loss = 1.7377
[2023-10-04 16:37:49] iter = 16110, loss = 1.4388
[2023-10-04 16:37:50] iter = 16120, loss = 1.4966
[2023-10-04 16:37:51] iter = 16130, loss = 1.5442
[2023-10-04 16:37:52] iter = 16140, loss = 1.5656
[2023-10-04 16:37:53] iter = 16150, loss = 1.5490
[2023-10-04 16:37:54] iter = 16160, loss = 1.4826
[2023-10-04 16:37:55] iter = 16170, loss = 1.6585
[2023-10-04 16:37:56] iter = 16180, loss = 1.4198
[2023-10-04 16:37:57] iter = 16190, loss = 1.5055
[2023-10-04 16:37:57] iter = 16200, loss = 1.4646
[2023-10-04 16:37:58] iter = 16210, loss = 1.6095
[2023-10-04 16:37:59] iter = 16220, loss = 1.3995
[2023-10-04 16:38:00] iter = 16230, loss = 1.4514
[2023-10-04 16:38:01] iter = 16240, loss = 1.4899
[2023-10-04 16:38:02] iter = 16250, loss = 1.4610
[2023-10-04 16:38:03] iter = 16260, loss = 1.5994
[2023-10-04 16:38:04] iter = 16270, loss = 1.6207
[2023-10-04 16:38:05] iter = 16280, loss = 1.4676
[2023-10-04 16:38:06] iter = 16290, loss = 1.5152
[2023-10-04 16:38:07] iter = 16300, loss = 1.6592
[2023-10-04 16:38:08] iter = 16310, loss = 1.4672
[2023-10-04 16:38:09] iter = 16320, loss = 1.5161
[2023-10-04 16:38:10] iter = 16330, loss = 1.4797
[2023-10-04 16:38:11] iter = 16340, loss = 1.5297
[2023-10-04 16:38:12] iter = 16350, loss = 1.5036
[2023-10-04 16:38:12] iter = 16360, loss = 1.5167
[2023-10-04 16:38:13] iter = 16370, loss = 1.4785
[2023-10-04 16:38:14] iter = 16380, loss = 1.7093
[2023-10-04 16:38:15] iter = 16390, loss = 1.5011
[2023-10-04 16:38:16] iter = 16400, loss = 1.3735
[2023-10-04 16:38:17] iter = 16410, loss = 1.4254
[2023-10-04 16:38:18] iter = 16420, loss = 1.5139
[2023-10-04 16:38:19] iter = 16430, loss = 1.4833
[2023-10-04 16:38:20] iter = 16440, loss = 1.5465
[2023-10-04 16:38:21] iter = 16450, loss = 1.5791
[2023-10-04 16:38:22] iter = 16460, loss = 1.4393
[2023-10-04 16:38:23] iter = 16470, loss = 1.6288
[2023-10-04 16:38:23] iter = 16480, loss = 1.4529
[2023-10-04 16:38:24] iter = 16490, loss = 1.5509
[2023-10-04 16:38:25] iter = 16500, loss = 1.3998
[2023-10-04 16:38:26] iter = 16510, loss = 1.5621
[2023-10-04 16:38:27] iter = 16520, loss = 1.5773
[2023-10-04 16:38:28] iter = 16530, loss = 1.5323
[2023-10-04 16:38:29] iter = 16540, loss = 1.6069
[2023-10-04 16:38:30] iter = 16550, loss = 1.5757
[2023-10-04 16:38:31] iter = 16560, loss = 1.4933
[2023-10-04 16:38:32] iter = 16570, loss = 1.4203
[2023-10-04 16:38:33] iter = 16580, loss = 1.4706
[2023-10-04 16:38:34] iter = 16590, loss = 1.5578
[2023-10-04 16:38:35] iter = 16600, loss = 1.3709
[2023-10-04 16:38:35] iter = 16610, loss = 1.5435
[2023-10-04 16:38:36] iter = 16620, loss = 1.4189
[2023-10-04 16:38:37] iter = 16630, loss = 1.4660
[2023-10-04 16:38:38] iter = 16640, loss = 1.6534
[2023-10-04 16:38:39] iter = 16650, loss = 1.4519
[2023-10-04 16:38:40] iter = 16660, loss = 1.5732
[2023-10-04 16:38:41] iter = 16670, loss = 1.5861
[2023-10-04 16:38:42] iter = 16680, loss = 1.5258
[2023-10-04 16:38:43] iter = 16690, loss = 1.6097
[2023-10-04 16:38:44] iter = 16700, loss = 1.5269
[2023-10-04 16:38:45] iter = 16710, loss = 1.6270
[2023-10-04 16:38:46] iter = 16720, loss = 1.3837
[2023-10-04 16:38:47] iter = 16730, loss = 1.4490
[2023-10-04 16:38:48] iter = 16740, loss = 1.5493
[2023-10-04 16:38:49] iter = 16750, loss = 1.6320
[2023-10-04 16:38:50] iter = 16760, loss = 1.4079
[2023-10-04 16:38:50] iter = 16770, loss = 1.5640
[2023-10-04 16:38:51] iter = 16780, loss = 1.7240
[2023-10-04 16:38:52] iter = 16790, loss = 1.5632
[2023-10-04 16:38:53] iter = 16800, loss = 1.6082
[2023-10-04 16:38:54] iter = 16810, loss = 1.4276
[2023-10-04 16:38:55] iter = 16820, loss = 1.4153
[2023-10-04 16:38:56] iter = 16830, loss = 1.4358
[2023-10-04 16:38:57] iter = 16840, loss = 1.5577
[2023-10-04 16:38:58] iter = 16850, loss = 1.4987
[2023-10-04 16:38:59] iter = 16860, loss = 1.4221
[2023-10-04 16:39:00] iter = 16870, loss = 1.4585
[2023-10-04 16:39:01] iter = 16880, loss = 1.6605
[2023-10-04 16:39:01] iter = 16890, loss = 1.3444
[2023-10-04 16:39:02] iter = 16900, loss = 1.4933
[2023-10-04 16:39:03] iter = 16910, loss = 1.6232
[2023-10-04 16:39:04] iter = 16920, loss = 1.4896
[2023-10-04 16:39:05] iter = 16930, loss = 1.5914
[2023-10-04 16:39:06] iter = 16940, loss = 1.5031
[2023-10-04 16:39:07] iter = 16950, loss = 1.4952
[2023-10-04 16:39:08] iter = 16960, loss = 1.4272
[2023-10-04 16:39:09] iter = 16970, loss = 1.4966
[2023-10-04 16:39:10] iter = 16980, loss = 1.4020
[2023-10-04 16:39:11] iter = 16990, loss = 1.5861
[2023-10-04 16:39:12] iter = 17000, loss = 1.5164
[2023-10-04 16:39:13] iter = 17010, loss = 1.6100
[2023-10-04 16:39:14] iter = 17020, loss = 1.5623
[2023-10-04 16:39:14] iter = 17030, loss = 1.5107
[2023-10-04 16:39:15] iter = 17040, loss = 1.4932
[2023-10-04 16:39:16] iter = 17050, loss = 1.5220
[2023-10-04 16:39:17] iter = 17060, loss = 1.6725
[2023-10-04 16:39:18] iter = 17070, loss = 1.3718
[2023-10-04 16:39:19] iter = 17080, loss = 1.4368
[2023-10-04 16:39:20] iter = 17090, loss = 1.4485
[2023-10-04 16:39:21] iter = 17100, loss = 1.4419
[2023-10-04 16:39:22] iter = 17110, loss = 1.3620
[2023-10-04 16:39:23] iter = 17120, loss = 1.3901
[2023-10-04 16:39:24] iter = 17130, loss = 1.4271
[2023-10-04 16:39:24] iter = 17140, loss = 1.4352
[2023-10-04 16:39:25] iter = 17150, loss = 1.5707
[2023-10-04 16:39:26] iter = 17160, loss = 1.4461
[2023-10-04 16:39:27] iter = 17170, loss = 1.4795
[2023-10-04 16:39:28] iter = 17180, loss = 1.4442
[2023-10-04 16:39:29] iter = 17190, loss = 1.4632
[2023-10-04 16:39:30] iter = 17200, loss = 1.4983
[2023-10-04 16:39:31] iter = 17210, loss = 1.6057
[2023-10-04 16:39:32] iter = 17220, loss = 1.4979
[2023-10-04 16:39:33] iter = 17230, loss = 1.5678
[2023-10-04 16:39:34] iter = 17240, loss = 1.5656
[2023-10-04 16:39:35] iter = 17250, loss = 1.4177
[2023-10-04 16:39:36] iter = 17260, loss = 1.3811
[2023-10-04 16:39:37] iter = 17270, loss = 1.5000
[2023-10-04 16:39:38] iter = 17280, loss = 1.4472
[2023-10-04 16:39:39] iter = 17290, loss = 1.4258
[2023-10-04 16:39:40] iter = 17300, loss = 1.6029
[2023-10-04 16:39:41] iter = 17310, loss = 1.5794
[2023-10-04 16:39:42] iter = 17320, loss = 1.6725
[2023-10-04 16:39:42] iter = 17330, loss = 1.3944
[2023-10-04 16:39:43] iter = 17340, loss = 1.4005
[2023-10-04 16:39:44] iter = 17350, loss = 1.3439
[2023-10-04 16:39:45] iter = 17360, loss = 1.6457
[2023-10-04 16:39:46] iter = 17370, loss = 1.5579
[2023-10-04 16:39:47] iter = 17380, loss = 1.6025
[2023-10-04 16:39:48] iter = 17390, loss = 1.3896
[2023-10-04 16:39:49] iter = 17400, loss = 1.5051
[2023-10-04 16:39:50] iter = 17410, loss = 1.4259
[2023-10-04 16:39:51] iter = 17420, loss = 1.5256
[2023-10-04 16:39:52] iter = 17430, loss = 1.4895
[2023-10-04 16:39:53] iter = 17440, loss = 1.3783
[2023-10-04 16:39:54] iter = 17450, loss = 1.4930
[2023-10-04 16:39:55] iter = 17460, loss = 1.4892
[2023-10-04 16:39:55] iter = 17470, loss = 1.5574
[2023-10-04 16:39:57] iter = 17480, loss = 1.5430
[2023-10-04 16:39:57] iter = 17490, loss = 1.6172
[2023-10-04 16:39:58] iter = 17500, loss = 1.6839
[2023-10-04 16:39:59] iter = 17510, loss = 1.5951
[2023-10-04 16:40:00] iter = 17520, loss = 1.3940
[2023-10-04 16:40:01] iter = 17530, loss = 1.5386
[2023-10-04 16:40:02] iter = 17540, loss = 1.4495
[2023-10-04 16:40:03] iter = 17550, loss = 1.5547
[2023-10-04 16:40:04] iter = 17560, loss = 1.5818
[2023-10-04 16:40:05] iter = 17570, loss = 1.5348
[2023-10-04 16:40:06] iter = 17580, loss = 1.4823
[2023-10-04 16:40:07] iter = 17590, loss = 1.3977
[2023-10-04 16:40:08] iter = 17600, loss = 1.6052
[2023-10-04 16:40:09] iter = 17610, loss = 1.4923
[2023-10-04 16:40:10] iter = 17620, loss = 1.3564
[2023-10-04 16:40:11] iter = 17630, loss = 1.3461
[2023-10-04 16:40:11] iter = 17640, loss = 1.4573
[2023-10-04 16:40:13] iter = 17650, loss = 1.5072
[2023-10-04 16:40:13] iter = 17660, loss = 1.3682
[2023-10-04 16:40:14] iter = 17670, loss = 1.4901
[2023-10-04 16:40:15] iter = 17680, loss = 1.4907
[2023-10-04 16:40:16] iter = 17690, loss = 1.5130
[2023-10-04 16:40:17] iter = 17700, loss = 1.6052
[2023-10-04 16:40:18] iter = 17710, loss = 1.2946
[2023-10-04 16:40:19] iter = 17720, loss = 1.3556
[2023-10-04 16:40:20] iter = 17730, loss = 1.5647
[2023-10-04 16:40:21] iter = 17740, loss = 1.5412
[2023-10-04 16:40:22] iter = 17750, loss = 1.5371
[2023-10-04 16:40:23] iter = 17760, loss = 1.4928
[2023-10-04 16:40:24] iter = 17770, loss = 1.3943
[2023-10-04 16:40:24] iter = 17780, loss = 1.5678
[2023-10-04 16:40:25] iter = 17790, loss = 1.5283
[2023-10-04 16:40:26] iter = 17800, loss = 1.6007
[2023-10-04 16:40:27] iter = 17810, loss = 1.3555
[2023-10-04 16:40:28] iter = 17820, loss = 1.4684
[2023-10-04 16:40:29] iter = 17830, loss = 1.4655
[2023-10-04 16:40:30] iter = 17840, loss = 1.3875
[2023-10-04 16:40:31] iter = 17850, loss = 1.5193
[2023-10-04 16:40:32] iter = 17860, loss = 1.4922
[2023-10-04 16:40:33] iter = 17870, loss = 1.5177
[2023-10-04 16:40:34] iter = 17880, loss = 1.3759
[2023-10-04 16:40:35] iter = 17890, loss = 1.5680
[2023-10-04 16:40:36] iter = 17900, loss = 1.5583
[2023-10-04 16:40:37] iter = 17910, loss = 1.4578
[2023-10-04 16:40:37] iter = 17920, loss = 1.4667
[2023-10-04 16:40:38] iter = 17930, loss = 1.5652
[2023-10-04 16:40:39] iter = 17940, loss = 1.5230
[2023-10-04 16:40:41] iter = 17950, loss = 1.5720
[2023-10-04 16:40:41] iter = 17960, loss = 1.4210
[2023-10-04 16:40:42] iter = 17970, loss = 1.4914
[2023-10-04 16:40:43] iter = 17980, loss = 1.4473
[2023-10-04 16:40:44] iter = 17990, loss = 1.5563
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 45429}
[2023-10-04 16:41:10] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.014646 train acc = 0.9980, test acc = 0.6271
[2023-10-04 16:41:34] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003424 train acc = 1.0000, test acc = 0.6269
[2023-10-04 16:41:59] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.017651 train acc = 1.0000, test acc = 0.6241
[2023-10-04 16:42:24] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.016278 train acc = 1.0000, test acc = 0.6242
[2023-10-04 16:42:48] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003613 train acc = 1.0000, test acc = 0.6220
[2023-10-04 16:43:13] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003509 train acc = 1.0000, test acc = 0.6206
[2023-10-04 16:43:37] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.013080 train acc = 1.0000, test acc = 0.6285
[2023-10-04 16:44:02] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.007148 train acc = 0.9980, test acc = 0.6309
[2023-10-04 16:44:26] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014868 train acc = 1.0000, test acc = 0.6267
[2023-10-04 16:44:51] Evaluate_09: epoch = 1000 train time = 23 s train loss = 0.006036 train acc = 1.0000, test acc = 0.6217
[2023-10-04 16:45:16] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004133 train acc = 1.0000, test acc = 0.6334
[2023-10-04 16:45:40] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002463 train acc = 1.0000, test acc = 0.6171
[2023-10-04 16:46:05] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.020194 train acc = 0.9980, test acc = 0.6266
[2023-10-04 16:46:30] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.014755 train acc = 1.0000, test acc = 0.6225
[2023-10-04 16:46:54] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004234 train acc = 1.0000, test acc = 0.6244
[2023-10-04 16:47:19] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002650 train acc = 1.0000, test acc = 0.6259
[2023-10-04 16:47:43] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003815 train acc = 1.0000, test acc = 0.6292
[2023-10-04 16:48:08] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.020193 train acc = 0.9980, test acc = 0.6315
[2023-10-04 16:48:32] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.022803 train acc = 0.9980, test acc = 0.6251
[2023-10-04 16:48:57] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004877 train acc = 1.0000, test acc = 0.6260
Evaluate 20 random ConvNet, mean = 0.6257 std = 0.0038
-------------------------
[2023-10-04 16:48:57] iter = 18000, loss = 1.3809
[2023-10-04 16:48:58] iter = 18010, loss = 1.5131
[2023-10-04 16:48:59] iter = 18020, loss = 1.5163
[2023-10-04 16:49:00] iter = 18030, loss = 1.3990
[2023-10-04 16:49:01] iter = 18040, loss = 1.4117
[2023-10-04 16:49:01] iter = 18050, loss = 1.3466
[2023-10-04 16:49:02] iter = 18060, loss = 1.5376
[2023-10-04 16:49:03] iter = 18070, loss = 1.4555
[2023-10-04 16:49:04] iter = 18080, loss = 1.5405
[2023-10-04 16:49:05] iter = 18090, loss = 1.4137
[2023-10-04 16:49:06] iter = 18100, loss = 1.4693
[2023-10-04 16:49:07] iter = 18110, loss = 1.3364
[2023-10-04 16:49:08] iter = 18120, loss = 1.4161
[2023-10-04 16:49:09] iter = 18130, loss = 1.4065
[2023-10-04 16:49:10] iter = 18140, loss = 1.4700
[2023-10-04 16:49:11] iter = 18150, loss = 1.4128
[2023-10-04 16:49:11] iter = 18160, loss = 1.3703
[2023-10-04 16:49:12] iter = 18170, loss = 1.3933
[2023-10-04 16:49:13] iter = 18180, loss = 1.3711
[2023-10-04 16:49:14] iter = 18190, loss = 1.4155
[2023-10-04 16:49:15] iter = 18200, loss = 1.5367
[2023-10-04 16:49:16] iter = 18210, loss = 1.5868
[2023-10-04 16:49:17] iter = 18220, loss = 1.5631
[2023-10-04 16:49:18] iter = 18230, loss = 1.4251
[2023-10-04 16:49:19] iter = 18240, loss = 1.4257
[2023-10-04 16:49:20] iter = 18250, loss = 1.5108
[2023-10-04 16:49:21] iter = 18260, loss = 1.4038
[2023-10-04 16:49:22] iter = 18270, loss = 1.4452
[2023-10-04 16:49:23] iter = 18280, loss = 1.3843
[2023-10-04 16:49:24] iter = 18290, loss = 1.4256
[2023-10-04 16:49:25] iter = 18300, loss = 1.6420
[2023-10-04 16:49:25] iter = 18310, loss = 1.3654
[2023-10-04 16:49:26] iter = 18320, loss = 1.3761
[2023-10-04 16:49:27] iter = 18330, loss = 1.4508
[2023-10-04 16:49:28] iter = 18340, loss = 1.5521
[2023-10-04 16:49:29] iter = 18350, loss = 1.3283
[2023-10-04 16:49:30] iter = 18360, loss = 1.4447
[2023-10-04 16:49:31] iter = 18370, loss = 1.5242
[2023-10-04 16:49:32] iter = 18380, loss = 1.5703
[2023-10-04 16:49:33] iter = 18390, loss = 1.6083
[2023-10-04 16:49:34] iter = 18400, loss = 1.4988
[2023-10-04 16:49:35] iter = 18410, loss = 1.3677
[2023-10-04 16:49:36] iter = 18420, loss = 1.5731
[2023-10-04 16:49:37] iter = 18430, loss = 1.4301
[2023-10-04 16:49:37] iter = 18440, loss = 1.5138
[2023-10-04 16:49:38] iter = 18450, loss = 1.5939
[2023-10-04 16:49:39] iter = 18460, loss = 1.3969
[2023-10-04 16:49:40] iter = 18470, loss = 1.4976
[2023-10-04 16:49:41] iter = 18480, loss = 1.5851
[2023-10-04 16:49:42] iter = 18490, loss = 1.5122
[2023-10-04 16:49:43] iter = 18500, loss = 1.4397
[2023-10-04 16:49:44] iter = 18510, loss = 1.5452
[2023-10-04 16:49:45] iter = 18520, loss = 1.4899
[2023-10-04 16:49:46] iter = 18530, loss = 1.3886
[2023-10-04 16:49:47] iter = 18540, loss = 1.7923
[2023-10-04 16:49:48] iter = 18550, loss = 1.4542
[2023-10-04 16:49:49] iter = 18560, loss = 1.5439
[2023-10-04 16:49:50] iter = 18570, loss = 1.3467
[2023-10-04 16:49:51] iter = 18580, loss = 1.4802
[2023-10-04 16:49:51] iter = 18590, loss = 1.5585
[2023-10-04 16:49:52] iter = 18600, loss = 1.4864
[2023-10-04 16:49:53] iter = 18610, loss = 1.4654
[2023-10-04 16:49:54] iter = 18620, loss = 1.5373
[2023-10-04 16:49:55] iter = 18630, loss = 1.4437
[2023-10-04 16:49:56] iter = 18640, loss = 1.4912
[2023-10-04 16:49:57] iter = 18650, loss = 1.4785
[2023-10-04 16:49:58] iter = 18660, loss = 1.5455
[2023-10-04 16:49:59] iter = 18670, loss = 1.4931
[2023-10-04 16:50:00] iter = 18680, loss = 1.5156
[2023-10-04 16:50:00] iter = 18690, loss = 1.4201
[2023-10-04 16:50:01] iter = 18700, loss = 1.4140
[2023-10-04 16:50:02] iter = 18710, loss = 1.4055
[2023-10-04 16:50:03] iter = 18720, loss = 1.4657
[2023-10-04 16:50:04] iter = 18730, loss = 1.4897
[2023-10-04 16:50:05] iter = 18740, loss = 1.5751
[2023-10-04 16:50:06] iter = 18750, loss = 1.5205
[2023-10-04 16:50:07] iter = 18760, loss = 1.4727
[2023-10-04 16:50:08] iter = 18770, loss = 1.4099
[2023-10-04 16:50:09] iter = 18780, loss = 1.3870
[2023-10-04 16:50:10] iter = 18790, loss = 1.4429
[2023-10-04 16:50:11] iter = 18800, loss = 1.4530
[2023-10-04 16:50:12] iter = 18810, loss = 1.6638
[2023-10-04 16:50:12] iter = 18820, loss = 1.4256
[2023-10-04 16:50:13] iter = 18830, loss = 1.5461
[2023-10-04 16:50:14] iter = 18840, loss = 1.5223
[2023-10-04 16:50:15] iter = 18850, loss = 1.5885
[2023-10-04 16:50:16] iter = 18860, loss = 1.4280
[2023-10-04 16:50:17] iter = 18870, loss = 1.5952
[2023-10-04 16:50:18] iter = 18880, loss = 1.3985
[2023-10-04 16:50:19] iter = 18890, loss = 1.4387
[2023-10-04 16:50:20] iter = 18900, loss = 1.4747
[2023-10-04 16:50:21] iter = 18910, loss = 1.4483
[2023-10-04 16:50:22] iter = 18920, loss = 1.4037
[2023-10-04 16:50:23] iter = 18930, loss = 1.4959
[2023-10-04 16:50:23] iter = 18940, loss = 1.5843
[2023-10-04 16:50:24] iter = 18950, loss = 1.5168
[2023-10-04 16:50:25] iter = 18960, loss = 1.5166
[2023-10-04 16:50:26] iter = 18970, loss = 1.6278
[2023-10-04 16:50:27] iter = 18980, loss = 1.3784
[2023-10-04 16:50:28] iter = 18990, loss = 1.4735
[2023-10-04 16:50:29] iter = 19000, loss = 1.5642
[2023-10-04 16:50:30] iter = 19010, loss = 1.5787
[2023-10-04 16:50:31] iter = 19020, loss = 1.4774
[2023-10-04 16:50:32] iter = 19030, loss = 1.6228
[2023-10-04 16:50:33] iter = 19040, loss = 1.5563
[2023-10-04 16:50:34] iter = 19050, loss = 1.5692
[2023-10-04 16:50:34] iter = 19060, loss = 1.4802
[2023-10-04 16:50:35] iter = 19070, loss = 1.5685
[2023-10-04 16:50:36] iter = 19080, loss = 1.5133
[2023-10-04 16:50:37] iter = 19090, loss = 1.5244
[2023-10-04 16:50:38] iter = 19100, loss = 1.4651
[2023-10-04 16:50:39] iter = 19110, loss = 1.4701
[2023-10-04 16:50:40] iter = 19120, loss = 1.4786
[2023-10-04 16:50:41] iter = 19130, loss = 1.5693
[2023-10-04 16:50:42] iter = 19140, loss = 1.6750
[2023-10-04 16:50:43] iter = 19150, loss = 1.5511
[2023-10-04 16:50:44] iter = 19160, loss = 1.5015
[2023-10-04 16:50:45] iter = 19170, loss = 1.4345
[2023-10-04 16:50:46] iter = 19180, loss = 1.4260
[2023-10-04 16:50:47] iter = 19190, loss = 1.5821
[2023-10-04 16:50:48] iter = 19200, loss = 1.5128
[2023-10-04 16:50:48] iter = 19210, loss = 1.5202
[2023-10-04 16:50:49] iter = 19220, loss = 1.4895
[2023-10-04 16:50:50] iter = 19230, loss = 1.4477
[2023-10-04 16:50:51] iter = 19240, loss = 1.4725
[2023-10-04 16:50:52] iter = 19250, loss = 1.3218
[2023-10-04 16:50:53] iter = 19260, loss = 1.5261
[2023-10-04 16:50:54] iter = 19270, loss = 1.4498
[2023-10-04 16:50:55] iter = 19280, loss = 1.6059
[2023-10-04 16:50:56] iter = 19290, loss = 1.4709
[2023-10-04 16:50:57] iter = 19300, loss = 1.4514
[2023-10-04 16:50:58] iter = 19310, loss = 1.3526
[2023-10-04 16:50:58] iter = 19320, loss = 1.4986
[2023-10-04 16:50:59] iter = 19330, loss = 1.3952
[2023-10-04 16:51:00] iter = 19340, loss = 1.4981
[2023-10-04 16:51:01] iter = 19350, loss = 1.4902
[2023-10-04 16:51:02] iter = 19360, loss = 1.4122
[2023-10-04 16:51:03] iter = 19370, loss = 1.4755
[2023-10-04 16:51:04] iter = 19380, loss = 1.5282
[2023-10-04 16:51:05] iter = 19390, loss = 1.4505
[2023-10-04 16:51:06] iter = 19400, loss = 1.5335
[2023-10-04 16:51:07] iter = 19410, loss = 1.6228
[2023-10-04 16:51:08] iter = 19420, loss = 1.6645
[2023-10-04 16:51:09] iter = 19430, loss = 1.4449
[2023-10-04 16:51:09] iter = 19440, loss = 1.5482
[2023-10-04 16:51:10] iter = 19450, loss = 1.4656
[2023-10-04 16:51:11] iter = 19460, loss = 1.5221
[2023-10-04 16:51:12] iter = 19470, loss = 1.4647
[2023-10-04 16:51:13] iter = 19480, loss = 1.4766
[2023-10-04 16:51:14] iter = 19490, loss = 1.4727
[2023-10-04 16:51:15] iter = 19500, loss = 1.4041
[2023-10-04 16:51:16] iter = 19510, loss = 1.5610
[2023-10-04 16:51:17] iter = 19520, loss = 1.7126
[2023-10-04 16:51:18] iter = 19530, loss = 1.4552
[2023-10-04 16:51:19] iter = 19540, loss = 1.4111
[2023-10-04 16:51:20] iter = 19550, loss = 1.5449
[2023-10-04 16:51:21] iter = 19560, loss = 1.5445
[2023-10-04 16:51:22] iter = 19570, loss = 1.3877
[2023-10-04 16:51:23] iter = 19580, loss = 1.3613
[2023-10-04 16:51:24] iter = 19590, loss = 1.4099
[2023-10-04 16:51:25] iter = 19600, loss = 1.5005
[2023-10-04 16:51:26] iter = 19610, loss = 1.4305
[2023-10-04 16:51:27] iter = 19620, loss = 1.4503
[2023-10-04 16:51:28] iter = 19630, loss = 1.6335
[2023-10-04 16:51:29] iter = 19640, loss = 1.6620
[2023-10-04 16:51:30] iter = 19650, loss = 1.3602
[2023-10-04 16:51:30] iter = 19660, loss = 1.4305
[2023-10-04 16:51:31] iter = 19670, loss = 1.3417
[2023-10-04 16:51:32] iter = 19680, loss = 1.6547
[2023-10-04 16:51:33] iter = 19690, loss = 1.3993
[2023-10-04 16:51:34] iter = 19700, loss = 1.3108
[2023-10-04 16:51:35] iter = 19710, loss = 1.3832
[2023-10-04 16:51:36] iter = 19720, loss = 1.5626
[2023-10-04 16:51:37] iter = 19730, loss = 1.5677
[2023-10-04 16:51:38] iter = 19740, loss = 1.5180
[2023-10-04 16:51:39] iter = 19750, loss = 1.4846
[2023-10-04 16:51:40] iter = 19760, loss = 1.5279
[2023-10-04 16:51:41] iter = 19770, loss = 1.4491
[2023-10-04 16:51:41] iter = 19780, loss = 1.5311
[2023-10-04 16:51:42] iter = 19790, loss = 1.5187
[2023-10-04 16:51:43] iter = 19800, loss = 1.4945
[2023-10-04 16:51:44] iter = 19810, loss = 1.3979
[2023-10-04 16:51:45] iter = 19820, loss = 1.4869
[2023-10-04 16:51:46] iter = 19830, loss = 1.3492
[2023-10-04 16:51:47] iter = 19840, loss = 1.4974
[2023-10-04 16:51:48] iter = 19850, loss = 1.4313
[2023-10-04 16:51:49] iter = 19860, loss = 1.5493
[2023-10-04 16:51:50] iter = 19870, loss = 1.4630
[2023-10-04 16:51:51] iter = 19880, loss = 1.5394
[2023-10-04 16:51:52] iter = 19890, loss = 1.4219
[2023-10-04 16:51:53] iter = 19900, loss = 1.4511
[2023-10-04 16:51:53] iter = 19910, loss = 1.4639
[2023-10-04 16:51:54] iter = 19920, loss = 1.4628
[2023-10-04 16:51:55] iter = 19930, loss = 1.5919
[2023-10-04 16:51:56] iter = 19940, loss = 1.4876
[2023-10-04 16:51:57] iter = 19950, loss = 1.4179
[2023-10-04 16:51:58] iter = 19960, loss = 1.4945
[2023-10-04 16:51:59] iter = 19970, loss = 1.4974
[2023-10-04 16:52:00] iter = 19980, loss = 1.3440
[2023-10-04 16:52:01] iter = 19990, loss = 1.5186
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 22466}
[2023-10-04 16:52:27] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005090 train acc = 1.0000, test acc = 0.6189
[2023-10-04 16:52:51] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.017811 train acc = 1.0000, test acc = 0.6310
[2023-10-04 16:53:16] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002552 train acc = 1.0000, test acc = 0.6250
[2023-10-04 16:53:40] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003906 train acc = 1.0000, test acc = 0.6266
[2023-10-04 16:54:05] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.019591 train acc = 1.0000, test acc = 0.6331
[2023-10-04 16:54:29] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002863 train acc = 1.0000, test acc = 0.6306
[2023-10-04 16:54:54] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002026 train acc = 1.0000, test acc = 0.6227
[2023-10-04 16:55:18] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.015709 train acc = 0.9980, test acc = 0.6258
[2023-10-04 16:55:43] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014031 train acc = 1.0000, test acc = 0.6281
[2023-10-04 16:56:08] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003581 train acc = 1.0000, test acc = 0.6295
[2023-10-04 16:56:32] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.018620 train acc = 1.0000, test acc = 0.6284
[2023-10-04 16:56:57] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.026633 train acc = 1.0000, test acc = 0.6229
[2023-10-04 16:57:21] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001757 train acc = 1.0000, test acc = 0.6284
[2023-10-04 16:57:46] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002229 train acc = 1.0000, test acc = 0.6222
[2023-10-04 16:58:10] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.017153 train acc = 1.0000, test acc = 0.6307
[2023-10-04 16:58:35] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.006330 train acc = 1.0000, test acc = 0.6233
[2023-10-04 16:58:59] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002301 train acc = 1.0000, test acc = 0.6292
[2023-10-04 16:59:24] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.017540 train acc = 1.0000, test acc = 0.6281
[2023-10-04 16:59:49] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.022035 train acc = 0.9980, test acc = 0.6247
[2023-10-04 17:00:13] Evaluate_19: epoch = 1000 train time = 23 s train loss = 0.006387 train acc = 1.0000, test acc = 0.6267
Evaluate 20 random ConvNet, mean = 0.6268 std = 0.0035
-------------------------
[2023-10-04 17:00:14] iter = 20000, loss = 1.5138

================== Exp 2 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5173a57f40>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-04 17:00:31] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 14237}
[2023-10-04 17:00:56] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.001016 train acc = 1.0000, test acc = 0.5075
[2023-10-04 17:01:20] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.015948 train acc = 1.0000, test acc = 0.5052
[2023-10-04 17:01:45] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.004951 train acc = 1.0000, test acc = 0.4981
[2023-10-04 17:02:09] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.008049 train acc = 1.0000, test acc = 0.5033
[2023-10-04 17:02:34] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.008966 train acc = 1.0000, test acc = 0.4989
[2023-10-04 17:02:58] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003761 train acc = 1.0000, test acc = 0.5064
[2023-10-04 17:03:23] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003156 train acc = 1.0000, test acc = 0.5001
[2023-10-04 17:03:48] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.000913 train acc = 1.0000, test acc = 0.4964
[2023-10-04 17:04:12] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.005573 train acc = 1.0000, test acc = 0.5009
[2023-10-04 17:04:36] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.004658 train acc = 1.0000, test acc = 0.5020
[2023-10-04 17:05:01] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003004 train acc = 1.0000, test acc = 0.4911
[2023-10-04 17:05:25] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003117 train acc = 1.0000, test acc = 0.5068
[2023-10-04 17:05:50] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.005243 train acc = 1.0000, test acc = 0.5116
[2023-10-04 17:06:15] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.015008 train acc = 0.9980, test acc = 0.5121
[2023-10-04 17:06:39] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.006967 train acc = 1.0000, test acc = 0.4974
[2023-10-04 17:07:04] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.012803 train acc = 1.0000, test acc = 0.5033
[2023-10-04 17:07:28] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.011242 train acc = 1.0000, test acc = 0.5012
[2023-10-04 17:07:53] Evaluate_17: epoch = 1000 train time = 23 s train loss = 0.003241 train acc = 1.0000, test acc = 0.4983
[2023-10-04 17:08:17] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004399 train acc = 1.0000, test acc = 0.4974
[2023-10-04 17:08:42] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.001973 train acc = 1.0000, test acc = 0.5033
Evaluate 20 random ConvNet, mean = 0.5021 std = 0.0051
-------------------------
[2023-10-04 17:08:42] iter = 00000, loss = 5.8863
[2023-10-04 17:08:43] iter = 00010, loss = 5.0926
[2023-10-04 17:08:44] iter = 00020, loss = 4.7465
[2023-10-04 17:08:45] iter = 00030, loss = 4.1557
[2023-10-04 17:08:46] iter = 00040, loss = 3.9677
[2023-10-04 17:08:47] iter = 00050, loss = 3.8413
[2023-10-04 17:08:48] iter = 00060, loss = 3.6507
[2023-10-04 17:08:49] iter = 00070, loss = 3.2613
[2023-10-04 17:08:49] iter = 00080, loss = 3.2274
[2023-10-04 17:08:50] iter = 00090, loss = 3.1488
[2023-10-04 17:08:51] iter = 00100, loss = 2.9255
[2023-10-04 17:08:52] iter = 00110, loss = 3.1921
[2023-10-04 17:08:53] iter = 00120, loss = 3.2448
[2023-10-04 17:08:54] iter = 00130, loss = 3.0620
[2023-10-04 17:08:55] iter = 00140, loss = 2.9169
[2023-10-04 17:08:56] iter = 00150, loss = 2.8230
[2023-10-04 17:08:57] iter = 00160, loss = 2.8292
[2023-10-04 17:08:58] iter = 00170, loss = 2.8052
[2023-10-04 17:08:59] iter = 00180, loss = 2.7629
[2023-10-04 17:09:00] iter = 00190, loss = 2.7535
[2023-10-04 17:09:01] iter = 00200, loss = 2.6353
[2023-10-04 17:09:02] iter = 00210, loss = 2.6090
[2023-10-04 17:09:02] iter = 00220, loss = 2.7487
[2023-10-04 17:09:03] iter = 00230, loss = 2.6731
[2023-10-04 17:09:04] iter = 00240, loss = 2.6081
[2023-10-04 17:09:05] iter = 00250, loss = 2.4335
[2023-10-04 17:09:06] iter = 00260, loss = 2.5685
[2023-10-04 17:09:07] iter = 00270, loss = 2.6580
[2023-10-04 17:09:08] iter = 00280, loss = 2.4305
[2023-10-04 17:09:09] iter = 00290, loss = 2.4183
[2023-10-04 17:09:10] iter = 00300, loss = 2.4884
[2023-10-04 17:09:11] iter = 00310, loss = 2.6042
[2023-10-04 17:09:12] iter = 00320, loss = 2.3241
[2023-10-04 17:09:13] iter = 00330, loss = 2.3958
[2023-10-04 17:09:13] iter = 00340, loss = 2.3839
[2023-10-04 17:09:14] iter = 00350, loss = 2.4310
[2023-10-04 17:09:15] iter = 00360, loss = 2.4635
[2023-10-04 17:09:16] iter = 00370, loss = 2.3385
[2023-10-04 17:09:17] iter = 00380, loss = 2.4601
[2023-10-04 17:09:18] iter = 00390, loss = 2.2768
[2023-10-04 17:09:19] iter = 00400, loss = 2.3027
[2023-10-04 17:09:20] iter = 00410, loss = 2.3870
[2023-10-04 17:09:21] iter = 00420, loss = 2.2492
[2023-10-04 17:09:21] iter = 00430, loss = 2.0853
[2023-10-04 17:09:22] iter = 00440, loss = 2.3708
[2023-10-04 17:09:23] iter = 00450, loss = 2.5414
[2023-10-04 17:09:24] iter = 00460, loss = 2.3120
[2023-10-04 17:09:25] iter = 00470, loss = 2.4081
[2023-10-04 17:09:26] iter = 00480, loss = 2.1371
[2023-10-04 17:09:27] iter = 00490, loss = 2.1728
[2023-10-04 17:09:28] iter = 00500, loss = 2.1069
[2023-10-04 17:09:29] iter = 00510, loss = 2.1932
[2023-10-04 17:09:29] iter = 00520, loss = 2.2351
[2023-10-04 17:09:30] iter = 00530, loss = 2.1224
[2023-10-04 17:09:31] iter = 00540, loss = 2.2218
[2023-10-04 17:09:32] iter = 00550, loss = 2.0807
[2023-10-04 17:09:33] iter = 00560, loss = 2.2623
[2023-10-04 17:09:34] iter = 00570, loss = 2.3419
[2023-10-04 17:09:35] iter = 00580, loss = 2.3033
[2023-10-04 17:09:36] iter = 00590, loss = 2.0751
[2023-10-04 17:09:37] iter = 00600, loss = 2.2453
[2023-10-04 17:09:38] iter = 00610, loss = 2.3216
[2023-10-04 17:09:39] iter = 00620, loss = 2.2484
[2023-10-04 17:09:40] iter = 00630, loss = 2.1651
[2023-10-04 17:09:40] iter = 00640, loss = 2.0960
[2023-10-04 17:09:41] iter = 00650, loss = 2.2426
[2023-10-04 17:09:42] iter = 00660, loss = 2.0880
[2023-10-04 17:09:43] iter = 00670, loss = 2.1599
[2023-10-04 17:09:44] iter = 00680, loss = 2.2044
[2023-10-04 17:09:45] iter = 00690, loss = 2.0675
[2023-10-04 17:09:46] iter = 00700, loss = 2.1539
[2023-10-04 17:09:47] iter = 00710, loss = 2.0518
[2023-10-04 17:09:48] iter = 00720, loss = 2.0363
[2023-10-04 17:09:49] iter = 00730, loss = 2.0889
[2023-10-04 17:09:50] iter = 00740, loss = 2.1471
[2023-10-04 17:09:51] iter = 00750, loss = 2.0517
[2023-10-04 17:09:52] iter = 00760, loss = 2.1218
[2023-10-04 17:09:53] iter = 00770, loss = 2.0799
[2023-10-04 17:09:54] iter = 00780, loss = 2.0137
[2023-10-04 17:09:55] iter = 00790, loss = 2.0969
[2023-10-04 17:09:56] iter = 00800, loss = 2.1144
[2023-10-04 17:09:56] iter = 00810, loss = 2.2472
[2023-10-04 17:09:57] iter = 00820, loss = 2.1352
[2023-10-04 17:09:58] iter = 00830, loss = 2.1671
[2023-10-04 17:09:59] iter = 00840, loss = 1.9198
[2023-10-04 17:10:00] iter = 00850, loss = 2.1071
[2023-10-04 17:10:01] iter = 00860, loss = 2.0613
[2023-10-04 17:10:02] iter = 00870, loss = 2.1577
[2023-10-04 17:10:03] iter = 00880, loss = 2.0850
[2023-10-04 17:10:04] iter = 00890, loss = 1.8427
[2023-10-04 17:10:05] iter = 00900, loss = 2.2188
[2023-10-04 17:10:06] iter = 00910, loss = 2.2377
[2023-10-04 17:10:07] iter = 00920, loss = 2.0360
[2023-10-04 17:10:07] iter = 00930, loss = 2.0417
[2023-10-04 17:10:08] iter = 00940, loss = 1.9721
[2023-10-04 17:10:09] iter = 00950, loss = 2.0036
[2023-10-04 17:10:10] iter = 00960, loss = 1.9737
[2023-10-04 17:10:11] iter = 00970, loss = 2.1805
[2023-10-04 17:10:12] iter = 00980, loss = 2.1290
[2023-10-04 17:10:13] iter = 00990, loss = 1.9739
[2023-10-04 17:10:14] iter = 01000, loss = 2.0550
[2023-10-04 17:10:15] iter = 01010, loss = 2.2827
[2023-10-04 17:10:16] iter = 01020, loss = 1.9460
[2023-10-04 17:10:17] iter = 01030, loss = 2.0409
[2023-10-04 17:10:18] iter = 01040, loss = 1.9144
[2023-10-04 17:10:18] iter = 01050, loss = 2.1616
[2023-10-04 17:10:19] iter = 01060, loss = 2.0417
[2023-10-04 17:10:20] iter = 01070, loss = 2.1605
[2023-10-04 17:10:21] iter = 01080, loss = 1.8096
[2023-10-04 17:10:22] iter = 01090, loss = 2.0748
[2023-10-04 17:10:23] iter = 01100, loss = 1.8841
[2023-10-04 17:10:24] iter = 01110, loss = 2.0019
[2023-10-04 17:10:25] iter = 01120, loss = 2.0144
[2023-10-04 17:10:26] iter = 01130, loss = 2.1091
[2023-10-04 17:10:27] iter = 01140, loss = 1.9054
[2023-10-04 17:10:28] iter = 01150, loss = 2.1584
[2023-10-04 17:10:28] iter = 01160, loss = 2.0099
[2023-10-04 17:10:29] iter = 01170, loss = 1.9604
[2023-10-04 17:10:30] iter = 01180, loss = 1.9248
[2023-10-04 17:10:31] iter = 01190, loss = 2.0492
[2023-10-04 17:10:32] iter = 01200, loss = 1.8788
[2023-10-04 17:10:33] iter = 01210, loss = 2.0620
[2023-10-04 17:10:34] iter = 01220, loss = 1.9765
[2023-10-04 17:10:35] iter = 01230, loss = 2.0008
[2023-10-04 17:10:36] iter = 01240, loss = 1.9427
[2023-10-04 17:10:37] iter = 01250, loss = 1.9757
[2023-10-04 17:10:38] iter = 01260, loss = 1.9702
[2023-10-04 17:10:39] iter = 01270, loss = 2.0731
[2023-10-04 17:10:40] iter = 01280, loss = 1.9343
[2023-10-04 17:10:41] iter = 01290, loss = 2.0135
[2023-10-04 17:10:41] iter = 01300, loss = 1.9114
[2023-10-04 17:10:42] iter = 01310, loss = 1.8958
[2023-10-04 17:10:43] iter = 01320, loss = 1.8394
[2023-10-04 17:10:44] iter = 01330, loss = 2.0062
[2023-10-04 17:10:45] iter = 01340, loss = 1.9548
[2023-10-04 17:10:46] iter = 01350, loss = 1.9892
[2023-10-04 17:10:47] iter = 01360, loss = 1.9699
[2023-10-04 17:10:48] iter = 01370, loss = 1.9638
[2023-10-04 17:10:49] iter = 01380, loss = 1.8348
[2023-10-04 17:10:50] iter = 01390, loss = 1.7812
[2023-10-04 17:10:51] iter = 01400, loss = 1.9954
[2023-10-04 17:10:51] iter = 01410, loss = 1.8456
[2023-10-04 17:10:52] iter = 01420, loss = 1.9723
[2023-10-04 17:10:53] iter = 01430, loss = 1.9459
[2023-10-04 17:10:54] iter = 01440, loss = 1.8353
[2023-10-04 17:10:55] iter = 01450, loss = 2.0140
[2023-10-04 17:10:56] iter = 01460, loss = 1.9908
[2023-10-04 17:10:57] iter = 01470, loss = 1.8953
[2023-10-04 17:10:58] iter = 01480, loss = 2.0997
[2023-10-04 17:10:59] iter = 01490, loss = 1.9542
[2023-10-04 17:11:00] iter = 01500, loss = 1.8861
[2023-10-04 17:11:01] iter = 01510, loss = 1.9831
[2023-10-04 17:11:02] iter = 01520, loss = 1.9077
[2023-10-04 17:11:02] iter = 01530, loss = 2.0088
[2023-10-04 17:11:03] iter = 01540, loss = 2.0215
[2023-10-04 17:11:04] iter = 01550, loss = 1.9135
[2023-10-04 17:11:05] iter = 01560, loss = 2.0783
[2023-10-04 17:11:06] iter = 01570, loss = 1.9553
[2023-10-04 17:11:07] iter = 01580, loss = 1.9061
[2023-10-04 17:11:08] iter = 01590, loss = 1.7779
[2023-10-04 17:11:09] iter = 01600, loss = 2.0472
[2023-10-04 17:11:10] iter = 01610, loss = 1.9605
[2023-10-04 17:11:11] iter = 01620, loss = 1.8871
[2023-10-04 17:11:12] iter = 01630, loss = 1.8075
[2023-10-04 17:11:13] iter = 01640, loss = 1.9668
[2023-10-04 17:11:14] iter = 01650, loss = 1.7650
[2023-10-04 17:11:15] iter = 01660, loss = 1.8701
[2023-10-04 17:11:16] iter = 01670, loss = 1.8369
[2023-10-04 17:11:16] iter = 01680, loss = 1.8975
[2023-10-04 17:11:17] iter = 01690, loss = 1.9301
[2023-10-04 17:11:18] iter = 01700, loss = 1.9475
[2023-10-04 17:11:19] iter = 01710, loss = 1.8347
[2023-10-04 17:11:20] iter = 01720, loss = 1.9398
[2023-10-04 17:11:21] iter = 01730, loss = 1.9173
[2023-10-04 17:11:22] iter = 01740, loss = 1.8536
[2023-10-04 17:11:23] iter = 01750, loss = 1.8038
[2023-10-04 17:11:24] iter = 01760, loss = 1.9119
[2023-10-04 17:11:25] iter = 01770, loss = 1.7652
[2023-10-04 17:11:26] iter = 01780, loss = 1.8550
[2023-10-04 17:11:27] iter = 01790, loss = 1.7592
[2023-10-04 17:11:28] iter = 01800, loss = 1.9062
[2023-10-04 17:11:28] iter = 01810, loss = 1.9967
[2023-10-04 17:11:29] iter = 01820, loss = 1.8540
[2023-10-04 17:11:30] iter = 01830, loss = 1.8219
[2023-10-04 17:11:31] iter = 01840, loss = 1.9408
[2023-10-04 17:11:32] iter = 01850, loss = 1.7138
[2023-10-04 17:11:33] iter = 01860, loss = 1.7559
[2023-10-04 17:11:34] iter = 01870, loss = 1.9244
[2023-10-04 17:11:35] iter = 01880, loss = 1.7757
[2023-10-04 17:11:36] iter = 01890, loss = 1.7343
[2023-10-04 17:11:37] iter = 01900, loss = 1.7095
[2023-10-04 17:11:38] iter = 01910, loss = 1.8766
[2023-10-04 17:11:39] iter = 01920, loss = 1.7102
[2023-10-04 17:11:40] iter = 01930, loss = 1.8293
[2023-10-04 17:11:40] iter = 01940, loss = 1.8835
[2023-10-04 17:11:41] iter = 01950, loss = 1.8760
[2023-10-04 17:11:42] iter = 01960, loss = 1.8087
[2023-10-04 17:11:43] iter = 01970, loss = 1.9553
[2023-10-04 17:11:44] iter = 01980, loss = 1.7735
[2023-10-04 17:11:45] iter = 01990, loss = 1.7342
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 6328}
[2023-10-04 17:12:10] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.007717 train acc = 1.0000, test acc = 0.5792
[2023-10-04 17:12:35] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004857 train acc = 1.0000, test acc = 0.5819
[2023-10-04 17:13:00] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.006009 train acc = 1.0000, test acc = 0.5888
[2023-10-04 17:13:24] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.013511 train acc = 1.0000, test acc = 0.5836
[2023-10-04 17:13:49] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011887 train acc = 1.0000, test acc = 0.5837
[2023-10-04 17:14:13] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011617 train acc = 1.0000, test acc = 0.5835
[2023-10-04 17:14:38] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.015062 train acc = 1.0000, test acc = 0.5891
[2023-10-04 17:15:02] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012164 train acc = 1.0000, test acc = 0.5851
[2023-10-04 17:15:27] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.004285 train acc = 1.0000, test acc = 0.5864
[2023-10-04 17:15:52] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.006835 train acc = 1.0000, test acc = 0.5895
[2023-10-04 17:16:16] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003252 train acc = 1.0000, test acc = 0.5832
[2023-10-04 17:16:41] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.013705 train acc = 1.0000, test acc = 0.5880
[2023-10-04 17:17:06] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.014367 train acc = 1.0000, test acc = 0.5804
[2023-10-04 17:17:30] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.022549 train acc = 1.0000, test acc = 0.5826
[2023-10-04 17:17:54] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.007089 train acc = 1.0000, test acc = 0.5878
[2023-10-04 17:18:19] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004984 train acc = 1.0000, test acc = 0.5819
[2023-10-04 17:18:43] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.016688 train acc = 1.0000, test acc = 0.5847
[2023-10-04 17:19:08] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004490 train acc = 1.0000, test acc = 0.5901
[2023-10-04 17:19:32] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004284 train acc = 1.0000, test acc = 0.5839
[2023-10-04 17:19:57] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012776 train acc = 1.0000, test acc = 0.5838
Evaluate 20 random ConvNet, mean = 0.5849 std = 0.0031
-------------------------
[2023-10-04 17:19:57] iter = 02000, loss = 1.7548
[2023-10-04 17:19:58] iter = 02010, loss = 1.8630
[2023-10-04 17:19:59] iter = 02020, loss = 1.9058
[2023-10-04 17:20:00] iter = 02030, loss = 1.8691
[2023-10-04 17:20:01] iter = 02040, loss = 1.8545
[2023-10-04 17:20:02] iter = 02050, loss = 1.8051
[2023-10-04 17:20:03] iter = 02060, loss = 1.9500
[2023-10-04 17:20:04] iter = 02070, loss = 1.6655
[2023-10-04 17:20:05] iter = 02080, loss = 1.8336
[2023-10-04 17:20:05] iter = 02090, loss = 1.9269
[2023-10-04 17:20:06] iter = 02100, loss = 1.7038
[2023-10-04 17:20:07] iter = 02110, loss = 1.9556
[2023-10-04 17:20:08] iter = 02120, loss = 1.7146
[2023-10-04 17:20:09] iter = 02130, loss = 1.8902
[2023-10-04 17:20:10] iter = 02140, loss = 1.8444
[2023-10-04 17:20:11] iter = 02150, loss = 1.8080
[2023-10-04 17:20:12] iter = 02160, loss = 1.7472
[2023-10-04 17:20:13] iter = 02170, loss = 1.7981
[2023-10-04 17:20:14] iter = 02180, loss = 1.9143
[2023-10-04 17:20:15] iter = 02190, loss = 1.8001
[2023-10-04 17:20:15] iter = 02200, loss = 1.7027
[2023-10-04 17:20:16] iter = 02210, loss = 1.8716
[2023-10-04 17:20:17] iter = 02220, loss = 1.8146
[2023-10-04 17:20:18] iter = 02230, loss = 1.8016
[2023-10-04 17:20:19] iter = 02240, loss = 1.9622
[2023-10-04 17:20:20] iter = 02250, loss = 1.8142
[2023-10-04 17:20:21] iter = 02260, loss = 1.8260
[2023-10-04 17:20:22] iter = 02270, loss = 1.8486
[2023-10-04 17:20:23] iter = 02280, loss = 1.7845
[2023-10-04 17:20:24] iter = 02290, loss = 1.9907
[2023-10-04 17:20:25] iter = 02300, loss = 1.7509
[2023-10-04 17:20:26] iter = 02310, loss = 1.8524
[2023-10-04 17:20:26] iter = 02320, loss = 1.6280
[2023-10-04 17:20:27] iter = 02330, loss = 1.7185
[2023-10-04 17:20:28] iter = 02340, loss = 1.8404
[2023-10-04 17:20:29] iter = 02350, loss = 1.7670
[2023-10-04 17:20:30] iter = 02360, loss = 1.6453
[2023-10-04 17:20:31] iter = 02370, loss = 1.7950
[2023-10-04 17:20:32] iter = 02380, loss = 1.7180
[2023-10-04 17:20:33] iter = 02390, loss = 1.7296
[2023-10-04 17:20:34] iter = 02400, loss = 1.8658
[2023-10-04 17:20:35] iter = 02410, loss = 1.7911
[2023-10-04 17:20:36] iter = 02420, loss = 1.7261
[2023-10-04 17:20:37] iter = 02430, loss = 1.8697
[2023-10-04 17:20:38] iter = 02440, loss = 1.7776
[2023-10-04 17:20:38] iter = 02450, loss = 1.7982
[2023-10-04 17:20:39] iter = 02460, loss = 1.9057
[2023-10-04 17:20:40] iter = 02470, loss = 1.7490
[2023-10-04 17:20:41] iter = 02480, loss = 1.7656
[2023-10-04 17:20:42] iter = 02490, loss = 1.8550
[2023-10-04 17:20:43] iter = 02500, loss = 1.7846
[2023-10-04 17:20:44] iter = 02510, loss = 1.7665
[2023-10-04 17:20:45] iter = 02520, loss = 1.7281
[2023-10-04 17:20:46] iter = 02530, loss = 1.7574
[2023-10-04 17:20:47] iter = 02540, loss = 2.0172
[2023-10-04 17:20:48] iter = 02550, loss = 1.7370
[2023-10-04 17:20:49] iter = 02560, loss = 1.7210
[2023-10-04 17:20:50] iter = 02570, loss = 1.9212
[2023-10-04 17:20:50] iter = 02580, loss = 1.7105
[2023-10-04 17:20:51] iter = 02590, loss = 1.8559
[2023-10-04 17:20:52] iter = 02600, loss = 1.6555
[2023-10-04 17:20:53] iter = 02610, loss = 1.7313
[2023-10-04 17:20:54] iter = 02620, loss = 1.8594
[2023-10-04 17:20:55] iter = 02630, loss = 1.8235
[2023-10-04 17:20:56] iter = 02640, loss = 1.9102
[2023-10-04 17:20:57] iter = 02650, loss = 1.8044
[2023-10-04 17:20:58] iter = 02660, loss = 1.8340
[2023-10-04 17:20:59] iter = 02670, loss = 1.8106
[2023-10-04 17:21:00] iter = 02680, loss = 1.7954
[2023-10-04 17:21:01] iter = 02690, loss = 1.9250
[2023-10-04 17:21:01] iter = 02700, loss = 1.7852
[2023-10-04 17:21:02] iter = 02710, loss = 1.7145
[2023-10-04 17:21:03] iter = 02720, loss = 1.6677
[2023-10-04 17:21:04] iter = 02730, loss = 1.6980
[2023-10-04 17:21:05] iter = 02740, loss = 1.6567
[2023-10-04 17:21:06] iter = 02750, loss = 1.7087
[2023-10-04 17:21:07] iter = 02760, loss = 1.8464
[2023-10-04 17:21:08] iter = 02770, loss = 1.8086
[2023-10-04 17:21:09] iter = 02780, loss = 1.7103
[2023-10-04 17:21:10] iter = 02790, loss = 1.6589
[2023-10-04 17:21:10] iter = 02800, loss = 1.8018
[2023-10-04 17:21:11] iter = 02810, loss = 1.7737
[2023-10-04 17:21:12] iter = 02820, loss = 1.7229
[2023-10-04 17:21:13] iter = 02830, loss = 1.8673
[2023-10-04 17:21:14] iter = 02840, loss = 1.8335
[2023-10-04 17:21:15] iter = 02850, loss = 1.8826
[2023-10-04 17:21:16] iter = 02860, loss = 1.6977
[2023-10-04 17:21:17] iter = 02870, loss = 1.7227
[2023-10-04 17:21:18] iter = 02880, loss = 1.7667
[2023-10-04 17:21:19] iter = 02890, loss = 1.9679
[2023-10-04 17:21:19] iter = 02900, loss = 1.9178
[2023-10-04 17:21:20] iter = 02910, loss = 1.7635
[2023-10-04 17:21:21] iter = 02920, loss = 1.7331
[2023-10-04 17:21:22] iter = 02930, loss = 1.6904
[2023-10-04 17:21:23] iter = 02940, loss = 1.7299
[2023-10-04 17:21:24] iter = 02950, loss = 1.7182
[2023-10-04 17:21:25] iter = 02960, loss = 2.0610
[2023-10-04 17:21:26] iter = 02970, loss = 1.8818
[2023-10-04 17:21:27] iter = 02980, loss = 1.7847
[2023-10-04 17:21:28] iter = 02990, loss = 1.7534
[2023-10-04 17:21:29] iter = 03000, loss = 1.7315
[2023-10-04 17:21:30] iter = 03010, loss = 1.7701
[2023-10-04 17:21:31] iter = 03020, loss = 1.7560
[2023-10-04 17:21:32] iter = 03030, loss = 1.8662
[2023-10-04 17:21:32] iter = 03040, loss = 1.6639
[2023-10-04 17:21:33] iter = 03050, loss = 1.7997
[2023-10-04 17:21:34] iter = 03060, loss = 1.6413
[2023-10-04 17:21:35] iter = 03070, loss = 1.5577
[2023-10-04 17:21:36] iter = 03080, loss = 1.7007
[2023-10-04 17:21:37] iter = 03090, loss = 2.1267
[2023-10-04 17:21:38] iter = 03100, loss = 1.7868
[2023-10-04 17:21:39] iter = 03110, loss = 1.7633
[2023-10-04 17:21:40] iter = 03120, loss = 1.6465
[2023-10-04 17:21:41] iter = 03130, loss = 1.9006
[2023-10-04 17:21:42] iter = 03140, loss = 1.7365
[2023-10-04 17:21:42] iter = 03150, loss = 1.6609
[2023-10-04 17:21:43] iter = 03160, loss = 1.7916
[2023-10-04 17:21:44] iter = 03170, loss = 1.7131
[2023-10-04 17:21:45] iter = 03180, loss = 1.6867
[2023-10-04 17:21:46] iter = 03190, loss = 1.5856
[2023-10-04 17:21:47] iter = 03200, loss = 1.8153
[2023-10-04 17:21:48] iter = 03210, loss = 1.7129
[2023-10-04 17:21:49] iter = 03220, loss = 1.8525
[2023-10-04 17:21:50] iter = 03230, loss = 1.7248
[2023-10-04 17:21:51] iter = 03240, loss = 1.7733
[2023-10-04 17:21:52] iter = 03250, loss = 1.6605
[2023-10-04 17:21:53] iter = 03260, loss = 1.7762
[2023-10-04 17:21:53] iter = 03270, loss = 1.8576
[2023-10-04 17:21:54] iter = 03280, loss = 1.7326
[2023-10-04 17:21:55] iter = 03290, loss = 1.7727
[2023-10-04 17:21:56] iter = 03300, loss = 1.7618
[2023-10-04 17:21:57] iter = 03310, loss = 1.7068
[2023-10-04 17:21:58] iter = 03320, loss = 1.7055
[2023-10-04 17:21:59] iter = 03330, loss = 1.6328
[2023-10-04 17:22:00] iter = 03340, loss = 1.8187
[2023-10-04 17:22:01] iter = 03350, loss = 1.7629
[2023-10-04 17:22:02] iter = 03360, loss = 1.8861
[2023-10-04 17:22:03] iter = 03370, loss = 1.7889
[2023-10-04 17:22:03] iter = 03380, loss = 1.8709
[2023-10-04 17:22:04] iter = 03390, loss = 1.7576
[2023-10-04 17:22:05] iter = 03400, loss = 1.7462
[2023-10-04 17:22:06] iter = 03410, loss = 1.6904
[2023-10-04 17:22:07] iter = 03420, loss = 1.7889
[2023-10-04 17:22:08] iter = 03430, loss = 1.9523
[2023-10-04 17:22:09] iter = 03440, loss = 1.7037
[2023-10-04 17:22:10] iter = 03450, loss = 1.7768
[2023-10-04 17:22:11] iter = 03460, loss = 1.6257
[2023-10-04 17:22:12] iter = 03470, loss = 1.6523
[2023-10-04 17:22:13] iter = 03480, loss = 1.6371
[2023-10-04 17:22:13] iter = 03490, loss = 1.6865
[2023-10-04 17:22:14] iter = 03500, loss = 1.6988
[2023-10-04 17:22:15] iter = 03510, loss = 1.8159
[2023-10-04 17:22:16] iter = 03520, loss = 1.8275
[2023-10-04 17:22:17] iter = 03530, loss = 1.6359
[2023-10-04 17:22:18] iter = 03540, loss = 1.7504
[2023-10-04 17:22:19] iter = 03550, loss = 1.6971
[2023-10-04 17:22:20] iter = 03560, loss = 1.7674
[2023-10-04 17:22:21] iter = 03570, loss = 1.7520
[2023-10-04 17:22:22] iter = 03580, loss = 1.8090
[2023-10-04 17:22:22] iter = 03590, loss = 1.6444
[2023-10-04 17:22:23] iter = 03600, loss = 1.8173
[2023-10-04 17:22:24] iter = 03610, loss = 1.5471
[2023-10-04 17:22:25] iter = 03620, loss = 1.7585
[2023-10-04 17:22:26] iter = 03630, loss = 1.6882
[2023-10-04 17:22:27] iter = 03640, loss = 1.8850
[2023-10-04 17:22:28] iter = 03650, loss = 1.6930
[2023-10-04 17:22:29] iter = 03660, loss = 1.6405
[2023-10-04 17:22:30] iter = 03670, loss = 1.8900
[2023-10-04 17:22:31] iter = 03680, loss = 1.5559
[2023-10-04 17:22:32] iter = 03690, loss = 1.7859
[2023-10-04 17:22:32] iter = 03700, loss = 1.5067
[2023-10-04 17:22:33] iter = 03710, loss = 1.6393
[2023-10-04 17:22:34] iter = 03720, loss = 1.6444
[2023-10-04 17:22:35] iter = 03730, loss = 1.7940
[2023-10-04 17:22:36] iter = 03740, loss = 1.7783
[2023-10-04 17:22:37] iter = 03750, loss = 1.8049
[2023-10-04 17:22:38] iter = 03760, loss = 1.6376
[2023-10-04 17:22:39] iter = 03770, loss = 1.6325
[2023-10-04 17:22:40] iter = 03780, loss = 1.5668
[2023-10-04 17:22:41] iter = 03790, loss = 1.6883
[2023-10-04 17:22:42] iter = 03800, loss = 1.6648
[2023-10-04 17:22:42] iter = 03810, loss = 1.6247
[2023-10-04 17:22:43] iter = 03820, loss = 1.7414
[2023-10-04 17:22:44] iter = 03830, loss = 1.5966
[2023-10-04 17:22:45] iter = 03840, loss = 1.7419
[2023-10-04 17:22:46] iter = 03850, loss = 1.7921
[2023-10-04 17:22:47] iter = 03860, loss = 1.6817
[2023-10-04 17:22:48] iter = 03870, loss = 1.7412
[2023-10-04 17:22:49] iter = 03880, loss = 1.6954
[2023-10-04 17:22:50] iter = 03890, loss = 1.6590
[2023-10-04 17:22:51] iter = 03900, loss = 1.7213
[2023-10-04 17:22:51] iter = 03910, loss = 1.8282
[2023-10-04 17:22:52] iter = 03920, loss = 1.8386
[2023-10-04 17:22:53] iter = 03930, loss = 1.6330
[2023-10-04 17:22:54] iter = 03940, loss = 1.6576
[2023-10-04 17:22:55] iter = 03950, loss = 1.8711
[2023-10-04 17:22:56] iter = 03960, loss = 1.6164
[2023-10-04 17:22:57] iter = 03970, loss = 1.6133
[2023-10-04 17:22:58] iter = 03980, loss = 1.6551
[2023-10-04 17:22:59] iter = 03990, loss = 1.7997
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 79973}
[2023-10-04 17:23:24] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.003438 train acc = 1.0000, test acc = 0.6072
[2023-10-04 17:23:49] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010395 train acc = 1.0000, test acc = 0.5949
[2023-10-04 17:24:13] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.007304 train acc = 1.0000, test acc = 0.5990
[2023-10-04 17:24:37] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.005895 train acc = 1.0000, test acc = 0.6064
[2023-10-04 17:25:02] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.007538 train acc = 1.0000, test acc = 0.6051
[2023-10-04 17:25:26] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.018308 train acc = 0.9980, test acc = 0.5993
[2023-10-04 17:25:51] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.006421 train acc = 0.9980, test acc = 0.6040
[2023-10-04 17:26:15] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002304 train acc = 1.0000, test acc = 0.6078
[2023-10-04 17:26:40] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.008930 train acc = 1.0000, test acc = 0.6044
[2023-10-04 17:27:04] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.020086 train acc = 1.0000, test acc = 0.5993
[2023-10-04 17:27:28] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003236 train acc = 1.0000, test acc = 0.6002
[2023-10-04 17:27:52] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004698 train acc = 1.0000, test acc = 0.6034
[2023-10-04 17:28:17] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003064 train acc = 1.0000, test acc = 0.5990
[2023-10-04 17:28:41] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003226 train acc = 1.0000, test acc = 0.6093
[2023-10-04 17:29:06] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001689 train acc = 1.0000, test acc = 0.5971
[2023-10-04 17:29:30] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001549 train acc = 1.0000, test acc = 0.5965
[2023-10-04 17:29:55] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004881 train acc = 1.0000, test acc = 0.5930
[2023-10-04 17:30:19] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003944 train acc = 1.0000, test acc = 0.5937
[2023-10-04 17:30:44] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.008263 train acc = 1.0000, test acc = 0.6009
[2023-10-04 17:31:08] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003539 train acc = 1.0000, test acc = 0.5966
Evaluate 20 random ConvNet, mean = 0.6009 std = 0.0047
-------------------------
[2023-10-04 17:31:08] iter = 04000, loss = 1.5966
[2023-10-04 17:31:09] iter = 04010, loss = 1.6580
[2023-10-04 17:31:10] iter = 04020, loss = 1.5534
[2023-10-04 17:31:11] iter = 04030, loss = 1.7049
[2023-10-04 17:31:12] iter = 04040, loss = 1.7134
[2023-10-04 17:31:13] iter = 04050, loss = 1.6646
[2023-10-04 17:31:14] iter = 04060, loss = 1.7732
[2023-10-04 17:31:15] iter = 04070, loss = 1.5887
[2023-10-04 17:31:16] iter = 04080, loss = 1.7851
[2023-10-04 17:31:17] iter = 04090, loss = 1.5493
[2023-10-04 17:31:18] iter = 04100, loss = 1.6361
[2023-10-04 17:31:18] iter = 04110, loss = 1.7941
[2023-10-04 17:31:19] iter = 04120, loss = 1.7862
[2023-10-04 17:31:20] iter = 04130, loss = 1.6938
[2023-10-04 17:31:21] iter = 04140, loss = 1.8579
[2023-10-04 17:31:22] iter = 04150, loss = 1.6192
[2023-10-04 17:31:23] iter = 04160, loss = 1.6313
[2023-10-04 17:31:24] iter = 04170, loss = 1.6685
[2023-10-04 17:31:25] iter = 04180, loss = 1.7517
[2023-10-04 17:31:26] iter = 04190, loss = 1.5229
[2023-10-04 17:31:27] iter = 04200, loss = 1.6930
[2023-10-04 17:31:28] iter = 04210, loss = 1.7420
[2023-10-04 17:31:29] iter = 04220, loss = 1.8535
[2023-10-04 17:31:30] iter = 04230, loss = 1.6690
[2023-10-04 17:31:31] iter = 04240, loss = 1.6219
[2023-10-04 17:31:32] iter = 04250, loss = 1.6379
[2023-10-04 17:31:32] iter = 04260, loss = 1.6980
[2023-10-04 17:31:33] iter = 04270, loss = 1.8666
[2023-10-04 17:31:34] iter = 04280, loss = 1.7131
[2023-10-04 17:31:35] iter = 04290, loss = 1.6360
[2023-10-04 17:31:36] iter = 04300, loss = 1.5986
[2023-10-04 17:31:37] iter = 04310, loss = 1.6215
[2023-10-04 17:31:38] iter = 04320, loss = 1.7853
[2023-10-04 17:31:39] iter = 04330, loss = 1.7174
[2023-10-04 17:31:40] iter = 04340, loss = 1.7567
[2023-10-04 17:31:41] iter = 04350, loss = 1.4872
[2023-10-04 17:31:42] iter = 04360, loss = 1.6556
[2023-10-04 17:31:42] iter = 04370, loss = 1.8285
[2023-10-04 17:31:43] iter = 04380, loss = 1.6722
[2023-10-04 17:31:44] iter = 04390, loss = 1.6898
[2023-10-04 17:31:45] iter = 04400, loss = 1.8430
[2023-10-04 17:31:46] iter = 04410, loss = 1.8024
[2023-10-04 17:31:47] iter = 04420, loss = 1.7362
[2023-10-04 17:31:48] iter = 04430, loss = 1.6720
[2023-10-04 17:31:49] iter = 04440, loss = 1.7229
[2023-10-04 17:31:50] iter = 04450, loss = 1.6409
[2023-10-04 17:31:51] iter = 04460, loss = 1.7450
[2023-10-04 17:31:52] iter = 04470, loss = 1.8033
[2023-10-04 17:31:52] iter = 04480, loss = 1.6280
[2023-10-04 17:31:53] iter = 04490, loss = 1.5536
[2023-10-04 17:31:54] iter = 04500, loss = 1.7440
[2023-10-04 17:31:55] iter = 04510, loss = 1.6710
[2023-10-04 17:31:56] iter = 04520, loss = 1.6958
[2023-10-04 17:31:57] iter = 04530, loss = 1.6415
[2023-10-04 17:31:58] iter = 04540, loss = 1.6166
[2023-10-04 17:31:59] iter = 04550, loss = 1.5649
[2023-10-04 17:32:00] iter = 04560, loss = 1.5255
[2023-10-04 17:32:01] iter = 04570, loss = 1.7959
[2023-10-04 17:32:02] iter = 04580, loss = 1.6806
[2023-10-04 17:32:03] iter = 04590, loss = 1.6053
[2023-10-04 17:32:03] iter = 04600, loss = 1.5601
[2023-10-04 17:32:04] iter = 04610, loss = 1.6887
[2023-10-04 17:32:05] iter = 04620, loss = 1.7535
[2023-10-04 17:32:06] iter = 04630, loss = 1.5696
[2023-10-04 17:32:07] iter = 04640, loss = 1.6223
[2023-10-04 17:32:08] iter = 04650, loss = 1.6619
[2023-10-04 17:32:09] iter = 04660, loss = 1.5840
[2023-10-04 17:32:10] iter = 04670, loss = 1.6483
[2023-10-04 17:32:11] iter = 04680, loss = 1.6932
[2023-10-04 17:32:12] iter = 04690, loss = 1.6649
[2023-10-04 17:32:13] iter = 04700, loss = 1.7857
[2023-10-04 17:32:13] iter = 04710, loss = 1.5018
[2023-10-04 17:32:14] iter = 04720, loss = 1.5882
[2023-10-04 17:32:15] iter = 04730, loss = 1.8481
[2023-10-04 17:32:16] iter = 04740, loss = 1.5667
[2023-10-04 17:32:17] iter = 04750, loss = 1.6834
[2023-10-04 17:32:18] iter = 04760, loss = 1.6497
[2023-10-04 17:32:19] iter = 04770, loss = 1.6430
[2023-10-04 17:32:20] iter = 04780, loss = 1.8657
[2023-10-04 17:32:21] iter = 04790, loss = 1.6034
[2023-10-04 17:32:21] iter = 04800, loss = 1.7390
[2023-10-04 17:32:22] iter = 04810, loss = 1.6292
[2023-10-04 17:32:23] iter = 04820, loss = 1.6069
[2023-10-04 17:32:24] iter = 04830, loss = 1.7378
[2023-10-04 17:32:25] iter = 04840, loss = 1.7659
[2023-10-04 17:32:26] iter = 04850, loss = 1.6378
[2023-10-04 17:32:27] iter = 04860, loss = 1.6033
[2023-10-04 17:32:28] iter = 04870, loss = 1.6467
[2023-10-04 17:32:29] iter = 04880, loss = 1.6653
[2023-10-04 17:32:30] iter = 04890, loss = 1.6186
[2023-10-04 17:32:31] iter = 04900, loss = 1.7159
[2023-10-04 17:32:32] iter = 04910, loss = 1.6094
[2023-10-04 17:32:32] iter = 04920, loss = 1.6424
[2023-10-04 17:32:33] iter = 04930, loss = 1.7877
[2023-10-04 17:32:34] iter = 04940, loss = 1.4455
[2023-10-04 17:32:35] iter = 04950, loss = 1.6654
[2023-10-04 17:32:36] iter = 04960, loss = 1.4886
[2023-10-04 17:32:37] iter = 04970, loss = 1.6163
[2023-10-04 17:32:38] iter = 04980, loss = 1.8148
[2023-10-04 17:32:39] iter = 04990, loss = 1.6055
[2023-10-04 17:32:40] iter = 05000, loss = 1.6803
[2023-10-04 17:32:41] iter = 05010, loss = 1.5728
[2023-10-04 17:32:42] iter = 05020, loss = 1.6591
[2023-10-04 17:32:43] iter = 05030, loss = 1.5955
[2023-10-04 17:32:44] iter = 05040, loss = 1.6696
[2023-10-04 17:32:44] iter = 05050, loss = 1.8520
[2023-10-04 17:32:45] iter = 05060, loss = 1.5630
[2023-10-04 17:32:46] iter = 05070, loss = 1.7398
[2023-10-04 17:32:47] iter = 05080, loss = 1.7450
[2023-10-04 17:32:48] iter = 05090, loss = 1.5405
[2023-10-04 17:32:49] iter = 05100, loss = 1.5168
[2023-10-04 17:32:50] iter = 05110, loss = 1.7832
[2023-10-04 17:32:51] iter = 05120, loss = 1.6267
[2023-10-04 17:32:52] iter = 05130, loss = 1.6913
[2023-10-04 17:32:52] iter = 05140, loss = 1.6918
[2023-10-04 17:32:53] iter = 05150, loss = 1.5134
[2023-10-04 17:32:54] iter = 05160, loss = 1.6613
[2023-10-04 17:32:55] iter = 05170, loss = 1.7134
[2023-10-04 17:32:56] iter = 05180, loss = 1.7193
[2023-10-04 17:32:57] iter = 05190, loss = 1.6180
[2023-10-04 17:32:58] iter = 05200, loss = 1.5548
[2023-10-04 17:32:59] iter = 05210, loss = 1.5427
[2023-10-04 17:33:00] iter = 05220, loss = 1.7633
[2023-10-04 17:33:01] iter = 05230, loss = 1.5204
[2023-10-04 17:33:02] iter = 05240, loss = 1.7185
[2023-10-04 17:33:03] iter = 05250, loss = 1.6105
[2023-10-04 17:33:04] iter = 05260, loss = 1.7736
[2023-10-04 17:33:04] iter = 05270, loss = 1.5029
[2023-10-04 17:33:05] iter = 05280, loss = 1.5698
[2023-10-04 17:33:06] iter = 05290, loss = 1.4823
[2023-10-04 17:33:07] iter = 05300, loss = 1.6179
[2023-10-04 17:33:08] iter = 05310, loss = 1.5450
[2023-10-04 17:33:09] iter = 05320, loss = 1.6698
[2023-10-04 17:33:10] iter = 05330, loss = 1.5772
[2023-10-04 17:33:11] iter = 05340, loss = 1.6286
[2023-10-04 17:33:12] iter = 05350, loss = 1.6228
[2023-10-04 17:33:13] iter = 05360, loss = 1.6973
[2023-10-04 17:33:14] iter = 05370, loss = 1.6782
[2023-10-04 17:33:14] iter = 05380, loss = 1.6285
[2023-10-04 17:33:15] iter = 05390, loss = 1.7177
[2023-10-04 17:33:16] iter = 05400, loss = 1.6457
[2023-10-04 17:33:17] iter = 05410, loss = 1.6268
[2023-10-04 17:33:18] iter = 05420, loss = 1.5210
[2023-10-04 17:33:19] iter = 05430, loss = 1.5401
[2023-10-04 17:33:20] iter = 05440, loss = 1.6835
[2023-10-04 17:33:21] iter = 05450, loss = 1.6965
[2023-10-04 17:33:22] iter = 05460, loss = 1.5845
[2023-10-04 17:33:23] iter = 05470, loss = 1.5867
[2023-10-04 17:33:24] iter = 05480, loss = 1.7150
[2023-10-04 17:33:24] iter = 05490, loss = 1.6765
[2023-10-04 17:33:25] iter = 05500, loss = 1.6999
[2023-10-04 17:33:26] iter = 05510, loss = 1.6086
[2023-10-04 17:33:27] iter = 05520, loss = 1.5787
[2023-10-04 17:33:28] iter = 05530, loss = 1.6648
[2023-10-04 17:33:29] iter = 05540, loss = 1.5646
[2023-10-04 17:33:30] iter = 05550, loss = 1.5673
[2023-10-04 17:33:31] iter = 05560, loss = 1.6024
[2023-10-04 17:33:32] iter = 05570, loss = 1.7007
[2023-10-04 17:33:33] iter = 05580, loss = 1.5675
[2023-10-04 17:33:34] iter = 05590, loss = 1.5658
[2023-10-04 17:33:35] iter = 05600, loss = 1.7266
[2023-10-04 17:33:36] iter = 05610, loss = 1.6613
[2023-10-04 17:33:37] iter = 05620, loss = 1.7024
[2023-10-04 17:33:38] iter = 05630, loss = 1.5983
[2023-10-04 17:33:38] iter = 05640, loss = 1.5282
[2023-10-04 17:33:39] iter = 05650, loss = 1.6986
[2023-10-04 17:33:40] iter = 05660, loss = 1.6562
[2023-10-04 17:33:41] iter = 05670, loss = 1.6447
[2023-10-04 17:33:42] iter = 05680, loss = 1.6205
[2023-10-04 17:33:43] iter = 05690, loss = 1.5963
[2023-10-04 17:33:44] iter = 05700, loss = 1.5656
[2023-10-04 17:33:45] iter = 05710, loss = 1.4955
[2023-10-04 17:33:46] iter = 05720, loss = 1.7584
[2023-10-04 17:33:47] iter = 05730, loss = 1.7889
[2023-10-04 17:33:47] iter = 05740, loss = 1.5261
[2023-10-04 17:33:48] iter = 05750, loss = 1.5619
[2023-10-04 17:33:49] iter = 05760, loss = 1.6135
[2023-10-04 17:33:50] iter = 05770, loss = 1.5624
[2023-10-04 17:33:51] iter = 05780, loss = 1.5735
[2023-10-04 17:33:52] iter = 05790, loss = 1.6181
[2023-10-04 17:33:53] iter = 05800, loss = 1.7524
[2023-10-04 17:33:54] iter = 05810, loss = 1.5870
[2023-10-04 17:33:55] iter = 05820, loss = 1.6418
[2023-10-04 17:33:56] iter = 05830, loss = 1.6073
[2023-10-04 17:33:57] iter = 05840, loss = 1.5479
[2023-10-04 17:33:58] iter = 05850, loss = 1.5998
[2023-10-04 17:33:58] iter = 05860, loss = 1.7723
[2023-10-04 17:33:59] iter = 05870, loss = 1.6229
[2023-10-04 17:34:00] iter = 05880, loss = 1.6000
[2023-10-04 17:34:01] iter = 05890, loss = 1.5295
[2023-10-04 17:34:02] iter = 05900, loss = 1.6572
[2023-10-04 17:34:03] iter = 05910, loss = 1.6106
[2023-10-04 17:34:04] iter = 05920, loss = 1.5495
[2023-10-04 17:34:05] iter = 05930, loss = 1.6925
[2023-10-04 17:34:06] iter = 05940, loss = 1.6153
[2023-10-04 17:34:07] iter = 05950, loss = 1.6693
[2023-10-04 17:34:07] iter = 05960, loss = 1.5943
[2023-10-04 17:34:08] iter = 05970, loss = 1.5564
[2023-10-04 17:34:09] iter = 05980, loss = 1.6803
[2023-10-04 17:34:10] iter = 05990, loss = 1.5287
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 51346}
[2023-10-04 17:34:35] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.001617 train acc = 1.0000, test acc = 0.6027
[2023-10-04 17:35:00] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.005677 train acc = 1.0000, test acc = 0.6084
[2023-10-04 17:35:24] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.021308 train acc = 0.9980, test acc = 0.6125
[2023-10-04 17:35:49] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.007296 train acc = 1.0000, test acc = 0.6112
[2023-10-04 17:36:13] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.013355 train acc = 0.9980, test acc = 0.6052
[2023-10-04 17:36:37] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.007101 train acc = 1.0000, test acc = 0.6015
[2023-10-04 17:37:02] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012302 train acc = 1.0000, test acc = 0.6089
[2023-10-04 17:37:26] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004010 train acc = 1.0000, test acc = 0.6067
[2023-10-04 17:37:50] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.001930 train acc = 1.0000, test acc = 0.6093
[2023-10-04 17:38:15] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003240 train acc = 1.0000, test acc = 0.6099
[2023-10-04 17:38:39] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004059 train acc = 1.0000, test acc = 0.6104
[2023-10-04 17:39:04] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001733 train acc = 1.0000, test acc = 0.6081
[2023-10-04 17:39:28] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.018161 train acc = 1.0000, test acc = 0.6144
[2023-10-04 17:39:53] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.026578 train acc = 0.9960, test acc = 0.6087
[2023-10-04 17:40:17] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013338 train acc = 1.0000, test acc = 0.6120
[2023-10-04 17:40:41] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003467 train acc = 1.0000, test acc = 0.6091
[2023-10-04 17:41:06] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.008200 train acc = 1.0000, test acc = 0.6071
[2023-10-04 17:41:30] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003642 train acc = 1.0000, test acc = 0.6090
[2023-10-04 17:41:55] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.014758 train acc = 0.9960, test acc = 0.6133
[2023-10-04 17:42:19] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.015673 train acc = 1.0000, test acc = 0.6020
Evaluate 20 random ConvNet, mean = 0.6085 std = 0.0035
-------------------------
[2023-10-04 17:42:19] iter = 06000, loss = 1.6444
[2023-10-04 17:42:20] iter = 06010, loss = 1.6259
[2023-10-04 17:42:21] iter = 06020, loss = 1.6378
[2023-10-04 17:42:22] iter = 06030, loss = 1.5666
[2023-10-04 17:42:23] iter = 06040, loss = 1.5879
[2023-10-04 17:42:24] iter = 06050, loss = 1.5518
[2023-10-04 17:42:25] iter = 06060, loss = 1.4450
[2023-10-04 17:42:26] iter = 06070, loss = 1.6267
[2023-10-04 17:42:27] iter = 06080, loss = 1.7461
[2023-10-04 17:42:28] iter = 06090, loss = 1.5786
[2023-10-04 17:42:28] iter = 06100, loss = 1.5753
[2023-10-04 17:42:29] iter = 06110, loss = 1.7052
[2023-10-04 17:42:30] iter = 06120, loss = 1.5693
[2023-10-04 17:42:31] iter = 06130, loss = 1.5916
[2023-10-04 17:42:32] iter = 06140, loss = 1.7129
[2023-10-04 17:42:33] iter = 06150, loss = 1.6230
[2023-10-04 17:42:34] iter = 06160, loss = 1.6733
[2023-10-04 17:42:35] iter = 06170, loss = 1.7050
[2023-10-04 17:42:36] iter = 06180, loss = 1.6245
[2023-10-04 17:42:37] iter = 06190, loss = 1.5085
[2023-10-04 17:42:37] iter = 06200, loss = 1.6547
[2023-10-04 17:42:38] iter = 06210, loss = 1.5453
[2023-10-04 17:42:39] iter = 06220, loss = 1.6792
[2023-10-04 17:42:40] iter = 06230, loss = 1.5480
[2023-10-04 17:42:41] iter = 06240, loss = 1.5386
[2023-10-04 17:42:42] iter = 06250, loss = 1.5455
[2023-10-04 17:42:43] iter = 06260, loss = 1.5362
[2023-10-04 17:42:44] iter = 06270, loss = 1.4649
[2023-10-04 17:42:45] iter = 06280, loss = 1.7599
[2023-10-04 17:42:46] iter = 06290, loss = 1.5526
[2023-10-04 17:42:46] iter = 06300, loss = 1.5341
[2023-10-04 17:42:47] iter = 06310, loss = 1.6172
[2023-10-04 17:42:48] iter = 06320, loss = 1.6207
[2023-10-04 17:42:49] iter = 06330, loss = 1.7468
[2023-10-04 17:42:50] iter = 06340, loss = 1.4677
[2023-10-04 17:42:51] iter = 06350, loss = 1.7516
[2023-10-04 17:42:52] iter = 06360, loss = 1.5396
[2023-10-04 17:42:53] iter = 06370, loss = 1.7101
[2023-10-04 17:42:54] iter = 06380, loss = 1.4314
[2023-10-04 17:42:55] iter = 06390, loss = 1.6255
[2023-10-04 17:42:56] iter = 06400, loss = 1.6585
[2023-10-04 17:42:56] iter = 06410, loss = 1.6537
[2023-10-04 17:42:57] iter = 06420, loss = 1.5439
[2023-10-04 17:42:58] iter = 06430, loss = 1.5492
[2023-10-04 17:42:59] iter = 06440, loss = 1.6034
[2023-10-04 17:43:00] iter = 06450, loss = 1.5634
[2023-10-04 17:43:01] iter = 06460, loss = 1.6086
[2023-10-04 17:43:02] iter = 06470, loss = 1.5873
[2023-10-04 17:43:03] iter = 06480, loss = 1.6070
[2023-10-04 17:43:04] iter = 06490, loss = 1.5874
[2023-10-04 17:43:05] iter = 06500, loss = 1.6485
[2023-10-04 17:43:06] iter = 06510, loss = 1.5665
[2023-10-04 17:43:07] iter = 06520, loss = 1.5296
[2023-10-04 17:43:08] iter = 06530, loss = 1.6250
[2023-10-04 17:43:09] iter = 06540, loss = 1.6532
[2023-10-04 17:43:09] iter = 06550, loss = 1.6512
[2023-10-04 17:43:10] iter = 06560, loss = 1.5816
[2023-10-04 17:43:11] iter = 06570, loss = 1.6302
[2023-10-04 17:43:12] iter = 06580, loss = 1.6171
[2023-10-04 17:43:13] iter = 06590, loss = 1.6304
[2023-10-04 17:43:14] iter = 06600, loss = 1.6254
[2023-10-04 17:43:15] iter = 06610, loss = 1.7521
[2023-10-04 17:43:16] iter = 06620, loss = 1.7167
[2023-10-04 17:43:17] iter = 06630, loss = 1.6350
[2023-10-04 17:43:18] iter = 06640, loss = 1.4799
[2023-10-04 17:43:19] iter = 06650, loss = 1.5859
[2023-10-04 17:43:20] iter = 06660, loss = 1.4623
[2023-10-04 17:43:20] iter = 06670, loss = 1.5529
[2023-10-04 17:43:21] iter = 06680, loss = 1.5388
[2023-10-04 17:43:22] iter = 06690, loss = 1.6656
[2023-10-04 17:43:23] iter = 06700, loss = 1.5480
[2023-10-04 17:43:24] iter = 06710, loss = 1.5330
[2023-10-04 17:43:25] iter = 06720, loss = 1.5961
[2023-10-04 17:43:26] iter = 06730, loss = 1.5848
[2023-10-04 17:43:27] iter = 06740, loss = 1.6965
[2023-10-04 17:43:28] iter = 06750, loss = 1.5986
[2023-10-04 17:43:29] iter = 06760, loss = 1.5795
[2023-10-04 17:43:29] iter = 06770, loss = 1.5209
[2023-10-04 17:43:30] iter = 06780, loss = 1.6513
[2023-10-04 17:43:31] iter = 06790, loss = 1.6915
[2023-10-04 17:43:32] iter = 06800, loss = 1.6941
[2023-10-04 17:43:33] iter = 06810, loss = 1.7068
[2023-10-04 17:43:34] iter = 06820, loss = 1.5993
[2023-10-04 17:43:35] iter = 06830, loss = 1.5359
[2023-10-04 17:43:36] iter = 06840, loss = 1.3995
[2023-10-04 17:43:37] iter = 06850, loss = 1.5268
[2023-10-04 17:43:38] iter = 06860, loss = 1.5993
[2023-10-04 17:43:39] iter = 06870, loss = 1.7628
[2023-10-04 17:43:40] iter = 06880, loss = 1.5603
[2023-10-04 17:43:40] iter = 06890, loss = 1.5878
[2023-10-04 17:43:41] iter = 06900, loss = 1.5657
[2023-10-04 17:43:42] iter = 06910, loss = 1.5639
[2023-10-04 17:43:43] iter = 06920, loss = 1.5088
[2023-10-04 17:43:44] iter = 06930, loss = 1.4543
[2023-10-04 17:43:45] iter = 06940, loss = 1.6286
[2023-10-04 17:43:46] iter = 06950, loss = 1.5851
[2023-10-04 17:43:47] iter = 06960, loss = 1.6001
[2023-10-04 17:43:48] iter = 06970, loss = 1.5183
[2023-10-04 17:43:49] iter = 06980, loss = 1.6834
[2023-10-04 17:43:50] iter = 06990, loss = 1.5260
[2023-10-04 17:43:50] iter = 07000, loss = 1.6264
[2023-10-04 17:43:51] iter = 07010, loss = 1.5289
[2023-10-04 17:43:52] iter = 07020, loss = 1.5329
[2023-10-04 17:43:53] iter = 07030, loss = 1.6215
[2023-10-04 17:43:54] iter = 07040, loss = 1.6615
[2023-10-04 17:43:55] iter = 07050, loss = 1.6731
[2023-10-04 17:43:56] iter = 07060, loss = 1.5341
[2023-10-04 17:43:57] iter = 07070, loss = 1.6633
[2023-10-04 17:43:58] iter = 07080, loss = 1.6877
[2023-10-04 17:43:59] iter = 07090, loss = 1.6331
[2023-10-04 17:43:59] iter = 07100, loss = 1.6099
[2023-10-04 17:44:00] iter = 07110, loss = 1.4577
[2023-10-04 17:44:01] iter = 07120, loss = 1.7375
[2023-10-04 17:44:02] iter = 07130, loss = 1.4827
[2023-10-04 17:44:03] iter = 07140, loss = 1.5802
[2023-10-04 17:44:04] iter = 07150, loss = 1.5532
[2023-10-04 17:44:05] iter = 07160, loss = 1.5633
[2023-10-04 17:44:06] iter = 07170, loss = 1.6137
[2023-10-04 17:44:07] iter = 07180, loss = 1.5591
[2023-10-04 17:44:08] iter = 07190, loss = 1.6281
[2023-10-04 17:44:08] iter = 07200, loss = 1.6070
[2023-10-04 17:44:09] iter = 07210, loss = 1.5226
[2023-10-04 17:44:10] iter = 07220, loss = 1.5710
[2023-10-04 17:44:11] iter = 07230, loss = 1.6744
[2023-10-04 17:44:12] iter = 07240, loss = 1.4777
[2023-10-04 17:44:13] iter = 07250, loss = 1.6394
[2023-10-04 17:44:14] iter = 07260, loss = 1.6603
[2023-10-04 17:44:15] iter = 07270, loss = 1.7731
[2023-10-04 17:44:16] iter = 07280, loss = 1.5916
[2023-10-04 17:44:17] iter = 07290, loss = 1.7552
[2023-10-04 17:44:18] iter = 07300, loss = 1.6334
[2023-10-04 17:44:18] iter = 07310, loss = 1.5233
[2023-10-04 17:44:19] iter = 07320, loss = 1.5501
[2023-10-04 17:44:20] iter = 07330, loss = 1.6009
[2023-10-04 17:44:21] iter = 07340, loss = 1.5622
[2023-10-04 17:44:22] iter = 07350, loss = 1.5701
[2023-10-04 17:44:23] iter = 07360, loss = 1.5084
[2023-10-04 17:44:24] iter = 07370, loss = 1.7031
[2023-10-04 17:44:25] iter = 07380, loss = 1.5620
[2023-10-04 17:44:26] iter = 07390, loss = 1.5572
[2023-10-04 17:44:27] iter = 07400, loss = 1.4588
[2023-10-04 17:44:28] iter = 07410, loss = 1.6127
[2023-10-04 17:44:28] iter = 07420, loss = 1.5625
[2023-10-04 17:44:30] iter = 07430, loss = 1.5631
[2023-10-04 17:44:30] iter = 07440, loss = 1.5214
[2023-10-04 17:44:31] iter = 07450, loss = 1.6263
[2023-10-04 17:44:32] iter = 07460, loss = 1.6756
[2023-10-04 17:44:33] iter = 07470, loss = 1.5610
[2023-10-04 17:44:34] iter = 07480, loss = 1.6398
[2023-10-04 17:44:35] iter = 07490, loss = 1.6854
[2023-10-04 17:44:36] iter = 07500, loss = 1.5948
[2023-10-04 17:44:37] iter = 07510, loss = 1.5782
[2023-10-04 17:44:38] iter = 07520, loss = 1.5518
[2023-10-04 17:44:39] iter = 07530, loss = 1.5832
[2023-10-04 17:44:40] iter = 07540, loss = 1.6452
[2023-10-04 17:44:41] iter = 07550, loss = 1.5806
[2023-10-04 17:44:42] iter = 07560, loss = 1.7249
[2023-10-04 17:44:43] iter = 07570, loss = 1.5805
[2023-10-04 17:44:44] iter = 07580, loss = 1.5120
[2023-10-04 17:44:45] iter = 07590, loss = 1.5895
[2023-10-04 17:44:45] iter = 07600, loss = 1.6521
[2023-10-04 17:44:47] iter = 07610, loss = 1.5128
[2023-10-04 17:44:47] iter = 07620, loss = 1.3946
[2023-10-04 17:44:48] iter = 07630, loss = 1.5740
[2023-10-04 17:44:49] iter = 07640, loss = 1.6922
[2023-10-04 17:44:50] iter = 07650, loss = 1.5616
[2023-10-04 17:44:51] iter = 07660, loss = 1.5739
[2023-10-04 17:44:52] iter = 07670, loss = 1.5582
[2023-10-04 17:44:53] iter = 07680, loss = 1.6656
[2023-10-04 17:44:54] iter = 07690, loss = 1.6414
[2023-10-04 17:44:55] iter = 07700, loss = 1.8230
[2023-10-04 17:44:56] iter = 07710, loss = 1.5580
[2023-10-04 17:44:56] iter = 07720, loss = 1.6094
[2023-10-04 17:44:57] iter = 07730, loss = 1.5310
[2023-10-04 17:44:58] iter = 07740, loss = 1.5112
[2023-10-04 17:44:59] iter = 07750, loss = 1.6565
[2023-10-04 17:45:00] iter = 07760, loss = 1.6123
[2023-10-04 17:45:01] iter = 07770, loss = 1.6162
[2023-10-04 17:45:02] iter = 07780, loss = 1.5264
[2023-10-04 17:45:03] iter = 07790, loss = 1.6568
[2023-10-04 17:45:04] iter = 07800, loss = 1.5337
[2023-10-04 17:45:05] iter = 07810, loss = 1.7281
[2023-10-04 17:45:06] iter = 07820, loss = 1.5189
[2023-10-04 17:45:07] iter = 07830, loss = 1.5486
[2023-10-04 17:45:07] iter = 07840, loss = 1.7303
[2023-10-04 17:45:08] iter = 07850, loss = 1.5461
[2023-10-04 17:45:09] iter = 07860, loss = 1.6088
[2023-10-04 17:45:10] iter = 07870, loss = 1.5624
[2023-10-04 17:45:11] iter = 07880, loss = 1.5772
[2023-10-04 17:45:12] iter = 07890, loss = 1.6953
[2023-10-04 17:45:13] iter = 07900, loss = 1.6244
[2023-10-04 17:45:14] iter = 07910, loss = 1.5253
[2023-10-04 17:45:15] iter = 07920, loss = 1.5619
[2023-10-04 17:45:16] iter = 07930, loss = 1.6410
[2023-10-04 17:45:17] iter = 07940, loss = 1.6662
[2023-10-04 17:45:18] iter = 07950, loss = 1.4967
[2023-10-04 17:45:18] iter = 07960, loss = 1.6707
[2023-10-04 17:45:19] iter = 07970, loss = 1.5367
[2023-10-04 17:45:20] iter = 07980, loss = 1.4832
[2023-10-04 17:45:21] iter = 07990, loss = 1.5503
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 22736}
[2023-10-04 17:45:47] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002995 train acc = 1.0000, test acc = 0.6168
[2023-10-04 17:46:11] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003457 train acc = 1.0000, test acc = 0.6190
[2023-10-04 17:46:36] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002901 train acc = 1.0000, test acc = 0.6139
[2023-10-04 17:47:00] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.010396 train acc = 1.0000, test acc = 0.6090
[2023-10-04 17:47:24] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011125 train acc = 1.0000, test acc = 0.6150
[2023-10-04 17:47:49] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.008632 train acc = 1.0000, test acc = 0.6125
[2023-10-04 17:48:13] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.010971 train acc = 0.9980, test acc = 0.6149
[2023-10-04 17:48:37] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.017489 train acc = 0.9980, test acc = 0.6179
[2023-10-04 17:49:02] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.010114 train acc = 1.0000, test acc = 0.6095
[2023-10-04 17:49:26] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002493 train acc = 1.0000, test acc = 0.6123
[2023-10-04 17:49:51] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.010052 train acc = 1.0000, test acc = 0.6091
[2023-10-04 17:50:15] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.006589 train acc = 1.0000, test acc = 0.6072
[2023-10-04 17:50:39] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.016783 train acc = 1.0000, test acc = 0.6169
[2023-10-04 17:51:03] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003369 train acc = 1.0000, test acc = 0.6142
[2023-10-04 17:51:28] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.019184 train acc = 1.0000, test acc = 0.6116
[2023-10-04 17:51:52] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.012390 train acc = 1.0000, test acc = 0.6175
[2023-10-04 17:52:16] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.005380 train acc = 1.0000, test acc = 0.6167
[2023-10-04 17:52:41] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003630 train acc = 1.0000, test acc = 0.6122
[2023-10-04 17:53:05] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.008758 train acc = 1.0000, test acc = 0.6166
[2023-10-04 17:53:29] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004407 train acc = 1.0000, test acc = 0.6204
Evaluate 20 random ConvNet, mean = 0.6142 std = 0.0036
-------------------------
[2023-10-04 17:53:30] iter = 08000, loss = 1.7193
[2023-10-04 17:53:31] iter = 08010, loss = 1.5564
[2023-10-04 17:53:31] iter = 08020, loss = 1.5629
[2023-10-04 17:53:32] iter = 08030, loss = 1.5878
[2023-10-04 17:53:33] iter = 08040, loss = 1.8181
[2023-10-04 17:53:34] iter = 08050, loss = 1.6867
[2023-10-04 17:53:35] iter = 08060, loss = 1.6021
[2023-10-04 17:53:36] iter = 08070, loss = 1.4413
[2023-10-04 17:53:37] iter = 08080, loss = 1.6135
[2023-10-04 17:53:38] iter = 08090, loss = 1.4662
[2023-10-04 17:53:39] iter = 08100, loss = 1.8242
[2023-10-04 17:53:39] iter = 08110, loss = 1.5845
[2023-10-04 17:53:40] iter = 08120, loss = 1.7687
[2023-10-04 17:53:41] iter = 08130, loss = 1.5558
[2023-10-04 17:53:42] iter = 08140, loss = 1.6791
[2023-10-04 17:53:43] iter = 08150, loss = 1.5350
[2023-10-04 17:53:44] iter = 08160, loss = 1.5965
[2023-10-04 17:53:45] iter = 08170, loss = 1.4940
[2023-10-04 17:53:46] iter = 08180, loss = 1.6238
[2023-10-04 17:53:47] iter = 08190, loss = 1.4913
[2023-10-04 17:53:48] iter = 08200, loss = 1.5995
[2023-10-04 17:53:49] iter = 08210, loss = 1.6282
[2023-10-04 17:53:50] iter = 08220, loss = 1.6916
[2023-10-04 17:53:51] iter = 08230, loss = 1.7819
[2023-10-04 17:53:52] iter = 08240, loss = 1.5361
[2023-10-04 17:53:52] iter = 08250, loss = 1.4822
[2023-10-04 17:53:53] iter = 08260, loss = 1.6659
[2023-10-04 17:53:54] iter = 08270, loss = 1.6067
[2023-10-04 17:53:55] iter = 08280, loss = 1.7644
[2023-10-04 17:53:56] iter = 08290, loss = 1.5690
[2023-10-04 17:53:57] iter = 08300, loss = 1.6678
[2023-10-04 17:53:58] iter = 08310, loss = 1.6009
[2023-10-04 17:53:59] iter = 08320, loss = 1.4958
[2023-10-04 17:54:00] iter = 08330, loss = 1.6104
[2023-10-04 17:54:01] iter = 08340, loss = 1.7003
[2023-10-04 17:54:02] iter = 08350, loss = 1.5527
[2023-10-04 17:54:03] iter = 08360, loss = 1.6302
[2023-10-04 17:54:04] iter = 08370, loss = 1.6974
[2023-10-04 17:54:04] iter = 08380, loss = 1.5629
[2023-10-04 17:54:05] iter = 08390, loss = 1.4518
[2023-10-04 17:54:06] iter = 08400, loss = 1.5679
[2023-10-04 17:54:07] iter = 08410, loss = 1.4557
[2023-10-04 17:54:08] iter = 08420, loss = 1.5779
[2023-10-04 17:54:09] iter = 08430, loss = 1.6401
[2023-10-04 17:54:10] iter = 08440, loss = 1.4671
[2023-10-04 17:54:11] iter = 08450, loss = 1.4548
[2023-10-04 17:54:12] iter = 08460, loss = 1.6340
[2023-10-04 17:54:13] iter = 08470, loss = 1.5597
[2023-10-04 17:54:13] iter = 08480, loss = 1.5552
[2023-10-04 17:54:14] iter = 08490, loss = 1.7453
[2023-10-04 17:54:15] iter = 08500, loss = 1.4939
[2023-10-04 17:54:16] iter = 08510, loss = 1.6865
[2023-10-04 17:54:17] iter = 08520, loss = 1.7220
[2023-10-04 17:54:18] iter = 08530, loss = 1.5297
[2023-10-04 17:54:19] iter = 08540, loss = 1.4247
[2023-10-04 17:54:20] iter = 08550, loss = 1.4538
[2023-10-04 17:54:21] iter = 08560, loss = 1.4868
[2023-10-04 17:54:22] iter = 08570, loss = 1.5049
[2023-10-04 17:54:23] iter = 08580, loss = 1.4794
[2023-10-04 17:54:23] iter = 08590, loss = 1.6240
[2023-10-04 17:54:24] iter = 08600, loss = 1.6246
[2023-10-04 17:54:25] iter = 08610, loss = 1.5702
[2023-10-04 17:54:26] iter = 08620, loss = 1.5549
[2023-10-04 17:54:27] iter = 08630, loss = 1.7312
[2023-10-04 17:54:28] iter = 08640, loss = 1.6043
[2023-10-04 17:54:29] iter = 08650, loss = 1.4716
[2023-10-04 17:54:30] iter = 08660, loss = 1.4255
[2023-10-04 17:54:31] iter = 08670, loss = 1.4327
[2023-10-04 17:54:31] iter = 08680, loss = 1.5595
[2023-10-04 17:54:32] iter = 08690, loss = 1.6029
[2023-10-04 17:54:33] iter = 08700, loss = 1.6409
[2023-10-04 17:54:34] iter = 08710, loss = 1.7372
[2023-10-04 17:54:35] iter = 08720, loss = 1.6795
[2023-10-04 17:54:36] iter = 08730, loss = 1.5859
[2023-10-04 17:54:37] iter = 08740, loss = 1.6922
[2023-10-04 17:54:38] iter = 08750, loss = 1.6515
[2023-10-04 17:54:39] iter = 08760, loss = 1.5618
[2023-10-04 17:54:40] iter = 08770, loss = 1.5074
[2023-10-04 17:54:41] iter = 08780, loss = 1.4877
[2023-10-04 17:54:42] iter = 08790, loss = 1.6917
[2023-10-04 17:54:43] iter = 08800, loss = 1.4443
[2023-10-04 17:54:44] iter = 08810, loss = 1.6557
[2023-10-04 17:54:45] iter = 08820, loss = 1.5360
[2023-10-04 17:54:46] iter = 08830, loss = 1.7056
[2023-10-04 17:54:46] iter = 08840, loss = 1.4623
[2023-10-04 17:54:47] iter = 08850, loss = 1.4673
[2023-10-04 17:54:48] iter = 08860, loss = 1.5839
[2023-10-04 17:54:49] iter = 08870, loss = 1.6084
[2023-10-04 17:54:50] iter = 08880, loss = 1.5591
[2023-10-04 17:54:51] iter = 08890, loss = 1.4573
[2023-10-04 17:54:52] iter = 08900, loss = 1.5091
[2023-10-04 17:54:53] iter = 08910, loss = 1.6636
[2023-10-04 17:54:54] iter = 08920, loss = 1.6944
[2023-10-04 17:54:55] iter = 08930, loss = 1.5745
[2023-10-04 17:54:56] iter = 08940, loss = 1.5252
[2023-10-04 17:54:56] iter = 08950, loss = 1.6489
[2023-10-04 17:54:57] iter = 08960, loss = 1.6786
[2023-10-04 17:54:58] iter = 08970, loss = 1.5673
[2023-10-04 17:54:59] iter = 08980, loss = 1.5210
[2023-10-04 17:55:00] iter = 08990, loss = 1.4782
[2023-10-04 17:55:01] iter = 09000, loss = 1.6457
[2023-10-04 17:55:02] iter = 09010, loss = 1.4105
[2023-10-04 17:55:03] iter = 09020, loss = 1.5821
[2023-10-04 17:55:04] iter = 09030, loss = 1.5787
[2023-10-04 17:55:05] iter = 09040, loss = 1.6074
[2023-10-04 17:55:05] iter = 09050, loss = 1.5639
[2023-10-04 17:55:06] iter = 09060, loss = 1.5734
[2023-10-04 17:55:07] iter = 09070, loss = 1.5094
[2023-10-04 17:55:08] iter = 09080, loss = 1.6235
[2023-10-04 17:55:09] iter = 09090, loss = 1.7278
[2023-10-04 17:55:10] iter = 09100, loss = 1.4896
[2023-10-04 17:55:11] iter = 09110, loss = 1.4941
[2023-10-04 17:55:12] iter = 09120, loss = 1.6044
[2023-10-04 17:55:13] iter = 09130, loss = 1.4429
[2023-10-04 17:55:14] iter = 09140, loss = 1.5641
[2023-10-04 17:55:15] iter = 09150, loss = 1.4891
[2023-10-04 17:55:16] iter = 09160, loss = 1.5874
[2023-10-04 17:55:17] iter = 09170, loss = 1.6177
[2023-10-04 17:55:17] iter = 09180, loss = 1.5144
[2023-10-04 17:55:18] iter = 09190, loss = 1.6488
[2023-10-04 17:55:19] iter = 09200, loss = 1.5179
[2023-10-04 17:55:20] iter = 09210, loss = 1.5307
[2023-10-04 17:55:21] iter = 09220, loss = 1.5028
[2023-10-04 17:55:22] iter = 09230, loss = 1.3945
[2023-10-04 17:55:23] iter = 09240, loss = 1.5804
[2023-10-04 17:55:24] iter = 09250, loss = 1.5480
[2023-10-04 17:55:25] iter = 09260, loss = 1.5522
[2023-10-04 17:55:25] iter = 09270, loss = 1.4965
[2023-10-04 17:55:26] iter = 09280, loss = 1.4763
[2023-10-04 17:55:27] iter = 09290, loss = 1.5249
[2023-10-04 17:55:28] iter = 09300, loss = 1.5871
[2023-10-04 17:55:29] iter = 09310, loss = 1.6821
[2023-10-04 17:55:30] iter = 09320, loss = 1.5914
[2023-10-04 17:55:31] iter = 09330, loss = 1.5230
[2023-10-04 17:55:32] iter = 09340, loss = 1.4655
[2023-10-04 17:55:33] iter = 09350, loss = 1.5548
[2023-10-04 17:55:34] iter = 09360, loss = 1.3944
[2023-10-04 17:55:35] iter = 09370, loss = 1.5707
[2023-10-04 17:55:35] iter = 09380, loss = 1.6455
[2023-10-04 17:55:36] iter = 09390, loss = 1.5422
[2023-10-04 17:55:37] iter = 09400, loss = 1.7061
[2023-10-04 17:55:38] iter = 09410, loss = 1.5183
[2023-10-04 17:55:39] iter = 09420, loss = 1.6299
[2023-10-04 17:55:40] iter = 09430, loss = 1.6398
[2023-10-04 17:55:41] iter = 09440, loss = 1.6305
[2023-10-04 17:55:42] iter = 09450, loss = 1.4890
[2023-10-04 17:55:43] iter = 09460, loss = 1.5980
[2023-10-04 17:55:44] iter = 09470, loss = 1.5944
[2023-10-04 17:55:45] iter = 09480, loss = 1.6146
[2023-10-04 17:55:45] iter = 09490, loss = 1.6038
[2023-10-04 17:55:47] iter = 09500, loss = 1.6901
[2023-10-04 17:55:47] iter = 09510, loss = 1.5473
[2023-10-04 17:55:48] iter = 09520, loss = 1.7588
[2023-10-04 17:55:49] iter = 09530, loss = 1.5784
[2023-10-04 17:55:50] iter = 09540, loss = 1.5542
[2023-10-04 17:55:51] iter = 09550, loss = 1.5784
[2023-10-04 17:55:52] iter = 09560, loss = 1.6788
[2023-10-04 17:55:53] iter = 09570, loss = 1.4716
[2023-10-04 17:55:54] iter = 09580, loss = 1.4716
[2023-10-04 17:55:54] iter = 09590, loss = 1.5715
[2023-10-04 17:55:55] iter = 09600, loss = 1.4625
[2023-10-04 17:55:56] iter = 09610, loss = 1.5374
[2023-10-04 17:55:57] iter = 09620, loss = 1.6013
[2023-10-04 17:55:58] iter = 09630, loss = 1.5874
[2023-10-04 17:55:59] iter = 09640, loss = 1.4414
[2023-10-04 17:56:00] iter = 09650, loss = 1.4517
[2023-10-04 17:56:01] iter = 09660, loss = 1.4947
[2023-10-04 17:56:02] iter = 09670, loss = 1.6920
[2023-10-04 17:56:03] iter = 09680, loss = 1.5493
[2023-10-04 17:56:04] iter = 09690, loss = 1.5456
[2023-10-04 17:56:05] iter = 09700, loss = 1.6020
[2023-10-04 17:56:06] iter = 09710, loss = 1.5472
[2023-10-04 17:56:06] iter = 09720, loss = 1.7098
[2023-10-04 17:56:07] iter = 09730, loss = 1.4348
[2023-10-04 17:56:08] iter = 09740, loss = 1.5284
[2023-10-04 17:56:09] iter = 09750, loss = 1.5801
[2023-10-04 17:56:10] iter = 09760, loss = 1.6420
[2023-10-04 17:56:11] iter = 09770, loss = 1.5655
[2023-10-04 17:56:12] iter = 09780, loss = 1.5039
[2023-10-04 17:56:13] iter = 09790, loss = 1.5113
[2023-10-04 17:56:14] iter = 09800, loss = 1.6629
[2023-10-04 17:56:15] iter = 09810, loss = 1.5604
[2023-10-04 17:56:16] iter = 09820, loss = 1.6748
[2023-10-04 17:56:17] iter = 09830, loss = 1.5239
[2023-10-04 17:56:18] iter = 09840, loss = 1.5644
[2023-10-04 17:56:18] iter = 09850, loss = 1.5982
[2023-10-04 17:56:19] iter = 09860, loss = 1.4862
[2023-10-04 17:56:20] iter = 09870, loss = 1.6033
[2023-10-04 17:56:21] iter = 09880, loss = 1.5311
[2023-10-04 17:56:22] iter = 09890, loss = 1.5654
[2023-10-04 17:56:23] iter = 09900, loss = 1.6491
[2023-10-04 17:56:24] iter = 09910, loss = 1.4893
[2023-10-04 17:56:25] iter = 09920, loss = 1.6035
[2023-10-04 17:56:26] iter = 09930, loss = 1.5405
[2023-10-04 17:56:27] iter = 09940, loss = 1.5026
[2023-10-04 17:56:28] iter = 09950, loss = 1.3903
[2023-10-04 17:56:28] iter = 09960, loss = 1.5515
[2023-10-04 17:56:29] iter = 09970, loss = 1.5271
[2023-10-04 17:56:30] iter = 09980, loss = 1.5964
[2023-10-04 17:56:31] iter = 09990, loss = 1.5658
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 92717}
[2023-10-04 17:56:57] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.025901 train acc = 1.0000, test acc = 0.6149
[2023-10-04 17:57:21] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.016887 train acc = 1.0000, test acc = 0.6154
[2023-10-04 17:57:46] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002170 train acc = 1.0000, test acc = 0.6147
[2023-10-04 17:58:10] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003784 train acc = 1.0000, test acc = 0.6125
[2023-10-04 17:58:35] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.028627 train acc = 1.0000, test acc = 0.6144
[2023-10-04 17:58:59] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.012920 train acc = 1.0000, test acc = 0.6143
[2023-10-04 17:59:23] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.005200 train acc = 1.0000, test acc = 0.6113
[2023-10-04 17:59:48] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004638 train acc = 1.0000, test acc = 0.6156
[2023-10-04 18:00:12] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.004979 train acc = 1.0000, test acc = 0.6190
[2023-10-04 18:00:36] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013102 train acc = 1.0000, test acc = 0.6186
[2023-10-04 18:01:01] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.028398 train acc = 0.9980, test acc = 0.6175
[2023-10-04 18:01:25] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.013281 train acc = 1.0000, test acc = 0.6150
[2023-10-04 18:01:50] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.008600 train acc = 1.0000, test acc = 0.6141
[2023-10-04 18:02:14] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.009116 train acc = 1.0000, test acc = 0.6156
[2023-10-04 18:02:38] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.028466 train acc = 1.0000, test acc = 0.6187
[2023-10-04 18:03:03] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.005417 train acc = 1.0000, test acc = 0.6247
[2023-10-04 18:03:27] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.029687 train acc = 0.9980, test acc = 0.6158
[2023-10-04 18:03:51] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.013363 train acc = 1.0000, test acc = 0.6238
[2023-10-04 18:04:16] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.016335 train acc = 1.0000, test acc = 0.6228
[2023-10-04 18:04:40] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.011573 train acc = 1.0000, test acc = 0.6126
Evaluate 20 random ConvNet, mean = 0.6166 std = 0.0036
-------------------------
[2023-10-04 18:04:40] iter = 10000, loss = 1.6533
[2023-10-04 18:04:41] iter = 10010, loss = 1.4642
[2023-10-04 18:04:42] iter = 10020, loss = 1.5374
[2023-10-04 18:04:43] iter = 10030, loss = 1.7176
[2023-10-04 18:04:44] iter = 10040, loss = 1.4488
[2023-10-04 18:04:45] iter = 10050, loss = 1.5038
[2023-10-04 18:04:46] iter = 10060, loss = 1.5051
[2023-10-04 18:04:47] iter = 10070, loss = 1.6497
[2023-10-04 18:04:48] iter = 10080, loss = 1.5434
[2023-10-04 18:04:49] iter = 10090, loss = 1.5472
[2023-10-04 18:04:50] iter = 10100, loss = 1.5326
[2023-10-04 18:04:51] iter = 10110, loss = 1.6461
[2023-10-04 18:04:52] iter = 10120, loss = 1.5486
[2023-10-04 18:04:52] iter = 10130, loss = 1.6154
[2023-10-04 18:04:53] iter = 10140, loss = 1.4324
[2023-10-04 18:04:54] iter = 10150, loss = 1.5170
[2023-10-04 18:04:55] iter = 10160, loss = 1.5260
[2023-10-04 18:04:56] iter = 10170, loss = 1.7686
[2023-10-04 18:04:57] iter = 10180, loss = 1.5013
[2023-10-04 18:04:58] iter = 10190, loss = 1.4259
[2023-10-04 18:04:59] iter = 10200, loss = 1.5934
[2023-10-04 18:05:00] iter = 10210, loss = 1.7555
[2023-10-04 18:05:01] iter = 10220, loss = 1.4852
[2023-10-04 18:05:01] iter = 10230, loss = 1.6747
[2023-10-04 18:05:02] iter = 10240, loss = 1.6858
[2023-10-04 18:05:03] iter = 10250, loss = 1.4831
[2023-10-04 18:05:04] iter = 10260, loss = 1.4452
[2023-10-04 18:05:05] iter = 10270, loss = 1.5930
[2023-10-04 18:05:06] iter = 10280, loss = 1.5973
[2023-10-04 18:05:07] iter = 10290, loss = 1.4863
[2023-10-04 18:05:08] iter = 10300, loss = 1.5005
[2023-10-04 18:05:09] iter = 10310, loss = 1.5616
[2023-10-04 18:05:10] iter = 10320, loss = 1.5021
[2023-10-04 18:05:11] iter = 10330, loss = 1.5686
[2023-10-04 18:05:12] iter = 10340, loss = 1.6954
[2023-10-04 18:05:13] iter = 10350, loss = 1.4589
[2023-10-04 18:05:13] iter = 10360, loss = 1.7401
[2023-10-04 18:05:14] iter = 10370, loss = 1.4989
[2023-10-04 18:05:15] iter = 10380, loss = 1.5518
[2023-10-04 18:05:16] iter = 10390, loss = 1.5424
[2023-10-04 18:05:17] iter = 10400, loss = 1.4005
[2023-10-04 18:05:18] iter = 10410, loss = 1.7321
[2023-10-04 18:05:19] iter = 10420, loss = 1.4949
[2023-10-04 18:05:20] iter = 10430, loss = 1.5377
[2023-10-04 18:05:21] iter = 10440, loss = 1.6506
[2023-10-04 18:05:22] iter = 10450, loss = 1.5448
[2023-10-04 18:05:23] iter = 10460, loss = 1.5689
[2023-10-04 18:05:24] iter = 10470, loss = 1.5696
[2023-10-04 18:05:25] iter = 10480, loss = 1.5248
[2023-10-04 18:05:26] iter = 10490, loss = 1.7603
[2023-10-04 18:05:27] iter = 10500, loss = 1.5963
[2023-10-04 18:05:27] iter = 10510, loss = 1.4402
[2023-10-04 18:05:28] iter = 10520, loss = 1.4446
[2023-10-04 18:05:29] iter = 10530, loss = 1.5168
[2023-10-04 18:05:30] iter = 10540, loss = 1.6120
[2023-10-04 18:05:31] iter = 10550, loss = 1.7707
[2023-10-04 18:05:32] iter = 10560, loss = 1.4726
[2023-10-04 18:05:33] iter = 10570, loss = 1.5179
[2023-10-04 18:05:34] iter = 10580, loss = 1.4640
[2023-10-04 18:05:35] iter = 10590, loss = 1.4971
[2023-10-04 18:05:36] iter = 10600, loss = 1.5382
[2023-10-04 18:05:36] iter = 10610, loss = 1.4117
[2023-10-04 18:05:37] iter = 10620, loss = 1.5056
[2023-10-04 18:05:38] iter = 10630, loss = 1.6893
[2023-10-04 18:05:39] iter = 10640, loss = 1.5582
[2023-10-04 18:05:40] iter = 10650, loss = 1.6485
[2023-10-04 18:05:41] iter = 10660, loss = 1.5513
[2023-10-04 18:05:42] iter = 10670, loss = 1.5092
[2023-10-04 18:05:43] iter = 10680, loss = 1.5049
[2023-10-04 18:05:44] iter = 10690, loss = 1.6269
[2023-10-04 18:05:45] iter = 10700, loss = 1.5053
[2023-10-04 18:05:46] iter = 10710, loss = 1.5865
[2023-10-04 18:05:46] iter = 10720, loss = 1.6436
[2023-10-04 18:05:47] iter = 10730, loss = 1.4868
[2023-10-04 18:05:48] iter = 10740, loss = 1.3954
[2023-10-04 18:05:49] iter = 10750, loss = 1.6449
[2023-10-04 18:05:50] iter = 10760, loss = 1.5187
[2023-10-04 18:05:51] iter = 10770, loss = 1.6401
[2023-10-04 18:05:52] iter = 10780, loss = 1.5240
[2023-10-04 18:05:53] iter = 10790, loss = 1.5738
[2023-10-04 18:05:54] iter = 10800, loss = 1.4672
[2023-10-04 18:05:55] iter = 10810, loss = 1.4507
[2023-10-04 18:05:55] iter = 10820, loss = 1.5830
[2023-10-04 18:05:56] iter = 10830, loss = 1.5746
[2023-10-04 18:05:57] iter = 10840, loss = 1.5336
[2023-10-04 18:05:58] iter = 10850, loss = 1.5758
[2023-10-04 18:05:59] iter = 10860, loss = 1.5238
[2023-10-04 18:06:00] iter = 10870, loss = 1.5847
[2023-10-04 18:06:01] iter = 10880, loss = 1.3877
[2023-10-04 18:06:02] iter = 10890, loss = 1.6243
[2023-10-04 18:06:03] iter = 10900, loss = 1.4706
[2023-10-04 18:06:04] iter = 10910, loss = 1.4506
[2023-10-04 18:06:04] iter = 10920, loss = 1.6626
[2023-10-04 18:06:05] iter = 10930, loss = 1.6969
[2023-10-04 18:06:06] iter = 10940, loss = 1.4857
[2023-10-04 18:06:07] iter = 10950, loss = 1.4543
[2023-10-04 18:06:08] iter = 10960, loss = 1.5806
[2023-10-04 18:06:09] iter = 10970, loss = 1.5369
[2023-10-04 18:06:10] iter = 10980, loss = 1.4028
[2023-10-04 18:06:11] iter = 10990, loss = 1.4601
[2023-10-04 18:06:12] iter = 11000, loss = 1.5612
[2023-10-04 18:06:13] iter = 11010, loss = 1.4973
[2023-10-04 18:06:14] iter = 11020, loss = 1.4947
[2023-10-04 18:06:15] iter = 11030, loss = 1.5686
[2023-10-04 18:06:16] iter = 11040, loss = 1.7157
[2023-10-04 18:06:16] iter = 11050, loss = 1.6961
[2023-10-04 18:06:17] iter = 11060, loss = 1.7598
[2023-10-04 18:06:18] iter = 11070, loss = 1.6734
[2023-10-04 18:06:19] iter = 11080, loss = 1.4984
[2023-10-04 18:06:20] iter = 11090, loss = 1.5460
[2023-10-04 18:06:21] iter = 11100, loss = 1.6581
[2023-10-04 18:06:22] iter = 11110, loss = 1.5482
[2023-10-04 18:06:23] iter = 11120, loss = 1.4807
[2023-10-04 18:06:24] iter = 11130, loss = 1.4222
[2023-10-04 18:06:25] iter = 11140, loss = 1.5493
[2023-10-04 18:06:26] iter = 11150, loss = 1.6150
[2023-10-04 18:06:27] iter = 11160, loss = 1.4331
[2023-10-04 18:06:28] iter = 11170, loss = 1.5354
[2023-10-04 18:06:28] iter = 11180, loss = 1.4950
[2023-10-04 18:06:29] iter = 11190, loss = 1.5909
[2023-10-04 18:06:30] iter = 11200, loss = 1.6077
[2023-10-04 18:06:31] iter = 11210, loss = 1.5289
[2023-10-04 18:06:32] iter = 11220, loss = 1.4378
[2023-10-04 18:06:33] iter = 11230, loss = 1.5277
[2023-10-04 18:06:34] iter = 11240, loss = 1.7711
[2023-10-04 18:06:35] iter = 11250, loss = 1.4956
[2023-10-04 18:06:36] iter = 11260, loss = 1.6641
[2023-10-04 18:06:36] iter = 11270, loss = 1.5176
[2023-10-04 18:06:37] iter = 11280, loss = 1.5494
[2023-10-04 18:06:38] iter = 11290, loss = 1.4905
[2023-10-04 18:06:39] iter = 11300, loss = 1.4651
[2023-10-04 18:06:40] iter = 11310, loss = 1.7596
[2023-10-04 18:06:41] iter = 11320, loss = 1.5724
[2023-10-04 18:06:42] iter = 11330, loss = 1.4812
[2023-10-04 18:06:43] iter = 11340, loss = 1.4779
[2023-10-04 18:06:44] iter = 11350, loss = 1.5212
[2023-10-04 18:06:45] iter = 11360, loss = 1.5285
[2023-10-04 18:06:46] iter = 11370, loss = 1.5966
[2023-10-04 18:06:47] iter = 11380, loss = 1.5876
[2023-10-04 18:06:48] iter = 11390, loss = 1.5989
[2023-10-04 18:06:49] iter = 11400, loss = 1.5510
[2023-10-04 18:06:50] iter = 11410, loss = 1.5539
[2023-10-04 18:06:50] iter = 11420, loss = 1.4736
[2023-10-04 18:06:51] iter = 11430, loss = 1.5993
[2023-10-04 18:06:52] iter = 11440, loss = 1.5284
[2023-10-04 18:06:53] iter = 11450, loss = 1.3226
[2023-10-04 18:06:54] iter = 11460, loss = 1.6372
[2023-10-04 18:06:55] iter = 11470, loss = 1.4811
[2023-10-04 18:06:56] iter = 11480, loss = 1.5649
[2023-10-04 18:06:57] iter = 11490, loss = 1.5320
[2023-10-04 18:06:58] iter = 11500, loss = 1.5142
[2023-10-04 18:06:59] iter = 11510, loss = 1.5826
[2023-10-04 18:07:00] iter = 11520, loss = 1.4485
[2023-10-04 18:07:01] iter = 11530, loss = 1.5917
[2023-10-04 18:07:01] iter = 11540, loss = 1.4698
[2023-10-04 18:07:02] iter = 11550, loss = 1.5077
[2023-10-04 18:07:03] iter = 11560, loss = 1.4930
[2023-10-04 18:07:04] iter = 11570, loss = 1.5765
[2023-10-04 18:07:05] iter = 11580, loss = 1.5524
[2023-10-04 18:07:06] iter = 11590, loss = 1.5146
[2023-10-04 18:07:07] iter = 11600, loss = 1.4430
[2023-10-04 18:07:08] iter = 11610, loss = 1.4598
[2023-10-04 18:07:09] iter = 11620, loss = 1.5560
[2023-10-04 18:07:10] iter = 11630, loss = 1.5008
[2023-10-04 18:07:11] iter = 11640, loss = 1.4704
[2023-10-04 18:07:12] iter = 11650, loss = 1.5792
[2023-10-04 18:07:13] iter = 11660, loss = 1.4651
[2023-10-04 18:07:13] iter = 11670, loss = 1.5490
[2023-10-04 18:07:14] iter = 11680, loss = 1.4979
[2023-10-04 18:07:15] iter = 11690, loss = 1.5606
[2023-10-04 18:07:16] iter = 11700, loss = 1.6357
[2023-10-04 18:07:17] iter = 11710, loss = 1.6360
[2023-10-04 18:07:18] iter = 11720, loss = 1.5802
[2023-10-04 18:07:19] iter = 11730, loss = 1.6855
[2023-10-04 18:07:20] iter = 11740, loss = 1.5160
[2023-10-04 18:07:21] iter = 11750, loss = 1.5878
[2023-10-04 18:07:22] iter = 11760, loss = 1.5012
[2023-10-04 18:07:23] iter = 11770, loss = 1.5954
[2023-10-04 18:07:24] iter = 11780, loss = 1.5078
[2023-10-04 18:07:24] iter = 11790, loss = 1.4917
[2023-10-04 18:07:25] iter = 11800, loss = 1.4885
[2023-10-04 18:07:26] iter = 11810, loss = 1.5636
[2023-10-04 18:07:27] iter = 11820, loss = 1.5201
[2023-10-04 18:07:28] iter = 11830, loss = 1.5466
[2023-10-04 18:07:29] iter = 11840, loss = 1.6242
[2023-10-04 18:07:30] iter = 11850, loss = 1.6741
[2023-10-04 18:07:31] iter = 11860, loss = 1.4478
[2023-10-04 18:07:32] iter = 11870, loss = 1.7373
[2023-10-04 18:07:33] iter = 11880, loss = 1.5723
[2023-10-04 18:07:34] iter = 11890, loss = 1.5823
[2023-10-04 18:07:35] iter = 11900, loss = 1.9252
[2023-10-04 18:07:35] iter = 11910, loss = 1.5068
[2023-10-04 18:07:36] iter = 11920, loss = 1.4811
[2023-10-04 18:07:37] iter = 11930, loss = 1.4684
[2023-10-04 18:07:38] iter = 11940, loss = 1.4729
[2023-10-04 18:07:39] iter = 11950, loss = 1.5375
[2023-10-04 18:07:40] iter = 11960, loss = 1.4213
[2023-10-04 18:07:41] iter = 11970, loss = 1.4887
[2023-10-04 18:07:42] iter = 11980, loss = 1.5584
[2023-10-04 18:07:43] iter = 11990, loss = 1.4415
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 64016}
[2023-10-04 18:08:08] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.012093 train acc = 1.0000, test acc = 0.6273
[2023-10-04 18:08:33] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.016895 train acc = 1.0000, test acc = 0.6157
[2023-10-04 18:08:57] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003131 train acc = 1.0000, test acc = 0.6215
[2023-10-04 18:09:21] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003761 train acc = 1.0000, test acc = 0.6193
[2023-10-04 18:09:46] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.017400 train acc = 1.0000, test acc = 0.6197
[2023-10-04 18:10:10] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001922 train acc = 1.0000, test acc = 0.6261
[2023-10-04 18:10:35] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.015267 train acc = 1.0000, test acc = 0.6198
[2023-10-04 18:10:59] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.006112 train acc = 1.0000, test acc = 0.6247
[2023-10-04 18:11:24] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003800 train acc = 1.0000, test acc = 0.6243
[2023-10-04 18:11:48] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.009205 train acc = 0.9980, test acc = 0.6243
[2023-10-04 18:12:12] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.023673 train acc = 0.9980, test acc = 0.6196
[2023-10-04 18:12:37] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015409 train acc = 1.0000, test acc = 0.6150
[2023-10-04 18:13:01] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011190 train acc = 1.0000, test acc = 0.6114
[2023-10-04 18:13:25] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003596 train acc = 1.0000, test acc = 0.6224
[2023-10-04 18:13:50] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.010506 train acc = 1.0000, test acc = 0.6267
[2023-10-04 18:14:14] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.023638 train acc = 1.0000, test acc = 0.6225
[2023-10-04 18:14:39] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.017420 train acc = 1.0000, test acc = 0.6193
[2023-10-04 18:15:03] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.017063 train acc = 0.9980, test acc = 0.6182
[2023-10-04 18:15:28] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013336 train acc = 1.0000, test acc = 0.6237
[2023-10-04 18:15:52] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003808 train acc = 1.0000, test acc = 0.6224
Evaluate 20 random ConvNet, mean = 0.6212 std = 0.0040
-------------------------
[2023-10-04 18:15:52] iter = 12000, loss = 1.5107
[2023-10-04 18:15:53] iter = 12010, loss = 1.5739
[2023-10-04 18:15:54] iter = 12020, loss = 1.4929
[2023-10-04 18:15:55] iter = 12030, loss = 1.3778
[2023-10-04 18:15:56] iter = 12040, loss = 1.5176
[2023-10-04 18:15:57] iter = 12050, loss = 1.5339
[2023-10-04 18:15:58] iter = 12060, loss = 1.4951
[2023-10-04 18:15:59] iter = 12070, loss = 1.5564
[2023-10-04 18:16:00] iter = 12080, loss = 1.4265
[2023-10-04 18:16:01] iter = 12090, loss = 1.5144
[2023-10-04 18:16:02] iter = 12100, loss = 1.5205
[2023-10-04 18:16:02] iter = 12110, loss = 1.5593
[2023-10-04 18:16:03] iter = 12120, loss = 1.4447
[2023-10-04 18:16:04] iter = 12130, loss = 1.5712
[2023-10-04 18:16:05] iter = 12140, loss = 1.5121
[2023-10-04 18:16:06] iter = 12150, loss = 1.5387
[2023-10-04 18:16:07] iter = 12160, loss = 1.4859
[2023-10-04 18:16:08] iter = 12170, loss = 1.3189
[2023-10-04 18:16:09] iter = 12180, loss = 1.5151
[2023-10-04 18:16:10] iter = 12190, loss = 1.4659
[2023-10-04 18:16:10] iter = 12200, loss = 1.4492
[2023-10-04 18:16:11] iter = 12210, loss = 1.4758
[2023-10-04 18:16:12] iter = 12220, loss = 1.7172
[2023-10-04 18:16:13] iter = 12230, loss = 1.6123
[2023-10-04 18:16:14] iter = 12240, loss = 1.4469
[2023-10-04 18:16:15] iter = 12250, loss = 1.4297
[2023-10-04 18:16:16] iter = 12260, loss = 1.5137
[2023-10-04 18:16:17] iter = 12270, loss = 1.4165
[2023-10-04 18:16:18] iter = 12280, loss = 1.3575
[2023-10-04 18:16:19] iter = 12290, loss = 1.4670
[2023-10-04 18:16:20] iter = 12300, loss = 1.6516
[2023-10-04 18:16:20] iter = 12310, loss = 1.6187
[2023-10-04 18:16:21] iter = 12320, loss = 1.4190
[2023-10-04 18:16:22] iter = 12330, loss = 1.5485
[2023-10-04 18:16:23] iter = 12340, loss = 1.4284
[2023-10-04 18:16:24] iter = 12350, loss = 1.6593
[2023-10-04 18:16:25] iter = 12360, loss = 1.5743
[2023-10-04 18:16:26] iter = 12370, loss = 1.5711
[2023-10-04 18:16:27] iter = 12380, loss = 1.4383
[2023-10-04 18:16:28] iter = 12390, loss = 1.5300
[2023-10-04 18:16:29] iter = 12400, loss = 1.4468
[2023-10-04 18:16:30] iter = 12410, loss = 1.5244
[2023-10-04 18:16:31] iter = 12420, loss = 1.3762
[2023-10-04 18:16:31] iter = 12430, loss = 1.5597
[2023-10-04 18:16:32] iter = 12440, loss = 1.5972
[2023-10-04 18:16:33] iter = 12450, loss = 1.5829
[2023-10-04 18:16:34] iter = 12460, loss = 1.5509
[2023-10-04 18:16:35] iter = 12470, loss = 1.5059
[2023-10-04 18:16:36] iter = 12480, loss = 1.5503
[2023-10-04 18:16:37] iter = 12490, loss = 1.5564
[2023-10-04 18:16:38] iter = 12500, loss = 1.4806
[2023-10-04 18:16:39] iter = 12510, loss = 1.4800
[2023-10-04 18:16:39] iter = 12520, loss = 1.5551
[2023-10-04 18:16:40] iter = 12530, loss = 1.6263
[2023-10-04 18:16:41] iter = 12540, loss = 1.5558
[2023-10-04 18:16:42] iter = 12550, loss = 1.4124
[2023-10-04 18:16:43] iter = 12560, loss = 1.5260
[2023-10-04 18:16:44] iter = 12570, loss = 1.4487
[2023-10-04 18:16:45] iter = 12580, loss = 1.4604
[2023-10-04 18:16:46] iter = 12590, loss = 1.5616
[2023-10-04 18:16:47] iter = 12600, loss = 1.5085
[2023-10-04 18:16:48] iter = 12610, loss = 1.4747
[2023-10-04 18:16:49] iter = 12620, loss = 1.3017
[2023-10-04 18:16:50] iter = 12630, loss = 1.4969
[2023-10-04 18:16:50] iter = 12640, loss = 1.6105
[2023-10-04 18:16:51] iter = 12650, loss = 1.6904
[2023-10-04 18:16:52] iter = 12660, loss = 1.5674
[2023-10-04 18:16:53] iter = 12670, loss = 1.5007
[2023-10-04 18:16:54] iter = 12680, loss = 1.4653
[2023-10-04 18:16:55] iter = 12690, loss = 1.4983
[2023-10-04 18:16:56] iter = 12700, loss = 1.6877
[2023-10-04 18:16:57] iter = 12710, loss = 1.4980
[2023-10-04 18:16:58] iter = 12720, loss = 1.5045
[2023-10-04 18:16:59] iter = 12730, loss = 1.5052
[2023-10-04 18:17:00] iter = 12740, loss = 1.4219
[2023-10-04 18:17:00] iter = 12750, loss = 1.4975
[2023-10-04 18:17:01] iter = 12760, loss = 1.5150
[2023-10-04 18:17:02] iter = 12770, loss = 1.4876
[2023-10-04 18:17:03] iter = 12780, loss = 1.5137
[2023-10-04 18:17:04] iter = 12790, loss = 1.3680
[2023-10-04 18:17:05] iter = 12800, loss = 1.6119
[2023-10-04 18:17:06] iter = 12810, loss = 1.4809
[2023-10-04 18:17:07] iter = 12820, loss = 1.5298
[2023-10-04 18:17:08] iter = 12830, loss = 1.4719
[2023-10-04 18:17:09] iter = 12840, loss = 1.4731
[2023-10-04 18:17:10] iter = 12850, loss = 1.5024
[2023-10-04 18:17:11] iter = 12860, loss = 1.5976
[2023-10-04 18:17:11] iter = 12870, loss = 1.7185
[2023-10-04 18:17:12] iter = 12880, loss = 1.5340
[2023-10-04 18:17:13] iter = 12890, loss = 1.5767
[2023-10-04 18:17:14] iter = 12900, loss = 1.4347
[2023-10-04 18:17:15] iter = 12910, loss = 1.4964
[2023-10-04 18:17:16] iter = 12920, loss = 1.4508
[2023-10-04 18:17:17] iter = 12930, loss = 1.5554
[2023-10-04 18:17:18] iter = 12940, loss = 1.5616
[2023-10-04 18:17:19] iter = 12950, loss = 1.4681
[2023-10-04 18:17:20] iter = 12960, loss = 1.5958
[2023-10-04 18:17:21] iter = 12970, loss = 1.5300
[2023-10-04 18:17:22] iter = 12980, loss = 1.5811
[2023-10-04 18:17:23] iter = 12990, loss = 1.5249
[2023-10-04 18:17:24] iter = 13000, loss = 1.4533
[2023-10-04 18:17:25] iter = 13010, loss = 1.4291
[2023-10-04 18:17:25] iter = 13020, loss = 1.5334
[2023-10-04 18:17:26] iter = 13030, loss = 1.4060
[2023-10-04 18:17:27] iter = 13040, loss = 1.4945
[2023-10-04 18:17:28] iter = 13050, loss = 1.4890
[2023-10-04 18:17:29] iter = 13060, loss = 1.4794
[2023-10-04 18:17:30] iter = 13070, loss = 1.4513
[2023-10-04 18:17:31] iter = 13080, loss = 1.5374
[2023-10-04 18:17:32] iter = 13090, loss = 1.5460
[2023-10-04 18:17:33] iter = 13100, loss = 1.5013
[2023-10-04 18:17:34] iter = 13110, loss = 1.5530
[2023-10-04 18:17:35] iter = 13120, loss = 1.5358
[2023-10-04 18:17:35] iter = 13130, loss = 1.3477
[2023-10-04 18:17:36] iter = 13140, loss = 1.5100
[2023-10-04 18:17:37] iter = 13150, loss = 1.5842
[2023-10-04 18:17:38] iter = 13160, loss = 1.4831
[2023-10-04 18:17:39] iter = 13170, loss = 1.4302
[2023-10-04 18:17:40] iter = 13180, loss = 1.4993
[2023-10-04 18:17:41] iter = 13190, loss = 1.4081
[2023-10-04 18:17:42] iter = 13200, loss = 1.5100
[2023-10-04 18:17:43] iter = 13210, loss = 1.6884
[2023-10-04 18:17:44] iter = 13220, loss = 1.6286
[2023-10-04 18:17:44] iter = 13230, loss = 1.5273
[2023-10-04 18:17:45] iter = 13240, loss = 1.5430
[2023-10-04 18:17:46] iter = 13250, loss = 1.4537
[2023-10-04 18:17:47] iter = 13260, loss = 1.3714
[2023-10-04 18:17:48] iter = 13270, loss = 1.6311
[2023-10-04 18:17:49] iter = 13280, loss = 1.4525
[2023-10-04 18:17:50] iter = 13290, loss = 1.4128
[2023-10-04 18:17:51] iter = 13300, loss = 1.5104
[2023-10-04 18:17:52] iter = 13310, loss = 1.4400
[2023-10-04 18:17:53] iter = 13320, loss = 1.4731
[2023-10-04 18:17:53] iter = 13330, loss = 1.4589
[2023-10-04 18:17:54] iter = 13340, loss = 1.5881
[2023-10-04 18:17:55] iter = 13350, loss = 1.6343
[2023-10-04 18:17:56] iter = 13360, loss = 1.4460
[2023-10-04 18:17:57] iter = 13370, loss = 1.5695
[2023-10-04 18:17:58] iter = 13380, loss = 1.3818
[2023-10-04 18:17:59] iter = 13390, loss = 1.5149
[2023-10-04 18:18:00] iter = 13400, loss = 1.4199
[2023-10-04 18:18:01] iter = 13410, loss = 1.5138
[2023-10-04 18:18:02] iter = 13420, loss = 1.5698
[2023-10-04 18:18:03] iter = 13430, loss = 1.3989
[2023-10-04 18:18:04] iter = 13440, loss = 1.6285
[2023-10-04 18:18:04] iter = 13450, loss = 1.4843
[2023-10-04 18:18:05] iter = 13460, loss = 1.6268
[2023-10-04 18:18:06] iter = 13470, loss = 1.5330
[2023-10-04 18:18:07] iter = 13480, loss = 1.5432
[2023-10-04 18:18:08] iter = 13490, loss = 1.5427
[2023-10-04 18:18:09] iter = 13500, loss = 1.4954
[2023-10-04 18:18:10] iter = 13510, loss = 1.4527
[2023-10-04 18:18:11] iter = 13520, loss = 1.3975
[2023-10-04 18:18:12] iter = 13530, loss = 1.4301
[2023-10-04 18:18:13] iter = 13540, loss = 1.6054
[2023-10-04 18:18:14] iter = 13550, loss = 1.4650
[2023-10-04 18:18:14] iter = 13560, loss = 1.5704
[2023-10-04 18:18:15] iter = 13570, loss = 1.4834
[2023-10-04 18:18:16] iter = 13580, loss = 1.5063
[2023-10-04 18:18:17] iter = 13590, loss = 1.4898
[2023-10-04 18:18:18] iter = 13600, loss = 1.4969
[2023-10-04 18:18:19] iter = 13610, loss = 1.6343
[2023-10-04 18:18:20] iter = 13620, loss = 1.5778
[2023-10-04 18:18:21] iter = 13630, loss = 1.5974
[2023-10-04 18:18:22] iter = 13640, loss = 1.6311
[2023-10-04 18:18:23] iter = 13650, loss = 1.4721
[2023-10-04 18:18:24] iter = 13660, loss = 1.5636
[2023-10-04 18:18:25] iter = 13670, loss = 1.5200
[2023-10-04 18:18:26] iter = 13680, loss = 1.5854
[2023-10-04 18:18:27] iter = 13690, loss = 1.6746
[2023-10-04 18:18:28] iter = 13700, loss = 1.4637
[2023-10-04 18:18:29] iter = 13710, loss = 1.6033
[2023-10-04 18:18:29] iter = 13720, loss = 1.5169
[2023-10-04 18:18:30] iter = 13730, loss = 1.5996
[2023-10-04 18:18:31] iter = 13740, loss = 1.4338
[2023-10-04 18:18:32] iter = 13750, loss = 1.5446
[2023-10-04 18:18:33] iter = 13760, loss = 1.5382
[2023-10-04 18:18:34] iter = 13770, loss = 1.4783
[2023-10-04 18:18:35] iter = 13780, loss = 1.4747
[2023-10-04 18:18:36] iter = 13790, loss = 1.7058
[2023-10-04 18:18:37] iter = 13800, loss = 1.5878
[2023-10-04 18:18:38] iter = 13810, loss = 1.4770
[2023-10-04 18:18:39] iter = 13820, loss = 1.5103
[2023-10-04 18:18:39] iter = 13830, loss = 1.5626
[2023-10-04 18:18:40] iter = 13840, loss = 1.7103
[2023-10-04 18:18:41] iter = 13850, loss = 1.6323
[2023-10-04 18:18:42] iter = 13860, loss = 1.5721
[2023-10-04 18:18:43] iter = 13870, loss = 1.5519
[2023-10-04 18:18:44] iter = 13880, loss = 1.4510
[2023-10-04 18:18:45] iter = 13890, loss = 1.6532
[2023-10-04 18:18:46] iter = 13900, loss = 1.7158
[2023-10-04 18:18:47] iter = 13910, loss = 1.4158
[2023-10-04 18:18:48] iter = 13920, loss = 1.6224
[2023-10-04 18:18:49] iter = 13930, loss = 1.6190
[2023-10-04 18:18:49] iter = 13940, loss = 1.5975
[2023-10-04 18:18:50] iter = 13950, loss = 1.4218
[2023-10-04 18:18:51] iter = 13960, loss = 1.6259
[2023-10-04 18:18:52] iter = 13970, loss = 1.4726
[2023-10-04 18:18:53] iter = 13980, loss = 1.3683
[2023-10-04 18:18:54] iter = 13990, loss = 1.5538
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 35509}
[2023-10-04 18:19:19] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.018494 train acc = 0.9960, test acc = 0.6264
[2023-10-04 18:19:44] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.024102 train acc = 0.9980, test acc = 0.6278
[2023-10-04 18:20:08] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010643 train acc = 1.0000, test acc = 0.6195
[2023-10-04 18:20:33] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002996 train acc = 1.0000, test acc = 0.6267
[2023-10-04 18:20:57] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.021625 train acc = 0.9980, test acc = 0.6300
[2023-10-04 18:21:21] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.007024 train acc = 1.0000, test acc = 0.6307
[2023-10-04 18:21:45] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.018777 train acc = 1.0000, test acc = 0.6258
[2023-10-04 18:22:10] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.015355 train acc = 1.0000, test acc = 0.6233
[2023-10-04 18:22:34] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.011536 train acc = 1.0000, test acc = 0.6196
[2023-10-04 18:22:59] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.011280 train acc = 1.0000, test acc = 0.6245
[2023-10-04 18:23:23] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003260 train acc = 1.0000, test acc = 0.6220
[2023-10-04 18:23:47] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.017239 train acc = 0.9980, test acc = 0.6227
[2023-10-04 18:24:12] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.017707 train acc = 0.9980, test acc = 0.6273
[2023-10-04 18:24:36] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.015839 train acc = 1.0000, test acc = 0.6225
[2023-10-04 18:25:01] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.006159 train acc = 1.0000, test acc = 0.6257
[2023-10-04 18:25:25] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004124 train acc = 1.0000, test acc = 0.6237
[2023-10-04 18:25:50] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.007534 train acc = 1.0000, test acc = 0.6178
[2023-10-04 18:26:14] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.011159 train acc = 1.0000, test acc = 0.6168
[2023-10-04 18:26:39] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003528 train acc = 1.0000, test acc = 0.6159
[2023-10-04 18:27:03] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.014166 train acc = 1.0000, test acc = 0.6330
Evaluate 20 random ConvNet, mean = 0.6241 std = 0.0045
-------------------------
[2023-10-04 18:27:03] iter = 14000, loss = 1.4623
[2023-10-04 18:27:04] iter = 14010, loss = 1.5079
[2023-10-04 18:27:05] iter = 14020, loss = 1.4490
[2023-10-04 18:27:06] iter = 14030, loss = 1.4268
[2023-10-04 18:27:07] iter = 14040, loss = 1.4200
[2023-10-04 18:27:08] iter = 14050, loss = 1.6031
[2023-10-04 18:27:09] iter = 14060, loss = 1.3496
[2023-10-04 18:27:10] iter = 14070, loss = 1.4302
[2023-10-04 18:27:11] iter = 14080, loss = 1.5447
[2023-10-04 18:27:12] iter = 14090, loss = 1.5493
[2023-10-04 18:27:13] iter = 14100, loss = 1.4297
[2023-10-04 18:27:14] iter = 14110, loss = 1.5258
[2023-10-04 18:27:14] iter = 14120, loss = 1.5055
[2023-10-04 18:27:15] iter = 14130, loss = 1.4565
[2023-10-04 18:27:16] iter = 14140, loss = 1.5820
[2023-10-04 18:27:17] iter = 14150, loss = 1.6934
[2023-10-04 18:27:18] iter = 14160, loss = 1.5112
[2023-10-04 18:27:19] iter = 14170, loss = 1.4363
[2023-10-04 18:27:20] iter = 14180, loss = 1.6070
[2023-10-04 18:27:21] iter = 14190, loss = 1.5314
[2023-10-04 18:27:22] iter = 14200, loss = 1.4605
[2023-10-04 18:27:23] iter = 14210, loss = 1.3956
[2023-10-04 18:27:24] iter = 14220, loss = 1.4322
[2023-10-04 18:27:25] iter = 14230, loss = 1.4615
[2023-10-04 18:27:25] iter = 14240, loss = 1.4249
[2023-10-04 18:27:26] iter = 14250, loss = 1.5859
[2023-10-04 18:27:27] iter = 14260, loss = 1.3983
[2023-10-04 18:27:28] iter = 14270, loss = 1.5560
[2023-10-04 18:27:29] iter = 14280, loss = 1.5527
[2023-10-04 18:27:30] iter = 14290, loss = 1.5108
[2023-10-04 18:27:31] iter = 14300, loss = 1.4691
[2023-10-04 18:27:32] iter = 14310, loss = 1.5441
[2023-10-04 18:27:33] iter = 14320, loss = 1.5816
[2023-10-04 18:27:34] iter = 14330, loss = 1.4827
[2023-10-04 18:27:35] iter = 14340, loss = 1.5255
[2023-10-04 18:27:35] iter = 14350, loss = 1.4595
[2023-10-04 18:27:36] iter = 14360, loss = 1.4455
[2023-10-04 18:27:37] iter = 14370, loss = 1.4861
[2023-10-04 18:27:38] iter = 14380, loss = 1.6833
[2023-10-04 18:27:39] iter = 14390, loss = 1.5708
[2023-10-04 18:27:40] iter = 14400, loss = 1.5860
[2023-10-04 18:27:41] iter = 14410, loss = 1.4858
[2023-10-04 18:27:42] iter = 14420, loss = 1.5467
[2023-10-04 18:27:43] iter = 14430, loss = 1.5212
[2023-10-04 18:27:44] iter = 14440, loss = 1.5103
[2023-10-04 18:27:45] iter = 14450, loss = 1.5542
[2023-10-04 18:27:45] iter = 14460, loss = 1.5251
[2023-10-04 18:27:46] iter = 14470, loss = 1.6222
[2023-10-04 18:27:47] iter = 14480, loss = 1.4907
[2023-10-04 18:27:48] iter = 14490, loss = 1.5695
[2023-10-04 18:27:49] iter = 14500, loss = 1.5741
[2023-10-04 18:27:50] iter = 14510, loss = 1.4792
[2023-10-04 18:27:51] iter = 14520, loss = 1.5460
[2023-10-04 18:27:52] iter = 14530, loss = 1.5853
[2023-10-04 18:27:53] iter = 14540, loss = 1.5075
[2023-10-04 18:27:54] iter = 14550, loss = 1.4429
[2023-10-04 18:27:55] iter = 14560, loss = 1.5195
[2023-10-04 18:27:56] iter = 14570, loss = 1.5600
[2023-10-04 18:27:57] iter = 14580, loss = 1.5201
[2023-10-04 18:27:58] iter = 14590, loss = 1.4771
[2023-10-04 18:27:59] iter = 14600, loss = 1.4335
[2023-10-04 18:27:59] iter = 14610, loss = 1.5177
[2023-10-04 18:28:00] iter = 14620, loss = 1.4406
[2023-10-04 18:28:01] iter = 14630, loss = 1.4702
[2023-10-04 18:28:02] iter = 14640, loss = 1.5737
[2023-10-04 18:28:03] iter = 14650, loss = 1.4807
[2023-10-04 18:28:04] iter = 14660, loss = 1.3983
[2023-10-04 18:28:05] iter = 14670, loss = 1.3606
[2023-10-04 18:28:06] iter = 14680, loss = 1.4138
[2023-10-04 18:28:07] iter = 14690, loss = 1.4208
[2023-10-04 18:28:08] iter = 14700, loss = 1.5235
[2023-10-04 18:28:09] iter = 14710, loss = 1.4563
[2023-10-04 18:28:10] iter = 14720, loss = 1.5315
[2023-10-04 18:28:11] iter = 14730, loss = 1.3741
[2023-10-04 18:28:11] iter = 14740, loss = 1.4778
[2023-10-04 18:28:12] iter = 14750, loss = 1.5341
[2023-10-04 18:28:13] iter = 14760, loss = 1.5230
[2023-10-04 18:28:14] iter = 14770, loss = 1.5697
[2023-10-04 18:28:15] iter = 14780, loss = 1.5949
[2023-10-04 18:28:16] iter = 14790, loss = 1.4191
[2023-10-04 18:28:17] iter = 14800, loss = 1.3335
[2023-10-04 18:28:18] iter = 14810, loss = 1.4898
[2023-10-04 18:28:19] iter = 14820, loss = 1.4749
[2023-10-04 18:28:20] iter = 14830, loss = 1.4568
[2023-10-04 18:28:21] iter = 14840, loss = 1.4758
[2023-10-04 18:28:22] iter = 14850, loss = 1.4804
[2023-10-04 18:28:22] iter = 14860, loss = 1.5074
[2023-10-04 18:28:23] iter = 14870, loss = 1.5037
[2023-10-04 18:28:24] iter = 14880, loss = 1.4801
[2023-10-04 18:28:25] iter = 14890, loss = 1.5083
[2023-10-04 18:28:26] iter = 14900, loss = 1.5864
[2023-10-04 18:28:27] iter = 14910, loss = 1.3927
[2023-10-04 18:28:28] iter = 14920, loss = 1.4850
[2023-10-04 18:28:29] iter = 14930, loss = 1.5868
[2023-10-04 18:28:30] iter = 14940, loss = 1.5000
[2023-10-04 18:28:31] iter = 14950, loss = 1.4188
[2023-10-04 18:28:32] iter = 14960, loss = 1.5559
[2023-10-04 18:28:33] iter = 14970, loss = 1.5714
[2023-10-04 18:28:34] iter = 14980, loss = 1.4540
[2023-10-04 18:28:34] iter = 14990, loss = 1.6119
[2023-10-04 18:28:35] iter = 15000, loss = 1.5364
[2023-10-04 18:28:36] iter = 15010, loss = 1.4261
[2023-10-04 18:28:37] iter = 15020, loss = 1.6038
[2023-10-04 18:28:38] iter = 15030, loss = 1.3717
[2023-10-04 18:28:39] iter = 15040, loss = 1.4899
[2023-10-04 18:28:40] iter = 15050, loss = 1.5276
[2023-10-04 18:28:41] iter = 15060, loss = 1.6474
[2023-10-04 18:28:42] iter = 15070, loss = 1.5486
[2023-10-04 18:28:43] iter = 15080, loss = 1.4861
[2023-10-04 18:28:44] iter = 15090, loss = 1.3636
[2023-10-04 18:28:45] iter = 15100, loss = 1.5124
[2023-10-04 18:28:46] iter = 15110, loss = 1.4291
[2023-10-04 18:28:46] iter = 15120, loss = 1.3788
[2023-10-04 18:28:47] iter = 15130, loss = 1.6381
[2023-10-04 18:28:48] iter = 15140, loss = 1.4742
[2023-10-04 18:28:49] iter = 15150, loss = 1.4792
[2023-10-04 18:28:50] iter = 15160, loss = 1.4592
[2023-10-04 18:28:51] iter = 15170, loss = 1.5065
[2023-10-04 18:28:52] iter = 15180, loss = 1.4489
[2023-10-04 18:28:53] iter = 15190, loss = 1.3874
[2023-10-04 18:28:54] iter = 15200, loss = 1.4200
[2023-10-04 18:28:55] iter = 15210, loss = 1.4830
[2023-10-04 18:28:56] iter = 15220, loss = 1.5168
[2023-10-04 18:28:57] iter = 15230, loss = 1.4192
[2023-10-04 18:28:57] iter = 15240, loss = 1.6319
[2023-10-04 18:28:58] iter = 15250, loss = 1.6094
[2023-10-04 18:28:59] iter = 15260, loss = 1.4900
[2023-10-04 18:29:00] iter = 15270, loss = 1.4616
[2023-10-04 18:29:01] iter = 15280, loss = 1.4598
[2023-10-04 18:29:02] iter = 15290, loss = 1.4394
[2023-10-04 18:29:03] iter = 15300, loss = 1.3898
[2023-10-04 18:29:04] iter = 15310, loss = 1.5086
[2023-10-04 18:29:05] iter = 15320, loss = 1.4673
[2023-10-04 18:29:06] iter = 15330, loss = 1.3844
[2023-10-04 18:29:07] iter = 15340, loss = 1.3615
[2023-10-04 18:29:08] iter = 15350, loss = 1.4482
[2023-10-04 18:29:08] iter = 15360, loss = 1.6156
[2023-10-04 18:29:09] iter = 15370, loss = 1.5844
[2023-10-04 18:29:10] iter = 15380, loss = 1.3604
[2023-10-04 18:29:11] iter = 15390, loss = 1.3619
[2023-10-04 18:29:12] iter = 15400, loss = 1.6101
[2023-10-04 18:29:13] iter = 15410, loss = 1.4332
[2023-10-04 18:29:14] iter = 15420, loss = 1.4741
[2023-10-04 18:29:15] iter = 15430, loss = 1.4772
[2023-10-04 18:29:16] iter = 15440, loss = 1.4822
[2023-10-04 18:29:17] iter = 15450, loss = 1.5063
[2023-10-04 18:29:18] iter = 15460, loss = 1.5331
[2023-10-04 18:29:18] iter = 15470, loss = 1.4768
[2023-10-04 18:29:19] iter = 15480, loss = 1.5278
[2023-10-04 18:29:20] iter = 15490, loss = 1.4575
[2023-10-04 18:29:21] iter = 15500, loss = 1.4181
[2023-10-04 18:29:22] iter = 15510, loss = 1.4470
[2023-10-04 18:29:23] iter = 15520, loss = 1.4985
[2023-10-04 18:29:24] iter = 15530, loss = 1.4429
[2023-10-04 18:29:25] iter = 15540, loss = 1.3997
[2023-10-04 18:29:26] iter = 15550, loss = 1.4915
[2023-10-04 18:29:26] iter = 15560, loss = 1.5808
[2023-10-04 18:29:27] iter = 15570, loss = 1.4676
[2023-10-04 18:29:28] iter = 15580, loss = 1.4778
[2023-10-04 18:29:29] iter = 15590, loss = 1.4891
[2023-10-04 18:29:30] iter = 15600, loss = 1.4916
[2023-10-04 18:29:31] iter = 15610, loss = 1.5419
[2023-10-04 18:29:32] iter = 15620, loss = 1.4521
[2023-10-04 18:29:33] iter = 15630, loss = 1.4612
[2023-10-04 18:29:34] iter = 15640, loss = 1.5461
[2023-10-04 18:29:35] iter = 15650, loss = 1.4706
[2023-10-04 18:29:36] iter = 15660, loss = 1.4688
[2023-10-04 18:29:37] iter = 15670, loss = 1.4993
[2023-10-04 18:29:37] iter = 15680, loss = 1.6133
[2023-10-04 18:29:38] iter = 15690, loss = 1.4918
[2023-10-04 18:29:39] iter = 15700, loss = 1.5337
[2023-10-04 18:29:40] iter = 15710, loss = 1.5110
[2023-10-04 18:29:41] iter = 15720, loss = 1.3591
[2023-10-04 18:29:42] iter = 15730, loss = 1.5603
[2023-10-04 18:29:43] iter = 15740, loss = 1.5386
[2023-10-04 18:29:44] iter = 15750, loss = 1.5354
[2023-10-04 18:29:45] iter = 15760, loss = 1.4609
[2023-10-04 18:29:46] iter = 15770, loss = 1.5534
[2023-10-04 18:29:47] iter = 15780, loss = 1.5766
[2023-10-04 18:29:48] iter = 15790, loss = 1.5586
[2023-10-04 18:29:48] iter = 15800, loss = 1.3645
[2023-10-04 18:29:49] iter = 15810, loss = 1.5323
[2023-10-04 18:29:51] iter = 15820, loss = 1.6832
[2023-10-04 18:29:51] iter = 15830, loss = 1.5495
[2023-10-04 18:29:52] iter = 15840, loss = 1.4567
[2023-10-04 18:29:53] iter = 15850, loss = 1.4940
[2023-10-04 18:29:54] iter = 15860, loss = 1.7448
[2023-10-04 18:29:55] iter = 15870, loss = 1.4520
[2023-10-04 18:29:56] iter = 15880, loss = 1.4030
[2023-10-04 18:29:57] iter = 15890, loss = 1.4529
[2023-10-04 18:29:58] iter = 15900, loss = 1.5417
[2023-10-04 18:29:59] iter = 15910, loss = 1.4474
[2023-10-04 18:30:00] iter = 15920, loss = 1.6044
[2023-10-04 18:30:01] iter = 15930, loss = 1.5745
[2023-10-04 18:30:02] iter = 15940, loss = 1.4849
[2023-10-04 18:30:03] iter = 15950, loss = 1.4570
[2023-10-04 18:30:04] iter = 15960, loss = 1.6585
[2023-10-04 18:30:04] iter = 15970, loss = 1.5795
[2023-10-04 18:30:05] iter = 15980, loss = 1.6585
[2023-10-04 18:30:06] iter = 15990, loss = 1.4214
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 7527}
[2023-10-04 18:30:31] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.011919 train acc = 1.0000, test acc = 0.6205
[2023-10-04 18:30:55] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.026908 train acc = 0.9960, test acc = 0.6281
[2023-10-04 18:31:20] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010391 train acc = 0.9980, test acc = 0.6234
[2023-10-04 18:31:44] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.005612 train acc = 1.0000, test acc = 0.6176
[2023-10-04 18:32:09] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002253 train acc = 1.0000, test acc = 0.6262
[2023-10-04 18:32:33] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.018686 train acc = 1.0000, test acc = 0.6271
[2023-10-04 18:32:58] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.020291 train acc = 1.0000, test acc = 0.6235
[2023-10-04 18:33:22] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.016277 train acc = 1.0000, test acc = 0.6222
[2023-10-04 18:33:47] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003442 train acc = 1.0000, test acc = 0.6272
[2023-10-04 18:34:11] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.012435 train acc = 1.0000, test acc = 0.6298
[2023-10-04 18:34:36] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002196 train acc = 1.0000, test acc = 0.6329
[2023-10-04 18:35:00] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.029529 train acc = 0.9980, test acc = 0.6255
[2023-10-04 18:35:25] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.005903 train acc = 1.0000, test acc = 0.6240
[2023-10-04 18:35:49] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.011979 train acc = 0.9980, test acc = 0.6287
[2023-10-04 18:36:13] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.024321 train acc = 1.0000, test acc = 0.6260
[2023-10-04 18:36:38] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.034413 train acc = 1.0000, test acc = 0.6202
[2023-10-04 18:37:02] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.017007 train acc = 1.0000, test acc = 0.6234
[2023-10-04 18:37:27] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004275 train acc = 1.0000, test acc = 0.6238
[2023-10-04 18:37:51] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.026416 train acc = 1.0000, test acc = 0.6157
[2023-10-04 18:38:16] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.025274 train acc = 1.0000, test acc = 0.6223
Evaluate 20 random ConvNet, mean = 0.6244 std = 0.0040
-------------------------
[2023-10-04 18:38:16] iter = 16000, loss = 1.3975
[2023-10-04 18:38:17] iter = 16010, loss = 1.3984
[2023-10-04 18:38:18] iter = 16020, loss = 1.3575
[2023-10-04 18:38:19] iter = 16030, loss = 1.4764
[2023-10-04 18:38:19] iter = 16040, loss = 1.3899
[2023-10-04 18:38:20] iter = 16050, loss = 1.4102
[2023-10-04 18:38:21] iter = 16060, loss = 1.5079
[2023-10-04 18:38:22] iter = 16070, loss = 1.5202
[2023-10-04 18:38:23] iter = 16080, loss = 1.4601
[2023-10-04 18:38:24] iter = 16090, loss = 1.3797
[2023-10-04 18:38:25] iter = 16100, loss = 1.4814
[2023-10-04 18:38:26] iter = 16110, loss = 1.5197
[2023-10-04 18:38:27] iter = 16120, loss = 1.4762
[2023-10-04 18:38:28] iter = 16130, loss = 1.5999
[2023-10-04 18:38:29] iter = 16140, loss = 1.4559
[2023-10-04 18:38:30] iter = 16150, loss = 1.7241
[2023-10-04 18:38:31] iter = 16160, loss = 1.4981
[2023-10-04 18:38:32] iter = 16170, loss = 1.4281
[2023-10-04 18:38:33] iter = 16180, loss = 1.5101
[2023-10-04 18:38:33] iter = 16190, loss = 1.3915
[2023-10-04 18:38:34] iter = 16200, loss = 1.3756
[2023-10-04 18:38:35] iter = 16210, loss = 1.5698
[2023-10-04 18:38:36] iter = 16220, loss = 1.4456
[2023-10-04 18:38:37] iter = 16230, loss = 1.5022
[2023-10-04 18:38:38] iter = 16240, loss = 1.4506
[2023-10-04 18:38:39] iter = 16250, loss = 1.4367
[2023-10-04 18:38:40] iter = 16260, loss = 1.4732
[2023-10-04 18:38:41] iter = 16270, loss = 1.6085
[2023-10-04 18:38:42] iter = 16280, loss = 1.4582
[2023-10-04 18:38:43] iter = 16290, loss = 1.5215
[2023-10-04 18:38:44] iter = 16300, loss = 1.4528
[2023-10-04 18:38:44] iter = 16310, loss = 1.5454
[2023-10-04 18:38:45] iter = 16320, loss = 1.4010
[2023-10-04 18:38:46] iter = 16330, loss = 1.4241
[2023-10-04 18:38:47] iter = 16340, loss = 1.7144
[2023-10-04 18:38:48] iter = 16350, loss = 1.4656
[2023-10-04 18:38:49] iter = 16360, loss = 1.5294
[2023-10-04 18:38:50] iter = 16370, loss = 1.4629
[2023-10-04 18:38:51] iter = 16380, loss = 1.5433
[2023-10-04 18:38:52] iter = 16390, loss = 1.6904
[2023-10-04 18:38:53] iter = 16400, loss = 1.3635
[2023-10-04 18:38:53] iter = 16410, loss = 1.6255
[2023-10-04 18:38:54] iter = 16420, loss = 1.4601
[2023-10-04 18:38:55] iter = 16430, loss = 1.5634
[2023-10-04 18:38:56] iter = 16440, loss = 1.4932
[2023-10-04 18:38:57] iter = 16450, loss = 1.5035
[2023-10-04 18:38:58] iter = 16460, loss = 1.5144
[2023-10-04 18:38:59] iter = 16470, loss = 1.5812
[2023-10-04 18:39:00] iter = 16480, loss = 1.4260
[2023-10-04 18:39:01] iter = 16490, loss = 1.5376
[2023-10-04 18:39:02] iter = 16500, loss = 1.5309
[2023-10-04 18:39:03] iter = 16510, loss = 1.5905
[2023-10-04 18:39:04] iter = 16520, loss = 1.5312
[2023-10-04 18:39:05] iter = 16530, loss = 1.5356
[2023-10-04 18:39:06] iter = 16540, loss = 1.5992
[2023-10-04 18:39:07] iter = 16550, loss = 1.5253
[2023-10-04 18:39:07] iter = 16560, loss = 1.5881
[2023-10-04 18:39:08] iter = 16570, loss = 1.4882
[2023-10-04 18:39:09] iter = 16580, loss = 1.2998
[2023-10-04 18:39:10] iter = 16590, loss = 1.4666
[2023-10-04 18:39:11] iter = 16600, loss = 1.3230
[2023-10-04 18:39:12] iter = 16610, loss = 1.4204
[2023-10-04 18:39:13] iter = 16620, loss = 1.5135
[2023-10-04 18:39:14] iter = 16630, loss = 1.4409
[2023-10-04 18:39:15] iter = 16640, loss = 1.4725
[2023-10-04 18:39:16] iter = 16650, loss = 1.4784
[2023-10-04 18:39:17] iter = 16660, loss = 1.4794
[2023-10-04 18:39:17] iter = 16670, loss = 1.3644
[2023-10-04 18:39:18] iter = 16680, loss = 1.7194
[2023-10-04 18:39:19] iter = 16690, loss = 1.4831
[2023-10-04 18:39:20] iter = 16700, loss = 1.4637
[2023-10-04 18:39:21] iter = 16710, loss = 1.4317
[2023-10-04 18:39:22] iter = 16720, loss = 1.5050
[2023-10-04 18:39:23] iter = 16730, loss = 1.4616
[2023-10-04 18:39:24] iter = 16740, loss = 1.4474
[2023-10-04 18:39:24] iter = 16750, loss = 1.4525
[2023-10-04 18:39:26] iter = 16760, loss = 1.3914
[2023-10-04 18:39:26] iter = 16770, loss = 1.5236
[2023-10-04 18:39:27] iter = 16780, loss = 1.5784
[2023-10-04 18:39:28] iter = 16790, loss = 1.4598
[2023-10-04 18:39:29] iter = 16800, loss = 1.4735
[2023-10-04 18:39:30] iter = 16810, loss = 1.4275
[2023-10-04 18:39:31] iter = 16820, loss = 1.4242
[2023-10-04 18:39:32] iter = 16830, loss = 1.4445
[2023-10-04 18:39:33] iter = 16840, loss = 1.4977
[2023-10-04 18:39:34] iter = 16850, loss = 1.4827
[2023-10-04 18:39:35] iter = 16860, loss = 1.5286
[2023-10-04 18:39:36] iter = 16870, loss = 1.4605
[2023-10-04 18:39:37] iter = 16880, loss = 1.5124
[2023-10-04 18:39:37] iter = 16890, loss = 1.4117
[2023-10-04 18:39:38] iter = 16900, loss = 1.4176
[2023-10-04 18:39:39] iter = 16910, loss = 1.5639
[2023-10-04 18:39:40] iter = 16920, loss = 1.5177
[2023-10-04 18:39:41] iter = 16930, loss = 1.3455
[2023-10-04 18:39:42] iter = 16940, loss = 1.4239
[2023-10-04 18:39:43] iter = 16950, loss = 1.5886
[2023-10-04 18:39:44] iter = 16960, loss = 1.5527
[2023-10-04 18:39:45] iter = 16970, loss = 1.4371
[2023-10-04 18:39:46] iter = 16980, loss = 1.5539
[2023-10-04 18:39:47] iter = 16990, loss = 1.5024
[2023-10-04 18:39:48] iter = 17000, loss = 1.5784
[2023-10-04 18:39:49] iter = 17010, loss = 1.5939
[2023-10-04 18:39:49] iter = 17020, loss = 1.3908
[2023-10-04 18:39:51] iter = 17030, loss = 1.6348
[2023-10-04 18:39:51] iter = 17040, loss = 1.5830
[2023-10-04 18:39:52] iter = 17050, loss = 1.5727
[2023-10-04 18:39:53] iter = 17060, loss = 1.4502
[2023-10-04 18:39:54] iter = 17070, loss = 1.6016
[2023-10-04 18:39:55] iter = 17080, loss = 1.6044
[2023-10-04 18:39:56] iter = 17090, loss = 1.4823
[2023-10-04 18:39:57] iter = 17100, loss = 1.4615
[2023-10-04 18:39:58] iter = 17110, loss = 1.5801
[2023-10-04 18:39:58] iter = 17120, loss = 1.3593
[2023-10-04 18:39:59] iter = 17130, loss = 1.5404
[2023-10-04 18:40:00] iter = 17140, loss = 1.4982
[2023-10-04 18:40:01] iter = 17150, loss = 1.5463
[2023-10-04 18:40:02] iter = 17160, loss = 1.5643
[2023-10-04 18:40:03] iter = 17170, loss = 1.4412
[2023-10-04 18:40:04] iter = 17180, loss = 1.5627
[2023-10-04 18:40:05] iter = 17190, loss = 1.4879
[2023-10-04 18:40:06] iter = 17200, loss = 1.5625
[2023-10-04 18:40:07] iter = 17210, loss = 1.5630
[2023-10-04 18:40:08] iter = 17220, loss = 1.4911
[2023-10-04 18:40:09] iter = 17230, loss = 1.5649
[2023-10-04 18:40:09] iter = 17240, loss = 1.3752
[2023-10-04 18:40:10] iter = 17250, loss = 1.5230
[2023-10-04 18:40:11] iter = 17260, loss = 1.5225
[2023-10-04 18:40:12] iter = 17270, loss = 1.4118
[2023-10-04 18:40:13] iter = 17280, loss = 1.4511
[2023-10-04 18:40:14] iter = 17290, loss = 1.3815
[2023-10-04 18:40:15] iter = 17300, loss = 1.4890
[2023-10-04 18:40:16] iter = 17310, loss = 1.4690
[2023-10-04 18:40:17] iter = 17320, loss = 1.5552
[2023-10-04 18:40:18] iter = 17330, loss = 1.4593
[2023-10-04 18:40:19] iter = 17340, loss = 1.6323
[2023-10-04 18:40:20] iter = 17350, loss = 1.5045
[2023-10-04 18:40:20] iter = 17360, loss = 1.5451
[2023-10-04 18:40:21] iter = 17370, loss = 1.6628
[2023-10-04 18:40:22] iter = 17380, loss = 1.3074
[2023-10-04 18:40:23] iter = 17390, loss = 1.4938
[2023-10-04 18:40:24] iter = 17400, loss = 1.6001
[2023-10-04 18:40:25] iter = 17410, loss = 1.4397
[2023-10-04 18:40:26] iter = 17420, loss = 1.4085
[2023-10-04 18:40:27] iter = 17430, loss = 1.4960
[2023-10-04 18:40:28] iter = 17440, loss = 1.5063
[2023-10-04 18:40:28] iter = 17450, loss = 1.5893
[2023-10-04 18:40:29] iter = 17460, loss = 1.6774
[2023-10-04 18:40:30] iter = 17470, loss = 1.5439
[2023-10-04 18:40:31] iter = 17480, loss = 1.4964
[2023-10-04 18:40:32] iter = 17490, loss = 1.3653
[2023-10-04 18:40:33] iter = 17500, loss = 1.5246
[2023-10-04 18:40:34] iter = 17510, loss = 1.4843
[2023-10-04 18:40:35] iter = 17520, loss = 1.4502
[2023-10-04 18:40:36] iter = 17530, loss = 1.4079
[2023-10-04 18:40:37] iter = 17540, loss = 1.6174
[2023-10-04 18:40:38] iter = 17550, loss = 1.5296
[2023-10-04 18:40:39] iter = 17560, loss = 1.5250
[2023-10-04 18:40:40] iter = 17570, loss = 1.4066
[2023-10-04 18:40:41] iter = 17580, loss = 1.4898
[2023-10-04 18:40:41] iter = 17590, loss = 1.5339
[2023-10-04 18:40:42] iter = 17600, loss = 1.5444
[2023-10-04 18:40:43] iter = 17610, loss = 1.4280
[2023-10-04 18:40:44] iter = 17620, loss = 1.5740
[2023-10-04 18:40:45] iter = 17630, loss = 1.6076
[2023-10-04 18:40:46] iter = 17640, loss = 1.4898
[2023-10-04 18:40:47] iter = 17650, loss = 1.5397
[2023-10-04 18:40:48] iter = 17660, loss = 1.4217
[2023-10-04 18:40:49] iter = 17670, loss = 1.3727
[2023-10-04 18:40:50] iter = 17680, loss = 1.5169
[2023-10-04 18:40:51] iter = 17690, loss = 1.4913
[2023-10-04 18:40:52] iter = 17700, loss = 1.6750
[2023-10-04 18:40:53] iter = 17710, loss = 1.4646
[2023-10-04 18:40:53] iter = 17720, loss = 1.4045
[2023-10-04 18:40:54] iter = 17730, loss = 1.5742
[2023-10-04 18:40:55] iter = 17740, loss = 1.5124
[2023-10-04 18:40:56] iter = 17750, loss = 1.5894
[2023-10-04 18:40:57] iter = 17760, loss = 1.5264
[2023-10-04 18:40:58] iter = 17770, loss = 1.3915
[2023-10-04 18:40:59] iter = 17780, loss = 1.5200
[2023-10-04 18:41:00] iter = 17790, loss = 1.5308
[2023-10-04 18:41:01] iter = 17800, loss = 1.5610
[2023-10-04 18:41:02] iter = 17810, loss = 1.4665
[2023-10-04 18:41:02] iter = 17820, loss = 1.5904
[2023-10-04 18:41:03] iter = 17830, loss = 1.5594
[2023-10-04 18:41:04] iter = 17840, loss = 1.3288
[2023-10-04 18:41:05] iter = 17850, loss = 1.5855
[2023-10-04 18:41:06] iter = 17860, loss = 1.4382
[2023-10-04 18:41:07] iter = 17870, loss = 1.6401
[2023-10-04 18:41:08] iter = 17880, loss = 1.4996
[2023-10-04 18:41:09] iter = 17890, loss = 1.5211
[2023-10-04 18:41:10] iter = 17900, loss = 1.5266
[2023-10-04 18:41:11] iter = 17910, loss = 1.4302
[2023-10-04 18:41:12] iter = 17920, loss = 1.3520
[2023-10-04 18:41:13] iter = 17930, loss = 1.3201
[2023-10-04 18:41:14] iter = 17940, loss = 1.4022
[2023-10-04 18:41:14] iter = 17950, loss = 1.3841
[2023-10-04 18:41:15] iter = 17960, loss = 1.6146
[2023-10-04 18:41:16] iter = 17970, loss = 1.4352
[2023-10-04 18:41:17] iter = 17980, loss = 1.5169
[2023-10-04 18:41:18] iter = 17990, loss = 1.5584
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 79566}
[2023-10-04 18:41:43] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.019233 train acc = 1.0000, test acc = 0.6279
[2023-10-04 18:42:08] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004821 train acc = 1.0000, test acc = 0.6260
[2023-10-04 18:42:32] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.007939 train acc = 1.0000, test acc = 0.6279
[2023-10-04 18:42:57] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.012331 train acc = 1.0000, test acc = 0.6243
[2023-10-04 18:43:21] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.012087 train acc = 0.9980, test acc = 0.6232
[2023-10-04 18:43:46] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003060 train acc = 1.0000, test acc = 0.6292
[2023-10-04 18:44:10] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003910 train acc = 1.0000, test acc = 0.6247
[2023-10-04 18:44:35] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.018497 train acc = 1.0000, test acc = 0.6227
[2023-10-04 18:44:59] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002056 train acc = 1.0000, test acc = 0.6301
[2023-10-04 18:45:24] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.007113 train acc = 0.9980, test acc = 0.6254
[2023-10-04 18:45:48] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.001751 train acc = 1.0000, test acc = 0.6293
[2023-10-04 18:46:12] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002127 train acc = 1.0000, test acc = 0.6341
[2023-10-04 18:46:37] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001882 train acc = 1.0000, test acc = 0.6289
[2023-10-04 18:47:02] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.011533 train acc = 0.9980, test acc = 0.6251
[2023-10-04 18:47:26] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.005188 train acc = 1.0000, test acc = 0.6326
[2023-10-04 18:47:50] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.013433 train acc = 0.9980, test acc = 0.6304
[2023-10-04 18:48:15] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.007332 train acc = 1.0000, test acc = 0.6248
[2023-10-04 18:48:39] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.014558 train acc = 1.0000, test acc = 0.6291
[2023-10-04 18:49:04] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004003 train acc = 1.0000, test acc = 0.6283
[2023-10-04 18:49:28] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004345 train acc = 1.0000, test acc = 0.6276
Evaluate 20 random ConvNet, mean = 0.6276 std = 0.0030
-------------------------
[2023-10-04 18:49:28] iter = 18000, loss = 1.4147
[2023-10-04 18:49:29] iter = 18010, loss = 1.6229
[2023-10-04 18:49:30] iter = 18020, loss = 1.4478
[2023-10-04 18:49:31] iter = 18030, loss = 1.5686
[2023-10-04 18:49:32] iter = 18040, loss = 1.4886
[2023-10-04 18:49:33] iter = 18050, loss = 1.5105
[2023-10-04 18:49:34] iter = 18060, loss = 1.6352
[2023-10-04 18:49:35] iter = 18070, loss = 1.5005
[2023-10-04 18:49:36] iter = 18080, loss = 1.4667
[2023-10-04 18:49:37] iter = 18090, loss = 1.3672
[2023-10-04 18:49:38] iter = 18100, loss = 1.5787
[2023-10-04 18:49:38] iter = 18110, loss = 1.5108
[2023-10-04 18:49:39] iter = 18120, loss = 1.4353
[2023-10-04 18:49:40] iter = 18130, loss = 1.4052
[2023-10-04 18:49:41] iter = 18140, loss = 1.5961
[2023-10-04 18:49:42] iter = 18150, loss = 1.4133
[2023-10-04 18:49:43] iter = 18160, loss = 1.4785
[2023-10-04 18:49:44] iter = 18170, loss = 1.4299
[2023-10-04 18:49:45] iter = 18180, loss = 1.5450
[2023-10-04 18:49:46] iter = 18190, loss = 1.5104
[2023-10-04 18:49:47] iter = 18200, loss = 1.4218
[2023-10-04 18:49:48] iter = 18210, loss = 1.3061
[2023-10-04 18:49:49] iter = 18220, loss = 1.4924
[2023-10-04 18:49:50] iter = 18230, loss = 1.5183
[2023-10-04 18:49:50] iter = 18240, loss = 1.4240
[2023-10-04 18:49:51] iter = 18250, loss = 1.4721
[2023-10-04 18:49:52] iter = 18260, loss = 1.4117
[2023-10-04 18:49:53] iter = 18270, loss = 1.5695
[2023-10-04 18:49:54] iter = 18280, loss = 1.5605
[2023-10-04 18:49:55] iter = 18290, loss = 1.4238
[2023-10-04 18:49:56] iter = 18300, loss = 1.4419
[2023-10-04 18:49:57] iter = 18310, loss = 1.5010
[2023-10-04 18:49:58] iter = 18320, loss = 1.4150
[2023-10-04 18:49:59] iter = 18330, loss = 1.4299
[2023-10-04 18:50:00] iter = 18340, loss = 1.4470
[2023-10-04 18:50:01] iter = 18350, loss = 1.3649
[2023-10-04 18:50:01] iter = 18360, loss = 1.4202
[2023-10-04 18:50:02] iter = 18370, loss = 1.4618
[2023-10-04 18:50:03] iter = 18380, loss = 1.4477
[2023-10-04 18:50:04] iter = 18390, loss = 1.5400
[2023-10-04 18:50:05] iter = 18400, loss = 1.6355
[2023-10-04 18:50:06] iter = 18410, loss = 1.6411
[2023-10-04 18:50:07] iter = 18420, loss = 1.5214
[2023-10-04 18:50:08] iter = 18430, loss = 1.3874
[2023-10-04 18:50:09] iter = 18440, loss = 1.5163
[2023-10-04 18:50:10] iter = 18450, loss = 1.4429
[2023-10-04 18:50:11] iter = 18460, loss = 1.4759
[2023-10-04 18:50:12] iter = 18470, loss = 1.3538
[2023-10-04 18:50:13] iter = 18480, loss = 1.3664
[2023-10-04 18:50:13] iter = 18490, loss = 1.5367
[2023-10-04 18:50:14] iter = 18500, loss = 1.4089
[2023-10-04 18:50:15] iter = 18510, loss = 1.4489
[2023-10-04 18:50:16] iter = 18520, loss = 1.5319
[2023-10-04 18:50:17] iter = 18530, loss = 1.4191
[2023-10-04 18:50:18] iter = 18540, loss = 1.3920
[2023-10-04 18:50:19] iter = 18550, loss = 1.4049
[2023-10-04 18:50:20] iter = 18560, loss = 1.4521
[2023-10-04 18:50:21] iter = 18570, loss = 1.6476
[2023-10-04 18:50:22] iter = 18580, loss = 1.4260
[2023-10-04 18:50:23] iter = 18590, loss = 1.4350
[2023-10-04 18:50:23] iter = 18600, loss = 1.4655
[2023-10-04 18:50:24] iter = 18610, loss = 1.5133
[2023-10-04 18:50:25] iter = 18620, loss = 1.4560
[2023-10-04 18:50:26] iter = 18630, loss = 1.5251
[2023-10-04 18:50:27] iter = 18640, loss = 1.4555
[2023-10-04 18:50:28] iter = 18650, loss = 1.6094
[2023-10-04 18:50:29] iter = 18660, loss = 1.4605
[2023-10-04 18:50:30] iter = 18670, loss = 1.5270
[2023-10-04 18:50:31] iter = 18680, loss = 1.4244
[2023-10-04 18:50:32] iter = 18690, loss = 1.5583
[2023-10-04 18:50:33] iter = 18700, loss = 1.5225
[2023-10-04 18:50:34] iter = 18710, loss = 1.5429
[2023-10-04 18:50:35] iter = 18720, loss = 1.5417
[2023-10-04 18:50:36] iter = 18730, loss = 1.4700
[2023-10-04 18:50:36] iter = 18740, loss = 1.5028
[2023-10-04 18:50:37] iter = 18750, loss = 1.3844
[2023-10-04 18:50:38] iter = 18760, loss = 1.6381
[2023-10-04 18:50:39] iter = 18770, loss = 1.5471
[2023-10-04 18:50:40] iter = 18780, loss = 1.4250
[2023-10-04 18:50:41] iter = 18790, loss = 1.4629
[2023-10-04 18:50:42] iter = 18800, loss = 1.5463
[2023-10-04 18:50:43] iter = 18810, loss = 1.5075
[2023-10-04 18:50:44] iter = 18820, loss = 1.3808
[2023-10-04 18:50:44] iter = 18830, loss = 1.4242
[2023-10-04 18:50:46] iter = 18840, loss = 1.4646
[2023-10-04 18:50:46] iter = 18850, loss = 1.6136
[2023-10-04 18:50:47] iter = 18860, loss = 1.5348
[2023-10-04 18:50:48] iter = 18870, loss = 1.2785
[2023-10-04 18:50:49] iter = 18880, loss = 1.5287
[2023-10-04 18:50:50] iter = 18890, loss = 1.4694
[2023-10-04 18:50:51] iter = 18900, loss = 1.5064
[2023-10-04 18:50:52] iter = 18910, loss = 1.5444
[2023-10-04 18:50:53] iter = 18920, loss = 1.4813
[2023-10-04 18:50:53] iter = 18930, loss = 1.5362
[2023-10-04 18:50:55] iter = 18940, loss = 1.3919
[2023-10-04 18:50:55] iter = 18950, loss = 1.5916
[2023-10-04 18:50:56] iter = 18960, loss = 1.4149
[2023-10-04 18:50:57] iter = 18970, loss = 1.5166
[2023-10-04 18:50:58] iter = 18980, loss = 1.3834
[2023-10-04 18:50:59] iter = 18990, loss = 1.5351
[2023-10-04 18:51:00] iter = 19000, loss = 1.5428
[2023-10-04 18:51:01] iter = 19010, loss = 1.4416
[2023-10-04 18:51:02] iter = 19020, loss = 1.4548
[2023-10-04 18:51:03] iter = 19030, loss = 1.5144
[2023-10-04 18:51:04] iter = 19040, loss = 1.4992
[2023-10-04 18:51:04] iter = 19050, loss = 1.4341
[2023-10-04 18:51:05] iter = 19060, loss = 1.4779
[2023-10-04 18:51:06] iter = 19070, loss = 1.4949
[2023-10-04 18:51:07] iter = 19080, loss = 1.5477
[2023-10-04 18:51:08] iter = 19090, loss = 1.5526
[2023-10-04 18:51:09] iter = 19100, loss = 1.6517
[2023-10-04 18:51:10] iter = 19110, loss = 1.6055
[2023-10-04 18:51:11] iter = 19120, loss = 1.4444
[2023-10-04 18:51:12] iter = 19130, loss = 1.3241
[2023-10-04 18:51:13] iter = 19140, loss = 1.5561
[2023-10-04 18:51:14] iter = 19150, loss = 1.5220
[2023-10-04 18:51:15] iter = 19160, loss = 1.4641
[2023-10-04 18:51:16] iter = 19170, loss = 1.5162
[2023-10-04 18:51:16] iter = 19180, loss = 1.4067
[2023-10-04 18:51:17] iter = 19190, loss = 1.4390
[2023-10-04 18:51:18] iter = 19200, loss = 1.3891
[2023-10-04 18:51:19] iter = 19210, loss = 1.4305
[2023-10-04 18:51:20] iter = 19220, loss = 1.4720
[2023-10-04 18:51:21] iter = 19230, loss = 1.5013
[2023-10-04 18:51:22] iter = 19240, loss = 1.3835
[2023-10-04 18:51:23] iter = 19250, loss = 1.5639
[2023-10-04 18:51:24] iter = 19260, loss = 1.5794
[2023-10-04 18:51:25] iter = 19270, loss = 1.3162
[2023-10-04 18:51:25] iter = 19280, loss = 1.4935
[2023-10-04 18:51:26] iter = 19290, loss = 1.6084
[2023-10-04 18:51:27] iter = 19300, loss = 1.4183
[2023-10-04 18:51:28] iter = 19310, loss = 1.4328
[2023-10-04 18:51:29] iter = 19320, loss = 1.5852
[2023-10-04 18:51:30] iter = 19330, loss = 1.6230
[2023-10-04 18:51:31] iter = 19340, loss = 1.4613
[2023-10-04 18:51:32] iter = 19350, loss = 1.5290
[2023-10-04 18:51:33] iter = 19360, loss = 1.4665
[2023-10-04 18:51:34] iter = 19370, loss = 1.5861
[2023-10-04 18:51:35] iter = 19380, loss = 1.4486
[2023-10-04 18:51:35] iter = 19390, loss = 1.7805
[2023-10-04 18:51:36] iter = 19400, loss = 1.3822
[2023-10-04 18:51:37] iter = 19410, loss = 1.5966
[2023-10-04 18:51:38] iter = 19420, loss = 1.4800
[2023-10-04 18:51:39] iter = 19430, loss = 1.4343
[2023-10-04 18:51:40] iter = 19440, loss = 1.4630
[2023-10-04 18:51:41] iter = 19450, loss = 1.4555
[2023-10-04 18:51:42] iter = 19460, loss = 1.4470
[2023-10-04 18:51:43] iter = 19470, loss = 1.4259
[2023-10-04 18:51:44] iter = 19480, loss = 1.3687
[2023-10-04 18:51:45] iter = 19490, loss = 1.3760
[2023-10-04 18:51:45] iter = 19500, loss = 1.3600
[2023-10-04 18:51:46] iter = 19510, loss = 1.5337
[2023-10-04 18:51:47] iter = 19520, loss = 1.5369
[2023-10-04 18:51:48] iter = 19530, loss = 1.4851
[2023-10-04 18:51:49] iter = 19540, loss = 1.4616
[2023-10-04 18:51:50] iter = 19550, loss = 1.5744
[2023-10-04 18:51:51] iter = 19560, loss = 1.4875
[2023-10-04 18:51:52] iter = 19570, loss = 1.3933
[2023-10-04 18:51:53] iter = 19580, loss = 1.5068
[2023-10-04 18:51:54] iter = 19590, loss = 1.5883
[2023-10-04 18:51:55] iter = 19600, loss = 1.4510
[2023-10-04 18:51:56] iter = 19610, loss = 1.4055
[2023-10-04 18:51:57] iter = 19620, loss = 1.4690
[2023-10-04 18:51:58] iter = 19630, loss = 1.5893
[2023-10-04 18:51:59] iter = 19640, loss = 1.6937
[2023-10-04 18:51:59] iter = 19650, loss = 1.5729
[2023-10-04 18:52:00] iter = 19660, loss = 1.3641
[2023-10-04 18:52:01] iter = 19670, loss = 1.4356
[2023-10-04 18:52:02] iter = 19680, loss = 1.5215
[2023-10-04 18:52:03] iter = 19690, loss = 1.3601
[2023-10-04 18:52:04] iter = 19700, loss = 1.4282
[2023-10-04 18:52:05] iter = 19710, loss = 1.4309
[2023-10-04 18:52:06] iter = 19720, loss = 1.4873
[2023-10-04 18:52:07] iter = 19730, loss = 1.6141
[2023-10-04 18:52:08] iter = 19740, loss = 1.5550
[2023-10-04 18:52:09] iter = 19750, loss = 1.4529
[2023-10-04 18:52:09] iter = 19760, loss = 1.5212
[2023-10-04 18:52:10] iter = 19770, loss = 1.5022
[2023-10-04 18:52:11] iter = 19780, loss = 1.4679
[2023-10-04 18:52:12] iter = 19790, loss = 1.5023
[2023-10-04 18:52:13] iter = 19800, loss = 1.4516
[2023-10-04 18:52:14] iter = 19810, loss = 1.4130
[2023-10-04 18:52:15] iter = 19820, loss = 1.4418
[2023-10-04 18:52:16] iter = 19830, loss = 1.4884
[2023-10-04 18:52:17] iter = 19840, loss = 1.4274
[2023-10-04 18:52:18] iter = 19850, loss = 1.5719
[2023-10-04 18:52:19] iter = 19860, loss = 1.5943
[2023-10-04 18:52:20] iter = 19870, loss = 1.4530
[2023-10-04 18:52:21] iter = 19880, loss = 1.4175
[2023-10-04 18:52:21] iter = 19890, loss = 1.5503
[2023-10-04 18:52:23] iter = 19900, loss = 1.4031
[2023-10-04 18:52:24] iter = 19910, loss = 1.4460
[2023-10-04 18:52:24] iter = 19920, loss = 1.3897
[2023-10-04 18:52:25] iter = 19930, loss = 1.4722
[2023-10-04 18:52:26] iter = 19940, loss = 1.4749
[2023-10-04 18:52:27] iter = 19950, loss = 1.4936
[2023-10-04 18:52:28] iter = 19960, loss = 1.4542
[2023-10-04 18:52:29] iter = 19970, loss = 1.4524
[2023-10-04 18:52:30] iter = 19980, loss = 1.4919
[2023-10-04 18:52:31] iter = 19990, loss = 1.5175
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 52032}
[2023-10-04 18:52:56] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005895 train acc = 0.9980, test acc = 0.6266
[2023-10-04 18:53:20] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003074 train acc = 1.0000, test acc = 0.6266
[2023-10-04 18:53:45] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.012062 train acc = 1.0000, test acc = 0.6269
[2023-10-04 18:54:09] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.020353 train acc = 0.9980, test acc = 0.6233
[2023-10-04 18:54:34] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.032657 train acc = 0.9980, test acc = 0.6283
[2023-10-04 18:54:58] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.019064 train acc = 0.9980, test acc = 0.6239
[2023-10-04 18:55:22] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012826 train acc = 1.0000, test acc = 0.6257
[2023-10-04 18:55:47] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002770 train acc = 1.0000, test acc = 0.6223
[2023-10-04 18:56:11] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.024541 train acc = 0.9960, test acc = 0.6270
[2023-10-04 18:56:36] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.018249 train acc = 1.0000, test acc = 0.6278
[2023-10-04 18:57:00] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.028057 train acc = 0.9960, test acc = 0.6380
[2023-10-04 18:57:24] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.005484 train acc = 1.0000, test acc = 0.6246
[2023-10-04 18:57:49] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.018009 train acc = 0.9980, test acc = 0.6256
[2023-10-04 18:58:13] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.009615 train acc = 1.0000, test acc = 0.6312
[2023-10-04 18:58:37] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.014142 train acc = 1.0000, test acc = 0.6244
[2023-10-04 18:59:02] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.007852 train acc = 1.0000, test acc = 0.6264
[2023-10-04 18:59:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.017359 train acc = 1.0000, test acc = 0.6264
[2023-10-04 18:59:50] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.009854 train acc = 1.0000, test acc = 0.6277
[2023-10-04 19:00:15] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013041 train acc = 1.0000, test acc = 0.6223
[2023-10-04 19:00:39] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.002703 train acc = 1.0000, test acc = 0.6262
Evaluate 20 random ConvNet, mean = 0.6266 std = 0.0033
-------------------------
[2023-10-04 19:00:40] iter = 20000, loss = 1.5077

================== Exp 3 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5173a57f40>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-04 19:00:57] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 40180}
[2023-10-04 19:01:21] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.000911 train acc = 1.0000, test acc = 0.5035
[2023-10-04 19:01:46] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.015335 train acc = 1.0000, test acc = 0.5020
[2023-10-04 19:02:10] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.011015 train acc = 1.0000, test acc = 0.5000
[2023-10-04 19:02:34] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.000775 train acc = 1.0000, test acc = 0.5057
[2023-10-04 19:02:59] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.008606 train acc = 1.0000, test acc = 0.5079
[2023-10-04 19:03:23] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011374 train acc = 1.0000, test acc = 0.5053
[2023-10-04 19:03:48] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003887 train acc = 1.0000, test acc = 0.5072
[2023-10-04 19:04:12] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002403 train acc = 1.0000, test acc = 0.4999
[2023-10-04 19:04:36] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002829 train acc = 1.0000, test acc = 0.4991
[2023-10-04 19:05:01] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002960 train acc = 1.0000, test acc = 0.4995
[2023-10-04 19:05:25] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002783 train acc = 1.0000, test acc = 0.5046
[2023-10-04 19:05:50] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002860 train acc = 1.0000, test acc = 0.5052
[2023-10-04 19:06:14] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.012306 train acc = 1.0000, test acc = 0.5081
[2023-10-04 19:06:38] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.005853 train acc = 1.0000, test acc = 0.5104
[2023-10-04 19:07:03] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.009513 train acc = 1.0000, test acc = 0.5080
[2023-10-04 19:07:27] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001608 train acc = 1.0000, test acc = 0.4976
[2023-10-04 19:07:52] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004915 train acc = 1.0000, test acc = 0.5070
[2023-10-04 19:08:16] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.006610 train acc = 1.0000, test acc = 0.5049
[2023-10-04 19:08:41] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.011225 train acc = 1.0000, test acc = 0.4996
[2023-10-04 19:09:05] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012428 train acc = 1.0000, test acc = 0.5024
Evaluate 20 random ConvNet, mean = 0.5039 std = 0.0036
-------------------------
[2023-10-04 19:09:05] iter = 00000, loss = 5.8302
[2023-10-04 19:09:06] iter = 00010, loss = 4.9278
[2023-10-04 19:09:07] iter = 00020, loss = 4.8491
[2023-10-04 19:09:08] iter = 00030, loss = 4.1829
[2023-10-04 19:09:09] iter = 00040, loss = 3.9829
[2023-10-04 19:09:10] iter = 00050, loss = 3.4974
[2023-10-04 19:09:11] iter = 00060, loss = 3.5334
[2023-10-04 19:09:12] iter = 00070, loss = 3.3191
[2023-10-04 19:09:13] iter = 00080, loss = 3.1984
[2023-10-04 19:09:14] iter = 00090, loss = 3.5580
[2023-10-04 19:09:15] iter = 00100, loss = 3.2208
[2023-10-04 19:09:16] iter = 00110, loss = 3.0569
[2023-10-04 19:09:16] iter = 00120, loss = 2.8479
[2023-10-04 19:09:17] iter = 00130, loss = 2.8147
[2023-10-04 19:09:18] iter = 00140, loss = 2.9953
[2023-10-04 19:09:19] iter = 00150, loss = 2.8471
[2023-10-04 19:09:20] iter = 00160, loss = 2.8343
[2023-10-04 19:09:21] iter = 00170, loss = 2.8288
[2023-10-04 19:09:22] iter = 00180, loss = 2.5819
[2023-10-04 19:09:23] iter = 00190, loss = 2.5583
[2023-10-04 19:09:24] iter = 00200, loss = 2.7213
[2023-10-04 19:09:25] iter = 00210, loss = 2.7364
[2023-10-04 19:09:26] iter = 00220, loss = 2.5775
[2023-10-04 19:09:26] iter = 00230, loss = 2.3597
[2023-10-04 19:09:27] iter = 00240, loss = 2.5561
[2023-10-04 19:09:28] iter = 00250, loss = 2.5642
[2023-10-04 19:09:29] iter = 00260, loss = 2.5889
[2023-10-04 19:09:30] iter = 00270, loss = 2.5240
[2023-10-04 19:09:31] iter = 00280, loss = 2.4551
[2023-10-04 19:09:32] iter = 00290, loss = 2.4386
[2023-10-04 19:09:33] iter = 00300, loss = 2.4138
[2023-10-04 19:09:34] iter = 00310, loss = 2.3632
[2023-10-04 19:09:35] iter = 00320, loss = 2.5554
[2023-10-04 19:09:36] iter = 00330, loss = 2.4131
[2023-10-04 19:09:36] iter = 00340, loss = 2.5480
[2023-10-04 19:09:37] iter = 00350, loss = 2.3872
[2023-10-04 19:09:38] iter = 00360, loss = 2.4327
[2023-10-04 19:09:39] iter = 00370, loss = 2.4532
[2023-10-04 19:09:40] iter = 00380, loss = 2.4627
[2023-10-04 19:09:41] iter = 00390, loss = 2.4500
[2023-10-04 19:09:42] iter = 00400, loss = 2.1340
[2023-10-04 19:09:43] iter = 00410, loss = 2.2525
[2023-10-04 19:09:44] iter = 00420, loss = 2.3915
[2023-10-04 19:09:45] iter = 00430, loss = 2.1842
[2023-10-04 19:09:46] iter = 00440, loss = 2.5792
[2023-10-04 19:09:47] iter = 00450, loss = 2.2051
[2023-10-04 19:09:47] iter = 00460, loss = 2.3153
[2023-10-04 19:09:48] iter = 00470, loss = 2.4965
[2023-10-04 19:09:49] iter = 00480, loss = 2.2750
[2023-10-04 19:09:50] iter = 00490, loss = 2.1128
[2023-10-04 19:09:51] iter = 00500, loss = 2.0622
[2023-10-04 19:09:52] iter = 00510, loss = 2.3429
[2023-10-04 19:09:53] iter = 00520, loss = 2.2162
[2023-10-04 19:09:54] iter = 00530, loss = 2.2265
[2023-10-04 19:09:55] iter = 00540, loss = 2.1308
[2023-10-04 19:09:56] iter = 00550, loss = 2.0795
[2023-10-04 19:09:57] iter = 00560, loss = 2.2996
[2023-10-04 19:09:58] iter = 00570, loss = 2.1804
[2023-10-04 19:09:58] iter = 00580, loss = 2.1511
[2023-10-04 19:09:59] iter = 00590, loss = 2.1575
[2023-10-04 19:10:00] iter = 00600, loss = 2.2586
[2023-10-04 19:10:01] iter = 00610, loss = 2.0578
[2023-10-04 19:10:02] iter = 00620, loss = 2.0019
[2023-10-04 19:10:03] iter = 00630, loss = 2.1591
[2023-10-04 19:10:04] iter = 00640, loss = 2.2552
[2023-10-04 19:10:05] iter = 00650, loss = 2.1485
[2023-10-04 19:10:06] iter = 00660, loss = 2.0859
[2023-10-04 19:10:07] iter = 00670, loss = 2.2242
[2023-10-04 19:10:08] iter = 00680, loss = 2.0208
[2023-10-04 19:10:09] iter = 00690, loss = 2.2185
[2023-10-04 19:10:10] iter = 00700, loss = 2.1002
[2023-10-04 19:10:11] iter = 00710, loss = 2.2050
[2023-10-04 19:10:11] iter = 00720, loss = 2.2470
[2023-10-04 19:10:12] iter = 00730, loss = 2.2827
[2023-10-04 19:10:13] iter = 00740, loss = 2.1279
[2023-10-04 19:10:14] iter = 00750, loss = 2.1283
[2023-10-04 19:10:15] iter = 00760, loss = 2.1923
[2023-10-04 19:10:16] iter = 00770, loss = 1.9682
[2023-10-04 19:10:17] iter = 00780, loss = 2.1710
[2023-10-04 19:10:18] iter = 00790, loss = 2.0899
[2023-10-04 19:10:19] iter = 00800, loss = 1.9770
[2023-10-04 19:10:20] iter = 00810, loss = 2.1072
[2023-10-04 19:10:21] iter = 00820, loss = 2.1028
[2023-10-04 19:10:22] iter = 00830, loss = 2.1751
[2023-10-04 19:10:23] iter = 00840, loss = 2.4160
[2023-10-04 19:10:23] iter = 00850, loss = 2.0524
[2023-10-04 19:10:24] iter = 00860, loss = 2.1175
[2023-10-04 19:10:25] iter = 00870, loss = 2.0969
[2023-10-04 19:10:26] iter = 00880, loss = 2.1281
[2023-10-04 19:10:27] iter = 00890, loss = 2.0116
[2023-10-04 19:10:28] iter = 00900, loss = 2.0987
[2023-10-04 19:10:29] iter = 00910, loss = 2.0965
[2023-10-04 19:10:30] iter = 00920, loss = 2.0814
[2023-10-04 19:10:31] iter = 00930, loss = 2.0264
[2023-10-04 19:10:32] iter = 00940, loss = 2.1249
[2023-10-04 19:10:33] iter = 00950, loss = 2.0578
[2023-10-04 19:10:33] iter = 00960, loss = 1.9964
[2023-10-04 19:10:34] iter = 00970, loss = 1.8024
[2023-10-04 19:10:35] iter = 00980, loss = 1.9709
[2023-10-04 19:10:36] iter = 00990, loss = 1.8235
[2023-10-04 19:10:37] iter = 01000, loss = 2.2139
[2023-10-04 19:10:38] iter = 01010, loss = 2.0105
[2023-10-04 19:10:39] iter = 01020, loss = 2.1122
[2023-10-04 19:10:40] iter = 01030, loss = 2.1015
[2023-10-04 19:10:41] iter = 01040, loss = 1.9900
[2023-10-04 19:10:42] iter = 01050, loss = 2.0031
[2023-10-04 19:10:42] iter = 01060, loss = 1.8665
[2023-10-04 19:10:43] iter = 01070, loss = 1.9440
[2023-10-04 19:10:44] iter = 01080, loss = 1.9622
[2023-10-04 19:10:45] iter = 01090, loss = 1.8692
[2023-10-04 19:10:46] iter = 01100, loss = 2.0211
[2023-10-04 19:10:47] iter = 01110, loss = 2.1930
[2023-10-04 19:10:48] iter = 01120, loss = 1.8671
[2023-10-04 19:10:49] iter = 01130, loss = 2.0091
[2023-10-04 19:10:50] iter = 01140, loss = 1.9450
[2023-10-04 19:10:51] iter = 01150, loss = 1.9052
[2023-10-04 19:10:51] iter = 01160, loss = 2.0665
[2023-10-04 19:10:52] iter = 01170, loss = 1.9228
[2023-10-04 19:10:53] iter = 01180, loss = 1.9093
[2023-10-04 19:10:54] iter = 01190, loss = 1.9620
[2023-10-04 19:10:55] iter = 01200, loss = 2.1809
[2023-10-04 19:10:56] iter = 01210, loss = 2.0916
[2023-10-04 19:10:57] iter = 01220, loss = 1.8706
[2023-10-04 19:10:58] iter = 01230, loss = 1.9314
[2023-10-04 19:10:59] iter = 01240, loss = 2.0660
[2023-10-04 19:11:00] iter = 01250, loss = 2.0889
[2023-10-04 19:11:01] iter = 01260, loss = 1.8103
[2023-10-04 19:11:01] iter = 01270, loss = 1.8691
[2023-10-04 19:11:02] iter = 01280, loss = 2.1258
[2023-10-04 19:11:03] iter = 01290, loss = 1.8444
[2023-10-04 19:11:04] iter = 01300, loss = 1.8837
[2023-10-04 19:11:05] iter = 01310, loss = 1.9390
[2023-10-04 19:11:06] iter = 01320, loss = 1.9568
[2023-10-04 19:11:07] iter = 01330, loss = 1.9520
[2023-10-04 19:11:08] iter = 01340, loss = 2.0569
[2023-10-04 19:11:09] iter = 01350, loss = 1.9964
[2023-10-04 19:11:10] iter = 01360, loss = 1.9227
[2023-10-04 19:11:11] iter = 01370, loss = 1.8776
[2023-10-04 19:11:11] iter = 01380, loss = 1.8974
[2023-10-04 19:11:12] iter = 01390, loss = 2.0314
[2023-10-04 19:11:13] iter = 01400, loss = 1.8557
[2023-10-04 19:11:14] iter = 01410, loss = 1.8410
[2023-10-04 19:11:15] iter = 01420, loss = 2.0303
[2023-10-04 19:11:16] iter = 01430, loss = 2.0093
[2023-10-04 19:11:17] iter = 01440, loss = 1.8902
[2023-10-04 19:11:18] iter = 01450, loss = 1.8654
[2023-10-04 19:11:19] iter = 01460, loss = 1.8566
[2023-10-04 19:11:20] iter = 01470, loss = 2.0567
[2023-10-04 19:11:21] iter = 01480, loss = 1.8508
[2023-10-04 19:11:22] iter = 01490, loss = 1.9040
[2023-10-04 19:11:23] iter = 01500, loss = 2.0266
[2023-10-04 19:11:24] iter = 01510, loss = 1.9156
[2023-10-04 19:11:25] iter = 01520, loss = 2.0621
[2023-10-04 19:11:26] iter = 01530, loss = 1.9460
[2023-10-04 19:11:26] iter = 01540, loss = 1.8911
[2023-10-04 19:11:27] iter = 01550, loss = 1.9928
[2023-10-04 19:11:28] iter = 01560, loss = 1.8664
[2023-10-04 19:11:29] iter = 01570, loss = 1.9735
[2023-10-04 19:11:30] iter = 01580, loss = 1.9756
[2023-10-04 19:11:31] iter = 01590, loss = 2.0869
[2023-10-04 19:11:32] iter = 01600, loss = 1.7155
[2023-10-04 19:11:33] iter = 01610, loss = 2.0522
[2023-10-04 19:11:34] iter = 01620, loss = 2.1206
[2023-10-04 19:11:35] iter = 01630, loss = 1.9329
[2023-10-04 19:11:36] iter = 01640, loss = 1.8819
[2023-10-04 19:11:37] iter = 01650, loss = 1.8391
[2023-10-04 19:11:38] iter = 01660, loss = 1.9408
[2023-10-04 19:11:38] iter = 01670, loss = 1.9277
[2023-10-04 19:11:39] iter = 01680, loss = 1.8657
[2023-10-04 19:11:40] iter = 01690, loss = 1.7531
[2023-10-04 19:11:41] iter = 01700, loss = 2.0605
[2023-10-04 19:11:42] iter = 01710, loss = 1.8152
[2023-10-04 19:11:43] iter = 01720, loss = 1.9662
[2023-10-04 19:11:44] iter = 01730, loss = 1.9191
[2023-10-04 19:11:45] iter = 01740, loss = 1.8910
[2023-10-04 19:11:46] iter = 01750, loss = 1.8005
[2023-10-04 19:11:47] iter = 01760, loss = 1.9068
[2023-10-04 19:11:48] iter = 01770, loss = 1.8896
[2023-10-04 19:11:49] iter = 01780, loss = 1.9656
[2023-10-04 19:11:49] iter = 01790, loss = 1.8825
[2023-10-04 19:11:50] iter = 01800, loss = 1.7700
[2023-10-04 19:11:51] iter = 01810, loss = 1.8381
[2023-10-04 19:11:52] iter = 01820, loss = 1.8322
[2023-10-04 19:11:53] iter = 01830, loss = 1.8191
[2023-10-04 19:11:54] iter = 01840, loss = 1.9775
[2023-10-04 19:11:55] iter = 01850, loss = 1.8395
[2023-10-04 19:11:56] iter = 01860, loss = 1.8335
[2023-10-04 19:11:57] iter = 01870, loss = 1.8818
[2023-10-04 19:11:58] iter = 01880, loss = 1.8449
[2023-10-04 19:11:59] iter = 01890, loss = 1.9153
[2023-10-04 19:11:59] iter = 01900, loss = 1.8224
[2023-10-04 19:12:00] iter = 01910, loss = 1.7188
[2023-10-04 19:12:01] iter = 01920, loss = 1.9631
[2023-10-04 19:12:02] iter = 01930, loss = 1.8439
[2023-10-04 19:12:03] iter = 01940, loss = 1.8730
[2023-10-04 19:12:04] iter = 01950, loss = 1.8334
[2023-10-04 19:12:05] iter = 01960, loss = 1.8463
[2023-10-04 19:12:06] iter = 01970, loss = 1.7293
[2023-10-04 19:12:07] iter = 01980, loss = 1.8555
[2023-10-04 19:12:08] iter = 01990, loss = 1.8215
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 29127}
[2023-10-04 19:12:33] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002760 train acc = 1.0000, test acc = 0.5834
[2023-10-04 19:12:58] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.017033 train acc = 1.0000, test acc = 0.5808
[2023-10-04 19:13:22] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.008720 train acc = 0.9980, test acc = 0.5837
[2023-10-04 19:13:46] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.014377 train acc = 0.9980, test acc = 0.5827
[2023-10-04 19:14:11] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002599 train acc = 1.0000, test acc = 0.5919
[2023-10-04 19:14:35] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.007307 train acc = 1.0000, test acc = 0.5804
[2023-10-04 19:15:00] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003256 train acc = 1.0000, test acc = 0.5792
[2023-10-04 19:15:24] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001350 train acc = 1.0000, test acc = 0.5890
[2023-10-04 19:15:49] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.009995 train acc = 1.0000, test acc = 0.5853
[2023-10-04 19:16:13] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002973 train acc = 1.0000, test acc = 0.5818
[2023-10-04 19:16:37] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004404 train acc = 1.0000, test acc = 0.5842
[2023-10-04 19:17:01] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.007917 train acc = 1.0000, test acc = 0.5864
[2023-10-04 19:17:26] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.006085 train acc = 1.0000, test acc = 0.5765
[2023-10-04 19:17:50] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.015709 train acc = 1.0000, test acc = 0.5919
[2023-10-04 19:18:15] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.007579 train acc = 1.0000, test acc = 0.5752
[2023-10-04 19:18:39] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004477 train acc = 1.0000, test acc = 0.5781
[2023-10-04 19:19:04] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002843 train acc = 1.0000, test acc = 0.5901
[2023-10-04 19:19:28] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.009222 train acc = 1.0000, test acc = 0.5872
[2023-10-04 19:19:53] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001394 train acc = 1.0000, test acc = 0.5894
[2023-10-04 19:20:17] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012424 train acc = 1.0000, test acc = 0.5786
Evaluate 20 random ConvNet, mean = 0.5838 std = 0.0049
-------------------------
[2023-10-04 19:20:17] iter = 02000, loss = 1.9646
[2023-10-04 19:20:18] iter = 02010, loss = 1.7885
[2023-10-04 19:20:19] iter = 02020, loss = 1.8468
[2023-10-04 19:20:20] iter = 02030, loss = 1.8262
[2023-10-04 19:20:21] iter = 02040, loss = 1.9607
[2023-10-04 19:20:21] iter = 02050, loss = 1.5954
[2023-10-04 19:20:22] iter = 02060, loss = 1.9503
[2023-10-04 19:20:23] iter = 02070, loss = 1.7015
[2023-10-04 19:20:24] iter = 02080, loss = 1.9064
[2023-10-04 19:20:25] iter = 02090, loss = 1.8145
[2023-10-04 19:20:26] iter = 02100, loss = 1.8735
[2023-10-04 19:20:27] iter = 02110, loss = 1.7218
[2023-10-04 19:20:28] iter = 02120, loss = 1.7644
[2023-10-04 19:20:29] iter = 02130, loss = 1.7783
[2023-10-04 19:20:30] iter = 02140, loss = 1.8449
[2023-10-04 19:20:31] iter = 02150, loss = 1.7574
[2023-10-04 19:20:32] iter = 02160, loss = 1.7417
[2023-10-04 19:20:33] iter = 02170, loss = 1.8947
[2023-10-04 19:20:33] iter = 02180, loss = 1.8340
[2023-10-04 19:20:34] iter = 02190, loss = 1.8356
[2023-10-04 19:20:35] iter = 02200, loss = 1.8065
[2023-10-04 19:20:36] iter = 02210, loss = 1.7353
[2023-10-04 19:20:37] iter = 02220, loss = 1.9146
[2023-10-04 19:20:38] iter = 02230, loss = 1.7687
[2023-10-04 19:20:39] iter = 02240, loss = 1.7126
[2023-10-04 19:20:40] iter = 02250, loss = 1.7979
[2023-10-04 19:20:41] iter = 02260, loss = 1.5825
[2023-10-04 19:20:42] iter = 02270, loss = 1.8074
[2023-10-04 19:20:43] iter = 02280, loss = 1.8128
[2023-10-04 19:20:44] iter = 02290, loss = 2.0210
[2023-10-04 19:20:45] iter = 02300, loss = 1.9607
[2023-10-04 19:20:46] iter = 02310, loss = 1.9229
[2023-10-04 19:20:47] iter = 02320, loss = 1.7593
[2023-10-04 19:20:47] iter = 02330, loss = 1.9159
[2023-10-04 19:20:48] iter = 02340, loss = 1.9775
[2023-10-04 19:20:49] iter = 02350, loss = 1.8309
[2023-10-04 19:20:50] iter = 02360, loss = 1.7975
[2023-10-04 19:20:51] iter = 02370, loss = 1.7775
[2023-10-04 19:20:52] iter = 02380, loss = 1.7461
[2023-10-04 19:20:53] iter = 02390, loss = 1.9812
[2023-10-04 19:20:54] iter = 02400, loss = 1.7791
[2023-10-04 19:20:55] iter = 02410, loss = 1.8488
[2023-10-04 19:20:55] iter = 02420, loss = 1.8332
[2023-10-04 19:20:56] iter = 02430, loss = 1.8925
[2023-10-04 19:20:57] iter = 02440, loss = 1.7035
[2023-10-04 19:20:58] iter = 02450, loss = 1.8802
[2023-10-04 19:20:59] iter = 02460, loss = 1.6596
[2023-10-04 19:21:00] iter = 02470, loss = 1.9362
[2023-10-04 19:21:01] iter = 02480, loss = 1.8389
[2023-10-04 19:21:02] iter = 02490, loss = 1.8267
[2023-10-04 19:21:03] iter = 02500, loss = 1.6574
[2023-10-04 19:21:04] iter = 02510, loss = 1.8437
[2023-10-04 19:21:05] iter = 02520, loss = 1.7589
[2023-10-04 19:21:05] iter = 02530, loss = 1.8473
[2023-10-04 19:21:06] iter = 02540, loss = 1.7393
[2023-10-04 19:21:07] iter = 02550, loss = 1.7447
[2023-10-04 19:21:08] iter = 02560, loss = 1.7730
[2023-10-04 19:21:09] iter = 02570, loss = 1.8660
[2023-10-04 19:21:10] iter = 02580, loss = 1.8015
[2023-10-04 19:21:11] iter = 02590, loss = 1.7510
[2023-10-04 19:21:12] iter = 02600, loss = 1.8450
[2023-10-04 19:21:13] iter = 02610, loss = 1.6399
[2023-10-04 19:21:14] iter = 02620, loss = 1.7714
[2023-10-04 19:21:15] iter = 02630, loss = 1.9451
[2023-10-04 19:21:16] iter = 02640, loss = 1.7600
[2023-10-04 19:21:16] iter = 02650, loss = 1.7617
[2023-10-04 19:21:17] iter = 02660, loss = 1.8777
[2023-10-04 19:21:18] iter = 02670, loss = 1.8086
[2023-10-04 19:21:19] iter = 02680, loss = 1.8386
[2023-10-04 19:21:20] iter = 02690, loss = 1.7178
[2023-10-04 19:21:21] iter = 02700, loss = 1.9105
[2023-10-04 19:21:22] iter = 02710, loss = 1.7460
[2023-10-04 19:21:23] iter = 02720, loss = 1.6060
[2023-10-04 19:21:24] iter = 02730, loss = 1.7845
[2023-10-04 19:21:25] iter = 02740, loss = 1.7407
[2023-10-04 19:21:26] iter = 02750, loss = 1.7180
[2023-10-04 19:21:27] iter = 02760, loss = 1.6957
[2023-10-04 19:21:28] iter = 02770, loss = 1.7773
[2023-10-04 19:21:28] iter = 02780, loss = 1.7368
[2023-10-04 19:21:29] iter = 02790, loss = 1.8039
[2023-10-04 19:21:30] iter = 02800, loss = 1.6924
[2023-10-04 19:21:31] iter = 02810, loss = 1.8120
[2023-10-04 19:21:32] iter = 02820, loss = 1.7560
[2023-10-04 19:21:33] iter = 02830, loss = 1.8497
[2023-10-04 19:21:34] iter = 02840, loss = 1.8145
[2023-10-04 19:21:35] iter = 02850, loss = 1.7714
[2023-10-04 19:21:36] iter = 02860, loss = 1.8004
[2023-10-04 19:21:37] iter = 02870, loss = 1.7794
[2023-10-04 19:21:38] iter = 02880, loss = 1.8146
[2023-10-04 19:21:38] iter = 02890, loss = 1.7223
[2023-10-04 19:21:39] iter = 02900, loss = 1.7227
[2023-10-04 19:21:40] iter = 02910, loss = 1.7757
[2023-10-04 19:21:41] iter = 02920, loss = 1.6932
[2023-10-04 19:21:42] iter = 02930, loss = 1.6344
[2023-10-04 19:21:43] iter = 02940, loss = 1.8093
[2023-10-04 19:21:44] iter = 02950, loss = 1.7516
[2023-10-04 19:21:45] iter = 02960, loss = 1.7348
[2023-10-04 19:21:45] iter = 02970, loss = 1.6970
[2023-10-04 19:21:47] iter = 02980, loss = 1.7414
[2023-10-04 19:21:47] iter = 02990, loss = 1.7148
[2023-10-04 19:21:48] iter = 03000, loss = 1.7258
[2023-10-04 19:21:49] iter = 03010, loss = 1.7826
[2023-10-04 19:21:50] iter = 03020, loss = 1.9597
[2023-10-04 19:21:51] iter = 03030, loss = 1.7333
[2023-10-04 19:21:52] iter = 03040, loss = 1.7929
[2023-10-04 19:21:53] iter = 03050, loss = 1.7048
[2023-10-04 19:21:54] iter = 03060, loss = 1.5433
[2023-10-04 19:21:55] iter = 03070, loss = 1.8252
[2023-10-04 19:21:55] iter = 03080, loss = 1.7722
[2023-10-04 19:21:57] iter = 03090, loss = 1.7120
[2023-10-04 19:21:57] iter = 03100, loss = 1.7328
[2023-10-04 19:21:58] iter = 03110, loss = 1.8499
[2023-10-04 19:21:59] iter = 03120, loss = 1.7208
[2023-10-04 19:22:00] iter = 03130, loss = 1.9105
[2023-10-04 19:22:01] iter = 03140, loss = 1.6223
[2023-10-04 19:22:02] iter = 03150, loss = 1.7962
[2023-10-04 19:22:03] iter = 03160, loss = 1.6119
[2023-10-04 19:22:04] iter = 03170, loss = 1.9051
[2023-10-04 19:22:05] iter = 03180, loss = 1.8169
[2023-10-04 19:22:06] iter = 03190, loss = 1.7309
[2023-10-04 19:22:07] iter = 03200, loss = 1.6144
[2023-10-04 19:22:08] iter = 03210, loss = 1.6791
[2023-10-04 19:22:09] iter = 03220, loss = 1.7022
[2023-10-04 19:22:09] iter = 03230, loss = 1.7518
[2023-10-04 19:22:10] iter = 03240, loss = 1.7617
[2023-10-04 19:22:11] iter = 03250, loss = 1.6089
[2023-10-04 19:22:12] iter = 03260, loss = 1.7450
[2023-10-04 19:22:13] iter = 03270, loss = 1.6953
[2023-10-04 19:22:14] iter = 03280, loss = 1.6238
[2023-10-04 19:22:15] iter = 03290, loss = 1.8535
[2023-10-04 19:22:16] iter = 03300, loss = 1.7409
[2023-10-04 19:22:17] iter = 03310, loss = 1.7019
[2023-10-04 19:22:18] iter = 03320, loss = 1.7108
[2023-10-04 19:22:19] iter = 03330, loss = 1.6565
[2023-10-04 19:22:19] iter = 03340, loss = 1.6893
[2023-10-04 19:22:20] iter = 03350, loss = 1.7050
[2023-10-04 19:22:21] iter = 03360, loss = 1.6979
[2023-10-04 19:22:22] iter = 03370, loss = 1.8078
[2023-10-04 19:22:23] iter = 03380, loss = 1.6901
[2023-10-04 19:22:24] iter = 03390, loss = 1.7980
[2023-10-04 19:22:25] iter = 03400, loss = 1.8639
[2023-10-04 19:22:26] iter = 03410, loss = 1.7779
[2023-10-04 19:22:27] iter = 03420, loss = 1.7748
[2023-10-04 19:22:28] iter = 03430, loss = 1.7021
[2023-10-04 19:22:28] iter = 03440, loss = 1.7522
[2023-10-04 19:22:29] iter = 03450, loss = 1.6764
[2023-10-04 19:22:30] iter = 03460, loss = 1.5561
[2023-10-04 19:22:31] iter = 03470, loss = 1.7836
[2023-10-04 19:22:32] iter = 03480, loss = 1.9088
[2023-10-04 19:22:33] iter = 03490, loss = 1.6064
[2023-10-04 19:22:34] iter = 03500, loss = 1.7412
[2023-10-04 19:22:35] iter = 03510, loss = 1.7844
[2023-10-04 19:22:36] iter = 03520, loss = 1.7791
[2023-10-04 19:22:37] iter = 03530, loss = 1.6355
[2023-10-04 19:22:38] iter = 03540, loss = 1.8527
[2023-10-04 19:22:38] iter = 03550, loss = 1.8075
[2023-10-04 19:22:39] iter = 03560, loss = 1.7412
[2023-10-04 19:22:40] iter = 03570, loss = 1.8711
[2023-10-04 19:22:41] iter = 03580, loss = 1.6556
[2023-10-04 19:22:42] iter = 03590, loss = 1.5983
[2023-10-04 19:22:43] iter = 03600, loss = 1.6784
[2023-10-04 19:22:44] iter = 03610, loss = 1.5390
[2023-10-04 19:22:45] iter = 03620, loss = 1.7011
[2023-10-04 19:22:46] iter = 03630, loss = 1.8590
[2023-10-04 19:22:47] iter = 03640, loss = 1.7592
[2023-10-04 19:22:47] iter = 03650, loss = 1.7855
[2023-10-04 19:22:48] iter = 03660, loss = 1.7969
[2023-10-04 19:22:49] iter = 03670, loss = 1.5765
[2023-10-04 19:22:50] iter = 03680, loss = 1.7835
[2023-10-04 19:22:51] iter = 03690, loss = 1.6368
[2023-10-04 19:22:52] iter = 03700, loss = 1.6035
[2023-10-04 19:22:53] iter = 03710, loss = 1.7382
[2023-10-04 19:22:54] iter = 03720, loss = 1.7342
[2023-10-04 19:22:55] iter = 03730, loss = 1.6478
[2023-10-04 19:22:56] iter = 03740, loss = 1.7823
[2023-10-04 19:22:56] iter = 03750, loss = 1.8192
[2023-10-04 19:22:57] iter = 03760, loss = 1.6837
[2023-10-04 19:22:58] iter = 03770, loss = 1.7431
[2023-10-04 19:22:59] iter = 03780, loss = 1.8856
[2023-10-04 19:23:00] iter = 03790, loss = 1.6876
[2023-10-04 19:23:01] iter = 03800, loss = 1.6088
[2023-10-04 19:23:02] iter = 03810, loss = 1.5706
[2023-10-04 19:23:03] iter = 03820, loss = 1.5915
[2023-10-04 19:23:04] iter = 03830, loss = 1.7998
[2023-10-04 19:23:05] iter = 03840, loss = 1.7585
[2023-10-04 19:23:06] iter = 03850, loss = 1.8537
[2023-10-04 19:23:07] iter = 03860, loss = 1.7744
[2023-10-04 19:23:07] iter = 03870, loss = 1.7755
[2023-10-04 19:23:08] iter = 03880, loss = 1.7665
[2023-10-04 19:23:09] iter = 03890, loss = 1.6483
[2023-10-04 19:23:10] iter = 03900, loss = 1.6944
[2023-10-04 19:23:11] iter = 03910, loss = 1.8317
[2023-10-04 19:23:12] iter = 03920, loss = 1.5946
[2023-10-04 19:23:13] iter = 03930, loss = 1.6543
[2023-10-04 19:23:14] iter = 03940, loss = 1.6339
[2023-10-04 19:23:15] iter = 03950, loss = 1.7580
[2023-10-04 19:23:16] iter = 03960, loss = 1.7311
[2023-10-04 19:23:16] iter = 03970, loss = 1.6485
[2023-10-04 19:23:18] iter = 03980, loss = 1.7501
[2023-10-04 19:23:18] iter = 03990, loss = 1.6549
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 99714}
[2023-10-04 19:23:44] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004905 train acc = 1.0000, test acc = 0.5893
[2023-10-04 19:24:08] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.026191 train acc = 0.9980, test acc = 0.5961
[2023-10-04 19:24:33] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.016305 train acc = 1.0000, test acc = 0.5985
[2023-10-04 19:24:57] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.013546 train acc = 1.0000, test acc = 0.6053
[2023-10-04 19:25:21] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.020576 train acc = 1.0000, test acc = 0.5926
[2023-10-04 19:25:46] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.010364 train acc = 1.0000, test acc = 0.5984
[2023-10-04 19:26:10] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.001299 train acc = 1.0000, test acc = 0.6031
[2023-10-04 19:26:35] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.006521 train acc = 0.9980, test acc = 0.5935
[2023-10-04 19:26:59] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.001425 train acc = 1.0000, test acc = 0.5957
[2023-10-04 19:27:23] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003826 train acc = 1.0000, test acc = 0.6002
[2023-10-04 19:27:48] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.008779 train acc = 1.0000, test acc = 0.5961
[2023-10-04 19:28:12] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003034 train acc = 1.0000, test acc = 0.6051
[2023-10-04 19:28:37] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.017820 train acc = 1.0000, test acc = 0.5982
[2023-10-04 19:29:01] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.015493 train acc = 1.0000, test acc = 0.6080
[2023-10-04 19:29:26] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.012494 train acc = 1.0000, test acc = 0.6075
[2023-10-04 19:29:50] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.011189 train acc = 0.9980, test acc = 0.5946
[2023-10-04 19:30:14] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.017767 train acc = 1.0000, test acc = 0.6101
[2023-10-04 19:30:39] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004261 train acc = 1.0000, test acc = 0.6050
[2023-10-04 19:31:03] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002941 train acc = 1.0000, test acc = 0.5989
[2023-10-04 19:31:27] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.009880 train acc = 1.0000, test acc = 0.5949
Evaluate 20 random ConvNet, mean = 0.5996 std = 0.0056
-------------------------
[2023-10-04 19:31:28] iter = 04000, loss = 1.7440
[2023-10-04 19:31:29] iter = 04010, loss = 1.6031
[2023-10-04 19:31:29] iter = 04020, loss = 1.7338
[2023-10-04 19:31:31] iter = 04030, loss = 1.7252
[2023-10-04 19:31:31] iter = 04040, loss = 1.7011
[2023-10-04 19:31:32] iter = 04050, loss = 1.6686
[2023-10-04 19:31:33] iter = 04060, loss = 1.7828
[2023-10-04 19:31:34] iter = 04070, loss = 1.6779
[2023-10-04 19:31:35] iter = 04080, loss = 1.7466
[2023-10-04 19:31:36] iter = 04090, loss = 1.6859
[2023-10-04 19:31:37] iter = 04100, loss = 1.9085
[2023-10-04 19:31:38] iter = 04110, loss = 1.7819
[2023-10-04 19:31:39] iter = 04120, loss = 1.5961
[2023-10-04 19:31:40] iter = 04130, loss = 1.7202
[2023-10-04 19:31:40] iter = 04140, loss = 1.7623
[2023-10-04 19:31:41] iter = 04150, loss = 1.8370
[2023-10-04 19:31:42] iter = 04160, loss = 1.7591
[2023-10-04 19:31:43] iter = 04170, loss = 1.6009
[2023-10-04 19:31:44] iter = 04180, loss = 1.6799
[2023-10-04 19:31:45] iter = 04190, loss = 1.6875
[2023-10-04 19:31:46] iter = 04200, loss = 1.5985
[2023-10-04 19:31:47] iter = 04210, loss = 1.5893
[2023-10-04 19:31:48] iter = 04220, loss = 1.6901
[2023-10-04 19:31:48] iter = 04230, loss = 1.7085
[2023-10-04 19:31:49] iter = 04240, loss = 1.8013
[2023-10-04 19:31:50] iter = 04250, loss = 1.5986
[2023-10-04 19:31:51] iter = 04260, loss = 1.7098
[2023-10-04 19:31:52] iter = 04270, loss = 1.7276
[2023-10-04 19:31:53] iter = 04280, loss = 1.6547
[2023-10-04 19:31:54] iter = 04290, loss = 1.6720
[2023-10-04 19:31:55] iter = 04300, loss = 1.8207
[2023-10-04 19:31:56] iter = 04310, loss = 1.6942
[2023-10-04 19:31:57] iter = 04320, loss = 1.9131
[2023-10-04 19:31:58] iter = 04330, loss = 1.5429
[2023-10-04 19:31:59] iter = 04340, loss = 1.5888
[2023-10-04 19:31:59] iter = 04350, loss = 1.7720
[2023-10-04 19:32:00] iter = 04360, loss = 1.7017
[2023-10-04 19:32:01] iter = 04370, loss = 1.6433
[2023-10-04 19:32:02] iter = 04380, loss = 1.6677
[2023-10-04 19:32:03] iter = 04390, loss = 1.5606
[2023-10-04 19:32:04] iter = 04400, loss = 1.6870
[2023-10-04 19:32:05] iter = 04410, loss = 1.6327
[2023-10-04 19:32:06] iter = 04420, loss = 1.7310
[2023-10-04 19:32:07] iter = 04430, loss = 1.6839
[2023-10-04 19:32:08] iter = 04440, loss = 1.6586
[2023-10-04 19:32:09] iter = 04450, loss = 1.5213
[2023-10-04 19:32:10] iter = 04460, loss = 1.7616
[2023-10-04 19:32:10] iter = 04470, loss = 1.5715
[2023-10-04 19:32:11] iter = 04480, loss = 1.6784
[2023-10-04 19:32:12] iter = 04490, loss = 1.5993
[2023-10-04 19:32:13] iter = 04500, loss = 1.6463
[2023-10-04 19:32:14] iter = 04510, loss = 1.5523
[2023-10-04 19:32:15] iter = 04520, loss = 1.5396
[2023-10-04 19:32:16] iter = 04530, loss = 1.6920
[2023-10-04 19:32:17] iter = 04540, loss = 1.5786
[2023-10-04 19:32:18] iter = 04550, loss = 1.6420
[2023-10-04 19:32:19] iter = 04560, loss = 1.7843
[2023-10-04 19:32:19] iter = 04570, loss = 1.5699
[2023-10-04 19:32:20] iter = 04580, loss = 1.6085
[2023-10-04 19:32:21] iter = 04590, loss = 1.8126
[2023-10-04 19:32:22] iter = 04600, loss = 1.5593
[2023-10-04 19:32:23] iter = 04610, loss = 1.6556
[2023-10-04 19:32:24] iter = 04620, loss = 1.5372
[2023-10-04 19:32:25] iter = 04630, loss = 1.5093
[2023-10-04 19:32:26] iter = 04640, loss = 1.6912
[2023-10-04 19:32:27] iter = 04650, loss = 1.5239
[2023-10-04 19:32:28] iter = 04660, loss = 1.5250
[2023-10-04 19:32:29] iter = 04670, loss = 1.6614
[2023-10-04 19:32:30] iter = 04680, loss = 1.6692
[2023-10-04 19:32:30] iter = 04690, loss = 1.6765
[2023-10-04 19:32:31] iter = 04700, loss = 1.5308
[2023-10-04 19:32:32] iter = 04710, loss = 1.7691
[2023-10-04 19:32:33] iter = 04720, loss = 1.6838
[2023-10-04 19:32:34] iter = 04730, loss = 1.6717
[2023-10-04 19:32:35] iter = 04740, loss = 1.6792
[2023-10-04 19:32:36] iter = 04750, loss = 1.7182
[2023-10-04 19:32:37] iter = 04760, loss = 1.6950
[2023-10-04 19:32:38] iter = 04770, loss = 1.6697
[2023-10-04 19:32:39] iter = 04780, loss = 1.6244
[2023-10-04 19:32:40] iter = 04790, loss = 1.7611
[2023-10-04 19:32:40] iter = 04800, loss = 1.7839
[2023-10-04 19:32:41] iter = 04810, loss = 1.6320
[2023-10-04 19:32:42] iter = 04820, loss = 1.6742
[2023-10-04 19:32:43] iter = 04830, loss = 1.6400
[2023-10-04 19:32:44] iter = 04840, loss = 1.7368
[2023-10-04 19:32:45] iter = 04850, loss = 1.7830
[2023-10-04 19:32:46] iter = 04860, loss = 1.5768
[2023-10-04 19:32:47] iter = 04870, loss = 1.6532
[2023-10-04 19:32:48] iter = 04880, loss = 1.6204
[2023-10-04 19:32:49] iter = 04890, loss = 1.5610
[2023-10-04 19:32:50] iter = 04900, loss = 1.5672
[2023-10-04 19:32:51] iter = 04910, loss = 1.6426
[2023-10-04 19:32:52] iter = 04920, loss = 1.5988
[2023-10-04 19:32:52] iter = 04930, loss = 1.7937
[2023-10-04 19:32:53] iter = 04940, loss = 1.6625
[2023-10-04 19:32:54] iter = 04950, loss = 2.0508
[2023-10-04 19:32:55] iter = 04960, loss = 1.5604
[2023-10-04 19:32:56] iter = 04970, loss = 1.6040
[2023-10-04 19:32:57] iter = 04980, loss = 1.5488
[2023-10-04 19:32:58] iter = 04990, loss = 1.6361
[2023-10-04 19:32:59] iter = 05000, loss = 1.7692
[2023-10-04 19:33:00] iter = 05010, loss = 1.5668
[2023-10-04 19:33:01] iter = 05020, loss = 1.6192
[2023-10-04 19:33:01] iter = 05030, loss = 1.5800
[2023-10-04 19:33:02] iter = 05040, loss = 1.6203
[2023-10-04 19:33:03] iter = 05050, loss = 1.7490
[2023-10-04 19:33:04] iter = 05060, loss = 1.6158
[2023-10-04 19:33:05] iter = 05070, loss = 1.5889
[2023-10-04 19:33:06] iter = 05080, loss = 1.7157
[2023-10-04 19:33:07] iter = 05090, loss = 1.6711
[2023-10-04 19:33:08] iter = 05100, loss = 1.6493
[2023-10-04 19:33:09] iter = 05110, loss = 1.5475
[2023-10-04 19:33:10] iter = 05120, loss = 1.6102
[2023-10-04 19:33:11] iter = 05130, loss = 1.6362
[2023-10-04 19:33:12] iter = 05140, loss = 1.4898
[2023-10-04 19:33:13] iter = 05150, loss = 1.6251
[2023-10-04 19:33:14] iter = 05160, loss = 1.6517
[2023-10-04 19:33:14] iter = 05170, loss = 1.5913
[2023-10-04 19:33:15] iter = 05180, loss = 1.5372
[2023-10-04 19:33:16] iter = 05190, loss = 1.6962
[2023-10-04 19:33:17] iter = 05200, loss = 1.5719
[2023-10-04 19:33:18] iter = 05210, loss = 1.5416
[2023-10-04 19:33:19] iter = 05220, loss = 1.7313
[2023-10-04 19:33:20] iter = 05230, loss = 1.4889
[2023-10-04 19:33:21] iter = 05240, loss = 1.5876
[2023-10-04 19:33:22] iter = 05250, loss = 1.5861
[2023-10-04 19:33:23] iter = 05260, loss = 1.8140
[2023-10-04 19:33:24] iter = 05270, loss = 1.5837
[2023-10-04 19:33:25] iter = 05280, loss = 1.6863
[2023-10-04 19:33:26] iter = 05290, loss = 1.6790
[2023-10-04 19:33:26] iter = 05300, loss = 1.6908
[2023-10-04 19:33:27] iter = 05310, loss = 1.7371
[2023-10-04 19:33:28] iter = 05320, loss = 1.8827
[2023-10-04 19:33:29] iter = 05330, loss = 1.7146
[2023-10-04 19:33:30] iter = 05340, loss = 1.7167
[2023-10-04 19:33:31] iter = 05350, loss = 1.5826
[2023-10-04 19:33:32] iter = 05360, loss = 1.7385
[2023-10-04 19:33:33] iter = 05370, loss = 1.5696
[2023-10-04 19:33:34] iter = 05380, loss = 1.6565
[2023-10-04 19:33:35] iter = 05390, loss = 1.5220
[2023-10-04 19:33:36] iter = 05400, loss = 1.8448
[2023-10-04 19:33:37] iter = 05410, loss = 1.6821
[2023-10-04 19:33:38] iter = 05420, loss = 1.6596
[2023-10-04 19:33:38] iter = 05430, loss = 1.6566
[2023-10-04 19:33:39] iter = 05440, loss = 1.6268
[2023-10-04 19:33:40] iter = 05450, loss = 1.6578
[2023-10-04 19:33:41] iter = 05460, loss = 1.6988
[2023-10-04 19:33:42] iter = 05470, loss = 1.6216
[2023-10-04 19:33:43] iter = 05480, loss = 1.6904
[2023-10-04 19:33:44] iter = 05490, loss = 1.4991
[2023-10-04 19:33:45] iter = 05500, loss = 1.5906
[2023-10-04 19:33:46] iter = 05510, loss = 1.5459
[2023-10-04 19:33:47] iter = 05520, loss = 1.7100
[2023-10-04 19:33:47] iter = 05530, loss = 1.7023
[2023-10-04 19:33:48] iter = 05540, loss = 1.6393
[2023-10-04 19:33:49] iter = 05550, loss = 1.5648
[2023-10-04 19:33:50] iter = 05560, loss = 1.4714
[2023-10-04 19:33:51] iter = 05570, loss = 1.6755
[2023-10-04 19:33:52] iter = 05580, loss = 1.4811
[2023-10-04 19:33:53] iter = 05590, loss = 1.6036
[2023-10-04 19:33:54] iter = 05600, loss = 1.5918
[2023-10-04 19:33:55] iter = 05610, loss = 1.5547
[2023-10-04 19:33:56] iter = 05620, loss = 1.7182
[2023-10-04 19:33:57] iter = 05630, loss = 1.6505
[2023-10-04 19:33:58] iter = 05640, loss = 1.7074
[2023-10-04 19:33:59] iter = 05650, loss = 1.5738
[2023-10-04 19:34:00] iter = 05660, loss = 1.5678
[2023-10-04 19:34:00] iter = 05670, loss = 1.5909
[2023-10-04 19:34:01] iter = 05680, loss = 1.5948
[2023-10-04 19:34:02] iter = 05690, loss = 1.7390
[2023-10-04 19:34:03] iter = 05700, loss = 1.7193
[2023-10-04 19:34:04] iter = 05710, loss = 1.6469
[2023-10-04 19:34:05] iter = 05720, loss = 1.6595
[2023-10-04 19:34:06] iter = 05730, loss = 1.6258
[2023-10-04 19:34:07] iter = 05740, loss = 1.5814
[2023-10-04 19:34:08] iter = 05750, loss = 1.5742
[2023-10-04 19:34:09] iter = 05760, loss = 1.5940
[2023-10-04 19:34:10] iter = 05770, loss = 1.6953
[2023-10-04 19:34:11] iter = 05780, loss = 1.5577
[2023-10-04 19:34:11] iter = 05790, loss = 1.6486
[2023-10-04 19:34:12] iter = 05800, loss = 1.6089
[2023-10-04 19:34:13] iter = 05810, loss = 1.5811
[2023-10-04 19:34:14] iter = 05820, loss = 1.5056
[2023-10-04 19:34:15] iter = 05830, loss = 1.5621
[2023-10-04 19:34:16] iter = 05840, loss = 1.4962
[2023-10-04 19:34:17] iter = 05850, loss = 1.7879
[2023-10-04 19:34:18] iter = 05860, loss = 1.6411
[2023-10-04 19:34:19] iter = 05870, loss = 1.6147
[2023-10-04 19:34:20] iter = 05880, loss = 1.6924
[2023-10-04 19:34:20] iter = 05890, loss = 1.6283
[2023-10-04 19:34:21] iter = 05900, loss = 1.4421
[2023-10-04 19:34:22] iter = 05910, loss = 1.7145
[2023-10-04 19:34:23] iter = 05920, loss = 1.6699
[2023-10-04 19:34:24] iter = 05930, loss = 1.5169
[2023-10-04 19:34:25] iter = 05940, loss = 1.5392
[2023-10-04 19:34:26] iter = 05950, loss = 1.6152
[2023-10-04 19:34:27] iter = 05960, loss = 1.6136
[2023-10-04 19:34:28] iter = 05970, loss = 1.5883
[2023-10-04 19:34:29] iter = 05980, loss = 1.6012
[2023-10-04 19:34:30] iter = 05990, loss = 1.6877
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 70911}
[2023-10-04 19:34:55] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.001987 train acc = 1.0000, test acc = 0.6096
[2023-10-04 19:35:19] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010662 train acc = 1.0000, test acc = 0.6117
[2023-10-04 19:35:44] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010320 train acc = 1.0000, test acc = 0.6054
[2023-10-04 19:36:08] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003875 train acc = 1.0000, test acc = 0.6105
[2023-10-04 19:36:32] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011112 train acc = 1.0000, test acc = 0.6116
[2023-10-04 19:36:57] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.009139 train acc = 1.0000, test acc = 0.6057
[2023-10-04 19:37:21] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.013726 train acc = 1.0000, test acc = 0.6037
[2023-10-04 19:37:45] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.010548 train acc = 1.0000, test acc = 0.6082
[2023-10-04 19:38:10] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003594 train acc = 1.0000, test acc = 0.6078
[2023-10-04 19:38:34] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003700 train acc = 1.0000, test acc = 0.6032
[2023-10-04 19:38:59] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.008976 train acc = 1.0000, test acc = 0.6102
[2023-10-04 19:39:23] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003539 train acc = 1.0000, test acc = 0.6057
[2023-10-04 19:39:48] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004058 train acc = 1.0000, test acc = 0.6085
[2023-10-04 19:40:12] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002445 train acc = 1.0000, test acc = 0.6103
[2023-10-04 19:40:36] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.014284 train acc = 1.0000, test acc = 0.6059
[2023-10-04 19:41:01] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.009793 train acc = 1.0000, test acc = 0.6166
[2023-10-04 19:41:25] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003119 train acc = 1.0000, test acc = 0.6086
[2023-10-04 19:41:50] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.016027 train acc = 1.0000, test acc = 0.6112
[2023-10-04 19:42:14] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.015664 train acc = 1.0000, test acc = 0.6128
[2023-10-04 19:42:39] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003777 train acc = 1.0000, test acc = 0.6165
Evaluate 20 random ConvNet, mean = 0.6092 std = 0.0036
-------------------------
[2023-10-04 19:42:39] iter = 06000, loss = 1.6537
[2023-10-04 19:42:40] iter = 06010, loss = 1.6335
[2023-10-04 19:42:41] iter = 06020, loss = 1.7084
[2023-10-04 19:42:42] iter = 06030, loss = 1.6506
[2023-10-04 19:42:43] iter = 06040, loss = 1.8093
[2023-10-04 19:42:43] iter = 06050, loss = 1.6708
[2023-10-04 19:42:44] iter = 06060, loss = 1.5566
[2023-10-04 19:42:45] iter = 06070, loss = 1.6556
[2023-10-04 19:42:46] iter = 06080, loss = 1.5901
[2023-10-04 19:42:47] iter = 06090, loss = 1.6066
[2023-10-04 19:42:48] iter = 06100, loss = 1.6375
[2023-10-04 19:42:49] iter = 06110, loss = 1.6903
[2023-10-04 19:42:50] iter = 06120, loss = 1.5891
[2023-10-04 19:42:51] iter = 06130, loss = 1.6079
[2023-10-04 19:42:52] iter = 06140, loss = 1.5117
[2023-10-04 19:42:52] iter = 06150, loss = 1.6313
[2023-10-04 19:42:53] iter = 06160, loss = 1.7121
[2023-10-04 19:42:54] iter = 06170, loss = 1.7232
[2023-10-04 19:42:55] iter = 06180, loss = 1.5866
[2023-10-04 19:42:56] iter = 06190, loss = 1.8684
[2023-10-04 19:42:57] iter = 06200, loss = 1.5277
[2023-10-04 19:42:58] iter = 06210, loss = 1.5739
[2023-10-04 19:42:59] iter = 06220, loss = 1.5431
[2023-10-04 19:43:00] iter = 06230, loss = 1.6911
[2023-10-04 19:43:01] iter = 06240, loss = 1.5452
[2023-10-04 19:43:02] iter = 06250, loss = 1.5283
[2023-10-04 19:43:02] iter = 06260, loss = 1.6540
[2023-10-04 19:43:03] iter = 06270, loss = 1.4274
[2023-10-04 19:43:04] iter = 06280, loss = 1.5776
[2023-10-04 19:43:05] iter = 06290, loss = 1.5008
[2023-10-04 19:43:06] iter = 06300, loss = 1.5357
[2023-10-04 19:43:07] iter = 06310, loss = 1.6828
[2023-10-04 19:43:08] iter = 06320, loss = 1.5873
[2023-10-04 19:43:09] iter = 06330, loss = 1.5597
[2023-10-04 19:43:10] iter = 06340, loss = 1.6038
[2023-10-04 19:43:11] iter = 06350, loss = 1.5890
[2023-10-04 19:43:12] iter = 06360, loss = 1.5806
[2023-10-04 19:43:13] iter = 06370, loss = 1.6627
[2023-10-04 19:43:14] iter = 06380, loss = 1.6094
[2023-10-04 19:43:15] iter = 06390, loss = 1.6515
[2023-10-04 19:43:15] iter = 06400, loss = 1.5514
[2023-10-04 19:43:16] iter = 06410, loss = 1.5228
[2023-10-04 19:43:17] iter = 06420, loss = 1.7700
[2023-10-04 19:43:18] iter = 06430, loss = 1.5360
[2023-10-04 19:43:19] iter = 06440, loss = 1.8368
[2023-10-04 19:43:20] iter = 06450, loss = 1.5741
[2023-10-04 19:43:21] iter = 06460, loss = 1.5600
[2023-10-04 19:43:22] iter = 06470, loss = 1.7253
[2023-10-04 19:43:23] iter = 06480, loss = 1.5727
[2023-10-04 19:43:24] iter = 06490, loss = 1.7903
[2023-10-04 19:43:25] iter = 06500, loss = 1.6962
[2023-10-04 19:43:26] iter = 06510, loss = 1.6351
[2023-10-04 19:43:27] iter = 06520, loss = 1.4922
[2023-10-04 19:43:28] iter = 06530, loss = 1.5585
[2023-10-04 19:43:28] iter = 06540, loss = 1.6452
[2023-10-04 19:43:29] iter = 06550, loss = 1.6200
[2023-10-04 19:43:30] iter = 06560, loss = 1.5452
[2023-10-04 19:43:31] iter = 06570, loss = 1.5995
[2023-10-04 19:43:32] iter = 06580, loss = 1.5612
[2023-10-04 19:43:33] iter = 06590, loss = 1.5708
[2023-10-04 19:43:34] iter = 06600, loss = 1.5281
[2023-10-04 19:43:35] iter = 06610, loss = 1.7075
[2023-10-04 19:43:36] iter = 06620, loss = 1.5492
[2023-10-04 19:43:37] iter = 06630, loss = 1.6280
[2023-10-04 19:43:38] iter = 06640, loss = 1.5283
[2023-10-04 19:43:38] iter = 06650, loss = 1.5439
[2023-10-04 19:43:39] iter = 06660, loss = 1.5174
[2023-10-04 19:43:40] iter = 06670, loss = 1.6981
[2023-10-04 19:43:41] iter = 06680, loss = 1.5782
[2023-10-04 19:43:42] iter = 06690, loss = 1.5074
[2023-10-04 19:43:43] iter = 06700, loss = 1.5896
[2023-10-04 19:43:44] iter = 06710, loss = 1.4940
[2023-10-04 19:43:45] iter = 06720, loss = 1.6626
[2023-10-04 19:43:46] iter = 06730, loss = 1.5078
[2023-10-04 19:43:47] iter = 06740, loss = 1.5472
[2023-10-04 19:43:48] iter = 06750, loss = 1.7637
[2023-10-04 19:43:49] iter = 06760, loss = 1.7845
[2023-10-04 19:43:49] iter = 06770, loss = 1.5943
[2023-10-04 19:43:50] iter = 06780, loss = 1.5399
[2023-10-04 19:43:51] iter = 06790, loss = 1.5320
[2023-10-04 19:43:52] iter = 06800, loss = 1.5828
[2023-10-04 19:43:53] iter = 06810, loss = 1.6057
[2023-10-04 19:43:54] iter = 06820, loss = 1.5778
[2023-10-04 19:43:55] iter = 06830, loss = 1.6402
[2023-10-04 19:43:56] iter = 06840, loss = 1.5499
[2023-10-04 19:43:57] iter = 06850, loss = 1.4467
[2023-10-04 19:43:58] iter = 06860, loss = 1.5849
[2023-10-04 19:43:59] iter = 06870, loss = 1.5295
[2023-10-04 19:43:59] iter = 06880, loss = 1.4572
[2023-10-04 19:44:00] iter = 06890, loss = 1.6436
[2023-10-04 19:44:01] iter = 06900, loss = 1.6080
[2023-10-04 19:44:02] iter = 06910, loss = 1.5183
[2023-10-04 19:44:03] iter = 06920, loss = 1.6745
[2023-10-04 19:44:04] iter = 06930, loss = 1.5681
[2023-10-04 19:44:05] iter = 06940, loss = 1.6239
[2023-10-04 19:44:06] iter = 06950, loss = 1.5192
[2023-10-04 19:44:07] iter = 06960, loss = 1.5139
[2023-10-04 19:44:08] iter = 06970, loss = 1.6845
[2023-10-04 19:44:08] iter = 06980, loss = 1.7667
[2023-10-04 19:44:09] iter = 06990, loss = 1.6698
[2023-10-04 19:44:10] iter = 07000, loss = 1.5197
[2023-10-04 19:44:11] iter = 07010, loss = 1.5103
[2023-10-04 19:44:12] iter = 07020, loss = 1.5959
[2023-10-04 19:44:13] iter = 07030, loss = 1.4209
[2023-10-04 19:44:14] iter = 07040, loss = 1.6426
[2023-10-04 19:44:15] iter = 07050, loss = 1.6823
[2023-10-04 19:44:16] iter = 07060, loss = 1.6875
[2023-10-04 19:44:17] iter = 07070, loss = 1.7362
[2023-10-04 19:44:17] iter = 07080, loss = 1.5591
[2023-10-04 19:44:18] iter = 07090, loss = 1.7084
[2023-10-04 19:44:19] iter = 07100, loss = 1.6175
[2023-10-04 19:44:20] iter = 07110, loss = 1.6377
[2023-10-04 19:44:21] iter = 07120, loss = 1.6656
[2023-10-04 19:44:22] iter = 07130, loss = 1.4464
[2023-10-04 19:44:23] iter = 07140, loss = 1.4988
[2023-10-04 19:44:24] iter = 07150, loss = 1.7037
[2023-10-04 19:44:25] iter = 07160, loss = 1.5539
[2023-10-04 19:44:26] iter = 07170, loss = 1.5435
[2023-10-04 19:44:27] iter = 07180, loss = 1.8301
[2023-10-04 19:44:28] iter = 07190, loss = 1.8412
[2023-10-04 19:44:29] iter = 07200, loss = 1.4509
[2023-10-04 19:44:30] iter = 07210, loss = 1.6268
[2023-10-04 19:44:30] iter = 07220, loss = 1.5544
[2023-10-04 19:44:31] iter = 07230, loss = 1.5180
[2023-10-04 19:44:32] iter = 07240, loss = 1.7380
[2023-10-04 19:44:33] iter = 07250, loss = 1.6721
[2023-10-04 19:44:34] iter = 07260, loss = 1.5433
[2023-10-04 19:44:35] iter = 07270, loss = 1.7110
[2023-10-04 19:44:36] iter = 07280, loss = 1.5920
[2023-10-04 19:44:37] iter = 07290, loss = 1.4372
[2023-10-04 19:44:38] iter = 07300, loss = 1.6930
[2023-10-04 19:44:39] iter = 07310, loss = 1.5295
[2023-10-04 19:44:40] iter = 07320, loss = 1.4913
[2023-10-04 19:44:41] iter = 07330, loss = 1.6082
[2023-10-04 19:44:41] iter = 07340, loss = 1.6397
[2023-10-04 19:44:42] iter = 07350, loss = 1.5971
[2023-10-04 19:44:43] iter = 07360, loss = 1.6486
[2023-10-04 19:44:44] iter = 07370, loss = 1.5418
[2023-10-04 19:44:45] iter = 07380, loss = 1.6104
[2023-10-04 19:44:46] iter = 07390, loss = 1.5276
[2023-10-04 19:44:47] iter = 07400, loss = 1.5969
[2023-10-04 19:44:48] iter = 07410, loss = 1.4765
[2023-10-04 19:44:49] iter = 07420, loss = 1.4744
[2023-10-04 19:44:50] iter = 07430, loss = 1.5350
[2023-10-04 19:44:51] iter = 07440, loss = 1.7522
[2023-10-04 19:44:51] iter = 07450, loss = 1.5167
[2023-10-04 19:44:52] iter = 07460, loss = 1.5761
[2023-10-04 19:44:53] iter = 07470, loss = 1.6825
[2023-10-04 19:44:54] iter = 07480, loss = 1.6912
[2023-10-04 19:44:55] iter = 07490, loss = 1.5900
[2023-10-04 19:44:56] iter = 07500, loss = 1.6908
[2023-10-04 19:44:57] iter = 07510, loss = 1.5737
[2023-10-04 19:44:58] iter = 07520, loss = 1.5818
[2023-10-04 19:44:59] iter = 07530, loss = 1.5558
[2023-10-04 19:45:00] iter = 07540, loss = 1.5330
[2023-10-04 19:45:00] iter = 07550, loss = 1.6953
[2023-10-04 19:45:01] iter = 07560, loss = 1.6098
[2023-10-04 19:45:02] iter = 07570, loss = 1.5438
[2023-10-04 19:45:03] iter = 07580, loss = 1.6714
[2023-10-04 19:45:04] iter = 07590, loss = 1.6332
[2023-10-04 19:45:05] iter = 07600, loss = 1.6305
[2023-10-04 19:45:06] iter = 07610, loss = 1.5745
[2023-10-04 19:45:07] iter = 07620, loss = 1.6545
[2023-10-04 19:45:08] iter = 07630, loss = 1.5173
[2023-10-04 19:45:09] iter = 07640, loss = 1.5397
[2023-10-04 19:45:10] iter = 07650, loss = 1.5662
[2023-10-04 19:45:11] iter = 07660, loss = 1.4934
[2023-10-04 19:45:12] iter = 07670, loss = 1.6849
[2023-10-04 19:45:12] iter = 07680, loss = 1.7333
[2023-10-04 19:45:13] iter = 07690, loss = 1.5903
[2023-10-04 19:45:14] iter = 07700, loss = 1.6179
[2023-10-04 19:45:15] iter = 07710, loss = 1.6119
[2023-10-04 19:45:16] iter = 07720, loss = 1.6392
[2023-10-04 19:45:17] iter = 07730, loss = 1.6005
[2023-10-04 19:45:18] iter = 07740, loss = 1.4678
[2023-10-04 19:45:19] iter = 07750, loss = 1.4653
[2023-10-04 19:45:20] iter = 07760, loss = 1.5791
[2023-10-04 19:45:21] iter = 07770, loss = 1.4942
[2023-10-04 19:45:22] iter = 07780, loss = 1.5015
[2023-10-04 19:45:23] iter = 07790, loss = 1.5812
[2023-10-04 19:45:23] iter = 07800, loss = 1.6011
[2023-10-04 19:45:24] iter = 07810, loss = 1.4873
[2023-10-04 19:45:25] iter = 07820, loss = 1.6872
[2023-10-04 19:45:26] iter = 07830, loss = 1.5006
[2023-10-04 19:45:27] iter = 07840, loss = 1.4935
[2023-10-04 19:45:28] iter = 07850, loss = 1.4975
[2023-10-04 19:45:29] iter = 07860, loss = 1.5550
[2023-10-04 19:45:30] iter = 07870, loss = 1.5292
[2023-10-04 19:45:31] iter = 07880, loss = 1.5896
[2023-10-04 19:45:32] iter = 07890, loss = 1.5406
[2023-10-04 19:45:33] iter = 07900, loss = 1.5442
[2023-10-04 19:45:34] iter = 07910, loss = 1.5615
[2023-10-04 19:45:35] iter = 07920, loss = 1.5595
[2023-10-04 19:45:35] iter = 07930, loss = 1.5679
[2023-10-04 19:45:36] iter = 07940, loss = 1.6381
[2023-10-04 19:45:37] iter = 07950, loss = 1.6941
[2023-10-04 19:45:38] iter = 07960, loss = 1.6018
[2023-10-04 19:45:39] iter = 07970, loss = 1.7143
[2023-10-04 19:45:40] iter = 07980, loss = 1.6083
[2023-10-04 19:45:41] iter = 07990, loss = 1.5760
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 42010}
[2023-10-04 19:46:06] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004846 train acc = 1.0000, test acc = 0.6147
[2023-10-04 19:46:30] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.008891 train acc = 1.0000, test acc = 0.6168
[2023-10-04 19:46:54] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.011378 train acc = 1.0000, test acc = 0.6208
[2023-10-04 19:47:19] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002257 train acc = 1.0000, test acc = 0.6208
[2023-10-04 19:47:44] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002054 train acc = 1.0000, test acc = 0.6122
[2023-10-04 19:48:08] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002183 train acc = 1.0000, test acc = 0.6114
[2023-10-04 19:48:32] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.015057 train acc = 1.0000, test acc = 0.6077
[2023-10-04 19:48:57] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.018289 train acc = 0.9980, test acc = 0.6107
[2023-10-04 19:49:21] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.017826 train acc = 0.9980, test acc = 0.6169
[2023-10-04 19:49:45] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.018203 train acc = 1.0000, test acc = 0.6154
[2023-10-04 19:50:10] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.025572 train acc = 0.9980, test acc = 0.6123
[2023-10-04 19:50:34] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.016303 train acc = 1.0000, test acc = 0.6094
[2023-10-04 19:50:59] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.015511 train acc = 0.9980, test acc = 0.6183
[2023-10-04 19:51:23] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.005226 train acc = 1.0000, test acc = 0.6193
[2023-10-04 19:51:47] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.019154 train acc = 1.0000, test acc = 0.6112
[2023-10-04 19:52:12] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.011334 train acc = 1.0000, test acc = 0.6114
[2023-10-04 19:52:37] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.015526 train acc = 1.0000, test acc = 0.6068
[2023-10-04 19:53:01] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004716 train acc = 1.0000, test acc = 0.6079
[2023-10-04 19:53:25] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001767 train acc = 1.0000, test acc = 0.6173
[2023-10-04 19:53:50] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.011704 train acc = 1.0000, test acc = 0.6121
Evaluate 20 random ConvNet, mean = 0.6137 std = 0.0042
-------------------------
[2023-10-04 19:53:50] iter = 08000, loss = 1.4731
[2023-10-04 19:53:51] iter = 08010, loss = 1.5657
[2023-10-04 19:53:52] iter = 08020, loss = 1.4702
[2023-10-04 19:53:53] iter = 08030, loss = 1.6513
[2023-10-04 19:53:54] iter = 08040, loss = 1.6381
[2023-10-04 19:53:55] iter = 08050, loss = 1.5820
[2023-10-04 19:53:56] iter = 08060, loss = 1.5700
[2023-10-04 19:53:57] iter = 08070, loss = 1.6013
[2023-10-04 19:53:57] iter = 08080, loss = 1.7402
[2023-10-04 19:53:58] iter = 08090, loss = 1.6337
[2023-10-04 19:53:59] iter = 08100, loss = 1.7029
[2023-10-04 19:54:00] iter = 08110, loss = 1.5084
[2023-10-04 19:54:01] iter = 08120, loss = 1.6773
[2023-10-04 19:54:02] iter = 08130, loss = 1.5551
[2023-10-04 19:54:03] iter = 08140, loss = 1.5537
[2023-10-04 19:54:04] iter = 08150, loss = 1.5054
[2023-10-04 19:54:05] iter = 08160, loss = 1.6541
[2023-10-04 19:54:06] iter = 08170, loss = 1.6894
[2023-10-04 19:54:07] iter = 08180, loss = 1.6407
[2023-10-04 19:54:08] iter = 08190, loss = 1.5479
[2023-10-04 19:54:08] iter = 08200, loss = 1.5090
[2023-10-04 19:54:09] iter = 08210, loss = 1.6457
[2023-10-04 19:54:10] iter = 08220, loss = 1.4827
[2023-10-04 19:54:11] iter = 08230, loss = 1.5554
[2023-10-04 19:54:12] iter = 08240, loss = 1.5501
[2023-10-04 19:54:13] iter = 08250, loss = 1.6497
[2023-10-04 19:54:14] iter = 08260, loss = 1.5033
[2023-10-04 19:54:15] iter = 08270, loss = 1.6739
[2023-10-04 19:54:16] iter = 08280, loss = 1.5675
[2023-10-04 19:54:17] iter = 08290, loss = 1.6100
[2023-10-04 19:54:17] iter = 08300, loss = 1.5561
[2023-10-04 19:54:18] iter = 08310, loss = 1.5423
[2023-10-04 19:54:19] iter = 08320, loss = 1.4613
[2023-10-04 19:54:20] iter = 08330, loss = 1.5243
[2023-10-04 19:54:21] iter = 08340, loss = 1.5515
[2023-10-04 19:54:22] iter = 08350, loss = 1.4640
[2023-10-04 19:54:23] iter = 08360, loss = 1.5248
[2023-10-04 19:54:24] iter = 08370, loss = 1.5135
[2023-10-04 19:54:25] iter = 08380, loss = 1.6789
[2023-10-04 19:54:26] iter = 08390, loss = 1.5452
[2023-10-04 19:54:27] iter = 08400, loss = 1.6907
[2023-10-04 19:54:27] iter = 08410, loss = 1.6171
[2023-10-04 19:54:28] iter = 08420, loss = 1.6873
[2023-10-04 19:54:29] iter = 08430, loss = 1.4829
[2023-10-04 19:54:30] iter = 08440, loss = 1.5010
[2023-10-04 19:54:31] iter = 08450, loss = 1.5376
[2023-10-04 19:54:32] iter = 08460, loss = 1.6391
[2023-10-04 19:54:33] iter = 08470, loss = 1.5719
[2023-10-04 19:54:34] iter = 08480, loss = 1.6343
[2023-10-04 19:54:35] iter = 08490, loss = 1.3940
[2023-10-04 19:54:36] iter = 08500, loss = 1.6401
[2023-10-04 19:54:37] iter = 08510, loss = 1.4742
[2023-10-04 19:54:37] iter = 08520, loss = 1.5030
[2023-10-04 19:54:38] iter = 08530, loss = 1.5445
[2023-10-04 19:54:39] iter = 08540, loss = 1.6424
[2023-10-04 19:54:40] iter = 08550, loss = 1.5380
[2023-10-04 19:54:41] iter = 08560, loss = 1.6356
[2023-10-04 19:54:42] iter = 08570, loss = 1.5120
[2023-10-04 19:54:43] iter = 08580, loss = 1.6302
[2023-10-04 19:54:44] iter = 08590, loss = 1.5440
[2023-10-04 19:54:45] iter = 08600, loss = 1.7403
[2023-10-04 19:54:45] iter = 08610, loss = 1.5590
[2023-10-04 19:54:46] iter = 08620, loss = 1.6261
[2023-10-04 19:54:47] iter = 08630, loss = 1.6465
[2023-10-04 19:54:48] iter = 08640, loss = 1.5890
[2023-10-04 19:54:49] iter = 08650, loss = 1.6565
[2023-10-04 19:54:50] iter = 08660, loss = 1.6402
[2023-10-04 19:54:51] iter = 08670, loss = 1.4686
[2023-10-04 19:54:52] iter = 08680, loss = 1.5346
[2023-10-04 19:54:53] iter = 08690, loss = 1.5448
[2023-10-04 19:54:54] iter = 08700, loss = 1.5390
[2023-10-04 19:54:54] iter = 08710, loss = 1.6562
[2023-10-04 19:54:55] iter = 08720, loss = 1.5873
[2023-10-04 19:54:56] iter = 08730, loss = 1.4877
[2023-10-04 19:54:57] iter = 08740, loss = 1.5388
[2023-10-04 19:54:58] iter = 08750, loss = 1.6756
[2023-10-04 19:54:59] iter = 08760, loss = 1.5381
[2023-10-04 19:55:00] iter = 08770, loss = 1.4996
[2023-10-04 19:55:01] iter = 08780, loss = 1.6706
[2023-10-04 19:55:01] iter = 08790, loss = 1.5598
[2023-10-04 19:55:02] iter = 08800, loss = 1.5365
[2023-10-04 19:55:03] iter = 08810, loss = 1.5740
[2023-10-04 19:55:04] iter = 08820, loss = 1.5492
[2023-10-04 19:55:05] iter = 08830, loss = 1.5156
[2023-10-04 19:55:06] iter = 08840, loss = 1.5628
[2023-10-04 19:55:07] iter = 08850, loss = 1.5502
[2023-10-04 19:55:07] iter = 08860, loss = 1.5732
[2023-10-04 19:55:08] iter = 08870, loss = 1.5759
[2023-10-04 19:55:09] iter = 08880, loss = 1.5434
[2023-10-04 19:55:10] iter = 08890, loss = 1.5884
[2023-10-04 19:55:11] iter = 08900, loss = 1.6329
[2023-10-04 19:55:12] iter = 08910, loss = 1.5633
[2023-10-04 19:55:13] iter = 08920, loss = 1.5216
[2023-10-04 19:55:14] iter = 08930, loss = 1.4927
[2023-10-04 19:55:15] iter = 08940, loss = 1.4757
[2023-10-04 19:55:16] iter = 08950, loss = 1.5001
[2023-10-04 19:55:17] iter = 08960, loss = 1.5705
[2023-10-04 19:55:18] iter = 08970, loss = 1.4853
[2023-10-04 19:55:19] iter = 08980, loss = 1.5599
[2023-10-04 19:55:19] iter = 08990, loss = 1.4898
[2023-10-04 19:55:20] iter = 09000, loss = 1.6395
[2023-10-04 19:55:21] iter = 09010, loss = 1.6474
[2023-10-04 19:55:22] iter = 09020, loss = 1.5524
[2023-10-04 19:55:23] iter = 09030, loss = 1.5066
[2023-10-04 19:55:24] iter = 09040, loss = 1.4920
[2023-10-04 19:55:25] iter = 09050, loss = 1.5584
[2023-10-04 19:55:26] iter = 09060, loss = 1.6265
[2023-10-04 19:55:27] iter = 09070, loss = 1.7741
[2023-10-04 19:55:28] iter = 09080, loss = 1.5735
[2023-10-04 19:55:29] iter = 09090, loss = 1.6289
[2023-10-04 19:55:30] iter = 09100, loss = 1.6802
[2023-10-04 19:55:30] iter = 09110, loss = 1.5857
[2023-10-04 19:55:31] iter = 09120, loss = 1.4561
[2023-10-04 19:55:32] iter = 09130, loss = 1.6008
[2023-10-04 19:55:33] iter = 09140, loss = 1.5394
[2023-10-04 19:55:34] iter = 09150, loss = 1.4571
[2023-10-04 19:55:35] iter = 09160, loss = 1.7048
[2023-10-04 19:55:36] iter = 09170, loss = 1.5266
[2023-10-04 19:55:37] iter = 09180, loss = 1.6398
[2023-10-04 19:55:38] iter = 09190, loss = 1.4466
[2023-10-04 19:55:38] iter = 09200, loss = 1.6270
[2023-10-04 19:55:39] iter = 09210, loss = 1.4940
[2023-10-04 19:55:40] iter = 09220, loss = 1.6377
[2023-10-04 19:55:41] iter = 09230, loss = 1.6001
[2023-10-04 19:55:42] iter = 09240, loss = 1.6626
[2023-10-04 19:55:43] iter = 09250, loss = 1.6864
[2023-10-04 19:55:44] iter = 09260, loss = 1.4831
[2023-10-04 19:55:45] iter = 09270, loss = 1.6132
[2023-10-04 19:55:46] iter = 09280, loss = 1.4964
[2023-10-04 19:55:46] iter = 09290, loss = 1.6360
[2023-10-04 19:55:47] iter = 09300, loss = 1.4822
[2023-10-04 19:55:48] iter = 09310, loss = 1.6097
[2023-10-04 19:55:49] iter = 09320, loss = 1.5183
[2023-10-04 19:55:50] iter = 09330, loss = 1.5478
[2023-10-04 19:55:51] iter = 09340, loss = 1.6031
[2023-10-04 19:55:52] iter = 09350, loss = 1.4726
[2023-10-04 19:55:53] iter = 09360, loss = 1.5108
[2023-10-04 19:55:54] iter = 09370, loss = 1.5132
[2023-10-04 19:55:55] iter = 09380, loss = 1.5577
[2023-10-04 19:55:55] iter = 09390, loss = 1.6783
[2023-10-04 19:55:56] iter = 09400, loss = 1.5531
[2023-10-04 19:55:57] iter = 09410, loss = 1.5218
[2023-10-04 19:55:58] iter = 09420, loss = 1.4912
[2023-10-04 19:55:59] iter = 09430, loss = 1.6730
[2023-10-04 19:56:00] iter = 09440, loss = 1.7661
[2023-10-04 19:56:01] iter = 09450, loss = 1.5901
[2023-10-04 19:56:02] iter = 09460, loss = 1.4837
[2023-10-04 19:56:03] iter = 09470, loss = 1.7633
[2023-10-04 19:56:04] iter = 09480, loss = 1.5707
[2023-10-04 19:56:04] iter = 09490, loss = 1.7882
[2023-10-04 19:56:05] iter = 09500, loss = 1.4685
[2023-10-04 19:56:06] iter = 09510, loss = 1.4661
[2023-10-04 19:56:07] iter = 09520, loss = 1.5186
[2023-10-04 19:56:08] iter = 09530, loss = 1.4266
[2023-10-04 19:56:09] iter = 09540, loss = 1.5739
[2023-10-04 19:56:10] iter = 09550, loss = 1.4273
[2023-10-04 19:56:11] iter = 09560, loss = 1.6036
[2023-10-04 19:56:12] iter = 09570, loss = 1.6951
[2023-10-04 19:56:13] iter = 09580, loss = 1.5930
[2023-10-04 19:56:14] iter = 09590, loss = 1.4778
[2023-10-04 19:56:15] iter = 09600, loss = 1.5040
[2023-10-04 19:56:15] iter = 09610, loss = 1.4275
[2023-10-04 19:56:16] iter = 09620, loss = 1.6368
[2023-10-04 19:56:17] iter = 09630, loss = 1.5887
[2023-10-04 19:56:18] iter = 09640, loss = 1.4779
[2023-10-04 19:56:19] iter = 09650, loss = 1.7133
[2023-10-04 19:56:20] iter = 09660, loss = 1.4894
[2023-10-04 19:56:21] iter = 09670, loss = 1.5687
[2023-10-04 19:56:22] iter = 09680, loss = 1.6484
[2023-10-04 19:56:23] iter = 09690, loss = 1.6013
[2023-10-04 19:56:24] iter = 09700, loss = 1.6879
[2023-10-04 19:56:24] iter = 09710, loss = 1.5777
[2023-10-04 19:56:26] iter = 09720, loss = 1.4927
[2023-10-04 19:56:26] iter = 09730, loss = 1.5308
[2023-10-04 19:56:27] iter = 09740, loss = 1.5499
[2023-10-04 19:56:28] iter = 09750, loss = 1.5431
[2023-10-04 19:56:29] iter = 09760, loss = 1.4835
[2023-10-04 19:56:30] iter = 09770, loss = 1.5550
[2023-10-04 19:56:31] iter = 09780, loss = 1.4651
[2023-10-04 19:56:32] iter = 09790, loss = 1.5672
[2023-10-04 19:56:33] iter = 09800, loss = 1.4717
[2023-10-04 19:56:34] iter = 09810, loss = 1.4821
[2023-10-04 19:56:35] iter = 09820, loss = 1.5327
[2023-10-04 19:56:36] iter = 09830, loss = 1.4300
[2023-10-04 19:56:37] iter = 09840, loss = 1.4480
[2023-10-04 19:56:38] iter = 09850, loss = 1.6723
[2023-10-04 19:56:38] iter = 09860, loss = 1.5687
[2023-10-04 19:56:39] iter = 09870, loss = 1.5869
[2023-10-04 19:56:40] iter = 09880, loss = 1.5350
[2023-10-04 19:56:41] iter = 09890, loss = 1.5665
[2023-10-04 19:56:42] iter = 09900, loss = 1.6208
[2023-10-04 19:56:43] iter = 09910, loss = 1.4998
[2023-10-04 19:56:44] iter = 09920, loss = 1.5536
[2023-10-04 19:56:45] iter = 09930, loss = 1.5267
[2023-10-04 19:56:46] iter = 09940, loss = 1.5596
[2023-10-04 19:56:47] iter = 09950, loss = 1.4769
[2023-10-04 19:56:48] iter = 09960, loss = 1.4515
[2023-10-04 19:56:49] iter = 09970, loss = 1.5167
[2023-10-04 19:56:49] iter = 09980, loss = 1.5538
[2023-10-04 19:56:50] iter = 09990, loss = 1.5491
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 11671}
[2023-10-04 19:57:16] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.013219 train acc = 1.0000, test acc = 0.6213
[2023-10-04 19:57:40] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.014415 train acc = 1.0000, test acc = 0.6107
[2023-10-04 19:58:04] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.006743 train acc = 0.9980, test acc = 0.6152
[2023-10-04 19:58:29] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.015557 train acc = 0.9960, test acc = 0.6184
[2023-10-04 19:58:53] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.012626 train acc = 1.0000, test acc = 0.6143
[2023-10-04 19:59:18] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.014056 train acc = 1.0000, test acc = 0.6130
[2023-10-04 19:59:42] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.019084 train acc = 1.0000, test acc = 0.6137
[2023-10-04 20:00:07] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.013977 train acc = 1.0000, test acc = 0.6157
[2023-10-04 20:00:31] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.011692 train acc = 1.0000, test acc = 0.6168
[2023-10-04 20:00:55] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.015628 train acc = 1.0000, test acc = 0.6216
[2023-10-04 20:01:20] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.006876 train acc = 1.0000, test acc = 0.6175
[2023-10-04 20:01:44] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001585 train acc = 1.0000, test acc = 0.6098
[2023-10-04 20:02:08] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003668 train acc = 1.0000, test acc = 0.6141
[2023-10-04 20:02:33] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.008358 train acc = 1.0000, test acc = 0.6119
[2023-10-04 20:02:57] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013490 train acc = 1.0000, test acc = 0.6135
[2023-10-04 20:03:21] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004063 train acc = 1.0000, test acc = 0.6242
[2023-10-04 20:03:45] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003579 train acc = 1.0000, test acc = 0.6170
[2023-10-04 20:04:10] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.016974 train acc = 1.0000, test acc = 0.6285
[2023-10-04 20:04:34] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002849 train acc = 1.0000, test acc = 0.6256
[2023-10-04 20:04:58] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.011995 train acc = 1.0000, test acc = 0.6200
Evaluate 20 random ConvNet, mean = 0.6171 std = 0.0049
-------------------------
[2023-10-04 20:04:59] iter = 10000, loss = 1.4720
[2023-10-04 20:05:00] iter = 10010, loss = 1.5042
[2023-10-04 20:05:01] iter = 10020, loss = 1.7295
[2023-10-04 20:05:01] iter = 10030, loss = 1.6503
[2023-10-04 20:05:02] iter = 10040, loss = 1.6254
[2023-10-04 20:05:03] iter = 10050, loss = 1.5759
[2023-10-04 20:05:04] iter = 10060, loss = 1.5125
[2023-10-04 20:05:05] iter = 10070, loss = 1.6194
[2023-10-04 20:05:06] iter = 10080, loss = 1.6545
[2023-10-04 20:05:07] iter = 10090, loss = 1.5318
[2023-10-04 20:05:08] iter = 10100, loss = 1.5227
[2023-10-04 20:05:09] iter = 10110, loss = 1.6189
[2023-10-04 20:05:10] iter = 10120, loss = 1.5233
[2023-10-04 20:05:11] iter = 10130, loss = 1.4730
[2023-10-04 20:05:12] iter = 10140, loss = 1.4871
[2023-10-04 20:05:12] iter = 10150, loss = 1.5928
[2023-10-04 20:05:13] iter = 10160, loss = 1.5263
[2023-10-04 20:05:14] iter = 10170, loss = 1.6803
[2023-10-04 20:05:15] iter = 10180, loss = 1.5527
[2023-10-04 20:05:16] iter = 10190, loss = 1.5424
[2023-10-04 20:05:17] iter = 10200, loss = 1.4188
[2023-10-04 20:05:18] iter = 10210, loss = 1.5150
[2023-10-04 20:05:19] iter = 10220, loss = 1.4295
[2023-10-04 20:05:20] iter = 10230, loss = 1.5175
[2023-10-04 20:05:21] iter = 10240, loss = 1.3806
[2023-10-04 20:05:22] iter = 10250, loss = 1.6511
[2023-10-04 20:05:23] iter = 10260, loss = 1.4009
[2023-10-04 20:05:23] iter = 10270, loss = 1.7254
[2023-10-04 20:05:24] iter = 10280, loss = 1.5402
[2023-10-04 20:05:25] iter = 10290, loss = 1.4826
[2023-10-04 20:05:26] iter = 10300, loss = 1.5086
[2023-10-04 20:05:27] iter = 10310, loss = 1.5281
[2023-10-04 20:05:28] iter = 10320, loss = 1.4562
[2023-10-04 20:05:29] iter = 10330, loss = 1.3837
[2023-10-04 20:05:30] iter = 10340, loss = 1.4462
[2023-10-04 20:05:31] iter = 10350, loss = 1.5371
[2023-10-04 20:05:32] iter = 10360, loss = 1.4522
[2023-10-04 20:05:33] iter = 10370, loss = 1.4842
[2023-10-04 20:05:34] iter = 10380, loss = 1.6172
[2023-10-04 20:05:35] iter = 10390, loss = 1.6643
[2023-10-04 20:05:36] iter = 10400, loss = 1.4577
[2023-10-04 20:05:37] iter = 10410, loss = 1.5098
[2023-10-04 20:05:38] iter = 10420, loss = 1.5973
[2023-10-04 20:05:38] iter = 10430, loss = 1.4604
[2023-10-04 20:05:39] iter = 10440, loss = 1.4443
[2023-10-04 20:05:40] iter = 10450, loss = 1.4675
[2023-10-04 20:05:41] iter = 10460, loss = 1.6760
[2023-10-04 20:05:42] iter = 10470, loss = 1.6546
[2023-10-04 20:05:43] iter = 10480, loss = 1.6098
[2023-10-04 20:05:44] iter = 10490, loss = 1.5367
[2023-10-04 20:05:45] iter = 10500, loss = 1.4496
[2023-10-04 20:05:46] iter = 10510, loss = 1.5183
[2023-10-04 20:05:47] iter = 10520, loss = 1.5181
[2023-10-04 20:05:48] iter = 10530, loss = 1.5387
[2023-10-04 20:05:49] iter = 10540, loss = 1.5427
[2023-10-04 20:05:50] iter = 10550, loss = 1.5126
[2023-10-04 20:05:51] iter = 10560, loss = 1.5856
[2023-10-04 20:05:52] iter = 10570, loss = 1.5454
[2023-10-04 20:05:53] iter = 10580, loss = 1.5463
[2023-10-04 20:05:54] iter = 10590, loss = 1.4222
[2023-10-04 20:05:55] iter = 10600, loss = 1.6569
[2023-10-04 20:05:55] iter = 10610, loss = 1.5918
[2023-10-04 20:05:56] iter = 10620, loss = 1.4362
[2023-10-04 20:05:57] iter = 10630, loss = 1.5835
[2023-10-04 20:05:58] iter = 10640, loss = 1.6212
[2023-10-04 20:05:59] iter = 10650, loss = 1.4764
[2023-10-04 20:06:00] iter = 10660, loss = 1.3691
[2023-10-04 20:06:01] iter = 10670, loss = 1.4645
[2023-10-04 20:06:02] iter = 10680, loss = 1.6260
[2023-10-04 20:06:03] iter = 10690, loss = 1.4873
[2023-10-04 20:06:04] iter = 10700, loss = 1.4461
[2023-10-04 20:06:05] iter = 10710, loss = 1.5724
[2023-10-04 20:06:06] iter = 10720, loss = 1.6758
[2023-10-04 20:06:06] iter = 10730, loss = 1.4920
[2023-10-04 20:06:07] iter = 10740, loss = 1.6192
[2023-10-04 20:06:08] iter = 10750, loss = 1.4712
[2023-10-04 20:06:09] iter = 10760, loss = 1.5187
[2023-10-04 20:06:10] iter = 10770, loss = 1.5377
[2023-10-04 20:06:11] iter = 10780, loss = 1.4934
[2023-10-04 20:06:12] iter = 10790, loss = 1.3940
[2023-10-04 20:06:13] iter = 10800, loss = 1.6252
[2023-10-04 20:06:14] iter = 10810, loss = 1.3523
[2023-10-04 20:06:15] iter = 10820, loss = 1.4740
[2023-10-04 20:06:15] iter = 10830, loss = 1.3597
[2023-10-04 20:06:16] iter = 10840, loss = 1.6164
[2023-10-04 20:06:17] iter = 10850, loss = 1.4008
[2023-10-04 20:06:18] iter = 10860, loss = 1.3745
[2023-10-04 20:06:19] iter = 10870, loss = 1.5301
[2023-10-04 20:06:20] iter = 10880, loss = 1.5693
[2023-10-04 20:06:21] iter = 10890, loss = 1.4694
[2023-10-04 20:06:22] iter = 10900, loss = 1.4242
[2023-10-04 20:06:23] iter = 10910, loss = 1.7535
[2023-10-04 20:06:24] iter = 10920, loss = 1.5565
[2023-10-04 20:06:25] iter = 10930, loss = 1.4215
[2023-10-04 20:06:26] iter = 10940, loss = 1.5677
[2023-10-04 20:06:27] iter = 10950, loss = 1.5539
[2023-10-04 20:06:28] iter = 10960, loss = 1.4218
[2023-10-04 20:06:29] iter = 10970, loss = 1.5264
[2023-10-04 20:06:30] iter = 10980, loss = 1.5715
[2023-10-04 20:06:31] iter = 10990, loss = 1.5189
[2023-10-04 20:06:31] iter = 11000, loss = 1.4437
[2023-10-04 20:06:32] iter = 11010, loss = 1.5968
[2023-10-04 20:06:33] iter = 11020, loss = 1.6373
[2023-10-04 20:06:34] iter = 11030, loss = 1.6219
[2023-10-04 20:06:35] iter = 11040, loss = 1.6690
[2023-10-04 20:06:36] iter = 11050, loss = 1.6627
[2023-10-04 20:06:37] iter = 11060, loss = 1.5330
[2023-10-04 20:06:38] iter = 11070, loss = 1.4289
[2023-10-04 20:06:39] iter = 11080, loss = 1.4137
[2023-10-04 20:06:40] iter = 11090, loss = 1.4389
[2023-10-04 20:06:40] iter = 11100, loss = 1.5811
[2023-10-04 20:06:41] iter = 11110, loss = 1.4954
[2023-10-04 20:06:42] iter = 11120, loss = 1.5923
[2023-10-04 20:06:43] iter = 11130, loss = 1.4999
[2023-10-04 20:06:44] iter = 11140, loss = 1.5477
[2023-10-04 20:06:45] iter = 11150, loss = 1.5960
[2023-10-04 20:06:46] iter = 11160, loss = 1.4989
[2023-10-04 20:06:47] iter = 11170, loss = 1.5587
[2023-10-04 20:06:48] iter = 11180, loss = 1.7139
[2023-10-04 20:06:49] iter = 11190, loss = 1.5035
[2023-10-04 20:06:49] iter = 11200, loss = 1.4584
[2023-10-04 20:06:50] iter = 11210, loss = 1.6416
[2023-10-04 20:06:51] iter = 11220, loss = 1.5407
[2023-10-04 20:06:52] iter = 11230, loss = 1.5084
[2023-10-04 20:06:53] iter = 11240, loss = 1.4269
[2023-10-04 20:06:54] iter = 11250, loss = 1.5371
[2023-10-04 20:06:55] iter = 11260, loss = 1.4808
[2023-10-04 20:06:56] iter = 11270, loss = 1.5621
[2023-10-04 20:06:57] iter = 11280, loss = 1.6311
[2023-10-04 20:06:58] iter = 11290, loss = 1.4397
[2023-10-04 20:06:59] iter = 11300, loss = 1.4194
[2023-10-04 20:07:00] iter = 11310, loss = 1.7758
[2023-10-04 20:07:00] iter = 11320, loss = 1.4468
[2023-10-04 20:07:01] iter = 11330, loss = 1.6599
[2023-10-04 20:07:02] iter = 11340, loss = 1.4975
[2023-10-04 20:07:03] iter = 11350, loss = 1.5174
[2023-10-04 20:07:04] iter = 11360, loss = 1.4487
[2023-10-04 20:07:05] iter = 11370, loss = 1.6448
[2023-10-04 20:07:06] iter = 11380, loss = 1.4165
[2023-10-04 20:07:07] iter = 11390, loss = 1.5002
[2023-10-04 20:07:08] iter = 11400, loss = 1.5398
[2023-10-04 20:07:09] iter = 11410, loss = 1.5553
[2023-10-04 20:07:10] iter = 11420, loss = 1.4878
[2023-10-04 20:07:10] iter = 11430, loss = 1.5415
[2023-10-04 20:07:11] iter = 11440, loss = 1.5030
[2023-10-04 20:07:13] iter = 11450, loss = 1.7329
[2023-10-04 20:07:13] iter = 11460, loss = 1.5718
[2023-10-04 20:07:14] iter = 11470, loss = 1.5170
[2023-10-04 20:07:15] iter = 11480, loss = 1.5618
[2023-10-04 20:07:16] iter = 11490, loss = 1.5149
[2023-10-04 20:07:17] iter = 11500, loss = 1.5379
[2023-10-04 20:07:18] iter = 11510, loss = 1.5161
[2023-10-04 20:07:19] iter = 11520, loss = 1.4821
[2023-10-04 20:07:20] iter = 11530, loss = 1.4846
[2023-10-04 20:07:21] iter = 11540, loss = 1.5803
[2023-10-04 20:07:22] iter = 11550, loss = 1.4409
[2023-10-04 20:07:23] iter = 11560, loss = 1.4547
[2023-10-04 20:07:23] iter = 11570, loss = 1.6856
[2023-10-04 20:07:24] iter = 11580, loss = 1.5670
[2023-10-04 20:07:25] iter = 11590, loss = 1.6365
[2023-10-04 20:07:26] iter = 11600, loss = 1.5194
[2023-10-04 20:07:27] iter = 11610, loss = 1.5233
[2023-10-04 20:07:28] iter = 11620, loss = 1.4383
[2023-10-04 20:07:29] iter = 11630, loss = 1.5958
[2023-10-04 20:07:30] iter = 11640, loss = 1.5947
[2023-10-04 20:07:31] iter = 11650, loss = 1.5020
[2023-10-04 20:07:32] iter = 11660, loss = 1.4835
[2023-10-04 20:07:33] iter = 11670, loss = 1.5557
[2023-10-04 20:07:33] iter = 11680, loss = 1.5222
[2023-10-04 20:07:34] iter = 11690, loss = 1.4627
[2023-10-04 20:07:35] iter = 11700, loss = 1.5363
[2023-10-04 20:07:36] iter = 11710, loss = 1.5316
[2023-10-04 20:07:37] iter = 11720, loss = 1.5741
[2023-10-04 20:07:38] iter = 11730, loss = 1.4479
[2023-10-04 20:07:39] iter = 11740, loss = 1.5474
[2023-10-04 20:07:40] iter = 11750, loss = 1.5013
[2023-10-04 20:07:41] iter = 11760, loss = 1.3814
[2023-10-04 20:07:42] iter = 11770, loss = 1.6229
[2023-10-04 20:07:43] iter = 11780, loss = 1.5890
[2023-10-04 20:07:44] iter = 11790, loss = 1.4671
[2023-10-04 20:07:44] iter = 11800, loss = 1.6477
[2023-10-04 20:07:45] iter = 11810, loss = 1.4581
[2023-10-04 20:07:46] iter = 11820, loss = 1.5382
[2023-10-04 20:07:47] iter = 11830, loss = 1.4692
[2023-10-04 20:07:48] iter = 11840, loss = 1.5514
[2023-10-04 20:07:49] iter = 11850, loss = 1.4753
[2023-10-04 20:07:50] iter = 11860, loss = 1.6086
[2023-10-04 20:07:51] iter = 11870, loss = 1.3871
[2023-10-04 20:07:52] iter = 11880, loss = 1.5086
[2023-10-04 20:07:53] iter = 11890, loss = 1.8054
[2023-10-04 20:07:54] iter = 11900, loss = 1.6070
[2023-10-04 20:07:55] iter = 11910, loss = 1.5746
[2023-10-04 20:07:56] iter = 11920, loss = 1.5776
[2023-10-04 20:07:56] iter = 11930, loss = 1.4863
[2023-10-04 20:07:57] iter = 11940, loss = 1.5410
[2023-10-04 20:07:58] iter = 11950, loss = 1.5810
[2023-10-04 20:07:59] iter = 11960, loss = 1.4450
[2023-10-04 20:08:00] iter = 11970, loss = 1.4839
[2023-10-04 20:08:01] iter = 11980, loss = 1.5862
[2023-10-04 20:08:02] iter = 11990, loss = 1.5321
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 83219}
[2023-10-04 20:08:27] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002266 train acc = 1.0000, test acc = 0.6170
[2023-10-04 20:08:51] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.020647 train acc = 1.0000, test acc = 0.6225
[2023-10-04 20:09:16] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.013324 train acc = 1.0000, test acc = 0.6292
[2023-10-04 20:09:41] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.013079 train acc = 1.0000, test acc = 0.6288
[2023-10-04 20:10:05] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.009999 train acc = 1.0000, test acc = 0.6254
[2023-10-04 20:10:30] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003654 train acc = 1.0000, test acc = 0.6170
[2023-10-04 20:10:54] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.024342 train acc = 1.0000, test acc = 0.6272
[2023-10-04 20:11:18] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.006182 train acc = 1.0000, test acc = 0.6251
[2023-10-04 20:11:43] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.010483 train acc = 1.0000, test acc = 0.6212
[2023-10-04 20:12:07] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005650 train acc = 1.0000, test acc = 0.6142
[2023-10-04 20:12:32] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.009470 train acc = 1.0000, test acc = 0.6193
[2023-10-04 20:12:56] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004700 train acc = 1.0000, test acc = 0.6182
[2023-10-04 20:13:21] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.002946 train acc = 1.0000, test acc = 0.6226
[2023-10-04 20:13:45] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002481 train acc = 1.0000, test acc = 0.6204
[2023-10-04 20:14:10] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004392 train acc = 1.0000, test acc = 0.6241
[2023-10-04 20:14:34] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.009889 train acc = 1.0000, test acc = 0.6137
[2023-10-04 20:14:58] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.005763 train acc = 1.0000, test acc = 0.6183
[2023-10-04 20:15:23] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003560 train acc = 1.0000, test acc = 0.6215
[2023-10-04 20:15:47] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001895 train acc = 1.0000, test acc = 0.6200
[2023-10-04 20:16:11] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012274 train acc = 1.0000, test acc = 0.6203
Evaluate 20 random ConvNet, mean = 0.6213 std = 0.0043
-------------------------
[2023-10-04 20:16:12] iter = 12000, loss = 1.5089
[2023-10-04 20:16:13] iter = 12010, loss = 1.3631
[2023-10-04 20:16:14] iter = 12020, loss = 1.3869
[2023-10-04 20:16:14] iter = 12030, loss = 1.5029
[2023-10-04 20:16:15] iter = 12040, loss = 1.4260
[2023-10-04 20:16:16] iter = 12050, loss = 1.4705
[2023-10-04 20:16:17] iter = 12060, loss = 1.4285
[2023-10-04 20:16:18] iter = 12070, loss = 1.4532
[2023-10-04 20:16:19] iter = 12080, loss = 1.4819
[2023-10-04 20:16:20] iter = 12090, loss = 1.5396
[2023-10-04 20:16:21] iter = 12100, loss = 1.5729
[2023-10-04 20:16:22] iter = 12110, loss = 1.4342
[2023-10-04 20:16:23] iter = 12120, loss = 1.4159
[2023-10-04 20:16:24] iter = 12130, loss = 1.4603
[2023-10-04 20:16:25] iter = 12140, loss = 1.5726
[2023-10-04 20:16:26] iter = 12150, loss = 1.5499
[2023-10-04 20:16:27] iter = 12160, loss = 1.5017
[2023-10-04 20:16:28] iter = 12170, loss = 1.4432
[2023-10-04 20:16:29] iter = 12180, loss = 1.5227
[2023-10-04 20:16:30] iter = 12190, loss = 1.5071
[2023-10-04 20:16:31] iter = 12200, loss = 1.6366
[2023-10-04 20:16:31] iter = 12210, loss = 1.3963
[2023-10-04 20:16:32] iter = 12220, loss = 1.5511
[2023-10-04 20:16:33] iter = 12230, loss = 1.4421
[2023-10-04 20:16:34] iter = 12240, loss = 1.4729
[2023-10-04 20:16:35] iter = 12250, loss = 1.6731
[2023-10-04 20:16:36] iter = 12260, loss = 1.4922
[2023-10-04 20:16:37] iter = 12270, loss = 1.4026
[2023-10-04 20:16:38] iter = 12280, loss = 1.5368
[2023-10-04 20:16:39] iter = 12290, loss = 1.4313
[2023-10-04 20:16:40] iter = 12300, loss = 1.3931
[2023-10-04 20:16:40] iter = 12310, loss = 1.6175
[2023-10-04 20:16:41] iter = 12320, loss = 1.4815
[2023-10-04 20:16:42] iter = 12330, loss = 1.5807
[2023-10-04 20:16:43] iter = 12340, loss = 1.5510
[2023-10-04 20:16:44] iter = 12350, loss = 1.5298
[2023-10-04 20:16:45] iter = 12360, loss = 1.5685
[2023-10-04 20:16:46] iter = 12370, loss = 1.5173
[2023-10-04 20:16:47] iter = 12380, loss = 1.5066
[2023-10-04 20:16:48] iter = 12390, loss = 1.5376
[2023-10-04 20:16:49] iter = 12400, loss = 1.5555
[2023-10-04 20:16:49] iter = 12410, loss = 1.4553
[2023-10-04 20:16:50] iter = 12420, loss = 1.6757
[2023-10-04 20:16:51] iter = 12430, loss = 1.5664
[2023-10-04 20:16:52] iter = 12440, loss = 1.5139
[2023-10-04 20:16:53] iter = 12450, loss = 1.6400
[2023-10-04 20:16:54] iter = 12460, loss = 1.4020
[2023-10-04 20:16:55] iter = 12470, loss = 1.4296
[2023-10-04 20:16:56] iter = 12480, loss = 1.6552
[2023-10-04 20:16:57] iter = 12490, loss = 1.5496
[2023-10-04 20:16:58] iter = 12500, loss = 1.3772
[2023-10-04 20:16:59] iter = 12510, loss = 1.5786
[2023-10-04 20:17:00] iter = 12520, loss = 1.4565
[2023-10-04 20:17:00] iter = 12530, loss = 1.4903
[2023-10-04 20:17:01] iter = 12540, loss = 1.5075
[2023-10-04 20:17:02] iter = 12550, loss = 1.6032
[2023-10-04 20:17:03] iter = 12560, loss = 1.4586
[2023-10-04 20:17:04] iter = 12570, loss = 1.5981
[2023-10-04 20:17:05] iter = 12580, loss = 1.4481
[2023-10-04 20:17:06] iter = 12590, loss = 1.4395
[2023-10-04 20:17:07] iter = 12600, loss = 1.4541
[2023-10-04 20:17:08] iter = 12610, loss = 1.5543
[2023-10-04 20:17:09] iter = 12620, loss = 1.4969
[2023-10-04 20:17:10] iter = 12630, loss = 1.4952
[2023-10-04 20:17:11] iter = 12640, loss = 1.5104
[2023-10-04 20:17:12] iter = 12650, loss = 1.7237
[2023-10-04 20:17:12] iter = 12660, loss = 1.4145
[2023-10-04 20:17:13] iter = 12670, loss = 1.4818
[2023-10-04 20:17:14] iter = 12680, loss = 1.4118
[2023-10-04 20:17:15] iter = 12690, loss = 1.6288
[2023-10-04 20:17:16] iter = 12700, loss = 1.6115
[2023-10-04 20:17:17] iter = 12710, loss = 1.5604
[2023-10-04 20:17:18] iter = 12720, loss = 1.4428
[2023-10-04 20:17:19] iter = 12730, loss = 1.5586
[2023-10-04 20:17:20] iter = 12740, loss = 1.5595
[2023-10-04 20:17:21] iter = 12750, loss = 1.4037
[2023-10-04 20:17:21] iter = 12760, loss = 1.5413
[2023-10-04 20:17:22] iter = 12770, loss = 1.4760
[2023-10-04 20:17:23] iter = 12780, loss = 1.5407
[2023-10-04 20:17:24] iter = 12790, loss = 1.7211
[2023-10-04 20:17:25] iter = 12800, loss = 1.5002
[2023-10-04 20:17:26] iter = 12810, loss = 1.4557
[2023-10-04 20:17:27] iter = 12820, loss = 1.5176
[2023-10-04 20:17:28] iter = 12830, loss = 1.5698
[2023-10-04 20:17:28] iter = 12840, loss = 1.5079
[2023-10-04 20:17:29] iter = 12850, loss = 1.4615
[2023-10-04 20:17:30] iter = 12860, loss = 1.4040
[2023-10-04 20:17:31] iter = 12870, loss = 1.5998
[2023-10-04 20:17:32] iter = 12880, loss = 1.3906
[2023-10-04 20:17:33] iter = 12890, loss = 1.4727
[2023-10-04 20:17:34] iter = 12900, loss = 1.5829
[2023-10-04 20:17:35] iter = 12910, loss = 1.6446
[2023-10-04 20:17:36] iter = 12920, loss = 1.4905
[2023-10-04 20:17:37] iter = 12930, loss = 1.5090
[2023-10-04 20:17:38] iter = 12940, loss = 1.4470
[2023-10-04 20:17:39] iter = 12950, loss = 1.5523
[2023-10-04 20:17:40] iter = 12960, loss = 1.4711
[2023-10-04 20:17:40] iter = 12970, loss = 1.6613
[2023-10-04 20:17:41] iter = 12980, loss = 1.5529
[2023-10-04 20:17:42] iter = 12990, loss = 1.5527
[2023-10-04 20:17:43] iter = 13000, loss = 1.5415
[2023-10-04 20:17:44] iter = 13010, loss = 1.5296
[2023-10-04 20:17:45] iter = 13020, loss = 1.5556
[2023-10-04 20:17:46] iter = 13030, loss = 1.4730
[2023-10-04 20:17:47] iter = 13040, loss = 1.5803
[2023-10-04 20:17:48] iter = 13050, loss = 1.3968
[2023-10-04 20:17:49] iter = 13060, loss = 1.6115
[2023-10-04 20:17:49] iter = 13070, loss = 1.4815
[2023-10-04 20:17:50] iter = 13080, loss = 1.4339
[2023-10-04 20:17:51] iter = 13090, loss = 1.4111
[2023-10-04 20:17:52] iter = 13100, loss = 1.5401
[2023-10-04 20:17:53] iter = 13110, loss = 1.5555
[2023-10-04 20:17:54] iter = 13120, loss = 1.5375
[2023-10-04 20:17:55] iter = 13130, loss = 1.4525
[2023-10-04 20:17:56] iter = 13140, loss = 1.5692
[2023-10-04 20:17:57] iter = 13150, loss = 1.6841
[2023-10-04 20:17:58] iter = 13160, loss = 1.5917
[2023-10-04 20:17:59] iter = 13170, loss = 1.6383
[2023-10-04 20:17:59] iter = 13180, loss = 1.5738
[2023-10-04 20:18:00] iter = 13190, loss = 1.5383
[2023-10-04 20:18:01] iter = 13200, loss = 1.5338
[2023-10-04 20:18:02] iter = 13210, loss = 1.5715
[2023-10-04 20:18:03] iter = 13220, loss = 1.4726
[2023-10-04 20:18:04] iter = 13230, loss = 1.6588
[2023-10-04 20:18:05] iter = 13240, loss = 1.5172
[2023-10-04 20:18:06] iter = 13250, loss = 1.7655
[2023-10-04 20:18:07] iter = 13260, loss = 1.4943
[2023-10-04 20:18:08] iter = 13270, loss = 1.5332
[2023-10-04 20:18:09] iter = 13280, loss = 1.6270
[2023-10-04 20:18:10] iter = 13290, loss = 1.7365
[2023-10-04 20:18:11] iter = 13300, loss = 1.5620
[2023-10-04 20:18:11] iter = 13310, loss = 1.4674
[2023-10-04 20:18:12] iter = 13320, loss = 1.5399
[2023-10-04 20:18:13] iter = 13330, loss = 1.4230
[2023-10-04 20:18:14] iter = 13340, loss = 1.5431
[2023-10-04 20:18:15] iter = 13350, loss = 1.4813
[2023-10-04 20:18:16] iter = 13360, loss = 1.6372
[2023-10-04 20:18:17] iter = 13370, loss = 1.3436
[2023-10-04 20:18:18] iter = 13380, loss = 1.3981
[2023-10-04 20:18:19] iter = 13390, loss = 1.5920
[2023-10-04 20:18:20] iter = 13400, loss = 1.5443
[2023-10-04 20:18:21] iter = 13410, loss = 1.6447
[2023-10-04 20:18:22] iter = 13420, loss = 1.4879
[2023-10-04 20:18:23] iter = 13430, loss = 1.5166
[2023-10-04 20:18:24] iter = 13440, loss = 1.6300
[2023-10-04 20:18:24] iter = 13450, loss = 1.4837
[2023-10-04 20:18:25] iter = 13460, loss = 1.4521
[2023-10-04 20:18:26] iter = 13470, loss = 1.5567
[2023-10-04 20:18:27] iter = 13480, loss = 1.5236
[2023-10-04 20:18:28] iter = 13490, loss = 1.7120
[2023-10-04 20:18:29] iter = 13500, loss = 1.5049
[2023-10-04 20:18:30] iter = 13510, loss = 1.5205
[2023-10-04 20:18:31] iter = 13520, loss = 1.5513
[2023-10-04 20:18:32] iter = 13530, loss = 1.5157
[2023-10-04 20:18:33] iter = 13540, loss = 1.4316
[2023-10-04 20:18:34] iter = 13550, loss = 1.3913
[2023-10-04 20:18:35] iter = 13560, loss = 1.4653
[2023-10-04 20:18:35] iter = 13570, loss = 1.5027
[2023-10-04 20:18:36] iter = 13580, loss = 1.6233
[2023-10-04 20:18:37] iter = 13590, loss = 1.6581
[2023-10-04 20:18:38] iter = 13600, loss = 1.5679
[2023-10-04 20:18:39] iter = 13610, loss = 1.4336
[2023-10-04 20:18:40] iter = 13620, loss = 1.5910
[2023-10-04 20:18:41] iter = 13630, loss = 1.4413
[2023-10-04 20:18:42] iter = 13640, loss = 1.5089
[2023-10-04 20:18:43] iter = 13650, loss = 1.5155
[2023-10-04 20:18:44] iter = 13660, loss = 1.4913
[2023-10-04 20:18:45] iter = 13670, loss = 1.5507
[2023-10-04 20:18:45] iter = 13680, loss = 1.4860
[2023-10-04 20:18:46] iter = 13690, loss = 1.6951
[2023-10-04 20:18:47] iter = 13700, loss = 1.5130
[2023-10-04 20:18:48] iter = 13710, loss = 1.5091
[2023-10-04 20:18:49] iter = 13720, loss = 1.4300
[2023-10-04 20:18:50] iter = 13730, loss = 1.5689
[2023-10-04 20:18:51] iter = 13740, loss = 1.5346
[2023-10-04 20:18:52] iter = 13750, loss = 1.5174
[2023-10-04 20:18:53] iter = 13760, loss = 1.5875
[2023-10-04 20:18:54] iter = 13770, loss = 1.5256
[2023-10-04 20:18:55] iter = 13780, loss = 1.4392
[2023-10-04 20:18:55] iter = 13790, loss = 1.4465
[2023-10-04 20:18:56] iter = 13800, loss = 1.6239
[2023-10-04 20:18:57] iter = 13810, loss = 1.4950
[2023-10-04 20:18:58] iter = 13820, loss = 1.3079
[2023-10-04 20:18:59] iter = 13830, loss = 1.4135
[2023-10-04 20:19:00] iter = 13840, loss = 1.5474
[2023-10-04 20:19:01] iter = 13850, loss = 1.5360
[2023-10-04 20:19:02] iter = 13860, loss = 1.5148
[2023-10-04 20:19:03] iter = 13870, loss = 1.4002
[2023-10-04 20:19:04] iter = 13880, loss = 1.4783
[2023-10-04 20:19:04] iter = 13890, loss = 1.5617
[2023-10-04 20:19:05] iter = 13900, loss = 1.4506
[2023-10-04 20:19:06] iter = 13910, loss = 1.6502
[2023-10-04 20:19:07] iter = 13920, loss = 1.4148
[2023-10-04 20:19:08] iter = 13930, loss = 1.4745
[2023-10-04 20:19:09] iter = 13940, loss = 1.5670
[2023-10-04 20:19:10] iter = 13950, loss = 1.6809
[2023-10-04 20:19:11] iter = 13960, loss = 1.7250
[2023-10-04 20:19:12] iter = 13970, loss = 1.5113
[2023-10-04 20:19:13] iter = 13980, loss = 1.4601
[2023-10-04 20:19:14] iter = 13990, loss = 1.5227
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 54871}
[2023-10-04 20:19:39] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.021520 train acc = 1.0000, test acc = 0.6224
[2023-10-04 20:20:04] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.011958 train acc = 1.0000, test acc = 0.6198
[2023-10-04 20:20:28] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.006077 train acc = 1.0000, test acc = 0.6191
[2023-10-04 20:20:52] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.012663 train acc = 0.9980, test acc = 0.6220
[2023-10-04 20:21:17] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.006089 train acc = 1.0000, test acc = 0.6283
[2023-10-04 20:21:41] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.018539 train acc = 0.9980, test acc = 0.6140
[2023-10-04 20:22:05] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004446 train acc = 1.0000, test acc = 0.6272
[2023-10-04 20:22:30] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.005610 train acc = 1.0000, test acc = 0.6169
[2023-10-04 20:22:54] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.018325 train acc = 1.0000, test acc = 0.6236
[2023-10-04 20:23:18] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.007654 train acc = 1.0000, test acc = 0.6216
[2023-10-04 20:23:43] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.016390 train acc = 1.0000, test acc = 0.6194
[2023-10-04 20:24:07] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.011459 train acc = 1.0000, test acc = 0.6220
[2023-10-04 20:24:31] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011861 train acc = 1.0000, test acc = 0.6221
[2023-10-04 20:24:55] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002548 train acc = 1.0000, test acc = 0.6273
[2023-10-04 20:25:20] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.022123 train acc = 0.9980, test acc = 0.6266
[2023-10-04 20:25:44] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.012952 train acc = 1.0000, test acc = 0.6207
[2023-10-04 20:26:08] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.020561 train acc = 1.0000, test acc = 0.6270
[2023-10-04 20:26:33] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003653 train acc = 1.0000, test acc = 0.6197
[2023-10-04 20:26:57] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.007259 train acc = 1.0000, test acc = 0.6222
[2023-10-04 20:27:22] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.026312 train acc = 1.0000, test acc = 0.6179
Evaluate 20 random ConvNet, mean = 0.6220 std = 0.0037
-------------------------
[2023-10-04 20:27:22] iter = 14000, loss = 1.6001
[2023-10-04 20:27:23] iter = 14010, loss = 1.5531
[2023-10-04 20:27:24] iter = 14020, loss = 1.5529
[2023-10-04 20:27:25] iter = 14030, loss = 1.8267
[2023-10-04 20:27:26] iter = 14040, loss = 1.5474
[2023-10-04 20:27:27] iter = 14050, loss = 1.4199
[2023-10-04 20:27:27] iter = 14060, loss = 1.2983
[2023-10-04 20:27:28] iter = 14070, loss = 1.5394
[2023-10-04 20:27:29] iter = 14080, loss = 1.3982
[2023-10-04 20:27:30] iter = 14090, loss = 1.5130
[2023-10-04 20:27:31] iter = 14100, loss = 1.4443
[2023-10-04 20:27:32] iter = 14110, loss = 1.5953
[2023-10-04 20:27:33] iter = 14120, loss = 1.4037
[2023-10-04 20:27:34] iter = 14130, loss = 1.4998
[2023-10-04 20:27:35] iter = 14140, loss = 1.4796
[2023-10-04 20:27:36] iter = 14150, loss = 1.5417
[2023-10-04 20:27:37] iter = 14160, loss = 1.5271
[2023-10-04 20:27:38] iter = 14170, loss = 1.5332
[2023-10-04 20:27:39] iter = 14180, loss = 1.4903
[2023-10-04 20:27:39] iter = 14190, loss = 1.5083
[2023-10-04 20:27:40] iter = 14200, loss = 1.5303
[2023-10-04 20:27:41] iter = 14210, loss = 1.5967
[2023-10-04 20:27:42] iter = 14220, loss = 1.6743
[2023-10-04 20:27:43] iter = 14230, loss = 1.7753
[2023-10-04 20:27:44] iter = 14240, loss = 1.4747
[2023-10-04 20:27:45] iter = 14250, loss = 1.4476
[2023-10-04 20:27:46] iter = 14260, loss = 1.4015
[2023-10-04 20:27:47] iter = 14270, loss = 1.4827
[2023-10-04 20:27:48] iter = 14280, loss = 1.4739
[2023-10-04 20:27:48] iter = 14290, loss = 1.3803
[2023-10-04 20:27:49] iter = 14300, loss = 1.5752
[2023-10-04 20:27:50] iter = 14310, loss = 1.4287
[2023-10-04 20:27:51] iter = 14320, loss = 1.4525
[2023-10-04 20:27:52] iter = 14330, loss = 1.6447
[2023-10-04 20:27:53] iter = 14340, loss = 1.4028
[2023-10-04 20:27:54] iter = 14350, loss = 1.5780
[2023-10-04 20:27:55] iter = 14360, loss = 1.4615
[2023-10-04 20:27:56] iter = 14370, loss = 1.4572
[2023-10-04 20:27:57] iter = 14380, loss = 1.5037
[2023-10-04 20:27:58] iter = 14390, loss = 1.5867
[2023-10-04 20:27:58] iter = 14400, loss = 1.5376
[2023-10-04 20:27:59] iter = 14410, loss = 1.5569
[2023-10-04 20:28:00] iter = 14420, loss = 1.4781
[2023-10-04 20:28:01] iter = 14430, loss = 1.5075
[2023-10-04 20:28:02] iter = 14440, loss = 1.5577
[2023-10-04 20:28:03] iter = 14450, loss = 1.4864
[2023-10-04 20:28:04] iter = 14460, loss = 1.4301
[2023-10-04 20:28:05] iter = 14470, loss = 1.5648
[2023-10-04 20:28:06] iter = 14480, loss = 1.5411
[2023-10-04 20:28:07] iter = 14490, loss = 1.4874
[2023-10-04 20:28:08] iter = 14500, loss = 1.5277
[2023-10-04 20:28:09] iter = 14510, loss = 1.5748
[2023-10-04 20:28:10] iter = 14520, loss = 1.4684
[2023-10-04 20:28:11] iter = 14530, loss = 1.6070
[2023-10-04 20:28:12] iter = 14540, loss = 1.4589
[2023-10-04 20:28:13] iter = 14550, loss = 1.5387
[2023-10-04 20:28:13] iter = 14560, loss = 1.4862
[2023-10-04 20:28:14] iter = 14570, loss = 1.3846
[2023-10-04 20:28:15] iter = 14580, loss = 1.4064
[2023-10-04 20:28:16] iter = 14590, loss = 1.6240
[2023-10-04 20:28:17] iter = 14600, loss = 1.7078
[2023-10-04 20:28:18] iter = 14610, loss = 1.4885
[2023-10-04 20:28:19] iter = 14620, loss = 1.4743
[2023-10-04 20:28:20] iter = 14630, loss = 1.4848
[2023-10-04 20:28:21] iter = 14640, loss = 1.5034
[2023-10-04 20:28:22] iter = 14650, loss = 1.5640
[2023-10-04 20:28:23] iter = 14660, loss = 1.5384
[2023-10-04 20:28:23] iter = 14670, loss = 1.4354
[2023-10-04 20:28:24] iter = 14680, loss = 1.5206
[2023-10-04 20:28:25] iter = 14690, loss = 1.5987
[2023-10-04 20:28:26] iter = 14700, loss = 1.6184
[2023-10-04 20:28:27] iter = 14710, loss = 1.5269
[2023-10-04 20:28:28] iter = 14720, loss = 1.5085
[2023-10-04 20:28:29] iter = 14730, loss = 1.5580
[2023-10-04 20:28:30] iter = 14740, loss = 1.6473
[2023-10-04 20:28:31] iter = 14750, loss = 1.3868
[2023-10-04 20:28:32] iter = 14760, loss = 1.3929
[2023-10-04 20:28:33] iter = 14770, loss = 1.4163
[2023-10-04 20:28:33] iter = 14780, loss = 1.5357
[2023-10-04 20:28:34] iter = 14790, loss = 1.5690
[2023-10-04 20:28:35] iter = 14800, loss = 1.5336
[2023-10-04 20:28:36] iter = 14810, loss = 1.4494
[2023-10-04 20:28:37] iter = 14820, loss = 1.4610
[2023-10-04 20:28:38] iter = 14830, loss = 1.5769
[2023-10-04 20:28:39] iter = 14840, loss = 1.4087
[2023-10-04 20:28:40] iter = 14850, loss = 1.4678
[2023-10-04 20:28:41] iter = 14860, loss = 1.4689
[2023-10-04 20:28:42] iter = 14870, loss = 1.4481
[2023-10-04 20:28:42] iter = 14880, loss = 1.5731
[2023-10-04 20:28:43] iter = 14890, loss = 1.5631
[2023-10-04 20:28:44] iter = 14900, loss = 1.3217
[2023-10-04 20:28:45] iter = 14910, loss = 1.5137
[2023-10-04 20:28:46] iter = 14920, loss = 1.5711
[2023-10-04 20:28:47] iter = 14930, loss = 1.5587
[2023-10-04 20:28:48] iter = 14940, loss = 1.5611
[2023-10-04 20:28:49] iter = 14950, loss = 1.5216
[2023-10-04 20:28:49] iter = 14960, loss = 1.5417
[2023-10-04 20:28:50] iter = 14970, loss = 1.4963
[2023-10-04 20:28:51] iter = 14980, loss = 1.4005
[2023-10-04 20:28:52] iter = 14990, loss = 1.5210
[2023-10-04 20:28:53] iter = 15000, loss = 1.5335
[2023-10-04 20:28:54] iter = 15010, loss = 1.6070
[2023-10-04 20:28:55] iter = 15020, loss = 1.4797
[2023-10-04 20:28:56] iter = 15030, loss = 1.5456
[2023-10-04 20:28:57] iter = 15040, loss = 1.4580
[2023-10-04 20:28:58] iter = 15050, loss = 1.5362
[2023-10-04 20:28:59] iter = 15060, loss = 1.3932
[2023-10-04 20:29:00] iter = 15070, loss = 1.5667
[2023-10-04 20:29:01] iter = 15080, loss = 1.5169
[2023-10-04 20:29:01] iter = 15090, loss = 1.4927
[2023-10-04 20:29:02] iter = 15100, loss = 1.7266
[2023-10-04 20:29:03] iter = 15110, loss = 1.5457
[2023-10-04 20:29:04] iter = 15120, loss = 1.5290
[2023-10-04 20:29:05] iter = 15130, loss = 1.5667
[2023-10-04 20:29:06] iter = 15140, loss = 1.5313
[2023-10-04 20:29:07] iter = 15150, loss = 1.4997
[2023-10-04 20:29:08] iter = 15160, loss = 1.4090
[2023-10-04 20:29:09] iter = 15170, loss = 1.4687
[2023-10-04 20:29:10] iter = 15180, loss = 1.4321
[2023-10-04 20:29:10] iter = 15190, loss = 1.3871
[2023-10-04 20:29:11] iter = 15200, loss = 1.4309
[2023-10-04 20:29:12] iter = 15210, loss = 1.4741
[2023-10-04 20:29:13] iter = 15220, loss = 1.6244
[2023-10-04 20:29:14] iter = 15230, loss = 1.4242
[2023-10-04 20:29:15] iter = 15240, loss = 1.3795
[2023-10-04 20:29:16] iter = 15250, loss = 1.4228
[2023-10-04 20:29:17] iter = 15260, loss = 1.5650
[2023-10-04 20:29:18] iter = 15270, loss = 1.3670
[2023-10-04 20:29:18] iter = 15280, loss = 1.4043
[2023-10-04 20:29:19] iter = 15290, loss = 1.5459
[2023-10-04 20:29:20] iter = 15300, loss = 1.4180
[2023-10-04 20:29:21] iter = 15310, loss = 1.6038
[2023-10-04 20:29:22] iter = 15320, loss = 1.4865
[2023-10-04 20:29:23] iter = 15330, loss = 1.5462
[2023-10-04 20:29:24] iter = 15340, loss = 1.4939
[2023-10-04 20:29:25] iter = 15350, loss = 1.4361
[2023-10-04 20:29:26] iter = 15360, loss = 1.4993
[2023-10-04 20:29:27] iter = 15370, loss = 1.6183
[2023-10-04 20:29:27] iter = 15380, loss = 1.4381
[2023-10-04 20:29:28] iter = 15390, loss = 1.6571
[2023-10-04 20:29:29] iter = 15400, loss = 1.4732
[2023-10-04 20:29:30] iter = 15410, loss = 1.3945
[2023-10-04 20:29:31] iter = 15420, loss = 1.5771
[2023-10-04 20:29:32] iter = 15430, loss = 1.4776
[2023-10-04 20:29:33] iter = 15440, loss = 1.4781
[2023-10-04 20:29:34] iter = 15450, loss = 1.4836
[2023-10-04 20:29:35] iter = 15460, loss = 1.6437
[2023-10-04 20:29:36] iter = 15470, loss = 1.5260
[2023-10-04 20:29:37] iter = 15480, loss = 1.5920
[2023-10-04 20:29:38] iter = 15490, loss = 1.6789
[2023-10-04 20:29:39] iter = 15500, loss = 1.5242
[2023-10-04 20:29:39] iter = 15510, loss = 1.4269
[2023-10-04 20:29:40] iter = 15520, loss = 1.5813
[2023-10-04 20:29:41] iter = 15530, loss = 1.4236
[2023-10-04 20:29:42] iter = 15540, loss = 1.5494
[2023-10-04 20:29:43] iter = 15550, loss = 1.5459
[2023-10-04 20:29:44] iter = 15560, loss = 1.4217
[2023-10-04 20:29:45] iter = 15570, loss = 1.5285
[2023-10-04 20:29:46] iter = 15580, loss = 1.6797
[2023-10-04 20:29:47] iter = 15590, loss = 1.4880
[2023-10-04 20:29:48] iter = 15600, loss = 1.6124
[2023-10-04 20:29:49] iter = 15610, loss = 1.5165
[2023-10-04 20:29:50] iter = 15620, loss = 1.4316
[2023-10-04 20:29:51] iter = 15630, loss = 1.5190
[2023-10-04 20:29:51] iter = 15640, loss = 1.5007
[2023-10-04 20:29:52] iter = 15650, loss = 1.3855
[2023-10-04 20:29:53] iter = 15660, loss = 1.6101
[2023-10-04 20:29:54] iter = 15670, loss = 1.3645
[2023-10-04 20:29:55] iter = 15680, loss = 1.5379
[2023-10-04 20:29:56] iter = 15690, loss = 1.5427
[2023-10-04 20:29:57] iter = 15700, loss = 1.5438
[2023-10-04 20:29:58] iter = 15710, loss = 1.4164
[2023-10-04 20:29:59] iter = 15720, loss = 1.5214
[2023-10-04 20:30:00] iter = 15730, loss = 1.5423
[2023-10-04 20:30:01] iter = 15740, loss = 1.4173
[2023-10-04 20:30:02] iter = 15750, loss = 1.5260
[2023-10-04 20:30:03] iter = 15760, loss = 1.6223
[2023-10-04 20:30:04] iter = 15770, loss = 1.4546
[2023-10-04 20:30:05] iter = 15780, loss = 1.5947
[2023-10-04 20:30:05] iter = 15790, loss = 1.4325
[2023-10-04 20:30:06] iter = 15800, loss = 1.3824
[2023-10-04 20:30:07] iter = 15810, loss = 1.5580
[2023-10-04 20:30:08] iter = 15820, loss = 1.4509
[2023-10-04 20:30:09] iter = 15830, loss = 1.5581
[2023-10-04 20:30:10] iter = 15840, loss = 1.4637
[2023-10-04 20:30:11] iter = 15850, loss = 1.6615
[2023-10-04 20:30:12] iter = 15860, loss = 1.6097
[2023-10-04 20:30:13] iter = 15870, loss = 1.4792
[2023-10-04 20:30:14] iter = 15880, loss = 1.4170
[2023-10-04 20:30:15] iter = 15890, loss = 1.5022
[2023-10-04 20:30:16] iter = 15900, loss = 1.4593
[2023-10-04 20:30:17] iter = 15910, loss = 1.5015
[2023-10-04 20:30:18] iter = 15920, loss = 1.3691
[2023-10-04 20:30:18] iter = 15930, loss = 1.6070
[2023-10-04 20:30:19] iter = 15940, loss = 1.5432
[2023-10-04 20:30:20] iter = 15950, loss = 1.4126
[2023-10-04 20:30:21] iter = 15960, loss = 1.4326
[2023-10-04 20:30:22] iter = 15970, loss = 1.6084
[2023-10-04 20:30:23] iter = 15980, loss = 1.5891
[2023-10-04 20:30:24] iter = 15990, loss = 1.5414
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 25186}
[2023-10-04 20:30:49] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.008715 train acc = 1.0000, test acc = 0.6235
[2023-10-04 20:31:14] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010529 train acc = 1.0000, test acc = 0.6226
[2023-10-04 20:31:38] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.015702 train acc = 0.9980, test acc = 0.6207
[2023-10-04 20:32:02] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.011550 train acc = 0.9980, test acc = 0.6221
[2023-10-04 20:32:27] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.013856 train acc = 0.9980, test acc = 0.6226
[2023-10-04 20:32:51] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.019694 train acc = 1.0000, test acc = 0.6259
[2023-10-04 20:33:15] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.019547 train acc = 0.9960, test acc = 0.6223
[2023-10-04 20:33:40] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002129 train acc = 1.0000, test acc = 0.6272
[2023-10-04 20:34:04] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.012379 train acc = 1.0000, test acc = 0.6277
[2023-10-04 20:34:28] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.011951 train acc = 1.0000, test acc = 0.6251
[2023-10-04 20:34:52] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004412 train acc = 1.0000, test acc = 0.6327
[2023-10-04 20:35:17] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.010308 train acc = 1.0000, test acc = 0.6264
[2023-10-04 20:35:41] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.021789 train acc = 0.9980, test acc = 0.6283
[2023-10-04 20:36:05] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.006146 train acc = 1.0000, test acc = 0.6236
[2023-10-04 20:36:30] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.011477 train acc = 1.0000, test acc = 0.6241
[2023-10-04 20:36:54] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002942 train acc = 1.0000, test acc = 0.6261
[2023-10-04 20:37:18] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.005371 train acc = 1.0000, test acc = 0.6181
[2023-10-04 20:37:43] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.017797 train acc = 1.0000, test acc = 0.6225
[2023-10-04 20:38:07] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.017394 train acc = 1.0000, test acc = 0.6244
[2023-10-04 20:38:31] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.020450 train acc = 0.9980, test acc = 0.6301
Evaluate 20 random ConvNet, mean = 0.6248 std = 0.0033
-------------------------
[2023-10-04 20:38:32] iter = 16000, loss = 1.5010
[2023-10-04 20:38:32] iter = 16010, loss = 1.3944
[2023-10-04 20:38:34] iter = 16020, loss = 1.3865
[2023-10-04 20:38:34] iter = 16030, loss = 1.3871
[2023-10-04 20:38:35] iter = 16040, loss = 1.4755
[2023-10-04 20:38:36] iter = 16050, loss = 1.3972
[2023-10-04 20:38:37] iter = 16060, loss = 1.4912
[2023-10-04 20:38:38] iter = 16070, loss = 1.4766
[2023-10-04 20:38:39] iter = 16080, loss = 1.4307
[2023-10-04 20:38:40] iter = 16090, loss = 1.3597
[2023-10-04 20:38:41] iter = 16100, loss = 1.5669
[2023-10-04 20:38:42] iter = 16110, loss = 1.4962
[2023-10-04 20:38:43] iter = 16120, loss = 1.4527
[2023-10-04 20:38:44] iter = 16130, loss = 1.5422
[2023-10-04 20:38:44] iter = 16140, loss = 1.4689
[2023-10-04 20:38:45] iter = 16150, loss = 1.3381
[2023-10-04 20:38:46] iter = 16160, loss = 1.3965
[2023-10-04 20:38:47] iter = 16170, loss = 1.5284
[2023-10-04 20:38:48] iter = 16180, loss = 1.5179
[2023-10-04 20:38:49] iter = 16190, loss = 1.4162
[2023-10-04 20:38:50] iter = 16200, loss = 1.5242
[2023-10-04 20:38:51] iter = 16210, loss = 1.5097
[2023-10-04 20:38:52] iter = 16220, loss = 1.5413
[2023-10-04 20:38:53] iter = 16230, loss = 1.4848
[2023-10-04 20:38:54] iter = 16240, loss = 1.4907
[2023-10-04 20:38:55] iter = 16250, loss = 1.5915
[2023-10-04 20:38:55] iter = 16260, loss = 1.5127
[2023-10-04 20:38:56] iter = 16270, loss = 1.5014
[2023-10-04 20:38:57] iter = 16280, loss = 1.4898
[2023-10-04 20:38:58] iter = 16290, loss = 1.4722
[2023-10-04 20:38:59] iter = 16300, loss = 1.4191
[2023-10-04 20:39:00] iter = 16310, loss = 1.6376
[2023-10-04 20:39:01] iter = 16320, loss = 1.4284
[2023-10-04 20:39:02] iter = 16330, loss = 1.5942
[2023-10-04 20:39:03] iter = 16340, loss = 1.4568
[2023-10-04 20:39:04] iter = 16350, loss = 1.2841
[2023-10-04 20:39:04] iter = 16360, loss = 1.4875
[2023-10-04 20:39:05] iter = 16370, loss = 1.5746
[2023-10-04 20:39:06] iter = 16380, loss = 1.4461
[2023-10-04 20:39:07] iter = 16390, loss = 1.5136
[2023-10-04 20:39:08] iter = 16400, loss = 1.5251
[2023-10-04 20:39:09] iter = 16410, loss = 1.5050
[2023-10-04 20:39:10] iter = 16420, loss = 1.4928
[2023-10-04 20:39:11] iter = 16430, loss = 1.5001
[2023-10-04 20:39:12] iter = 16440, loss = 1.6485
[2023-10-04 20:39:13] iter = 16450, loss = 1.4679
[2023-10-04 20:39:13] iter = 16460, loss = 1.5265
[2023-10-04 20:39:14] iter = 16470, loss = 1.5367
[2023-10-04 20:39:15] iter = 16480, loss = 1.4884
[2023-10-04 20:39:16] iter = 16490, loss = 1.3260
[2023-10-04 20:39:17] iter = 16500, loss = 1.5251
[2023-10-04 20:39:18] iter = 16510, loss = 1.4012
[2023-10-04 20:39:19] iter = 16520, loss = 1.5078
[2023-10-04 20:39:20] iter = 16530, loss = 1.4552
[2023-10-04 20:39:21] iter = 16540, loss = 1.3653
[2023-10-04 20:39:22] iter = 16550, loss = 1.3967
[2023-10-04 20:39:23] iter = 16560, loss = 1.6077
[2023-10-04 20:39:24] iter = 16570, loss = 1.4846
[2023-10-04 20:39:24] iter = 16580, loss = 1.4674
[2023-10-04 20:39:25] iter = 16590, loss = 1.4981
[2023-10-04 20:39:26] iter = 16600, loss = 1.4466
[2023-10-04 20:39:27] iter = 16610, loss = 1.4046
[2023-10-04 20:39:28] iter = 16620, loss = 1.5302
[2023-10-04 20:39:29] iter = 16630, loss = 1.4823
[2023-10-04 20:39:30] iter = 16640, loss = 1.3771
[2023-10-04 20:39:31] iter = 16650, loss = 1.3867
[2023-10-04 20:39:32] iter = 16660, loss = 1.4816
[2023-10-04 20:39:32] iter = 16670, loss = 1.5528
[2023-10-04 20:39:33] iter = 16680, loss = 1.5846
[2023-10-04 20:39:34] iter = 16690, loss = 1.3739
[2023-10-04 20:39:35] iter = 16700, loss = 1.4462
[2023-10-04 20:39:36] iter = 16710, loss = 1.4618
[2023-10-04 20:39:37] iter = 16720, loss = 1.6055
[2023-10-04 20:39:38] iter = 16730, loss = 1.4729
[2023-10-04 20:39:39] iter = 16740, loss = 1.4791
[2023-10-04 20:39:40] iter = 16750, loss = 1.4709
[2023-10-04 20:39:41] iter = 16760, loss = 1.5089
[2023-10-04 20:39:42] iter = 16770, loss = 1.5501
[2023-10-04 20:39:43] iter = 16780, loss = 1.6311
[2023-10-04 20:39:44] iter = 16790, loss = 1.5246
[2023-10-04 20:39:45] iter = 16800, loss = 1.4787
[2023-10-04 20:39:46] iter = 16810, loss = 1.3766
[2023-10-04 20:39:47] iter = 16820, loss = 1.4262
[2023-10-04 20:39:47] iter = 16830, loss = 1.6383
[2023-10-04 20:39:48] iter = 16840, loss = 1.4831
[2023-10-04 20:39:49] iter = 16850, loss = 1.3805
[2023-10-04 20:39:50] iter = 16860, loss = 1.5848
[2023-10-04 20:39:51] iter = 16870, loss = 1.5629
[2023-10-04 20:39:52] iter = 16880, loss = 1.7332
[2023-10-04 20:39:53] iter = 16890, loss = 1.4908
[2023-10-04 20:39:54] iter = 16900, loss = 1.3230
[2023-10-04 20:39:55] iter = 16910, loss = 1.5124
[2023-10-04 20:39:56] iter = 16920, loss = 1.4099
[2023-10-04 20:39:57] iter = 16930, loss = 1.4782
[2023-10-04 20:39:58] iter = 16940, loss = 1.4419
[2023-10-04 20:39:59] iter = 16950, loss = 1.4020
[2023-10-04 20:39:59] iter = 16960, loss = 1.5080
[2023-10-04 20:40:01] iter = 16970, loss = 1.6413
[2023-10-04 20:40:01] iter = 16980, loss = 1.4862
[2023-10-04 20:40:02] iter = 16990, loss = 1.4884
[2023-10-04 20:40:03] iter = 17000, loss = 1.4667
[2023-10-04 20:40:04] iter = 17010, loss = 1.4185
[2023-10-04 20:40:05] iter = 17020, loss = 1.5062
[2023-10-04 20:40:06] iter = 17030, loss = 1.5296
[2023-10-04 20:40:07] iter = 17040, loss = 1.5840
[2023-10-04 20:40:08] iter = 17050, loss = 1.4473
[2023-10-04 20:40:09] iter = 17060, loss = 1.5082
[2023-10-04 20:40:10] iter = 17070, loss = 1.5091
[2023-10-04 20:40:11] iter = 17080, loss = 1.4306
[2023-10-04 20:40:12] iter = 17090, loss = 1.4707
[2023-10-04 20:40:13] iter = 17100, loss = 1.4640
[2023-10-04 20:40:13] iter = 17110, loss = 1.3390
[2023-10-04 20:40:14] iter = 17120, loss = 1.5026
[2023-10-04 20:40:15] iter = 17130, loss = 1.6147
[2023-10-04 20:40:16] iter = 17140, loss = 1.3932
[2023-10-04 20:40:17] iter = 17150, loss = 1.4497
[2023-10-04 20:40:18] iter = 17160, loss = 1.5299
[2023-10-04 20:40:19] iter = 17170, loss = 1.4495
[2023-10-04 20:40:20] iter = 17180, loss = 1.5078
[2023-10-04 20:40:21] iter = 17190, loss = 1.4743
[2023-10-04 20:40:22] iter = 17200, loss = 1.4646
[2023-10-04 20:40:23] iter = 17210, loss = 1.3752
[2023-10-04 20:40:24] iter = 17220, loss = 1.4818
[2023-10-04 20:40:24] iter = 17230, loss = 1.3534
[2023-10-04 20:40:25] iter = 17240, loss = 1.4605
[2023-10-04 20:40:26] iter = 17250, loss = 1.5342
[2023-10-04 20:40:27] iter = 17260, loss = 1.4073
[2023-10-04 20:40:28] iter = 17270, loss = 1.4691
[2023-10-04 20:40:29] iter = 17280, loss = 1.5571
[2023-10-04 20:40:30] iter = 17290, loss = 1.5978
[2023-10-04 20:40:31] iter = 17300, loss = 1.5813
[2023-10-04 20:40:32] iter = 17310, loss = 1.5741
[2023-10-04 20:40:33] iter = 17320, loss = 1.3998
[2023-10-04 20:40:34] iter = 17330, loss = 1.5325
[2023-10-04 20:40:35] iter = 17340, loss = 1.3813
[2023-10-04 20:40:36] iter = 17350, loss = 1.4407
[2023-10-04 20:40:37] iter = 17360, loss = 1.5057
[2023-10-04 20:40:37] iter = 17370, loss = 1.5756
[2023-10-04 20:40:38] iter = 17380, loss = 1.3620
[2023-10-04 20:40:39] iter = 17390, loss = 1.5549
[2023-10-04 20:40:40] iter = 17400, loss = 1.2970
[2023-10-04 20:40:41] iter = 17410, loss = 1.6132
[2023-10-04 20:40:42] iter = 17420, loss = 1.5064
[2023-10-04 20:40:43] iter = 17430, loss = 1.4828
[2023-10-04 20:40:44] iter = 17440, loss = 1.7089
[2023-10-04 20:40:45] iter = 17450, loss = 1.3815
[2023-10-04 20:40:46] iter = 17460, loss = 1.4765
[2023-10-04 20:40:47] iter = 17470, loss = 1.3706
[2023-10-04 20:40:47] iter = 17480, loss = 1.3873
[2023-10-04 20:40:48] iter = 17490, loss = 1.4312
[2023-10-04 20:40:49] iter = 17500, loss = 1.4846
[2023-10-04 20:40:50] iter = 17510, loss = 1.4722
[2023-10-04 20:40:51] iter = 17520, loss = 1.4945
[2023-10-04 20:40:52] iter = 17530, loss = 1.3958
[2023-10-04 20:40:53] iter = 17540, loss = 1.3786
[2023-10-04 20:40:54] iter = 17550, loss = 1.3735
[2023-10-04 20:40:55] iter = 17560, loss = 1.7230
[2023-10-04 20:40:56] iter = 17570, loss = 1.3914
[2023-10-04 20:40:56] iter = 17580, loss = 1.2684
[2023-10-04 20:40:57] iter = 17590, loss = 1.6180
[2023-10-04 20:40:58] iter = 17600, loss = 1.4272
[2023-10-04 20:40:59] iter = 17610, loss = 1.4066
[2023-10-04 20:41:00] iter = 17620, loss = 1.4547
[2023-10-04 20:41:01] iter = 17630, loss = 1.5456
[2023-10-04 20:41:02] iter = 17640, loss = 1.5347
[2023-10-04 20:41:03] iter = 17650, loss = 1.4243
[2023-10-04 20:41:04] iter = 17660, loss = 1.4348
[2023-10-04 20:41:04] iter = 17670, loss = 1.3586
[2023-10-04 20:41:05] iter = 17680, loss = 1.4677
[2023-10-04 20:41:06] iter = 17690, loss = 1.4371
[2023-10-04 20:41:07] iter = 17700, loss = 1.5120
[2023-10-04 20:41:08] iter = 17710, loss = 1.5281
[2023-10-04 20:41:09] iter = 17720, loss = 1.4219
[2023-10-04 20:41:10] iter = 17730, loss = 1.4469
[2023-10-04 20:41:11] iter = 17740, loss = 1.4339
[2023-10-04 20:41:11] iter = 17750, loss = 1.5086
[2023-10-04 20:41:12] iter = 17760, loss = 1.5192
[2023-10-04 20:41:13] iter = 17770, loss = 1.4668
[2023-10-04 20:41:14] iter = 17780, loss = 1.5058
[2023-10-04 20:41:15] iter = 17790, loss = 1.5258
[2023-10-04 20:41:16] iter = 17800, loss = 1.4344
[2023-10-04 20:41:17] iter = 17810, loss = 1.7379
[2023-10-04 20:41:18] iter = 17820, loss = 1.4760
[2023-10-04 20:41:19] iter = 17830, loss = 1.3750
[2023-10-04 20:41:20] iter = 17840, loss = 1.5778
[2023-10-04 20:41:21] iter = 17850, loss = 1.3786
[2023-10-04 20:41:22] iter = 17860, loss = 1.5025
[2023-10-04 20:41:23] iter = 17870, loss = 1.4163
[2023-10-04 20:41:23] iter = 17880, loss = 1.3535
[2023-10-04 20:41:24] iter = 17890, loss = 1.4824
[2023-10-04 20:41:25] iter = 17900, loss = 1.5577
[2023-10-04 20:41:26] iter = 17910, loss = 1.4553
[2023-10-04 20:41:27] iter = 17920, loss = 1.4653
[2023-10-04 20:41:28] iter = 17930, loss = 1.4409
[2023-10-04 20:41:29] iter = 17940, loss = 1.4513
[2023-10-04 20:41:30] iter = 17950, loss = 1.6661
[2023-10-04 20:41:31] iter = 17960, loss = 1.4202
[2023-10-04 20:41:32] iter = 17970, loss = 1.4917
[2023-10-04 20:41:33] iter = 17980, loss = 1.5681
[2023-10-04 20:41:34] iter = 17990, loss = 1.4913
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 95060}
[2023-10-04 20:41:59] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.012809 train acc = 1.0000, test acc = 0.6246
[2023-10-04 20:42:24] Evaluate_01: epoch = 1000 train time = 23 s train loss = 0.014492 train acc = 1.0000, test acc = 0.6245
[2023-10-04 20:42:49] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003603 train acc = 1.0000, test acc = 0.6274
[2023-10-04 20:43:13] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001849 train acc = 1.0000, test acc = 0.6273
[2023-10-04 20:43:38] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.007569 train acc = 1.0000, test acc = 0.6283
[2023-10-04 20:44:02] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.015085 train acc = 1.0000, test acc = 0.6289
[2023-10-04 20:44:27] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004139 train acc = 1.0000, test acc = 0.6171
[2023-10-04 20:44:51] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.017308 train acc = 1.0000, test acc = 0.6241
[2023-10-04 20:45:15] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002667 train acc = 1.0000, test acc = 0.6296
[2023-10-04 20:45:40] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.027144 train acc = 1.0000, test acc = 0.6163
[2023-10-04 20:46:05] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003586 train acc = 1.0000, test acc = 0.6266
[2023-10-04 20:46:29] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.012053 train acc = 1.0000, test acc = 0.6300
[2023-10-04 20:46:53] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.018155 train acc = 1.0000, test acc = 0.6292
[2023-10-04 20:47:18] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004905 train acc = 1.0000, test acc = 0.6256
[2023-10-04 20:47:42] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.017205 train acc = 0.9980, test acc = 0.6251
[2023-10-04 20:48:07] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.017271 train acc = 1.0000, test acc = 0.6312
[2023-10-04 20:48:31] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.012738 train acc = 1.0000, test acc = 0.6271
[2023-10-04 20:48:56] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.010188 train acc = 1.0000, test acc = 0.6275
[2023-10-04 20:49:20] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.005118 train acc = 1.0000, test acc = 0.6229
[2023-10-04 20:49:45] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.013566 train acc = 1.0000, test acc = 0.6350
Evaluate 20 random ConvNet, mean = 0.6264 std = 0.0042
-------------------------
[2023-10-04 20:49:45] iter = 18000, loss = 1.5731
[2023-10-04 20:49:46] iter = 18010, loss = 1.5642
[2023-10-04 20:49:47] iter = 18020, loss = 1.3878
[2023-10-04 20:49:48] iter = 18030, loss = 1.3357
[2023-10-04 20:49:49] iter = 18040, loss = 1.5376
[2023-10-04 20:49:50] iter = 18050, loss = 1.5908
[2023-10-04 20:49:51] iter = 18060, loss = 1.7418
[2023-10-04 20:49:52] iter = 18070, loss = 1.3748
[2023-10-04 20:49:53] iter = 18080, loss = 1.5278
[2023-10-04 20:49:53] iter = 18090, loss = 1.4192
[2023-10-04 20:49:54] iter = 18100, loss = 1.5675
[2023-10-04 20:49:55] iter = 18110, loss = 1.4478
[2023-10-04 20:49:56] iter = 18120, loss = 1.4905
[2023-10-04 20:49:57] iter = 18130, loss = 1.4171
[2023-10-04 20:49:58] iter = 18140, loss = 1.5613
[2023-10-04 20:49:59] iter = 18150, loss = 1.4629
[2023-10-04 20:50:00] iter = 18160, loss = 1.4389
[2023-10-04 20:50:01] iter = 18170, loss = 1.3090
[2023-10-04 20:50:02] iter = 18180, loss = 1.4269
[2023-10-04 20:50:03] iter = 18190, loss = 1.4591
[2023-10-04 20:50:04] iter = 18200, loss = 1.4206
[2023-10-04 20:50:04] iter = 18210, loss = 1.3796
[2023-10-04 20:50:05] iter = 18220, loss = 1.4652
[2023-10-04 20:50:06] iter = 18230, loss = 1.4053
[2023-10-04 20:50:07] iter = 18240, loss = 1.5193
[2023-10-04 20:50:08] iter = 18250, loss = 1.4413
[2023-10-04 20:50:09] iter = 18260, loss = 1.4357
[2023-10-04 20:50:10] iter = 18270, loss = 1.5151
[2023-10-04 20:50:11] iter = 18280, loss = 1.3970
[2023-10-04 20:50:12] iter = 18290, loss = 1.4401
[2023-10-04 20:50:13] iter = 18300, loss = 1.4815
[2023-10-04 20:50:14] iter = 18310, loss = 1.5074
[2023-10-04 20:50:15] iter = 18320, loss = 1.4643
[2023-10-04 20:50:16] iter = 18330, loss = 1.4687
[2023-10-04 20:50:17] iter = 18340, loss = 1.4887
[2023-10-04 20:50:18] iter = 18350, loss = 1.5786
[2023-10-04 20:50:18] iter = 18360, loss = 1.5003
[2023-10-04 20:50:19] iter = 18370, loss = 1.5321
[2023-10-04 20:50:20] iter = 18380, loss = 1.5617
[2023-10-04 20:50:21] iter = 18390, loss = 1.4617
[2023-10-04 20:50:22] iter = 18400, loss = 1.3940
[2023-10-04 20:50:23] iter = 18410, loss = 1.3783
[2023-10-04 20:50:24] iter = 18420, loss = 1.4203
[2023-10-04 20:50:25] iter = 18430, loss = 1.5090
[2023-10-04 20:50:26] iter = 18440, loss = 1.4189
[2023-10-04 20:50:27] iter = 18450, loss = 1.3328
[2023-10-04 20:50:28] iter = 18460, loss = 1.4912
[2023-10-04 20:50:29] iter = 18470, loss = 1.4063
[2023-10-04 20:50:30] iter = 18480, loss = 1.5082
[2023-10-04 20:50:31] iter = 18490, loss = 1.3873
[2023-10-04 20:50:31] iter = 18500, loss = 1.4982
[2023-10-04 20:50:32] iter = 18510, loss = 1.3349
[2023-10-04 20:50:33] iter = 18520, loss = 1.4980
[2023-10-04 20:50:34] iter = 18530, loss = 1.4110
[2023-10-04 20:50:35] iter = 18540, loss = 1.3844
[2023-10-04 20:50:36] iter = 18550, loss = 1.3656
[2023-10-04 20:50:37] iter = 18560, loss = 1.3627
[2023-10-04 20:50:38] iter = 18570, loss = 1.5778
[2023-10-04 20:50:39] iter = 18580, loss = 1.8433
[2023-10-04 20:50:40] iter = 18590, loss = 1.4173
[2023-10-04 20:50:41] iter = 18600, loss = 1.6211
[2023-10-04 20:50:42] iter = 18610, loss = 1.4224
[2023-10-04 20:50:43] iter = 18620, loss = 1.3771
[2023-10-04 20:50:44] iter = 18630, loss = 1.5720
[2023-10-04 20:50:45] iter = 18640, loss = 1.5381
[2023-10-04 20:50:45] iter = 18650, loss = 1.3529
[2023-10-04 20:50:46] iter = 18660, loss = 1.5034
[2023-10-04 20:50:47] iter = 18670, loss = 1.5340
[2023-10-04 20:50:48] iter = 18680, loss = 1.5840
[2023-10-04 20:50:49] iter = 18690, loss = 1.3163
[2023-10-04 20:50:50] iter = 18700, loss = 1.3426
[2023-10-04 20:50:51] iter = 18710, loss = 1.3091
[2023-10-04 20:50:52] iter = 18720, loss = 1.3595
[2023-10-04 20:50:53] iter = 18730, loss = 1.4617
[2023-10-04 20:50:54] iter = 18740, loss = 1.4691
[2023-10-04 20:50:55] iter = 18750, loss = 1.4625
[2023-10-04 20:50:56] iter = 18760, loss = 1.4715
[2023-10-04 20:50:56] iter = 18770, loss = 1.4604
[2023-10-04 20:50:57] iter = 18780, loss = 1.5383
[2023-10-04 20:50:58] iter = 18790, loss = 1.4841
[2023-10-04 20:50:59] iter = 18800, loss = 1.4568
[2023-10-04 20:51:00] iter = 18810, loss = 1.6401
[2023-10-04 20:51:01] iter = 18820, loss = 1.4495
[2023-10-04 20:51:02] iter = 18830, loss = 1.5348
[2023-10-04 20:51:03] iter = 18840, loss = 1.6029
[2023-10-04 20:51:04] iter = 18850, loss = 1.4559
[2023-10-04 20:51:05] iter = 18860, loss = 1.5324
[2023-10-04 20:51:05] iter = 18870, loss = 1.4564
[2023-10-04 20:51:06] iter = 18880, loss = 1.4088
[2023-10-04 20:51:07] iter = 18890, loss = 1.4576
[2023-10-04 20:51:08] iter = 18900, loss = 1.5249
[2023-10-04 20:51:09] iter = 18910, loss = 1.6900
[2023-10-04 20:51:10] iter = 18920, loss = 1.3968
[2023-10-04 20:51:11] iter = 18930, loss = 1.5205
[2023-10-04 20:51:12] iter = 18940, loss = 1.3574
[2023-10-04 20:51:13] iter = 18950, loss = 1.4550
[2023-10-04 20:51:14] iter = 18960, loss = 1.4107
[2023-10-04 20:51:15] iter = 18970, loss = 1.4005
[2023-10-04 20:51:16] iter = 18980, loss = 1.4488
[2023-10-04 20:51:17] iter = 18990, loss = 1.5258
[2023-10-04 20:51:17] iter = 19000, loss = 1.4596
[2023-10-04 20:51:18] iter = 19010, loss = 1.4078
[2023-10-04 20:51:19] iter = 19020, loss = 1.3221
[2023-10-04 20:51:20] iter = 19030, loss = 1.4218
[2023-10-04 20:51:21] iter = 19040, loss = 1.4473
[2023-10-04 20:51:22] iter = 19050, loss = 1.4297
[2023-10-04 20:51:23] iter = 19060, loss = 1.6222
[2023-10-04 20:51:24] iter = 19070, loss = 1.6215
[2023-10-04 20:51:25] iter = 19080, loss = 1.3468
[2023-10-04 20:51:26] iter = 19090, loss = 1.4815
[2023-10-04 20:51:27] iter = 19100, loss = 1.5772
[2023-10-04 20:51:28] iter = 19110, loss = 1.4614
[2023-10-04 20:51:29] iter = 19120, loss = 1.4023
[2023-10-04 20:51:29] iter = 19130, loss = 1.3342
[2023-10-04 20:51:30] iter = 19140, loss = 1.4755
[2023-10-04 20:51:31] iter = 19150, loss = 1.3352
[2023-10-04 20:51:32] iter = 19160, loss = 1.4314
[2023-10-04 20:51:33] iter = 19170, loss = 1.5634
[2023-10-04 20:51:34] iter = 19180, loss = 1.4455
[2023-10-04 20:51:35] iter = 19190, loss = 1.5218
[2023-10-04 20:51:36] iter = 19200, loss = 1.4340
[2023-10-04 20:51:37] iter = 19210, loss = 1.4146
[2023-10-04 20:51:38] iter = 19220, loss = 1.3516
[2023-10-04 20:51:39] iter = 19230, loss = 1.6460
[2023-10-04 20:51:40] iter = 19240, loss = 1.3829
[2023-10-04 20:51:40] iter = 19250, loss = 1.4822
[2023-10-04 20:51:41] iter = 19260, loss = 1.5198
[2023-10-04 20:51:42] iter = 19270, loss = 1.6085
[2023-10-04 20:51:43] iter = 19280, loss = 1.3335
[2023-10-04 20:51:44] iter = 19290, loss = 1.5601
[2023-10-04 20:51:45] iter = 19300, loss = 1.4949
[2023-10-04 20:51:46] iter = 19310, loss = 1.5252
[2023-10-04 20:51:47] iter = 19320, loss = 1.3881
[2023-10-04 20:51:48] iter = 19330, loss = 1.4410
[2023-10-04 20:51:49] iter = 19340, loss = 1.4439
[2023-10-04 20:51:50] iter = 19350, loss = 1.5062
[2023-10-04 20:51:50] iter = 19360, loss = 1.5707
[2023-10-04 20:51:51] iter = 19370, loss = 1.4272
[2023-10-04 20:51:52] iter = 19380, loss = 1.5934
[2023-10-04 20:51:53] iter = 19390, loss = 1.5121
[2023-10-04 20:51:54] iter = 19400, loss = 1.5388
[2023-10-04 20:51:55] iter = 19410, loss = 1.5013
[2023-10-04 20:51:56] iter = 19420, loss = 1.3960
[2023-10-04 20:51:57] iter = 19430, loss = 1.4984
[2023-10-04 20:51:58] iter = 19440, loss = 1.4293
[2023-10-04 20:51:59] iter = 19450, loss = 1.4838
[2023-10-04 20:52:00] iter = 19460, loss = 1.6134
[2023-10-04 20:52:01] iter = 19470, loss = 1.4061
[2023-10-04 20:52:02] iter = 19480, loss = 1.4686
[2023-10-04 20:52:03] iter = 19490, loss = 1.3028
[2023-10-04 20:52:04] iter = 19500, loss = 1.4033
[2023-10-04 20:52:04] iter = 19510, loss = 1.4938
[2023-10-04 20:52:05] iter = 19520, loss = 1.5487
[2023-10-04 20:52:06] iter = 19530, loss = 1.5636
[2023-10-04 20:52:07] iter = 19540, loss = 1.4583
[2023-10-04 20:52:08] iter = 19550, loss = 1.4769
[2023-10-04 20:52:09] iter = 19560, loss = 1.4592
[2023-10-04 20:52:10] iter = 19570, loss = 1.3689
[2023-10-04 20:52:11] iter = 19580, loss = 1.6007
[2023-10-04 20:52:12] iter = 19590, loss = 1.5532
[2023-10-04 20:52:13] iter = 19600, loss = 1.5581
[2023-10-04 20:52:14] iter = 19610, loss = 1.5001
[2023-10-04 20:52:15] iter = 19620, loss = 1.5040
[2023-10-04 20:52:16] iter = 19630, loss = 1.4834
[2023-10-04 20:52:17] iter = 19640, loss = 1.4987
[2023-10-04 20:52:18] iter = 19650, loss = 1.4758
[2023-10-04 20:52:19] iter = 19660, loss = 1.4967
[2023-10-04 20:52:19] iter = 19670, loss = 1.3988
[2023-10-04 20:52:20] iter = 19680, loss = 1.4319
[2023-10-04 20:52:21] iter = 19690, loss = 1.5019
[2023-10-04 20:52:22] iter = 19700, loss = 1.4283
[2023-10-04 20:52:23] iter = 19710, loss = 1.5314
[2023-10-04 20:52:24] iter = 19720, loss = 1.4492
[2023-10-04 20:52:25] iter = 19730, loss = 1.4490
[2023-10-04 20:52:26] iter = 19740, loss = 1.4035
[2023-10-04 20:52:27] iter = 19750, loss = 1.4616
[2023-10-04 20:52:28] iter = 19760, loss = 1.4473
[2023-10-04 20:52:29] iter = 19770, loss = 1.4697
[2023-10-04 20:52:29] iter = 19780, loss = 1.4347
[2023-10-04 20:52:31] iter = 19790, loss = 1.3791
[2023-10-04 20:52:31] iter = 19800, loss = 1.4309
[2023-10-04 20:52:32] iter = 19810, loss = 1.4815
[2023-10-04 20:52:33] iter = 19820, loss = 1.4733
[2023-10-04 20:52:34] iter = 19830, loss = 1.3896
[2023-10-04 20:52:35] iter = 19840, loss = 1.3439
[2023-10-04 20:52:36] iter = 19850, loss = 1.3738
[2023-10-04 20:52:37] iter = 19860, loss = 1.4810
[2023-10-04 20:52:38] iter = 19870, loss = 1.3779
[2023-10-04 20:52:39] iter = 19880, loss = 1.5380
[2023-10-04 20:52:40] iter = 19890, loss = 1.4150
[2023-10-04 20:52:40] iter = 19900, loss = 1.4230
[2023-10-04 20:52:41] iter = 19910, loss = 1.5506
[2023-10-04 20:52:42] iter = 19920, loss = 1.5094
[2023-10-04 20:52:43] iter = 19930, loss = 1.3994
[2023-10-04 20:52:44] iter = 19940, loss = 1.3618
[2023-10-04 20:52:45] iter = 19950, loss = 1.5563
[2023-10-04 20:52:46] iter = 19960, loss = 1.5423
[2023-10-04 20:52:47] iter = 19970, loss = 1.4998
[2023-10-04 20:52:48] iter = 19980, loss = 1.5924
[2023-10-04 20:52:49] iter = 19990, loss = 1.5801
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 69946}
[2023-10-04 20:53:14] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002360 train acc = 1.0000, test acc = 0.6232
[2023-10-04 20:53:39] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.011997 train acc = 1.0000, test acc = 0.6300
[2023-10-04 20:54:03] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.015247 train acc = 1.0000, test acc = 0.6233
[2023-10-04 20:54:28] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004020 train acc = 1.0000, test acc = 0.6245
[2023-10-04 20:54:52] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.007847 train acc = 1.0000, test acc = 0.6226
[2023-10-04 20:55:17] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001821 train acc = 1.0000, test acc = 0.6300
[2023-10-04 20:55:41] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.011411 train acc = 1.0000, test acc = 0.6265
[2023-10-04 20:56:06] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004638 train acc = 1.0000, test acc = 0.6285
[2023-10-04 20:56:30] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.020379 train acc = 1.0000, test acc = 0.6284
[2023-10-04 20:56:55] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.015556 train acc = 0.9980, test acc = 0.6272
[2023-10-04 20:57:19] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.017909 train acc = 1.0000, test acc = 0.6306
[2023-10-04 20:57:44] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002945 train acc = 1.0000, test acc = 0.6336
[2023-10-04 20:58:08] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.013708 train acc = 1.0000, test acc = 0.6187
[2023-10-04 20:58:33] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.013765 train acc = 1.0000, test acc = 0.6255
[2023-10-04 20:58:57] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.023925 train acc = 1.0000, test acc = 0.6196
[2023-10-04 20:59:22] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.013375 train acc = 1.0000, test acc = 0.6263
[2023-10-04 20:59:46] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.018358 train acc = 1.0000, test acc = 0.6229
[2023-10-04 21:00:11] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.005011 train acc = 1.0000, test acc = 0.6237
[2023-10-04 21:00:35] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.018232 train acc = 0.9980, test acc = 0.6177
[2023-10-04 21:01:00] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.006628 train acc = 1.0000, test acc = 0.6223
Evaluate 20 random ConvNet, mean = 0.6253 std = 0.0041
-------------------------
[2023-10-04 21:01:00] iter = 20000, loss = 1.3916

================== Exp 4 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5173a57f40>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-04 21:01:17] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 60424}
[2023-10-04 21:01:42] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.012979 train acc = 1.0000, test acc = 0.4860
[2023-10-04 21:02:07] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.016296 train acc = 1.0000, test acc = 0.4935
[2023-10-04 21:02:31] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.014848 train acc = 1.0000, test acc = 0.4870
[2023-10-04 21:02:55] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001592 train acc = 1.0000, test acc = 0.4904
[2023-10-04 21:03:20] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.006075 train acc = 1.0000, test acc = 0.5036
[2023-10-04 21:03:44] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001050 train acc = 1.0000, test acc = 0.4920
[2023-10-04 21:04:09] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.017275 train acc = 1.0000, test acc = 0.4899
[2023-10-04 21:04:33] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002359 train acc = 1.0000, test acc = 0.4925
[2023-10-04 21:04:58] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.000880 train acc = 1.0000, test acc = 0.4962
[2023-10-04 21:05:23] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002970 train acc = 1.0000, test acc = 0.4865
[2023-10-04 21:05:47] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.006905 train acc = 0.9980, test acc = 0.4882
[2023-10-04 21:06:12] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002427 train acc = 1.0000, test acc = 0.4923
[2023-10-04 21:06:36] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.007660 train acc = 1.0000, test acc = 0.4862
[2023-10-04 21:07:01] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003422 train acc = 1.0000, test acc = 0.4965
[2023-10-04 21:07:25] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.012218 train acc = 1.0000, test acc = 0.4839
[2023-10-04 21:07:50] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003648 train acc = 1.0000, test acc = 0.4830
[2023-10-04 21:08:14] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004636 train acc = 1.0000, test acc = 0.4920
[2023-10-04 21:08:39] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.013575 train acc = 1.0000, test acc = 0.4913
[2023-10-04 21:09:03] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.006675 train acc = 1.0000, test acc = 0.4969
[2023-10-04 21:09:28] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.001000 train acc = 1.0000, test acc = 0.4914
Evaluate 20 random ConvNet, mean = 0.4910 std = 0.0049
-------------------------
[2023-10-04 21:09:28] iter = 00000, loss = 5.5537
[2023-10-04 21:09:29] iter = 00010, loss = 5.0900
[2023-10-04 21:09:30] iter = 00020, loss = 5.0630
[2023-10-04 21:09:31] iter = 00030, loss = 4.5784
[2023-10-04 21:09:32] iter = 00040, loss = 4.1172
[2023-10-04 21:09:33] iter = 00050, loss = 3.9357
[2023-10-04 21:09:33] iter = 00060, loss = 4.0993
[2023-10-04 21:09:34] iter = 00070, loss = 3.6953
[2023-10-04 21:09:35] iter = 00080, loss = 3.5187
[2023-10-04 21:09:36] iter = 00090, loss = 3.1703
[2023-10-04 21:09:37] iter = 00100, loss = 3.5306
[2023-10-04 21:09:38] iter = 00110, loss = 3.4469
[2023-10-04 21:09:39] iter = 00120, loss = 3.0518
[2023-10-04 21:09:40] iter = 00130, loss = 2.9273
[2023-10-04 21:09:41] iter = 00140, loss = 2.9338
[2023-10-04 21:09:42] iter = 00150, loss = 2.9704
[2023-10-04 21:09:43] iter = 00160, loss = 2.8345
[2023-10-04 21:09:44] iter = 00170, loss = 2.7699
[2023-10-04 21:09:45] iter = 00180, loss = 2.8003
[2023-10-04 21:09:46] iter = 00190, loss = 2.7732
[2023-10-04 21:09:46] iter = 00200, loss = 2.5775
[2023-10-04 21:09:47] iter = 00210, loss = 2.6398
[2023-10-04 21:09:48] iter = 00220, loss = 2.6723
[2023-10-04 21:09:49] iter = 00230, loss = 2.5915
[2023-10-04 21:09:50] iter = 00240, loss = 2.5731
[2023-10-04 21:09:51] iter = 00250, loss = 2.6712
[2023-10-04 21:09:52] iter = 00260, loss = 2.4892
[2023-10-04 21:09:53] iter = 00270, loss = 2.5734
[2023-10-04 21:09:54] iter = 00280, loss = 2.4813
[2023-10-04 21:09:55] iter = 00290, loss = 2.6608
[2023-10-04 21:09:56] iter = 00300, loss = 2.4399
[2023-10-04 21:09:56] iter = 00310, loss = 2.4553
[2023-10-04 21:09:57] iter = 00320, loss = 2.6608
[2023-10-04 21:09:58] iter = 00330, loss = 2.2645
[2023-10-04 21:09:59] iter = 00340, loss = 2.6825
[2023-10-04 21:10:00] iter = 00350, loss = 2.6577
[2023-10-04 21:10:01] iter = 00360, loss = 2.3165
[2023-10-04 21:10:02] iter = 00370, loss = 2.4601
[2023-10-04 21:10:03] iter = 00380, loss = 2.8625
[2023-10-04 21:10:04] iter = 00390, loss = 2.3953
[2023-10-04 21:10:05] iter = 00400, loss = 2.3853
[2023-10-04 21:10:05] iter = 00410, loss = 2.3257
[2023-10-04 21:10:06] iter = 00420, loss = 2.2840
[2023-10-04 21:10:07] iter = 00430, loss = 2.1386
[2023-10-04 21:10:08] iter = 00440, loss = 2.2690
[2023-10-04 21:10:09] iter = 00450, loss = 2.3098
[2023-10-04 21:10:10] iter = 00460, loss = 2.2223
[2023-10-04 21:10:11] iter = 00470, loss = 2.4526
[2023-10-04 21:10:12] iter = 00480, loss = 2.2508
[2023-10-04 21:10:13] iter = 00490, loss = 2.3260
[2023-10-04 21:10:14] iter = 00500, loss = 2.2388
[2023-10-04 21:10:15] iter = 00510, loss = 2.3922
[2023-10-04 21:10:16] iter = 00520, loss = 2.3026
[2023-10-04 21:10:17] iter = 00530, loss = 2.1179
[2023-10-04 21:10:18] iter = 00540, loss = 2.2477
[2023-10-04 21:10:18] iter = 00550, loss = 2.0887
[2023-10-04 21:10:19] iter = 00560, loss = 2.1334
[2023-10-04 21:10:20] iter = 00570, loss = 2.2804
[2023-10-04 21:10:21] iter = 00580, loss = 2.1419
[2023-10-04 21:10:22] iter = 00590, loss = 2.1788
[2023-10-04 21:10:23] iter = 00600, loss = 2.1570
[2023-10-04 21:10:24] iter = 00610, loss = 2.1632
[2023-10-04 21:10:25] iter = 00620, loss = 2.2095
[2023-10-04 21:10:26] iter = 00630, loss = 2.5134
[2023-10-04 21:10:27] iter = 00640, loss = 2.2561
[2023-10-04 21:10:28] iter = 00650, loss = 2.2323
[2023-10-04 21:10:28] iter = 00660, loss = 2.2532
[2023-10-04 21:10:29] iter = 00670, loss = 2.1593
[2023-10-04 21:10:30] iter = 00680, loss = 2.1274
[2023-10-04 21:10:31] iter = 00690, loss = 2.1699
[2023-10-04 21:10:32] iter = 00700, loss = 2.2567
[2023-10-04 21:10:33] iter = 00710, loss = 2.2145
[2023-10-04 21:10:34] iter = 00720, loss = 2.1549
[2023-10-04 21:10:35] iter = 00730, loss = 2.0636
[2023-10-04 21:10:36] iter = 00740, loss = 2.0404
[2023-10-04 21:10:37] iter = 00750, loss = 2.2050
[2023-10-04 21:10:37] iter = 00760, loss = 2.1184
[2023-10-04 21:10:38] iter = 00770, loss = 2.2676
[2023-10-04 21:10:39] iter = 00780, loss = 2.0572
[2023-10-04 21:10:40] iter = 00790, loss = 1.9971
[2023-10-04 21:10:41] iter = 00800, loss = 2.1711
[2023-10-04 21:10:42] iter = 00810, loss = 1.9247
[2023-10-04 21:10:43] iter = 00820, loss = 1.9496
[2023-10-04 21:10:44] iter = 00830, loss = 2.2156
[2023-10-04 21:10:45] iter = 00840, loss = 2.1632
[2023-10-04 21:10:46] iter = 00850, loss = 2.2024
[2023-10-04 21:10:47] iter = 00860, loss = 2.2142
[2023-10-04 21:10:47] iter = 00870, loss = 2.0924
[2023-10-04 21:10:48] iter = 00880, loss = 2.0670
[2023-10-04 21:10:49] iter = 00890, loss = 1.9663
[2023-10-04 21:10:50] iter = 00900, loss = 1.9659
[2023-10-04 21:10:51] iter = 00910, loss = 2.1822
[2023-10-04 21:10:52] iter = 00920, loss = 2.0898
[2023-10-04 21:10:53] iter = 00930, loss = 2.0165
[2023-10-04 21:10:54] iter = 00940, loss = 2.0333
[2023-10-04 21:10:55] iter = 00950, loss = 2.0875
[2023-10-04 21:10:56] iter = 00960, loss = 2.0039
[2023-10-04 21:10:57] iter = 00970, loss = 2.0409
[2023-10-04 21:10:57] iter = 00980, loss = 2.1610
[2023-10-04 21:10:58] iter = 00990, loss = 2.1734
[2023-10-04 21:10:59] iter = 01000, loss = 2.0524
[2023-10-04 21:11:00] iter = 01010, loss = 1.9815
[2023-10-04 21:11:01] iter = 01020, loss = 1.9728
[2023-10-04 21:11:02] iter = 01030, loss = 1.9660
[2023-10-04 21:11:03] iter = 01040, loss = 1.9869
[2023-10-04 21:11:04] iter = 01050, loss = 2.0934
[2023-10-04 21:11:05] iter = 01060, loss = 2.0871
[2023-10-04 21:11:06] iter = 01070, loss = 2.1283
[2023-10-04 21:11:07] iter = 01080, loss = 1.9507
[2023-10-04 21:11:08] iter = 01090, loss = 1.8574
[2023-10-04 21:11:08] iter = 01100, loss = 1.9925
[2023-10-04 21:11:09] iter = 01110, loss = 2.1503
[2023-10-04 21:11:10] iter = 01120, loss = 1.9044
[2023-10-04 21:11:11] iter = 01130, loss = 1.8793
[2023-10-04 21:11:12] iter = 01140, loss = 2.0539
[2023-10-04 21:11:13] iter = 01150, loss = 2.1345
[2023-10-04 21:11:14] iter = 01160, loss = 2.0312
[2023-10-04 21:11:15] iter = 01170, loss = 2.0611
[2023-10-04 21:11:16] iter = 01180, loss = 2.1820
[2023-10-04 21:11:17] iter = 01190, loss = 1.8629
[2023-10-04 21:11:18] iter = 01200, loss = 1.9593
[2023-10-04 21:11:18] iter = 01210, loss = 1.8740
[2023-10-04 21:11:19] iter = 01220, loss = 2.0170
[2023-10-04 21:11:20] iter = 01230, loss = 1.9246
[2023-10-04 21:11:21] iter = 01240, loss = 1.8930
[2023-10-04 21:11:22] iter = 01250, loss = 1.8658
[2023-10-04 21:11:23] iter = 01260, loss = 1.8807
[2023-10-04 21:11:24] iter = 01270, loss = 2.0128
[2023-10-04 21:11:25] iter = 01280, loss = 1.9330
[2023-10-04 21:11:26] iter = 01290, loss = 1.9319
[2023-10-04 21:11:27] iter = 01300, loss = 1.9264
[2023-10-04 21:11:28] iter = 01310, loss = 1.9797
[2023-10-04 21:11:28] iter = 01320, loss = 1.9160
[2023-10-04 21:11:29] iter = 01330, loss = 1.9435
[2023-10-04 21:11:30] iter = 01340, loss = 1.9982
[2023-10-04 21:11:31] iter = 01350, loss = 1.8293
[2023-10-04 21:11:32] iter = 01360, loss = 2.0633
[2023-10-04 21:11:33] iter = 01370, loss = 1.9143
[2023-10-04 21:11:34] iter = 01380, loss = 1.9572
[2023-10-04 21:11:35] iter = 01390, loss = 1.8762
[2023-10-04 21:11:36] iter = 01400, loss = 1.9333
[2023-10-04 21:11:37] iter = 01410, loss = 1.7958
[2023-10-04 21:11:38] iter = 01420, loss = 1.8280
[2023-10-04 21:11:38] iter = 01430, loss = 1.9751
[2023-10-04 21:11:39] iter = 01440, loss = 1.9767
[2023-10-04 21:11:40] iter = 01450, loss = 1.9112
[2023-10-04 21:11:41] iter = 01460, loss = 1.9645
[2023-10-04 21:11:42] iter = 01470, loss = 1.9025
[2023-10-04 21:11:43] iter = 01480, loss = 1.9703
[2023-10-04 21:11:44] iter = 01490, loss = 2.0035
[2023-10-04 21:11:45] iter = 01500, loss = 2.0722
[2023-10-04 21:11:46] iter = 01510, loss = 1.8531
[2023-10-04 21:11:47] iter = 01520, loss = 2.1114
[2023-10-04 21:11:48] iter = 01530, loss = 1.9728
[2023-10-04 21:11:49] iter = 01540, loss = 2.0341
[2023-10-04 21:11:49] iter = 01550, loss = 1.8008
[2023-10-04 21:11:50] iter = 01560, loss = 1.9264
[2023-10-04 21:11:52] iter = 01570, loss = 1.9169
[2023-10-04 21:11:52] iter = 01580, loss = 1.9025
[2023-10-04 21:11:53] iter = 01590, loss = 1.8291
[2023-10-04 21:11:54] iter = 01600, loss = 1.9094
[2023-10-04 21:11:55] iter = 01610, loss = 1.8815
[2023-10-04 21:11:56] iter = 01620, loss = 1.8667
[2023-10-04 21:11:57] iter = 01630, loss = 1.8314
[2023-10-04 21:11:58] iter = 01640, loss = 2.0255
[2023-10-04 21:11:59] iter = 01650, loss = 2.0170
[2023-10-04 21:12:00] iter = 01660, loss = 1.9381
[2023-10-04 21:12:01] iter = 01670, loss = 1.8699
[2023-10-04 21:12:02] iter = 01680, loss = 1.9140
[2023-10-04 21:12:03] iter = 01690, loss = 1.8862
[2023-10-04 21:12:03] iter = 01700, loss = 1.9311
[2023-10-04 21:12:04] iter = 01710, loss = 1.8392
[2023-10-04 21:12:05] iter = 01720, loss = 1.8704
[2023-10-04 21:12:06] iter = 01730, loss = 2.0009
[2023-10-04 21:12:07] iter = 01740, loss = 1.9390
[2023-10-04 21:12:08] iter = 01750, loss = 1.8092
[2023-10-04 21:12:09] iter = 01760, loss = 2.0452
[2023-10-04 21:12:10] iter = 01770, loss = 1.7380
[2023-10-04 21:12:11] iter = 01780, loss = 1.7332
[2023-10-04 21:12:12] iter = 01790, loss = 1.8061
[2023-10-04 21:12:13] iter = 01800, loss = 1.9391
[2023-10-04 21:12:14] iter = 01810, loss = 1.9575
[2023-10-04 21:12:14] iter = 01820, loss = 1.8848
[2023-10-04 21:12:15] iter = 01830, loss = 1.8576
[2023-10-04 21:12:16] iter = 01840, loss = 1.7259
[2023-10-04 21:12:17] iter = 01850, loss = 1.9530
[2023-10-04 21:12:18] iter = 01860, loss = 1.7973
[2023-10-04 21:12:19] iter = 01870, loss = 1.8670
[2023-10-04 21:12:20] iter = 01880, loss = 1.7380
[2023-10-04 21:12:21] iter = 01890, loss = 1.9900
[2023-10-04 21:12:22] iter = 01900, loss = 1.8722
[2023-10-04 21:12:23] iter = 01910, loss = 1.8256
[2023-10-04 21:12:23] iter = 01920, loss = 1.8532
[2023-10-04 21:12:24] iter = 01930, loss = 1.9052
[2023-10-04 21:12:25] iter = 01940, loss = 1.8733
[2023-10-04 21:12:26] iter = 01950, loss = 1.7473
[2023-10-04 21:12:27] iter = 01960, loss = 1.7773
[2023-10-04 21:12:28] iter = 01970, loss = 1.7364
[2023-10-04 21:12:29] iter = 01980, loss = 1.6800
[2023-10-04 21:12:30] iter = 01990, loss = 2.0361
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 51090}
[2023-10-04 21:12:55] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.006127 train acc = 1.0000, test acc = 0.5908
[2023-10-04 21:13:19] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003064 train acc = 1.0000, test acc = 0.5844
[2023-10-04 21:13:44] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.008948 train acc = 1.0000, test acc = 0.5994
[2023-10-04 21:14:08] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.017230 train acc = 1.0000, test acc = 0.5858
[2023-10-04 21:14:32] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.001441 train acc = 1.0000, test acc = 0.5851
[2023-10-04 21:14:57] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.010047 train acc = 1.0000, test acc = 0.5841
[2023-10-04 21:15:21] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.010335 train acc = 1.0000, test acc = 0.5886
[2023-10-04 21:15:46] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.007878 train acc = 1.0000, test acc = 0.5869
[2023-10-04 21:16:10] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.005113 train acc = 0.9980, test acc = 0.5922
[2023-10-04 21:16:35] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.008254 train acc = 1.0000, test acc = 0.5887
[2023-10-04 21:16:59] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.010208 train acc = 1.0000, test acc = 0.5863
[2023-10-04 21:17:23] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.017670 train acc = 1.0000, test acc = 0.5910
[2023-10-04 21:17:48] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.002931 train acc = 1.0000, test acc = 0.5885
[2023-10-04 21:18:12] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002349 train acc = 1.0000, test acc = 0.5964
[2023-10-04 21:18:37] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001409 train acc = 1.0000, test acc = 0.5806
[2023-10-04 21:19:01] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.006625 train acc = 1.0000, test acc = 0.5887
[2023-10-04 21:19:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003580 train acc = 1.0000, test acc = 0.5925
[2023-10-04 21:19:50] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.015131 train acc = 1.0000, test acc = 0.5938
[2023-10-04 21:20:15] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.012307 train acc = 1.0000, test acc = 0.5892
[2023-10-04 21:20:39] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.007163 train acc = 1.0000, test acc = 0.5812
Evaluate 20 random ConvNet, mean = 0.5887 std = 0.0046
-------------------------
[2023-10-04 21:20:39] iter = 02000, loss = 1.8323
[2023-10-04 21:20:40] iter = 02010, loss = 1.9591
[2023-10-04 21:20:41] iter = 02020, loss = 1.8996
[2023-10-04 21:20:42] iter = 02030, loss = 1.9308
[2023-10-04 21:20:43] iter = 02040, loss = 1.7351
[2023-10-04 21:20:44] iter = 02050, loss = 1.8430
[2023-10-04 21:20:45] iter = 02060, loss = 1.8233
[2023-10-04 21:20:46] iter = 02070, loss = 1.7090
[2023-10-04 21:20:47] iter = 02080, loss = 1.7101
[2023-10-04 21:20:48] iter = 02090, loss = 1.6362
[2023-10-04 21:20:49] iter = 02100, loss = 1.7831
[2023-10-04 21:20:49] iter = 02110, loss = 1.8531
[2023-10-04 21:20:50] iter = 02120, loss = 1.7477
[2023-10-04 21:20:51] iter = 02130, loss = 1.7054
[2023-10-04 21:20:52] iter = 02140, loss = 1.8704
[2023-10-04 21:20:53] iter = 02150, loss = 1.8679
[2023-10-04 21:20:54] iter = 02160, loss = 1.8037
[2023-10-04 21:20:55] iter = 02170, loss = 1.9901
[2023-10-04 21:20:56] iter = 02180, loss = 1.7496
[2023-10-04 21:20:57] iter = 02190, loss = 2.0011
[2023-10-04 21:20:58] iter = 02200, loss = 1.8751
[2023-10-04 21:20:59] iter = 02210, loss = 1.6821
[2023-10-04 21:20:59] iter = 02220, loss = 1.6460
[2023-10-04 21:21:00] iter = 02230, loss = 1.9381
[2023-10-04 21:21:01] iter = 02240, loss = 1.7903
[2023-10-04 21:21:02] iter = 02250, loss = 1.7821
[2023-10-04 21:21:03] iter = 02260, loss = 1.7487
[2023-10-04 21:21:04] iter = 02270, loss = 1.7441
[2023-10-04 21:21:05] iter = 02280, loss = 1.8354
[2023-10-04 21:21:06] iter = 02290, loss = 1.7602
[2023-10-04 21:21:07] iter = 02300, loss = 1.7451
[2023-10-04 21:21:08] iter = 02310, loss = 1.8158
[2023-10-04 21:21:08] iter = 02320, loss = 1.7609
[2023-10-04 21:21:09] iter = 02330, loss = 1.6408
[2023-10-04 21:21:10] iter = 02340, loss = 1.7813
[2023-10-04 21:21:11] iter = 02350, loss = 1.8256
[2023-10-04 21:21:12] iter = 02360, loss = 1.8425
[2023-10-04 21:21:13] iter = 02370, loss = 1.8424
[2023-10-04 21:21:14] iter = 02380, loss = 1.7130
[2023-10-04 21:21:15] iter = 02390, loss = 1.8522
[2023-10-04 21:21:16] iter = 02400, loss = 1.8891
[2023-10-04 21:21:17] iter = 02410, loss = 1.7575
[2023-10-04 21:21:18] iter = 02420, loss = 1.7965
[2023-10-04 21:21:19] iter = 02430, loss = 1.9437
[2023-10-04 21:21:20] iter = 02440, loss = 1.8218
[2023-10-04 21:21:21] iter = 02450, loss = 1.6866
[2023-10-04 21:21:22] iter = 02460, loss = 1.7482
[2023-10-04 21:21:23] iter = 02470, loss = 1.8694
[2023-10-04 21:21:23] iter = 02480, loss = 1.8148
[2023-10-04 21:21:24] iter = 02490, loss = 2.0004
[2023-10-04 21:21:25] iter = 02500, loss = 1.9032
[2023-10-04 21:21:26] iter = 02510, loss = 1.7598
[2023-10-04 21:21:27] iter = 02520, loss = 1.7126
[2023-10-04 21:21:28] iter = 02530, loss = 1.6100
[2023-10-04 21:21:29] iter = 02540, loss = 1.6823
[2023-10-04 21:21:30] iter = 02550, loss = 1.8306
[2023-10-04 21:21:31] iter = 02560, loss = 1.9447
[2023-10-04 21:21:32] iter = 02570, loss = 1.8484
[2023-10-04 21:21:33] iter = 02580, loss = 1.8162
[2023-10-04 21:21:34] iter = 02590, loss = 1.9233
[2023-10-04 21:21:35] iter = 02600, loss = 1.8149
[2023-10-04 21:21:35] iter = 02610, loss = 1.8043
[2023-10-04 21:21:36] iter = 02620, loss = 1.6464
[2023-10-04 21:21:37] iter = 02630, loss = 1.8952
[2023-10-04 21:21:38] iter = 02640, loss = 1.8777
[2023-10-04 21:21:39] iter = 02650, loss = 1.7181
[2023-10-04 21:21:40] iter = 02660, loss = 2.0062
[2023-10-04 21:21:41] iter = 02670, loss = 1.8800
[2023-10-04 21:21:42] iter = 02680, loss = 1.8469
[2023-10-04 21:21:43] iter = 02690, loss = 1.7246
[2023-10-04 21:21:44] iter = 02700, loss = 1.8431
[2023-10-04 21:21:44] iter = 02710, loss = 1.6537
[2023-10-04 21:21:45] iter = 02720, loss = 1.7327
[2023-10-04 21:21:46] iter = 02730, loss = 1.8328
[2023-10-04 21:21:47] iter = 02740, loss = 1.7099
[2023-10-04 21:21:48] iter = 02750, loss = 1.7489
[2023-10-04 21:21:49] iter = 02760, loss = 1.6380
[2023-10-04 21:21:50] iter = 02770, loss = 1.8599
[2023-10-04 21:21:51] iter = 02780, loss = 1.7320
[2023-10-04 21:21:52] iter = 02790, loss = 1.6756
[2023-10-04 21:21:53] iter = 02800, loss = 1.7648
[2023-10-04 21:21:54] iter = 02810, loss = 1.6843
[2023-10-04 21:21:55] iter = 02820, loss = 1.7049
[2023-10-04 21:21:56] iter = 02830, loss = 1.7940
[2023-10-04 21:21:57] iter = 02840, loss = 1.8015
[2023-10-04 21:21:58] iter = 02850, loss = 1.7002
[2023-10-04 21:21:58] iter = 02860, loss = 1.6530
[2023-10-04 21:21:59] iter = 02870, loss = 1.6585
[2023-10-04 21:22:00] iter = 02880, loss = 1.6604
[2023-10-04 21:22:01] iter = 02890, loss = 1.8572
[2023-10-04 21:22:02] iter = 02900, loss = 1.8200
[2023-10-04 21:22:03] iter = 02910, loss = 1.8194
[2023-10-04 21:22:04] iter = 02920, loss = 1.8267
[2023-10-04 21:22:05] iter = 02930, loss = 1.6937
[2023-10-04 21:22:06] iter = 02940, loss = 1.6879
[2023-10-04 21:22:07] iter = 02950, loss = 1.9103
[2023-10-04 21:22:08] iter = 02960, loss = 1.6886
[2023-10-04 21:22:09] iter = 02970, loss = 1.8903
[2023-10-04 21:22:09] iter = 02980, loss = 1.8188
[2023-10-04 21:22:10] iter = 02990, loss = 1.8371
[2023-10-04 21:22:11] iter = 03000, loss = 1.6648
[2023-10-04 21:22:12] iter = 03010, loss = 1.7600
[2023-10-04 21:22:13] iter = 03020, loss = 1.7265
[2023-10-04 21:22:14] iter = 03030, loss = 1.6313
[2023-10-04 21:22:15] iter = 03040, loss = 1.6495
[2023-10-04 21:22:16] iter = 03050, loss = 1.6863
[2023-10-04 21:22:17] iter = 03060, loss = 1.7509
[2023-10-04 21:22:18] iter = 03070, loss = 1.8236
[2023-10-04 21:22:19] iter = 03080, loss = 1.6838
[2023-10-04 21:22:20] iter = 03090, loss = 1.7026
[2023-10-04 21:22:20] iter = 03100, loss = 1.7569
[2023-10-04 21:22:21] iter = 03110, loss = 1.7636
[2023-10-04 21:22:22] iter = 03120, loss = 1.8497
[2023-10-04 21:22:23] iter = 03130, loss = 1.8022
[2023-10-04 21:22:24] iter = 03140, loss = 1.7271
[2023-10-04 21:22:25] iter = 03150, loss = 1.6244
[2023-10-04 21:22:26] iter = 03160, loss = 1.8131
[2023-10-04 21:22:27] iter = 03170, loss = 1.8525
[2023-10-04 21:22:28] iter = 03180, loss = 1.7239
[2023-10-04 21:22:29] iter = 03190, loss = 1.7658
[2023-10-04 21:22:29] iter = 03200, loss = 1.8183
[2023-10-04 21:22:30] iter = 03210, loss = 1.7501
[2023-10-04 21:22:31] iter = 03220, loss = 1.7406
[2023-10-04 21:22:32] iter = 03230, loss = 1.7231
[2023-10-04 21:22:33] iter = 03240, loss = 1.6048
[2023-10-04 21:22:34] iter = 03250, loss = 1.7372
[2023-10-04 21:22:35] iter = 03260, loss = 1.7439
[2023-10-04 21:22:36] iter = 03270, loss = 1.8713
[2023-10-04 21:22:37] iter = 03280, loss = 1.8272
[2023-10-04 21:22:38] iter = 03290, loss = 1.6812
[2023-10-04 21:22:39] iter = 03300, loss = 1.8286
[2023-10-04 21:22:39] iter = 03310, loss = 1.8009
[2023-10-04 21:22:40] iter = 03320, loss = 1.7028
[2023-10-04 21:22:41] iter = 03330, loss = 1.7773
[2023-10-04 21:22:42] iter = 03340, loss = 1.8471
[2023-10-04 21:22:43] iter = 03350, loss = 1.6090
[2023-10-04 21:22:44] iter = 03360, loss = 1.6613
[2023-10-04 21:22:45] iter = 03370, loss = 1.6401
[2023-10-04 21:22:46] iter = 03380, loss = 1.8594
[2023-10-04 21:22:47] iter = 03390, loss = 1.5340
[2023-10-04 21:22:48] iter = 03400, loss = 1.8251
[2023-10-04 21:22:49] iter = 03410, loss = 1.8199
[2023-10-04 21:22:50] iter = 03420, loss = 1.7967
[2023-10-04 21:22:50] iter = 03430, loss = 1.7209
[2023-10-04 21:22:52] iter = 03440, loss = 1.7894
[2023-10-04 21:22:52] iter = 03450, loss = 1.7398
[2023-10-04 21:22:53] iter = 03460, loss = 1.7206
[2023-10-04 21:22:54] iter = 03470, loss = 1.7426
[2023-10-04 21:22:55] iter = 03480, loss = 1.7756
[2023-10-04 21:22:56] iter = 03490, loss = 1.6412
[2023-10-04 21:22:57] iter = 03500, loss = 1.7306
[2023-10-04 21:22:58] iter = 03510, loss = 1.6966
[2023-10-04 21:22:59] iter = 03520, loss = 1.8850
[2023-10-04 21:23:00] iter = 03530, loss = 1.6934
[2023-10-04 21:23:01] iter = 03540, loss = 1.6035
[2023-10-04 21:23:02] iter = 03550, loss = 1.7028
[2023-10-04 21:23:03] iter = 03560, loss = 1.7514
[2023-10-04 21:23:04] iter = 03570, loss = 1.6433
[2023-10-04 21:23:05] iter = 03580, loss = 1.7154
[2023-10-04 21:23:05] iter = 03590, loss = 1.7876
[2023-10-04 21:23:06] iter = 03600, loss = 1.6934
[2023-10-04 21:23:07] iter = 03610, loss = 1.6405
[2023-10-04 21:23:08] iter = 03620, loss = 1.7940
[2023-10-04 21:23:09] iter = 03630, loss = 1.7949
[2023-10-04 21:23:10] iter = 03640, loss = 1.8082
[2023-10-04 21:23:11] iter = 03650, loss = 1.6212
[2023-10-04 21:23:12] iter = 03660, loss = 1.7545
[2023-10-04 21:23:13] iter = 03670, loss = 1.7247
[2023-10-04 21:23:14] iter = 03680, loss = 1.5416
[2023-10-04 21:23:15] iter = 03690, loss = 1.6317
[2023-10-04 21:23:16] iter = 03700, loss = 1.8400
[2023-10-04 21:23:16] iter = 03710, loss = 1.7069
[2023-10-04 21:23:17] iter = 03720, loss = 1.8021
[2023-10-04 21:23:18] iter = 03730, loss = 1.7783
[2023-10-04 21:23:19] iter = 03740, loss = 1.6586
[2023-10-04 21:23:20] iter = 03750, loss = 1.6560
[2023-10-04 21:23:21] iter = 03760, loss = 1.8830
[2023-10-04 21:23:22] iter = 03770, loss = 1.7736
[2023-10-04 21:23:23] iter = 03780, loss = 1.7297
[2023-10-04 21:23:24] iter = 03790, loss = 1.6652
[2023-10-04 21:23:25] iter = 03800, loss = 1.6067
[2023-10-04 21:23:26] iter = 03810, loss = 1.7331
[2023-10-04 21:23:27] iter = 03820, loss = 1.8100
[2023-10-04 21:23:28] iter = 03830, loss = 1.8065
[2023-10-04 21:23:29] iter = 03840, loss = 1.6324
[2023-10-04 21:23:29] iter = 03850, loss = 1.8374
[2023-10-04 21:23:30] iter = 03860, loss = 1.7681
[2023-10-04 21:23:31] iter = 03870, loss = 1.6466
[2023-10-04 21:23:32] iter = 03880, loss = 1.7902
[2023-10-04 21:23:33] iter = 03890, loss = 1.6931
[2023-10-04 21:23:34] iter = 03900, loss = 1.6498
[2023-10-04 21:23:35] iter = 03910, loss = 1.6328
[2023-10-04 21:23:36] iter = 03920, loss = 1.8138
[2023-10-04 21:23:37] iter = 03930, loss = 1.7367
[2023-10-04 21:23:38] iter = 03940, loss = 1.6358
[2023-10-04 21:23:39] iter = 03950, loss = 1.8756
[2023-10-04 21:23:40] iter = 03960, loss = 1.7430
[2023-10-04 21:23:40] iter = 03970, loss = 1.6679
[2023-10-04 21:23:41] iter = 03980, loss = 1.7260
[2023-10-04 21:23:42] iter = 03990, loss = 1.6507
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 23566}
[2023-10-04 21:24:08] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.010066 train acc = 1.0000, test acc = 0.6034
[2023-10-04 21:24:32] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.017269 train acc = 0.9980, test acc = 0.6040
[2023-10-04 21:24:57] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.001669 train acc = 1.0000, test acc = 0.6142
[2023-10-04 21:25:21] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003619 train acc = 1.0000, test acc = 0.6090
[2023-10-04 21:25:46] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.009138 train acc = 1.0000, test acc = 0.6041
[2023-10-04 21:26:10] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001640 train acc = 1.0000, test acc = 0.6014
[2023-10-04 21:26:34] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.009647 train acc = 1.0000, test acc = 0.6029
[2023-10-04 21:26:59] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001881 train acc = 1.0000, test acc = 0.6085
[2023-10-04 21:27:24] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.019379 train acc = 1.0000, test acc = 0.6052
[2023-10-04 21:27:48] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003435 train acc = 1.0000, test acc = 0.6107
[2023-10-04 21:28:13] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002189 train acc = 1.0000, test acc = 0.6073
[2023-10-04 21:28:38] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001783 train acc = 1.0000, test acc = 0.6056
[2023-10-04 21:29:02] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.010095 train acc = 1.0000, test acc = 0.6043
[2023-10-04 21:29:27] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.005886 train acc = 1.0000, test acc = 0.5992
[2023-10-04 21:29:51] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003304 train acc = 1.0000, test acc = 0.6044
[2023-10-04 21:30:16] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001808 train acc = 1.0000, test acc = 0.6069
[2023-10-04 21:30:40] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003743 train acc = 1.0000, test acc = 0.6036
[2023-10-04 21:31:05] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.001798 train acc = 1.0000, test acc = 0.6035
[2023-10-04 21:31:29] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001416 train acc = 1.0000, test acc = 0.5949
[2023-10-04 21:31:54] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012439 train acc = 1.0000, test acc = 0.6043
Evaluate 20 random ConvNet, mean = 0.6049 std = 0.0040
-------------------------
[2023-10-04 21:31:54] iter = 04000, loss = 1.6305
[2023-10-04 21:31:55] iter = 04010, loss = 1.7253
[2023-10-04 21:31:56] iter = 04020, loss = 1.5686
[2023-10-04 21:31:57] iter = 04030, loss = 1.7417
[2023-10-04 21:31:58] iter = 04040, loss = 1.6272
[2023-10-04 21:31:59] iter = 04050, loss = 1.6498
[2023-10-04 21:31:59] iter = 04060, loss = 1.7545
[2023-10-04 21:32:00] iter = 04070, loss = 1.8461
[2023-10-04 21:32:01] iter = 04080, loss = 1.7270
[2023-10-04 21:32:02] iter = 04090, loss = 1.6851
[2023-10-04 21:32:03] iter = 04100, loss = 1.6236
[2023-10-04 21:32:04] iter = 04110, loss = 1.7931
[2023-10-04 21:32:05] iter = 04120, loss = 1.7889
[2023-10-04 21:32:06] iter = 04130, loss = 1.5525
[2023-10-04 21:32:07] iter = 04140, loss = 1.5459
[2023-10-04 21:32:08] iter = 04150, loss = 1.5377
[2023-10-04 21:32:09] iter = 04160, loss = 1.6560
[2023-10-04 21:32:10] iter = 04170, loss = 1.5003
[2023-10-04 21:32:11] iter = 04180, loss = 1.5854
[2023-10-04 21:32:12] iter = 04190, loss = 1.8256
[2023-10-04 21:32:13] iter = 04200, loss = 1.4999
[2023-10-04 21:32:14] iter = 04210, loss = 1.7746
[2023-10-04 21:32:15] iter = 04220, loss = 1.6948
[2023-10-04 21:32:15] iter = 04230, loss = 1.7395
[2023-10-04 21:32:16] iter = 04240, loss = 1.7592
[2023-10-04 21:32:17] iter = 04250, loss = 1.7469
[2023-10-04 21:32:18] iter = 04260, loss = 1.6402
[2023-10-04 21:32:19] iter = 04270, loss = 1.7769
[2023-10-04 21:32:20] iter = 04280, loss = 1.6100
[2023-10-04 21:32:21] iter = 04290, loss = 1.6928
[2023-10-04 21:32:22] iter = 04300, loss = 1.6702
[2023-10-04 21:32:23] iter = 04310, loss = 1.5895
[2023-10-04 21:32:24] iter = 04320, loss = 1.5779
[2023-10-04 21:32:25] iter = 04330, loss = 1.6805
[2023-10-04 21:32:26] iter = 04340, loss = 1.8113
[2023-10-04 21:32:26] iter = 04350, loss = 1.7363
[2023-10-04 21:32:27] iter = 04360, loss = 1.6460
[2023-10-04 21:32:28] iter = 04370, loss = 1.6778
[2023-10-04 21:32:29] iter = 04380, loss = 1.6175
[2023-10-04 21:32:30] iter = 04390, loss = 1.6170
[2023-10-04 21:32:31] iter = 04400, loss = 1.6538
[2023-10-04 21:32:32] iter = 04410, loss = 1.6124
[2023-10-04 21:32:33] iter = 04420, loss = 1.5725
[2023-10-04 21:32:34] iter = 04430, loss = 1.6639
[2023-10-04 21:32:34] iter = 04440, loss = 1.6060
[2023-10-04 21:32:35] iter = 04450, loss = 1.6174
[2023-10-04 21:32:36] iter = 04460, loss = 1.6845
[2023-10-04 21:32:37] iter = 04470, loss = 1.7053
[2023-10-04 21:32:38] iter = 04480, loss = 1.6930
[2023-10-04 21:32:39] iter = 04490, loss = 1.6673
[2023-10-04 21:32:40] iter = 04500, loss = 1.7727
[2023-10-04 21:32:41] iter = 04510, loss = 1.7516
[2023-10-04 21:32:42] iter = 04520, loss = 1.6126
[2023-10-04 21:32:43] iter = 04530, loss = 1.5540
[2023-10-04 21:32:43] iter = 04540, loss = 1.7570
[2023-10-04 21:32:44] iter = 04550, loss = 1.6761
[2023-10-04 21:32:45] iter = 04560, loss = 1.6047
[2023-10-04 21:32:46] iter = 04570, loss = 1.8080
[2023-10-04 21:32:47] iter = 04580, loss = 1.5494
[2023-10-04 21:32:48] iter = 04590, loss = 1.6478
[2023-10-04 21:32:49] iter = 04600, loss = 1.6855
[2023-10-04 21:32:50] iter = 04610, loss = 1.6385
[2023-10-04 21:32:51] iter = 04620, loss = 1.5656
[2023-10-04 21:32:52] iter = 04630, loss = 1.7229
[2023-10-04 21:32:53] iter = 04640, loss = 1.7130
[2023-10-04 21:32:54] iter = 04650, loss = 1.7398
[2023-10-04 21:32:55] iter = 04660, loss = 1.6909
[2023-10-04 21:32:55] iter = 04670, loss = 1.7133
[2023-10-04 21:32:57] iter = 04680, loss = 1.5752
[2023-10-04 21:32:57] iter = 04690, loss = 1.5769
[2023-10-04 21:32:58] iter = 04700, loss = 1.6476
[2023-10-04 21:32:59] iter = 04710, loss = 1.5726
[2023-10-04 21:33:00] iter = 04720, loss = 1.7521
[2023-10-04 21:33:01] iter = 04730, loss = 1.5932
[2023-10-04 21:33:02] iter = 04740, loss = 1.4908
[2023-10-04 21:33:03] iter = 04750, loss = 1.6067
[2023-10-04 21:33:04] iter = 04760, loss = 1.6562
[2023-10-04 21:33:05] iter = 04770, loss = 1.7189
[2023-10-04 21:33:06] iter = 04780, loss = 1.7468
[2023-10-04 21:33:07] iter = 04790, loss = 1.6111
[2023-10-04 21:33:08] iter = 04800, loss = 1.6772
[2023-10-04 21:33:09] iter = 04810, loss = 1.7720
[2023-10-04 21:33:09] iter = 04820, loss = 1.6777
[2023-10-04 21:33:10] iter = 04830, loss = 1.7076
[2023-10-04 21:33:11] iter = 04840, loss = 1.6576
[2023-10-04 21:33:12] iter = 04850, loss = 1.6605
[2023-10-04 21:33:13] iter = 04860, loss = 1.6108
[2023-10-04 21:33:14] iter = 04870, loss = 1.6240
[2023-10-04 21:33:15] iter = 04880, loss = 1.6162
[2023-10-04 21:33:16] iter = 04890, loss = 1.6667
[2023-10-04 21:33:17] iter = 04900, loss = 1.6612
[2023-10-04 21:33:18] iter = 04910, loss = 1.4496
[2023-10-04 21:33:19] iter = 04920, loss = 1.5928
[2023-10-04 21:33:20] iter = 04930, loss = 1.5800
[2023-10-04 21:33:20] iter = 04940, loss = 1.6965
[2023-10-04 21:33:21] iter = 04950, loss = 1.6347
[2023-10-04 21:33:22] iter = 04960, loss = 1.6677
[2023-10-04 21:33:23] iter = 04970, loss = 1.6407
[2023-10-04 21:33:24] iter = 04980, loss = 1.6288
[2023-10-04 21:33:25] iter = 04990, loss = 1.5855
[2023-10-04 21:33:26] iter = 05000, loss = 1.5889
[2023-10-04 21:33:27] iter = 05010, loss = 1.7346
[2023-10-04 21:33:28] iter = 05020, loss = 1.6304
[2023-10-04 21:33:29] iter = 05030, loss = 1.6755
[2023-10-04 21:33:30] iter = 05040, loss = 1.6594
[2023-10-04 21:33:31] iter = 05050, loss = 1.6105
[2023-10-04 21:33:31] iter = 05060, loss = 1.5993
[2023-10-04 21:33:33] iter = 05070, loss = 1.7533
[2023-10-04 21:33:33] iter = 05080, loss = 1.5117
[2023-10-04 21:33:34] iter = 05090, loss = 1.8205
[2023-10-04 21:33:35] iter = 05100, loss = 1.7268
[2023-10-04 21:33:36] iter = 05110, loss = 1.5748
[2023-10-04 21:33:37] iter = 05120, loss = 1.5957
[2023-10-04 21:33:38] iter = 05130, loss = 1.6797
[2023-10-04 21:33:39] iter = 05140, loss = 1.6338
[2023-10-04 21:33:40] iter = 05150, loss = 1.7703
[2023-10-04 21:33:41] iter = 05160, loss = 1.6294
[2023-10-04 21:33:42] iter = 05170, loss = 1.6296
[2023-10-04 21:33:43] iter = 05180, loss = 1.6797
[2023-10-04 21:33:44] iter = 05190, loss = 1.6456
[2023-10-04 21:33:45] iter = 05200, loss = 1.6090
[2023-10-04 21:33:45] iter = 05210, loss = 1.5199
[2023-10-04 21:33:46] iter = 05220, loss = 1.5395
[2023-10-04 21:33:47] iter = 05230, loss = 1.6174
[2023-10-04 21:33:48] iter = 05240, loss = 1.6256
[2023-10-04 21:33:49] iter = 05250, loss = 1.6621
[2023-10-04 21:33:50] iter = 05260, loss = 1.6744
[2023-10-04 21:33:51] iter = 05270, loss = 1.7856
[2023-10-04 21:33:52] iter = 05280, loss = 1.6803
[2023-10-04 21:33:53] iter = 05290, loss = 1.6370
[2023-10-04 21:33:54] iter = 05300, loss = 1.8274
[2023-10-04 21:33:55] iter = 05310, loss = 1.5982
[2023-10-04 21:33:56] iter = 05320, loss = 1.7777
[2023-10-04 21:33:57] iter = 05330, loss = 1.6736
[2023-10-04 21:33:58] iter = 05340, loss = 1.7094
[2023-10-04 21:33:59] iter = 05350, loss = 1.5891
[2023-10-04 21:34:00] iter = 05360, loss = 1.6727
[2023-10-04 21:34:01] iter = 05370, loss = 1.5782
[2023-10-04 21:34:02] iter = 05380, loss = 1.6159
[2023-10-04 21:34:02] iter = 05390, loss = 1.5336
[2023-10-04 21:34:03] iter = 05400, loss = 1.5857
[2023-10-04 21:34:04] iter = 05410, loss = 1.7117
[2023-10-04 21:34:05] iter = 05420, loss = 1.7608
[2023-10-04 21:34:06] iter = 05430, loss = 1.4794
[2023-10-04 21:34:07] iter = 05440, loss = 1.5173
[2023-10-04 21:34:08] iter = 05450, loss = 1.5954
[2023-10-04 21:34:09] iter = 05460, loss = 1.6780
[2023-10-04 21:34:10] iter = 05470, loss = 1.6251
[2023-10-04 21:34:11] iter = 05480, loss = 1.5697
[2023-10-04 21:34:12] iter = 05490, loss = 1.5017
[2023-10-04 21:34:13] iter = 05500, loss = 1.5419
[2023-10-04 21:34:14] iter = 05510, loss = 1.7049
[2023-10-04 21:34:15] iter = 05520, loss = 1.5914
[2023-10-04 21:34:16] iter = 05530, loss = 1.5280
[2023-10-04 21:34:17] iter = 05540, loss = 1.7122
[2023-10-04 21:34:17] iter = 05550, loss = 1.7396
[2023-10-04 21:34:18] iter = 05560, loss = 1.6435
[2023-10-04 21:34:19] iter = 05570, loss = 1.5336
[2023-10-04 21:34:20] iter = 05580, loss = 1.6885
[2023-10-04 21:34:21] iter = 05590, loss = 1.6046
[2023-10-04 21:34:22] iter = 05600, loss = 1.5511
[2023-10-04 21:34:23] iter = 05610, loss = 1.6629
[2023-10-04 21:34:24] iter = 05620, loss = 1.6849
[2023-10-04 21:34:24] iter = 05630, loss = 1.6343
[2023-10-04 21:34:25] iter = 05640, loss = 1.5230
[2023-10-04 21:34:26] iter = 05650, loss = 1.6324
[2023-10-04 21:34:27] iter = 05660, loss = 1.5696
[2023-10-04 21:34:28] iter = 05670, loss = 1.6054
[2023-10-04 21:34:29] iter = 05680, loss = 1.5772
[2023-10-04 21:34:30] iter = 05690, loss = 1.6418
[2023-10-04 21:34:31] iter = 05700, loss = 1.6436
[2023-10-04 21:34:32] iter = 05710, loss = 1.7047
[2023-10-04 21:34:33] iter = 05720, loss = 1.5491
[2023-10-04 21:34:34] iter = 05730, loss = 1.5998
[2023-10-04 21:34:35] iter = 05740, loss = 1.5631
[2023-10-04 21:34:36] iter = 05750, loss = 1.5133
[2023-10-04 21:34:37] iter = 05760, loss = 1.6821
[2023-10-04 21:34:37] iter = 05770, loss = 1.5933
[2023-10-04 21:34:38] iter = 05780, loss = 1.7743
[2023-10-04 21:34:39] iter = 05790, loss = 1.5799
[2023-10-04 21:34:40] iter = 05800, loss = 1.6253
[2023-10-04 21:34:41] iter = 05810, loss = 1.7579
[2023-10-04 21:34:42] iter = 05820, loss = 1.8535
[2023-10-04 21:34:43] iter = 05830, loss = 1.7216
[2023-10-04 21:34:44] iter = 05840, loss = 1.5755
[2023-10-04 21:34:45] iter = 05850, loss = 1.7518
[2023-10-04 21:34:46] iter = 05860, loss = 1.6284
[2023-10-04 21:34:47] iter = 05870, loss = 1.4986
[2023-10-04 21:34:48] iter = 05880, loss = 1.4867
[2023-10-04 21:34:48] iter = 05890, loss = 1.5693
[2023-10-04 21:34:49] iter = 05900, loss = 1.6583
[2023-10-04 21:34:50] iter = 05910, loss = 1.7324
[2023-10-04 21:34:51] iter = 05920, loss = 1.5606
[2023-10-04 21:34:52] iter = 05930, loss = 1.6927
[2023-10-04 21:34:53] iter = 05940, loss = 1.7406
[2023-10-04 21:34:54] iter = 05950, loss = 1.6264
[2023-10-04 21:34:55] iter = 05960, loss = 1.7329
[2023-10-04 21:34:56] iter = 05970, loss = 1.6299
[2023-10-04 21:34:57] iter = 05980, loss = 1.6233
[2023-10-04 21:34:58] iter = 05990, loss = 1.6656
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 99138}
[2023-10-04 21:35:23] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005858 train acc = 1.0000, test acc = 0.6152
[2023-10-04 21:35:48] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001646 train acc = 1.0000, test acc = 0.6173
[2023-10-04 21:36:12] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.026207 train acc = 0.9980, test acc = 0.6154
[2023-10-04 21:36:37] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001612 train acc = 1.0000, test acc = 0.6214
[2023-10-04 21:37:01] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.013154 train acc = 1.0000, test acc = 0.6143
[2023-10-04 21:37:26] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.009760 train acc = 1.0000, test acc = 0.6158
[2023-10-04 21:37:51] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012918 train acc = 0.9980, test acc = 0.6128
[2023-10-04 21:38:15] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.015400 train acc = 1.0000, test acc = 0.6093
[2023-10-04 21:38:40] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014628 train acc = 1.0000, test acc = 0.6083
[2023-10-04 21:39:04] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.001625 train acc = 1.0000, test acc = 0.6134
[2023-10-04 21:39:29] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004378 train acc = 1.0000, test acc = 0.6191
[2023-10-04 21:39:53] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.010109 train acc = 1.0000, test acc = 0.6062
[2023-10-04 21:40:18] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.017451 train acc = 1.0000, test acc = 0.6125
[2023-10-04 21:40:42] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.009929 train acc = 0.9980, test acc = 0.6152
[2023-10-04 21:41:07] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002460 train acc = 1.0000, test acc = 0.6248
[2023-10-04 21:41:31] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001603 train acc = 1.0000, test acc = 0.6157
[2023-10-04 21:41:56] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.010804 train acc = 1.0000, test acc = 0.6116
[2023-10-04 21:42:20] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.019377 train acc = 1.0000, test acc = 0.6130
[2023-10-04 21:42:45] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003179 train acc = 1.0000, test acc = 0.6050
[2023-10-04 21:43:10] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.001414 train acc = 1.0000, test acc = 0.6119
Evaluate 20 random ConvNet, mean = 0.6139 std = 0.0046
-------------------------
[2023-10-04 21:43:10] iter = 06000, loss = 1.7112
[2023-10-04 21:43:11] iter = 06010, loss = 1.5729
[2023-10-04 21:43:12] iter = 06020, loss = 1.6430
[2023-10-04 21:43:13] iter = 06030, loss = 1.5766
[2023-10-04 21:43:13] iter = 06040, loss = 1.6253
[2023-10-04 21:43:14] iter = 06050, loss = 1.5619
[2023-10-04 21:43:15] iter = 06060, loss = 1.6522
[2023-10-04 21:43:16] iter = 06070, loss = 1.7200
[2023-10-04 21:43:17] iter = 06080, loss = 1.8977
[2023-10-04 21:43:18] iter = 06090, loss = 1.6804
[2023-10-04 21:43:19] iter = 06100, loss = 1.6236
[2023-10-04 21:43:20] iter = 06110, loss = 1.6818
[2023-10-04 21:43:21] iter = 06120, loss = 1.6093
[2023-10-04 21:43:22] iter = 06130, loss = 1.6182
[2023-10-04 21:43:23] iter = 06140, loss = 1.5681
[2023-10-04 21:43:24] iter = 06150, loss = 1.6299
[2023-10-04 21:43:25] iter = 06160, loss = 1.5058
[2023-10-04 21:43:26] iter = 06170, loss = 1.7618
[2023-10-04 21:43:27] iter = 06180, loss = 1.5989
[2023-10-04 21:43:28] iter = 06190, loss = 1.5858
[2023-10-04 21:43:29] iter = 06200, loss = 1.7634
[2023-10-04 21:43:29] iter = 06210, loss = 1.6978
[2023-10-04 21:43:30] iter = 06220, loss = 1.6805
[2023-10-04 21:43:31] iter = 06230, loss = 1.8706
[2023-10-04 21:43:32] iter = 06240, loss = 1.6356
[2023-10-04 21:43:33] iter = 06250, loss = 1.5550
[2023-10-04 21:43:34] iter = 06260, loss = 1.5174
[2023-10-04 21:43:35] iter = 06270, loss = 1.6334
[2023-10-04 21:43:36] iter = 06280, loss = 1.5580
[2023-10-04 21:43:37] iter = 06290, loss = 1.6946
[2023-10-04 21:43:38] iter = 06300, loss = 1.6510
[2023-10-04 21:43:39] iter = 06310, loss = 1.7076
[2023-10-04 21:43:40] iter = 06320, loss = 1.6146
[2023-10-04 21:43:41] iter = 06330, loss = 1.6684
[2023-10-04 21:43:41] iter = 06340, loss = 1.6103
[2023-10-04 21:43:42] iter = 06350, loss = 1.6371
[2023-10-04 21:43:43] iter = 06360, loss = 1.7279
[2023-10-04 21:43:44] iter = 06370, loss = 1.6027
[2023-10-04 21:43:45] iter = 06380, loss = 1.7395
[2023-10-04 21:43:46] iter = 06390, loss = 1.6300
[2023-10-04 21:43:47] iter = 06400, loss = 1.5255
[2023-10-04 21:43:48] iter = 06410, loss = 1.4542
[2023-10-04 21:43:49] iter = 06420, loss = 1.5875
[2023-10-04 21:43:50] iter = 06430, loss = 1.6048
[2023-10-04 21:43:51] iter = 06440, loss = 1.6452
[2023-10-04 21:43:52] iter = 06450, loss = 1.5283
[2023-10-04 21:43:53] iter = 06460, loss = 1.6906
[2023-10-04 21:43:54] iter = 06470, loss = 1.5909
[2023-10-04 21:43:54] iter = 06480, loss = 1.5093
[2023-10-04 21:43:55] iter = 06490, loss = 1.5583
[2023-10-04 21:43:56] iter = 06500, loss = 1.6639
[2023-10-04 21:43:57] iter = 06510, loss = 1.6901
[2023-10-04 21:43:58] iter = 06520, loss = 1.6323
[2023-10-04 21:43:59] iter = 06530, loss = 1.5773
[2023-10-04 21:44:00] iter = 06540, loss = 1.4231
[2023-10-04 21:44:01] iter = 06550, loss = 1.4776
[2023-10-04 21:44:02] iter = 06560, loss = 1.7327
[2023-10-04 21:44:03] iter = 06570, loss = 1.5610
[2023-10-04 21:44:04] iter = 06580, loss = 1.4343
[2023-10-04 21:44:05] iter = 06590, loss = 1.5008
[2023-10-04 21:44:06] iter = 06600, loss = 1.5208
[2023-10-04 21:44:06] iter = 06610, loss = 1.4969
[2023-10-04 21:44:07] iter = 06620, loss = 1.6367
[2023-10-04 21:44:08] iter = 06630, loss = 1.6410
[2023-10-04 21:44:09] iter = 06640, loss = 1.5845
[2023-10-04 21:44:10] iter = 06650, loss = 1.5247
[2023-10-04 21:44:11] iter = 06660, loss = 1.5165
[2023-10-04 21:44:12] iter = 06670, loss = 1.7261
[2023-10-04 21:44:13] iter = 06680, loss = 1.5545
[2023-10-04 21:44:14] iter = 06690, loss = 1.7256
[2023-10-04 21:44:15] iter = 06700, loss = 1.4884
[2023-10-04 21:44:16] iter = 06710, loss = 1.6594
[2023-10-04 21:44:17] iter = 06720, loss = 1.4903
[2023-10-04 21:44:18] iter = 06730, loss = 1.5682
[2023-10-04 21:44:19] iter = 06740, loss = 1.7030
[2023-10-04 21:44:19] iter = 06750, loss = 1.6289
[2023-10-04 21:44:20] iter = 06760, loss = 1.5095
[2023-10-04 21:44:21] iter = 06770, loss = 1.6205
[2023-10-04 21:44:22] iter = 06780, loss = 1.6676
[2023-10-04 21:44:23] iter = 06790, loss = 1.5684
[2023-10-04 21:44:24] iter = 06800, loss = 1.4622
[2023-10-04 21:44:25] iter = 06810, loss = 1.6386
[2023-10-04 21:44:26] iter = 06820, loss = 1.6010
[2023-10-04 21:44:27] iter = 06830, loss = 1.5194
[2023-10-04 21:44:28] iter = 06840, loss = 1.7036
[2023-10-04 21:44:29] iter = 06850, loss = 1.7594
[2023-10-04 21:44:29] iter = 06860, loss = 1.5826
[2023-10-04 21:44:30] iter = 06870, loss = 1.5284
[2023-10-04 21:44:31] iter = 06880, loss = 1.5005
[2023-10-04 21:44:32] iter = 06890, loss = 1.4244
[2023-10-04 21:44:33] iter = 06900, loss = 1.5363
[2023-10-04 21:44:34] iter = 06910, loss = 1.6389
[2023-10-04 21:44:35] iter = 06920, loss = 1.5858
[2023-10-04 21:44:36] iter = 06930, loss = 1.8069
[2023-10-04 21:44:37] iter = 06940, loss = 1.5074
[2023-10-04 21:44:38] iter = 06950, loss = 1.5695
[2023-10-04 21:44:39] iter = 06960, loss = 1.5998
[2023-10-04 21:44:40] iter = 06970, loss = 1.7302
[2023-10-04 21:44:41] iter = 06980, loss = 1.5227
[2023-10-04 21:44:42] iter = 06990, loss = 1.7040
[2023-10-04 21:44:42] iter = 07000, loss = 1.5651
[2023-10-04 21:44:43] iter = 07010, loss = 1.4926
[2023-10-04 21:44:44] iter = 07020, loss = 1.5579
[2023-10-04 21:44:45] iter = 07030, loss = 1.6791
[2023-10-04 21:44:46] iter = 07040, loss = 1.5976
[2023-10-04 21:44:47] iter = 07050, loss = 1.7340
[2023-10-04 21:44:48] iter = 07060, loss = 1.5274
[2023-10-04 21:44:49] iter = 07070, loss = 1.5425
[2023-10-04 21:44:50] iter = 07080, loss = 1.7005
[2023-10-04 21:44:51] iter = 07090, loss = 1.6148
[2023-10-04 21:44:52] iter = 07100, loss = 1.4756
[2023-10-04 21:44:53] iter = 07110, loss = 1.4073
[2023-10-04 21:44:54] iter = 07120, loss = 1.4525
[2023-10-04 21:44:54] iter = 07130, loss = 1.5555
[2023-10-04 21:44:55] iter = 07140, loss = 1.7654
[2023-10-04 21:44:56] iter = 07150, loss = 1.5409
[2023-10-04 21:44:57] iter = 07160, loss = 1.7551
[2023-10-04 21:44:58] iter = 07170, loss = 1.6648
[2023-10-04 21:44:59] iter = 07180, loss = 1.4491
[2023-10-04 21:45:00] iter = 07190, loss = 1.5955
[2023-10-04 21:45:01] iter = 07200, loss = 1.6506
[2023-10-04 21:45:01] iter = 07210, loss = 1.6113
[2023-10-04 21:45:02] iter = 07220, loss = 1.5210
[2023-10-04 21:45:03] iter = 07230, loss = 1.7404
[2023-10-04 21:45:04] iter = 07240, loss = 1.5863
[2023-10-04 21:45:05] iter = 07250, loss = 1.5536
[2023-10-04 21:45:06] iter = 07260, loss = 1.5071
[2023-10-04 21:45:07] iter = 07270, loss = 1.7539
[2023-10-04 21:45:08] iter = 07280, loss = 1.6056
[2023-10-04 21:45:09] iter = 07290, loss = 1.6350
[2023-10-04 21:45:10] iter = 07300, loss = 1.6025
[2023-10-04 21:45:11] iter = 07310, loss = 1.5749
[2023-10-04 21:45:11] iter = 07320, loss = 1.4772
[2023-10-04 21:45:12] iter = 07330, loss = 1.6136
[2023-10-04 21:45:13] iter = 07340, loss = 1.4893
[2023-10-04 21:45:14] iter = 07350, loss = 1.5673
[2023-10-04 21:45:15] iter = 07360, loss = 1.5805
[2023-10-04 21:45:16] iter = 07370, loss = 1.4692
[2023-10-04 21:45:17] iter = 07380, loss = 1.4886
[2023-10-04 21:45:18] iter = 07390, loss = 1.6702
[2023-10-04 21:45:19] iter = 07400, loss = 1.5012
[2023-10-04 21:45:20] iter = 07410, loss = 1.6176
[2023-10-04 21:45:21] iter = 07420, loss = 1.5950
[2023-10-04 21:45:22] iter = 07430, loss = 1.6709
[2023-10-04 21:45:23] iter = 07440, loss = 1.5438
[2023-10-04 21:45:24] iter = 07450, loss = 1.5345
[2023-10-04 21:45:24] iter = 07460, loss = 1.4639
[2023-10-04 21:45:25] iter = 07470, loss = 1.6802
[2023-10-04 21:45:26] iter = 07480, loss = 1.8063
[2023-10-04 21:45:27] iter = 07490, loss = 1.5848
[2023-10-04 21:45:28] iter = 07500, loss = 1.6630
[2023-10-04 21:45:29] iter = 07510, loss = 1.6405
[2023-10-04 21:45:30] iter = 07520, loss = 1.6356
[2023-10-04 21:45:31] iter = 07530, loss = 1.4618
[2023-10-04 21:45:32] iter = 07540, loss = 1.5610
[2023-10-04 21:45:33] iter = 07550, loss = 1.5721
[2023-10-04 21:45:34] iter = 07560, loss = 1.4583
[2023-10-04 21:45:34] iter = 07570, loss = 1.6267
[2023-10-04 21:45:35] iter = 07580, loss = 1.7552
[2023-10-04 21:45:36] iter = 07590, loss = 1.6475
[2023-10-04 21:45:37] iter = 07600, loss = 1.6852
[2023-10-04 21:45:38] iter = 07610, loss = 1.5907
[2023-10-04 21:45:39] iter = 07620, loss = 1.7268
[2023-10-04 21:45:40] iter = 07630, loss = 1.5230
[2023-10-04 21:45:41] iter = 07640, loss = 1.6808
[2023-10-04 21:45:42] iter = 07650, loss = 1.7916
[2023-10-04 21:45:43] iter = 07660, loss = 1.6865
[2023-10-04 21:45:44] iter = 07670, loss = 1.6538
[2023-10-04 21:45:45] iter = 07680, loss = 1.5790
[2023-10-04 21:45:46] iter = 07690, loss = 1.4009
[2023-10-04 21:45:46] iter = 07700, loss = 1.6523
[2023-10-04 21:45:47] iter = 07710, loss = 1.5618
[2023-10-04 21:45:48] iter = 07720, loss = 1.6089
[2023-10-04 21:45:49] iter = 07730, loss = 1.4339
[2023-10-04 21:45:50] iter = 07740, loss = 1.6676
[2023-10-04 21:45:51] iter = 07750, loss = 1.5636
[2023-10-04 21:45:52] iter = 07760, loss = 1.4985
[2023-10-04 21:45:53] iter = 07770, loss = 1.5342
[2023-10-04 21:45:54] iter = 07780, loss = 1.5995
[2023-10-04 21:45:55] iter = 07790, loss = 1.7270
[2023-10-04 21:45:55] iter = 07800, loss = 1.6536
[2023-10-04 21:45:57] iter = 07810, loss = 1.5556
[2023-10-04 21:45:57] iter = 07820, loss = 1.5530
[2023-10-04 21:45:58] iter = 07830, loss = 1.5984
[2023-10-04 21:45:59] iter = 07840, loss = 1.6051
[2023-10-04 21:46:00] iter = 07850, loss = 1.6324
[2023-10-04 21:46:01] iter = 07860, loss = 1.5763
[2023-10-04 21:46:02] iter = 07870, loss = 1.5558
[2023-10-04 21:46:03] iter = 07880, loss = 1.4741
[2023-10-04 21:46:04] iter = 07890, loss = 1.7124
[2023-10-04 21:46:05] iter = 07900, loss = 1.4241
[2023-10-04 21:46:06] iter = 07910, loss = 1.4218
[2023-10-04 21:46:07] iter = 07920, loss = 1.5464
[2023-10-04 21:46:08] iter = 07930, loss = 1.5986
[2023-10-04 21:46:08] iter = 07940, loss = 1.6790
[2023-10-04 21:46:09] iter = 07950, loss = 1.6312
[2023-10-04 21:46:10] iter = 07960, loss = 1.6919
[2023-10-04 21:46:11] iter = 07970, loss = 1.6802
[2023-10-04 21:46:12] iter = 07980, loss = 1.5505
[2023-10-04 21:46:13] iter = 07990, loss = 1.7880
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 74067}
[2023-10-04 21:46:38] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.001779 train acc = 1.0000, test acc = 0.6131
[2023-10-04 21:47:03] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006424 train acc = 1.0000, test acc = 0.6158
[2023-10-04 21:47:27] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003625 train acc = 1.0000, test acc = 0.6235
[2023-10-04 21:47:52] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.009840 train acc = 1.0000, test acc = 0.6207
[2023-10-04 21:48:16] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002121 train acc = 1.0000, test acc = 0.6191
[2023-10-04 21:48:41] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.014881 train acc = 0.9980, test acc = 0.6223
[2023-10-04 21:49:05] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.010403 train acc = 0.9980, test acc = 0.6141
[2023-10-04 21:49:30] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.020989 train acc = 0.9960, test acc = 0.6133
[2023-10-04 21:49:55] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.010186 train acc = 1.0000, test acc = 0.6182
[2023-10-04 21:50:19] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.014615 train acc = 1.0000, test acc = 0.6218
[2023-10-04 21:50:44] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.010185 train acc = 1.0000, test acc = 0.6161
[2023-10-04 21:51:08] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002131 train acc = 1.0000, test acc = 0.6221
[2023-10-04 21:51:32] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.012084 train acc = 1.0000, test acc = 0.6195
[2023-10-04 21:51:57] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.007967 train acc = 1.0000, test acc = 0.6226
[2023-10-04 21:52:21] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001984 train acc = 1.0000, test acc = 0.6160
[2023-10-04 21:52:46] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003309 train acc = 1.0000, test acc = 0.6168
[2023-10-04 21:53:10] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003440 train acc = 1.0000, test acc = 0.6204
[2023-10-04 21:53:35] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.015818 train acc = 0.9980, test acc = 0.6239
[2023-10-04 21:54:00] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004306 train acc = 1.0000, test acc = 0.6137
[2023-10-04 21:54:24] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004019 train acc = 1.0000, test acc = 0.6174
Evaluate 20 random ConvNet, mean = 0.6185 std = 0.0035
-------------------------
[2023-10-04 21:54:24] iter = 08000, loss = 1.5637
[2023-10-04 21:54:25] iter = 08010, loss = 1.5924
[2023-10-04 21:54:26] iter = 08020, loss = 1.6368
[2023-10-04 21:54:27] iter = 08030, loss = 1.5662
[2023-10-04 21:54:28] iter = 08040, loss = 1.3949
[2023-10-04 21:54:29] iter = 08050, loss = 1.5291
[2023-10-04 21:54:30] iter = 08060, loss = 1.4031
[2023-10-04 21:54:31] iter = 08070, loss = 1.4451
[2023-10-04 21:54:32] iter = 08080, loss = 1.5080
[2023-10-04 21:54:33] iter = 08090, loss = 1.5838
[2023-10-04 21:54:34] iter = 08100, loss = 1.5029
[2023-10-04 21:54:35] iter = 08110, loss = 1.5774
[2023-10-04 21:54:36] iter = 08120, loss = 1.5278
[2023-10-04 21:54:37] iter = 08130, loss = 1.6010
[2023-10-04 21:54:37] iter = 08140, loss = 1.5684
[2023-10-04 21:54:38] iter = 08150, loss = 1.5849
[2023-10-04 21:54:39] iter = 08160, loss = 1.6094
[2023-10-04 21:54:40] iter = 08170, loss = 1.5806
[2023-10-04 21:54:41] iter = 08180, loss = 1.5919
[2023-10-04 21:54:42] iter = 08190, loss = 1.5659
[2023-10-04 21:54:43] iter = 08200, loss = 1.5972
[2023-10-04 21:54:44] iter = 08210, loss = 1.5503
[2023-10-04 21:54:45] iter = 08220, loss = 1.5760
[2023-10-04 21:54:46] iter = 08230, loss = 1.5168
[2023-10-04 21:54:47] iter = 08240, loss = 1.5817
[2023-10-04 21:54:48] iter = 08250, loss = 1.5718
[2023-10-04 21:54:49] iter = 08260, loss = 1.5629
[2023-10-04 21:54:49] iter = 08270, loss = 1.6278
[2023-10-04 21:54:50] iter = 08280, loss = 1.5548
[2023-10-04 21:54:51] iter = 08290, loss = 1.5004
[2023-10-04 21:54:52] iter = 08300, loss = 1.5141
[2023-10-04 21:54:53] iter = 08310, loss = 1.6135
[2023-10-04 21:54:54] iter = 08320, loss = 1.6529
[2023-10-04 21:54:55] iter = 08330, loss = 1.5391
[2023-10-04 21:54:56] iter = 08340, loss = 1.6179
[2023-10-04 21:54:57] iter = 08350, loss = 1.5854
[2023-10-04 21:54:58] iter = 08360, loss = 1.6299
[2023-10-04 21:54:59] iter = 08370, loss = 1.3892
[2023-10-04 21:55:00] iter = 08380, loss = 1.5272
[2023-10-04 21:55:01] iter = 08390, loss = 1.4895
[2023-10-04 21:55:01] iter = 08400, loss = 1.5771
[2023-10-04 21:55:02] iter = 08410, loss = 1.4851
[2023-10-04 21:55:03] iter = 08420, loss = 1.7072
[2023-10-04 21:55:04] iter = 08430, loss = 1.5503
[2023-10-04 21:55:05] iter = 08440, loss = 1.4446
[2023-10-04 21:55:06] iter = 08450, loss = 1.5962
[2023-10-04 21:55:07] iter = 08460, loss = 1.4023
[2023-10-04 21:55:08] iter = 08470, loss = 1.6319
[2023-10-04 21:55:09] iter = 08480, loss = 1.5461
[2023-10-04 21:55:10] iter = 08490, loss = 1.6104
[2023-10-04 21:55:11] iter = 08500, loss = 1.6804
[2023-10-04 21:55:12] iter = 08510, loss = 1.6678
[2023-10-04 21:55:13] iter = 08520, loss = 1.7027
[2023-10-04 21:55:14] iter = 08530, loss = 1.7405
[2023-10-04 21:55:14] iter = 08540, loss = 1.6398
[2023-10-04 21:55:15] iter = 08550, loss = 1.5682
[2023-10-04 21:55:16] iter = 08560, loss = 1.5301
[2023-10-04 21:55:17] iter = 08570, loss = 1.7075
[2023-10-04 21:55:18] iter = 08580, loss = 1.6499
[2023-10-04 21:55:19] iter = 08590, loss = 1.5703
[2023-10-04 21:55:20] iter = 08600, loss = 1.7295
[2023-10-04 21:55:21] iter = 08610, loss = 1.5265
[2023-10-04 21:55:22] iter = 08620, loss = 1.4425
[2023-10-04 21:55:23] iter = 08630, loss = 1.5157
[2023-10-04 21:55:24] iter = 08640, loss = 1.4576
[2023-10-04 21:55:25] iter = 08650, loss = 1.8426
[2023-10-04 21:55:25] iter = 08660, loss = 1.5558
[2023-10-04 21:55:26] iter = 08670, loss = 1.5739
[2023-10-04 21:55:27] iter = 08680, loss = 1.6279
[2023-10-04 21:55:28] iter = 08690, loss = 1.4783
[2023-10-04 21:55:29] iter = 08700, loss = 1.4821
[2023-10-04 21:55:30] iter = 08710, loss = 1.5707
[2023-10-04 21:55:31] iter = 08720, loss = 1.6188
[2023-10-04 21:55:32] iter = 08730, loss = 1.5728
[2023-10-04 21:55:33] iter = 08740, loss = 1.4676
[2023-10-04 21:55:34] iter = 08750, loss = 1.4831
[2023-10-04 21:55:35] iter = 08760, loss = 1.7829
[2023-10-04 21:55:36] iter = 08770, loss = 1.3982
[2023-10-04 21:55:37] iter = 08780, loss = 1.6538
[2023-10-04 21:55:38] iter = 08790, loss = 1.6193
[2023-10-04 21:55:39] iter = 08800, loss = 1.4698
[2023-10-04 21:55:40] iter = 08810, loss = 1.6439
[2023-10-04 21:55:40] iter = 08820, loss = 1.6055
[2023-10-04 21:55:41] iter = 08830, loss = 1.6277
[2023-10-04 21:55:42] iter = 08840, loss = 1.5023
[2023-10-04 21:55:43] iter = 08850, loss = 1.5981
[2023-10-04 21:55:44] iter = 08860, loss = 1.6534
[2023-10-04 21:55:45] iter = 08870, loss = 1.6224
[2023-10-04 21:55:46] iter = 08880, loss = 1.5448
[2023-10-04 21:55:47] iter = 08890, loss = 1.5173
[2023-10-04 21:55:48] iter = 08900, loss = 1.5945
[2023-10-04 21:55:48] iter = 08910, loss = 1.5123
[2023-10-04 21:55:49] iter = 08920, loss = 1.4560
[2023-10-04 21:55:50] iter = 08930, loss = 1.5255
[2023-10-04 21:55:51] iter = 08940, loss = 1.5488
[2023-10-04 21:55:52] iter = 08950, loss = 1.5176
[2023-10-04 21:55:53] iter = 08960, loss = 1.5150
[2023-10-04 21:55:54] iter = 08970, loss = 1.6466
[2023-10-04 21:55:55] iter = 08980, loss = 1.5675
[2023-10-04 21:55:56] iter = 08990, loss = 1.6048
[2023-10-04 21:55:57] iter = 09000, loss = 1.6816
[2023-10-04 21:55:58] iter = 09010, loss = 1.6082
[2023-10-04 21:55:58] iter = 09020, loss = 1.6157
[2023-10-04 21:55:59] iter = 09030, loss = 1.6399
[2023-10-04 21:56:00] iter = 09040, loss = 1.4732
[2023-10-04 21:56:01] iter = 09050, loss = 1.5863
[2023-10-04 21:56:02] iter = 09060, loss = 1.5948
[2023-10-04 21:56:03] iter = 09070, loss = 1.4520
[2023-10-04 21:56:04] iter = 09080, loss = 1.6784
[2023-10-04 21:56:05] iter = 09090, loss = 1.6324
[2023-10-04 21:56:06] iter = 09100, loss = 1.5095
[2023-10-04 21:56:07] iter = 09110, loss = 1.5109
[2023-10-04 21:56:08] iter = 09120, loss = 1.7161
[2023-10-04 21:56:08] iter = 09130, loss = 1.5631
[2023-10-04 21:56:09] iter = 09140, loss = 1.5304
[2023-10-04 21:56:10] iter = 09150, loss = 1.5711
[2023-10-04 21:56:11] iter = 09160, loss = 1.4967
[2023-10-04 21:56:12] iter = 09170, loss = 1.4806
[2023-10-04 21:56:13] iter = 09180, loss = 1.6019
[2023-10-04 21:56:14] iter = 09190, loss = 1.5894
[2023-10-04 21:56:15] iter = 09200, loss = 1.4914
[2023-10-04 21:56:16] iter = 09210, loss = 1.6066
[2023-10-04 21:56:17] iter = 09220, loss = 1.4994
[2023-10-04 21:56:18] iter = 09230, loss = 1.5975
[2023-10-04 21:56:19] iter = 09240, loss = 1.5537
[2023-10-04 21:56:20] iter = 09250, loss = 1.5178
[2023-10-04 21:56:21] iter = 09260, loss = 1.4891
[2023-10-04 21:56:22] iter = 09270, loss = 1.6332
[2023-10-04 21:56:23] iter = 09280, loss = 1.6250
[2023-10-04 21:56:23] iter = 09290, loss = 1.4690
[2023-10-04 21:56:24] iter = 09300, loss = 1.6115
[2023-10-04 21:56:25] iter = 09310, loss = 1.5124
[2023-10-04 21:56:26] iter = 09320, loss = 1.5020
[2023-10-04 21:56:27] iter = 09330, loss = 1.6547
[2023-10-04 21:56:28] iter = 09340, loss = 1.5044
[2023-10-04 21:56:29] iter = 09350, loss = 1.3966
[2023-10-04 21:56:30] iter = 09360, loss = 1.4784
[2023-10-04 21:56:31] iter = 09370, loss = 1.6904
[2023-10-04 21:56:32] iter = 09380, loss = 1.5775
[2023-10-04 21:56:32] iter = 09390, loss = 1.5972
[2023-10-04 21:56:33] iter = 09400, loss = 1.4921
[2023-10-04 21:56:34] iter = 09410, loss = 1.4435
[2023-10-04 21:56:35] iter = 09420, loss = 1.5798
[2023-10-04 21:56:36] iter = 09430, loss = 1.5980
[2023-10-04 21:56:37] iter = 09440, loss = 1.5875
[2023-10-04 21:56:38] iter = 09450, loss = 1.5555
[2023-10-04 21:56:39] iter = 09460, loss = 1.4756
[2023-10-04 21:56:40] iter = 09470, loss = 1.6232
[2023-10-04 21:56:41] iter = 09480, loss = 1.5078
[2023-10-04 21:56:42] iter = 09490, loss = 1.5370
[2023-10-04 21:56:43] iter = 09500, loss = 1.5058
[2023-10-04 21:56:44] iter = 09510, loss = 1.7669
[2023-10-04 21:56:45] iter = 09520, loss = 1.4751
[2023-10-04 21:56:45] iter = 09530, loss = 1.4921
[2023-10-04 21:56:46] iter = 09540, loss = 1.4915
[2023-10-04 21:56:47] iter = 09550, loss = 1.5944
[2023-10-04 21:56:48] iter = 09560, loss = 1.5490
[2023-10-04 21:56:49] iter = 09570, loss = 1.5331
[2023-10-04 21:56:50] iter = 09580, loss = 1.5821
[2023-10-04 21:56:51] iter = 09590, loss = 1.5866
[2023-10-04 21:56:52] iter = 09600, loss = 1.7058
[2023-10-04 21:56:53] iter = 09610, loss = 1.5689
[2023-10-04 21:56:54] iter = 09620, loss = 1.6079
[2023-10-04 21:56:55] iter = 09630, loss = 1.5057
[2023-10-04 21:56:55] iter = 09640, loss = 1.5632
[2023-10-04 21:56:56] iter = 09650, loss = 1.5685
[2023-10-04 21:56:58] iter = 09660, loss = 1.6289
[2023-10-04 21:56:59] iter = 09670, loss = 1.5788
[2023-10-04 21:56:59] iter = 09680, loss = 1.4776
[2023-10-04 21:57:00] iter = 09690, loss = 1.5150
[2023-10-04 21:57:01] iter = 09700, loss = 1.5709
[2023-10-04 21:57:02] iter = 09710, loss = 1.5170
[2023-10-04 21:57:03] iter = 09720, loss = 1.6503
[2023-10-04 21:57:04] iter = 09730, loss = 1.4528
[2023-10-04 21:57:05] iter = 09740, loss = 1.5531
[2023-10-04 21:57:06] iter = 09750, loss = 1.5847
[2023-10-04 21:57:07] iter = 09760, loss = 1.5539
[2023-10-04 21:57:08] iter = 09770, loss = 1.5896
[2023-10-04 21:57:09] iter = 09780, loss = 1.5098
[2023-10-04 21:57:10] iter = 09790, loss = 1.4785
[2023-10-04 21:57:10] iter = 09800, loss = 1.6030
[2023-10-04 21:57:11] iter = 09810, loss = 1.5256
[2023-10-04 21:57:12] iter = 09820, loss = 1.5955
[2023-10-04 21:57:13] iter = 09830, loss = 1.6381
[2023-10-04 21:57:14] iter = 09840, loss = 1.6014
[2023-10-04 21:57:15] iter = 09850, loss = 1.6520
[2023-10-04 21:57:16] iter = 09860, loss = 1.7177
[2023-10-04 21:57:17] iter = 09870, loss = 1.5250
[2023-10-04 21:57:18] iter = 09880, loss = 1.7209
[2023-10-04 21:57:19] iter = 09890, loss = 1.7262
[2023-10-04 21:57:20] iter = 09900, loss = 1.6912
[2023-10-04 21:57:21] iter = 09910, loss = 1.6611
[2023-10-04 21:57:22] iter = 09920, loss = 1.5350
[2023-10-04 21:57:22] iter = 09930, loss = 1.6454
[2023-10-04 21:57:24] iter = 09940, loss = 1.4731
[2023-10-04 21:57:24] iter = 09950, loss = 1.5845
[2023-10-04 21:57:25] iter = 09960, loss = 1.5272
[2023-10-04 21:57:26] iter = 09970, loss = 1.5400
[2023-10-04 21:57:27] iter = 09980, loss = 1.6855
[2023-10-04 21:57:28] iter = 09990, loss = 1.6903
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 49294}
[2023-10-04 21:57:54] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.009947 train acc = 1.0000, test acc = 0.6144
[2023-10-04 21:58:18] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003839 train acc = 1.0000, test acc = 0.6250
[2023-10-04 21:58:43] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.013156 train acc = 1.0000, test acc = 0.6246
[2023-10-04 21:59:07] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.010679 train acc = 1.0000, test acc = 0.6200
[2023-10-04 21:59:32] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004299 train acc = 1.0000, test acc = 0.6250
[2023-10-04 21:59:56] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.005656 train acc = 1.0000, test acc = 0.6172
[2023-10-04 22:00:21] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016796 train acc = 1.0000, test acc = 0.6259
[2023-10-04 22:00:45] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003679 train acc = 1.0000, test acc = 0.6266
[2023-10-04 22:01:10] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.005311 train acc = 1.0000, test acc = 0.6229
[2023-10-04 22:01:34] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003423 train acc = 1.0000, test acc = 0.6241
[2023-10-04 22:01:59] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002165 train acc = 1.0000, test acc = 0.6180
[2023-10-04 22:02:24] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.019518 train acc = 1.0000, test acc = 0.6154
[2023-10-04 22:02:48] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004471 train acc = 1.0000, test acc = 0.6220
[2023-10-04 22:03:13] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.014799 train acc = 1.0000, test acc = 0.6200
[2023-10-04 22:03:37] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.009198 train acc = 1.0000, test acc = 0.6243
[2023-10-04 22:04:02] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.006860 train acc = 1.0000, test acc = 0.6225
[2023-10-04 22:04:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.016332 train acc = 0.9980, test acc = 0.6280
[2023-10-04 22:04:51] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.016049 train acc = 1.0000, test acc = 0.6190
[2023-10-04 22:05:16] Evaluate_18: epoch = 1000 train time = 23 s train loss = 0.017362 train acc = 1.0000, test acc = 0.6292
[2023-10-04 22:05:40] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.016795 train acc = 1.0000, test acc = 0.6173
Evaluate 20 random ConvNet, mean = 0.6221 std = 0.0041
-------------------------
[2023-10-04 22:05:41] iter = 10000, loss = 1.4167
[2023-10-04 22:05:42] iter = 10010, loss = 1.5029
[2023-10-04 22:05:43] iter = 10020, loss = 1.6218
[2023-10-04 22:05:44] iter = 10030, loss = 1.4188
[2023-10-04 22:05:44] iter = 10040, loss = 1.6604
[2023-10-04 22:05:45] iter = 10050, loss = 1.5599
[2023-10-04 22:05:46] iter = 10060, loss = 1.7530
[2023-10-04 22:05:47] iter = 10070, loss = 1.4261
[2023-10-04 22:05:48] iter = 10080, loss = 1.6973
[2023-10-04 22:05:49] iter = 10090, loss = 1.4485
[2023-10-04 22:05:50] iter = 10100, loss = 1.5426
[2023-10-04 22:05:51] iter = 10110, loss = 1.6180
[2023-10-04 22:05:52] iter = 10120, loss = 1.4820
[2023-10-04 22:05:53] iter = 10130, loss = 1.5433
[2023-10-04 22:05:54] iter = 10140, loss = 1.5380
[2023-10-04 22:05:55] iter = 10150, loss = 1.5902
[2023-10-04 22:05:55] iter = 10160, loss = 1.8188
[2023-10-04 22:05:56] iter = 10170, loss = 1.6612
[2023-10-04 22:05:57] iter = 10180, loss = 1.5502
[2023-10-04 22:05:58] iter = 10190, loss = 1.5882
[2023-10-04 22:05:59] iter = 10200, loss = 1.5445
[2023-10-04 22:06:00] iter = 10210, loss = 1.5781
[2023-10-04 22:06:01] iter = 10220, loss = 1.7270
[2023-10-04 22:06:02] iter = 10230, loss = 1.5950
[2023-10-04 22:06:03] iter = 10240, loss = 1.6295
[2023-10-04 22:06:04] iter = 10250, loss = 1.4972
[2023-10-04 22:06:04] iter = 10260, loss = 1.7538
[2023-10-04 22:06:05] iter = 10270, loss = 1.5005
[2023-10-04 22:06:06] iter = 10280, loss = 1.5586
[2023-10-04 22:06:07] iter = 10290, loss = 1.3922
[2023-10-04 22:06:08] iter = 10300, loss = 1.5489
[2023-10-04 22:06:09] iter = 10310, loss = 1.3664
[2023-10-04 22:06:10] iter = 10320, loss = 1.5454
[2023-10-04 22:06:11] iter = 10330, loss = 1.6626
[2023-10-04 22:06:12] iter = 10340, loss = 1.4170
[2023-10-04 22:06:13] iter = 10350, loss = 1.4737
[2023-10-04 22:06:14] iter = 10360, loss = 1.5714
[2023-10-04 22:06:15] iter = 10370, loss = 1.5327
[2023-10-04 22:06:15] iter = 10380, loss = 1.5994
[2023-10-04 22:06:16] iter = 10390, loss = 1.5962
[2023-10-04 22:06:17] iter = 10400, loss = 1.5625
[2023-10-04 22:06:18] iter = 10410, loss = 1.5059
[2023-10-04 22:06:19] iter = 10420, loss = 1.4457
[2023-10-04 22:06:20] iter = 10430, loss = 1.5216
[2023-10-04 22:06:21] iter = 10440, loss = 1.5222
[2023-10-04 22:06:22] iter = 10450, loss = 1.4402
[2023-10-04 22:06:23] iter = 10460, loss = 1.5662
[2023-10-04 22:06:24] iter = 10470, loss = 1.6628
[2023-10-04 22:06:25] iter = 10480, loss = 1.5360
[2023-10-04 22:06:26] iter = 10490, loss = 1.4691
[2023-10-04 22:06:27] iter = 10500, loss = 1.4346
[2023-10-04 22:06:28] iter = 10510, loss = 1.5889
[2023-10-04 22:06:29] iter = 10520, loss = 1.5369
[2023-10-04 22:06:29] iter = 10530, loss = 1.5521
[2023-10-04 22:06:30] iter = 10540, loss = 1.7739
[2023-10-04 22:06:31] iter = 10550, loss = 1.5282
[2023-10-04 22:06:32] iter = 10560, loss = 1.4940
[2023-10-04 22:06:33] iter = 10570, loss = 1.5387
[2023-10-04 22:06:34] iter = 10580, loss = 1.4888
[2023-10-04 22:06:35] iter = 10590, loss = 1.5666
[2023-10-04 22:06:36] iter = 10600, loss = 1.5413
[2023-10-04 22:06:37] iter = 10610, loss = 1.5439
[2023-10-04 22:06:38] iter = 10620, loss = 1.3563
[2023-10-04 22:06:39] iter = 10630, loss = 1.6005
[2023-10-04 22:06:40] iter = 10640, loss = 1.5093
[2023-10-04 22:06:41] iter = 10650, loss = 1.4482
[2023-10-04 22:06:42] iter = 10660, loss = 1.5596
[2023-10-04 22:06:43] iter = 10670, loss = 1.4542
[2023-10-04 22:06:44] iter = 10680, loss = 1.5631
[2023-10-04 22:06:45] iter = 10690, loss = 1.4548
[2023-10-04 22:06:46] iter = 10700, loss = 1.5698
[2023-10-04 22:06:47] iter = 10710, loss = 1.5132
[2023-10-04 22:06:47] iter = 10720, loss = 1.5535
[2023-10-04 22:06:48] iter = 10730, loss = 1.5325
[2023-10-04 22:06:49] iter = 10740, loss = 1.5269
[2023-10-04 22:06:50] iter = 10750, loss = 1.5536
[2023-10-04 22:06:51] iter = 10760, loss = 1.4706
[2023-10-04 22:06:52] iter = 10770, loss = 1.5505
[2023-10-04 22:06:53] iter = 10780, loss = 1.7049
[2023-10-04 22:06:54] iter = 10790, loss = 1.4647
[2023-10-04 22:06:55] iter = 10800, loss = 1.5917
[2023-10-04 22:06:56] iter = 10810, loss = 1.4769
[2023-10-04 22:06:57] iter = 10820, loss = 1.6205
[2023-10-04 22:06:57] iter = 10830, loss = 1.5477
[2023-10-04 22:06:58] iter = 10840, loss = 1.4967
[2023-10-04 22:06:59] iter = 10850, loss = 1.5775
[2023-10-04 22:07:00] iter = 10860, loss = 1.5515
[2023-10-04 22:07:01] iter = 10870, loss = 1.4763
[2023-10-04 22:07:02] iter = 10880, loss = 1.5930
[2023-10-04 22:07:03] iter = 10890, loss = 1.4377
[2023-10-04 22:07:04] iter = 10900, loss = 1.5750
[2023-10-04 22:07:05] iter = 10910, loss = 1.4569
[2023-10-04 22:07:05] iter = 10920, loss = 1.5557
[2023-10-04 22:07:06] iter = 10930, loss = 1.7631
[2023-10-04 22:07:07] iter = 10940, loss = 1.4310
[2023-10-04 22:07:08] iter = 10950, loss = 1.6541
[2023-10-04 22:07:09] iter = 10960, loss = 1.5548
[2023-10-04 22:07:10] iter = 10970, loss = 1.5418
[2023-10-04 22:07:11] iter = 10980, loss = 1.5839
[2023-10-04 22:07:12] iter = 10990, loss = 1.5775
[2023-10-04 22:07:13] iter = 11000, loss = 1.5540
[2023-10-04 22:07:14] iter = 11010, loss = 1.5489
[2023-10-04 22:07:15] iter = 11020, loss = 1.5580
[2023-10-04 22:07:16] iter = 11030, loss = 1.6170
[2023-10-04 22:07:17] iter = 11040, loss = 1.5991
[2023-10-04 22:07:18] iter = 11050, loss = 1.5975
[2023-10-04 22:07:19] iter = 11060, loss = 1.4405
[2023-10-04 22:07:20] iter = 11070, loss = 1.6229
[2023-10-04 22:07:20] iter = 11080, loss = 1.6145
[2023-10-04 22:07:21] iter = 11090, loss = 1.6459
[2023-10-04 22:07:22] iter = 11100, loss = 1.5127
[2023-10-04 22:07:23] iter = 11110, loss = 1.4645
[2023-10-04 22:07:24] iter = 11120, loss = 1.4131
[2023-10-04 22:07:25] iter = 11130, loss = 1.4843
[2023-10-04 22:07:26] iter = 11140, loss = 1.6631
[2023-10-04 22:07:27] iter = 11150, loss = 1.4625
[2023-10-04 22:07:28] iter = 11160, loss = 1.7572
[2023-10-04 22:07:29] iter = 11170, loss = 1.4047
[2023-10-04 22:07:30] iter = 11180, loss = 1.6476
[2023-10-04 22:07:30] iter = 11190, loss = 1.4154
[2023-10-04 22:07:31] iter = 11200, loss = 1.6907
[2023-10-04 22:07:32] iter = 11210, loss = 1.5191
[2023-10-04 22:07:33] iter = 11220, loss = 1.5775
[2023-10-04 22:07:34] iter = 11230, loss = 1.4212
[2023-10-04 22:07:35] iter = 11240, loss = 1.5024
[2023-10-04 22:07:36] iter = 11250, loss = 1.5429
[2023-10-04 22:07:37] iter = 11260, loss = 1.5764
[2023-10-04 22:07:38] iter = 11270, loss = 1.4684
[2023-10-04 22:07:39] iter = 11280, loss = 1.5411
[2023-10-04 22:07:39] iter = 11290, loss = 1.4760
[2023-10-04 22:07:40] iter = 11300, loss = 1.6123
[2023-10-04 22:07:41] iter = 11310, loss = 1.5271
[2023-10-04 22:07:42] iter = 11320, loss = 1.5401
[2023-10-04 22:07:43] iter = 11330, loss = 1.6158
[2023-10-04 22:07:44] iter = 11340, loss = 1.5913
[2023-10-04 22:07:45] iter = 11350, loss = 1.4122
[2023-10-04 22:07:46] iter = 11360, loss = 1.5264
[2023-10-04 22:07:47] iter = 11370, loss = 1.4584
[2023-10-04 22:07:48] iter = 11380, loss = 1.4969
[2023-10-04 22:07:49] iter = 11390, loss = 1.4626
[2023-10-04 22:07:50] iter = 11400, loss = 1.4317
[2023-10-04 22:07:50] iter = 11410, loss = 1.6681
[2023-10-04 22:07:51] iter = 11420, loss = 1.6646
[2023-10-04 22:07:52] iter = 11430, loss = 1.4885
[2023-10-04 22:07:53] iter = 11440, loss = 1.4406
[2023-10-04 22:07:54] iter = 11450, loss = 1.4562
[2023-10-04 22:07:55] iter = 11460, loss = 1.5215
[2023-10-04 22:07:56] iter = 11470, loss = 1.5087
[2023-10-04 22:07:57] iter = 11480, loss = 1.5199
[2023-10-04 22:07:58] iter = 11490, loss = 1.5249
[2023-10-04 22:07:59] iter = 11500, loss = 1.7128
[2023-10-04 22:08:00] iter = 11510, loss = 1.4539
[2023-10-04 22:08:00] iter = 11520, loss = 1.3923
[2023-10-04 22:08:01] iter = 11530, loss = 1.5000
[2023-10-04 22:08:02] iter = 11540, loss = 1.4699
[2023-10-04 22:08:03] iter = 11550, loss = 1.5198
[2023-10-04 22:08:04] iter = 11560, loss = 1.6454
[2023-10-04 22:08:05] iter = 11570, loss = 1.5958
[2023-10-04 22:08:06] iter = 11580, loss = 1.5077
[2023-10-04 22:08:07] iter = 11590, loss = 1.5450
[2023-10-04 22:08:08] iter = 11600, loss = 1.5675
[2023-10-04 22:08:09] iter = 11610, loss = 1.6875
[2023-10-04 22:08:10] iter = 11620, loss = 1.5908
[2023-10-04 22:08:11] iter = 11630, loss = 1.5088
[2023-10-04 22:08:11] iter = 11640, loss = 1.5309
[2023-10-04 22:08:12] iter = 11650, loss = 1.5686
[2023-10-04 22:08:13] iter = 11660, loss = 1.5073
[2023-10-04 22:08:14] iter = 11670, loss = 1.4892
[2023-10-04 22:08:15] iter = 11680, loss = 1.6900
[2023-10-04 22:08:16] iter = 11690, loss = 1.5997
[2023-10-04 22:08:17] iter = 11700, loss = 1.5330
[2023-10-04 22:08:18] iter = 11710, loss = 1.5393
[2023-10-04 22:08:19] iter = 11720, loss = 1.5525
[2023-10-04 22:08:20] iter = 11730, loss = 1.5152
[2023-10-04 22:08:21] iter = 11740, loss = 1.6176
[2023-10-04 22:08:21] iter = 11750, loss = 1.7766
[2023-10-04 22:08:22] iter = 11760, loss = 1.6054
[2023-10-04 22:08:23] iter = 11770, loss = 1.4191
[2023-10-04 22:08:24] iter = 11780, loss = 1.5204
[2023-10-04 22:08:25] iter = 11790, loss = 1.5544
[2023-10-04 22:08:26] iter = 11800, loss = 1.6666
[2023-10-04 22:08:27] iter = 11810, loss = 1.5466
[2023-10-04 22:08:28] iter = 11820, loss = 1.4471
[2023-10-04 22:08:29] iter = 11830, loss = 1.3740
[2023-10-04 22:08:30] iter = 11840, loss = 1.4696
[2023-10-04 22:08:31] iter = 11850, loss = 1.6571
[2023-10-04 22:08:32] iter = 11860, loss = 1.6359
[2023-10-04 22:08:33] iter = 11870, loss = 1.4927
[2023-10-04 22:08:33] iter = 11880, loss = 1.5828
[2023-10-04 22:08:34] iter = 11890, loss = 1.5972
[2023-10-04 22:08:35] iter = 11900, loss = 1.4717
[2023-10-04 22:08:36] iter = 11910, loss = 1.4262
[2023-10-04 22:08:37] iter = 11920, loss = 1.5467
[2023-10-04 22:08:38] iter = 11930, loss = 1.5414
[2023-10-04 22:08:39] iter = 11940, loss = 1.4361
[2023-10-04 22:08:40] iter = 11950, loss = 1.6503
[2023-10-04 22:08:41] iter = 11960, loss = 1.3848
[2023-10-04 22:08:42] iter = 11970, loss = 1.6791
[2023-10-04 22:08:43] iter = 11980, loss = 1.4573
[2023-10-04 22:08:44] iter = 11990, loss = 1.4817
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 25029}
[2023-10-04 22:09:09] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004701 train acc = 1.0000, test acc = 0.6329
[2023-10-04 22:09:34] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003041 train acc = 1.0000, test acc = 0.6239
[2023-10-04 22:09:59] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.008479 train acc = 1.0000, test acc = 0.6316
[2023-10-04 22:10:23] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004626 train acc = 1.0000, test acc = 0.6282
[2023-10-04 22:10:48] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.001832 train acc = 1.0000, test acc = 0.6261
[2023-10-04 22:11:12] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002811 train acc = 1.0000, test acc = 0.6173
[2023-10-04 22:11:37] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.013667 train acc = 1.0000, test acc = 0.6332
[2023-10-04 22:12:02] Evaluate_07: epoch = 1000 train time = 23 s train loss = 0.021091 train acc = 0.9960, test acc = 0.6290
[2023-10-04 22:12:27] Evaluate_08: epoch = 1000 train time = 23 s train loss = 0.014707 train acc = 0.9980, test acc = 0.6280
[2023-10-04 22:12:51] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.006303 train acc = 1.0000, test acc = 0.6278
[2023-10-04 22:13:16] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.001775 train acc = 1.0000, test acc = 0.6289
[2023-10-04 22:13:41] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.013992 train acc = 1.0000, test acc = 0.6262
[2023-10-04 22:14:05] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.015419 train acc = 0.9980, test acc = 0.6328
[2023-10-04 22:14:30] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003723 train acc = 1.0000, test acc = 0.6256
[2023-10-04 22:14:55] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.005731 train acc = 1.0000, test acc = 0.6228
[2023-10-04 22:15:20] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.005689 train acc = 1.0000, test acc = 0.6246
[2023-10-04 22:15:44] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.011053 train acc = 1.0000, test acc = 0.6318
[2023-10-04 22:16:09] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004592 train acc = 1.0000, test acc = 0.6272
[2023-10-04 22:16:34] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.014523 train acc = 1.0000, test acc = 0.6312
[2023-10-04 22:16:58] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004049 train acc = 1.0000, test acc = 0.6282
Evaluate 20 random ConvNet, mean = 0.6279 std = 0.0038
-------------------------
[2023-10-04 22:16:59] iter = 12000, loss = 1.4950
[2023-10-04 22:17:00] iter = 12010, loss = 1.5754
[2023-10-04 22:17:01] iter = 12020, loss = 1.5084
[2023-10-04 22:17:02] iter = 12030, loss = 1.5021
[2023-10-04 22:17:03] iter = 12040, loss = 1.5374
[2023-10-04 22:17:03] iter = 12050, loss = 1.5666
[2023-10-04 22:17:04] iter = 12060, loss = 1.4643
[2023-10-04 22:17:05] iter = 12070, loss = 1.5128
[2023-10-04 22:17:06] iter = 12080, loss = 1.4877
[2023-10-04 22:17:07] iter = 12090, loss = 1.4673
[2023-10-04 22:17:08] iter = 12100, loss = 1.6320
[2023-10-04 22:17:09] iter = 12110, loss = 1.6903
[2023-10-04 22:17:10] iter = 12120, loss = 1.4390
[2023-10-04 22:17:11] iter = 12130, loss = 1.5946
[2023-10-04 22:17:12] iter = 12140, loss = 1.5033
[2023-10-04 22:17:13] iter = 12150, loss = 1.5260
[2023-10-04 22:17:14] iter = 12160, loss = 1.5717
[2023-10-04 22:17:14] iter = 12170, loss = 1.4623
[2023-10-04 22:17:15] iter = 12180, loss = 1.4583
[2023-10-04 22:17:16] iter = 12190, loss = 1.4889
[2023-10-04 22:17:17] iter = 12200, loss = 1.5848
[2023-10-04 22:17:18] iter = 12210, loss = 1.4141
[2023-10-04 22:17:19] iter = 12220, loss = 1.4801
[2023-10-04 22:17:20] iter = 12230, loss = 1.6499
[2023-10-04 22:17:21] iter = 12240, loss = 1.5098
[2023-10-04 22:17:22] iter = 12250, loss = 1.6194
[2023-10-04 22:17:23] iter = 12260, loss = 1.3992
[2023-10-04 22:17:24] iter = 12270, loss = 1.4973
[2023-10-04 22:17:25] iter = 12280, loss = 1.6102
[2023-10-04 22:17:25] iter = 12290, loss = 1.5709
[2023-10-04 22:17:26] iter = 12300, loss = 1.6695
[2023-10-04 22:17:27] iter = 12310, loss = 1.4642
[2023-10-04 22:17:28] iter = 12320, loss = 1.5940
[2023-10-04 22:17:29] iter = 12330, loss = 1.4563
[2023-10-04 22:17:30] iter = 12340, loss = 1.5163
[2023-10-04 22:17:31] iter = 12350, loss = 1.3793
[2023-10-04 22:17:32] iter = 12360, loss = 1.4228
[2023-10-04 22:17:33] iter = 12370, loss = 1.6517
[2023-10-04 22:17:34] iter = 12380, loss = 1.5537
[2023-10-04 22:17:35] iter = 12390, loss = 1.5543
[2023-10-04 22:17:35] iter = 12400, loss = 1.5958
[2023-10-04 22:17:36] iter = 12410, loss = 1.6444
[2023-10-04 22:17:37] iter = 12420, loss = 1.6913
[2023-10-04 22:17:38] iter = 12430, loss = 1.5839
[2023-10-04 22:17:39] iter = 12440, loss = 1.4918
[2023-10-04 22:17:40] iter = 12450, loss = 1.4509
[2023-10-04 22:17:41] iter = 12460, loss = 1.4821
[2023-10-04 22:17:42] iter = 12470, loss = 1.5171
[2023-10-04 22:17:43] iter = 12480, loss = 1.5123
[2023-10-04 22:17:44] iter = 12490, loss = 1.5750
[2023-10-04 22:17:45] iter = 12500, loss = 1.5035
[2023-10-04 22:17:46] iter = 12510, loss = 1.4961
[2023-10-04 22:17:47] iter = 12520, loss = 1.4157
[2023-10-04 22:17:48] iter = 12530, loss = 1.4718
[2023-10-04 22:17:48] iter = 12540, loss = 1.5367
[2023-10-04 22:17:49] iter = 12550, loss = 1.4405
[2023-10-04 22:17:50] iter = 12560, loss = 1.5458
[2023-10-04 22:17:51] iter = 12570, loss = 1.6304
[2023-10-04 22:17:52] iter = 12580, loss = 1.5295
[2023-10-04 22:17:53] iter = 12590, loss = 1.4307
[2023-10-04 22:17:54] iter = 12600, loss = 1.5605
[2023-10-04 22:17:55] iter = 12610, loss = 1.5850
[2023-10-04 22:17:56] iter = 12620, loss = 1.4783
[2023-10-04 22:17:57] iter = 12630, loss = 1.3547
[2023-10-04 22:17:58] iter = 12640, loss = 1.4921
[2023-10-04 22:17:59] iter = 12650, loss = 1.6492
[2023-10-04 22:18:00] iter = 12660, loss = 1.3880
[2023-10-04 22:18:01] iter = 12670, loss = 1.5574
[2023-10-04 22:18:01] iter = 12680, loss = 1.5324
[2023-10-04 22:18:02] iter = 12690, loss = 1.4272
[2023-10-04 22:18:03] iter = 12700, loss = 1.5438
[2023-10-04 22:18:04] iter = 12710, loss = 1.5264
[2023-10-04 22:18:05] iter = 12720, loss = 1.5310
[2023-10-04 22:18:06] iter = 12730, loss = 1.4666
[2023-10-04 22:18:07] iter = 12740, loss = 1.5484
[2023-10-04 22:18:08] iter = 12750, loss = 1.3743
[2023-10-04 22:18:09] iter = 12760, loss = 1.5628
[2023-10-04 22:18:10] iter = 12770, loss = 1.5318
[2023-10-04 22:18:11] iter = 12780, loss = 1.4549
[2023-10-04 22:18:12] iter = 12790, loss = 1.4327
[2023-10-04 22:18:13] iter = 12800, loss = 1.5112
[2023-10-04 22:18:13] iter = 12810, loss = 1.3865
[2023-10-04 22:18:14] iter = 12820, loss = 1.4826
[2023-10-04 22:18:15] iter = 12830, loss = 1.5647
[2023-10-04 22:18:16] iter = 12840, loss = 1.5188
[2023-10-04 22:18:17] iter = 12850, loss = 1.4696
[2023-10-04 22:18:18] iter = 12860, loss = 1.4208
[2023-10-04 22:18:19] iter = 12870, loss = 1.4752
[2023-10-04 22:18:20] iter = 12880, loss = 1.4419
[2023-10-04 22:18:21] iter = 12890, loss = 1.5739
[2023-10-04 22:18:22] iter = 12900, loss = 1.6450
[2023-10-04 22:18:23] iter = 12910, loss = 1.6156
[2023-10-04 22:18:24] iter = 12920, loss = 1.4394
[2023-10-04 22:18:25] iter = 12930, loss = 1.4979
[2023-10-04 22:18:26] iter = 12940, loss = 1.5951
[2023-10-04 22:18:27] iter = 12950, loss = 1.3979
[2023-10-04 22:18:27] iter = 12960, loss = 1.5392
[2023-10-04 22:18:28] iter = 12970, loss = 1.5499
[2023-10-04 22:18:29] iter = 12980, loss = 1.5683
[2023-10-04 22:18:30] iter = 12990, loss = 1.6344
[2023-10-04 22:18:31] iter = 13000, loss = 1.5287
[2023-10-04 22:18:32] iter = 13010, loss = 1.5138
[2023-10-04 22:18:33] iter = 13020, loss = 1.5796
[2023-10-04 22:18:34] iter = 13030, loss = 1.5277
[2023-10-04 22:18:35] iter = 13040, loss = 1.4750
[2023-10-04 22:18:36] iter = 13050, loss = 1.5586
[2023-10-04 22:18:37] iter = 13060, loss = 1.5461
[2023-10-04 22:18:38] iter = 13070, loss = 1.6967
[2023-10-04 22:18:39] iter = 13080, loss = 1.6332
[2023-10-04 22:18:40] iter = 13090, loss = 1.6221
[2023-10-04 22:18:41] iter = 13100, loss = 1.5576
[2023-10-04 22:18:42] iter = 13110, loss = 1.5091
[2023-10-04 22:18:43] iter = 13120, loss = 1.5753
[2023-10-04 22:18:43] iter = 13130, loss = 1.4175
[2023-10-04 22:18:44] iter = 13140, loss = 1.5566
[2023-10-04 22:18:45] iter = 13150, loss = 1.5284
[2023-10-04 22:18:46] iter = 13160, loss = 1.4118
[2023-10-04 22:18:47] iter = 13170, loss = 1.5451
[2023-10-04 22:18:48] iter = 13180, loss = 1.5900
[2023-10-04 22:18:49] iter = 13190, loss = 1.4996
[2023-10-04 22:18:50] iter = 13200, loss = 1.4401
[2023-10-04 22:18:51] iter = 13210, loss = 1.5178
[2023-10-04 22:18:51] iter = 13220, loss = 1.5327
[2023-10-04 22:18:52] iter = 13230, loss = 1.5656
[2023-10-04 22:18:53] iter = 13240, loss = 1.5390
[2023-10-04 22:18:54] iter = 13250, loss = 1.5515
[2023-10-04 22:18:55] iter = 13260, loss = 1.5671
[2023-10-04 22:18:56] iter = 13270, loss = 1.3547
[2023-10-04 22:18:57] iter = 13280, loss = 1.4136
[2023-10-04 22:18:58] iter = 13290, loss = 1.5598
[2023-10-04 22:18:59] iter = 13300, loss = 1.5655
[2023-10-04 22:19:00] iter = 13310, loss = 1.4718
[2023-10-04 22:19:01] iter = 13320, loss = 1.5657
[2023-10-04 22:19:02] iter = 13330, loss = 1.5459
[2023-10-04 22:19:03] iter = 13340, loss = 1.5427
[2023-10-04 22:19:04] iter = 13350, loss = 1.4994
[2023-10-04 22:19:05] iter = 13360, loss = 1.4613
[2023-10-04 22:19:05] iter = 13370, loss = 1.4699
[2023-10-04 22:19:06] iter = 13380, loss = 1.4242
[2023-10-04 22:19:07] iter = 13390, loss = 1.4605
[2023-10-04 22:19:08] iter = 13400, loss = 1.4413
[2023-10-04 22:19:09] iter = 13410, loss = 1.5637
[2023-10-04 22:19:10] iter = 13420, loss = 1.6799
[2023-10-04 22:19:11] iter = 13430, loss = 1.5826
[2023-10-04 22:19:12] iter = 13440, loss = 1.7053
[2023-10-04 22:19:13] iter = 13450, loss = 1.5768
[2023-10-04 22:19:14] iter = 13460, loss = 1.6041
[2023-10-04 22:19:15] iter = 13470, loss = 1.5705
[2023-10-04 22:19:16] iter = 13480, loss = 1.5891
[2023-10-04 22:19:17] iter = 13490, loss = 1.5201
[2023-10-04 22:19:17] iter = 13500, loss = 1.3794
[2023-10-04 22:19:18] iter = 13510, loss = 1.6053
[2023-10-04 22:19:19] iter = 13520, loss = 1.3543
[2023-10-04 22:19:20] iter = 13530, loss = 1.5417
[2023-10-04 22:19:21] iter = 13540, loss = 1.4419
[2023-10-04 22:19:22] iter = 13550, loss = 1.4786
[2023-10-04 22:19:23] iter = 13560, loss = 1.5411
[2023-10-04 22:19:24] iter = 13570, loss = 1.5355
[2023-10-04 22:19:24] iter = 13580, loss = 1.5070
[2023-10-04 22:19:25] iter = 13590, loss = 1.3874
[2023-10-04 22:19:26] iter = 13600, loss = 1.3536
[2023-10-04 22:19:27] iter = 13610, loss = 1.5217
[2023-10-04 22:19:28] iter = 13620, loss = 1.4515
[2023-10-04 22:19:29] iter = 13630, loss = 1.4838
[2023-10-04 22:19:30] iter = 13640, loss = 1.5109
[2023-10-04 22:19:31] iter = 13650, loss = 1.5296
[2023-10-04 22:19:32] iter = 13660, loss = 1.6079
[2023-10-04 22:19:33] iter = 13670, loss = 1.5706
[2023-10-04 22:19:34] iter = 13680, loss = 1.5335
[2023-10-04 22:19:35] iter = 13690, loss = 1.4954
[2023-10-04 22:19:36] iter = 13700, loss = 1.4581
[2023-10-04 22:19:37] iter = 13710, loss = 1.6496
[2023-10-04 22:19:37] iter = 13720, loss = 1.4858
[2023-10-04 22:19:38] iter = 13730, loss = 1.5505
[2023-10-04 22:19:39] iter = 13740, loss = 1.5097
[2023-10-04 22:19:40] iter = 13750, loss = 1.4613
[2023-10-04 22:19:41] iter = 13760, loss = 1.6481
[2023-10-04 22:19:42] iter = 13770, loss = 1.6517
[2023-10-04 22:19:43] iter = 13780, loss = 1.4046
[2023-10-04 22:19:44] iter = 13790, loss = 1.7163
[2023-10-04 22:19:45] iter = 13800, loss = 1.5862
[2023-10-04 22:19:46] iter = 13810, loss = 1.7073
[2023-10-04 22:19:47] iter = 13820, loss = 1.5444
[2023-10-04 22:19:48] iter = 13830, loss = 1.4029
[2023-10-04 22:19:48] iter = 13840, loss = 1.4793
[2023-10-04 22:19:49] iter = 13850, loss = 1.4698
[2023-10-04 22:19:50] iter = 13860, loss = 1.3647
[2023-10-04 22:19:51] iter = 13870, loss = 1.4874
[2023-10-04 22:19:52] iter = 13880, loss = 1.4953
[2023-10-04 22:19:53] iter = 13890, loss = 1.4531
[2023-10-04 22:19:54] iter = 13900, loss = 1.4633
[2023-10-04 22:19:55] iter = 13910, loss = 1.4893
[2023-10-04 22:19:56] iter = 13920, loss = 1.5277
[2023-10-04 22:19:57] iter = 13930, loss = 1.3947
[2023-10-04 22:19:58] iter = 13940, loss = 1.5511
[2023-10-04 22:19:59] iter = 13950, loss = 1.3787
[2023-10-04 22:20:00] iter = 13960, loss = 1.4390
[2023-10-04 22:20:00] iter = 13970, loss = 1.5095
[2023-10-04 22:20:02] iter = 13980, loss = 1.4081
[2023-10-04 22:20:02] iter = 13990, loss = 1.5545
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 3733}
[2023-10-04 22:20:28] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005468 train acc = 1.0000, test acc = 0.6216
[2023-10-04 22:20:52] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003932 train acc = 1.0000, test acc = 0.6257
[2023-10-04 22:21:17] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.001904 train acc = 1.0000, test acc = 0.6277
[2023-10-04 22:21:41] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.022931 train acc = 1.0000, test acc = 0.6306
[2023-10-04 22:22:06] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.018809 train acc = 1.0000, test acc = 0.6267
[2023-10-04 22:22:31] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004892 train acc = 1.0000, test acc = 0.6325
[2023-10-04 22:22:55] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016539 train acc = 1.0000, test acc = 0.6284
[2023-10-04 22:23:20] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.011740 train acc = 1.0000, test acc = 0.6309
[2023-10-04 22:23:44] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003929 train acc = 1.0000, test acc = 0.6235
[2023-10-04 22:24:09] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.021848 train acc = 1.0000, test acc = 0.6286
[2023-10-04 22:24:34] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013722 train acc = 1.0000, test acc = 0.6259
[2023-10-04 22:24:58] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015843 train acc = 1.0000, test acc = 0.6208
[2023-10-04 22:25:23] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003381 train acc = 1.0000, test acc = 0.6301
[2023-10-04 22:25:48] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.026840 train acc = 1.0000, test acc = 0.6215
[2023-10-04 22:26:12] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.016499 train acc = 1.0000, test acc = 0.6266
[2023-10-04 22:26:37] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.014468 train acc = 1.0000, test acc = 0.6300
[2023-10-04 22:27:01] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.014718 train acc = 0.9960, test acc = 0.6203
[2023-10-04 22:27:26] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003921 train acc = 1.0000, test acc = 0.6333
[2023-10-04 22:27:50] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.019914 train acc = 0.9980, test acc = 0.6264
[2023-10-04 22:28:15] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.016255 train acc = 1.0000, test acc = 0.6297
Evaluate 20 random ConvNet, mean = 0.6270 std = 0.0038
-------------------------
[2023-10-04 22:28:15] iter = 14000, loss = 1.5946
[2023-10-04 22:28:16] iter = 14010, loss = 1.4298
[2023-10-04 22:28:17] iter = 14020, loss = 1.4492
[2023-10-04 22:28:18] iter = 14030, loss = 1.5018
[2023-10-04 22:28:19] iter = 14040, loss = 1.5793
[2023-10-04 22:28:20] iter = 14050, loss = 1.4512
[2023-10-04 22:28:21] iter = 14060, loss = 1.4177
[2023-10-04 22:28:22] iter = 14070, loss = 1.5514
[2023-10-04 22:28:23] iter = 14080, loss = 1.6083
[2023-10-04 22:28:23] iter = 14090, loss = 1.4523
[2023-10-04 22:28:24] iter = 14100, loss = 1.4113
[2023-10-04 22:28:25] iter = 14110, loss = 1.6250
[2023-10-04 22:28:26] iter = 14120, loss = 1.5026
[2023-10-04 22:28:27] iter = 14130, loss = 1.5505
[2023-10-04 22:28:28] iter = 14140, loss = 1.6361
[2023-10-04 22:28:29] iter = 14150, loss = 1.4992
[2023-10-04 22:28:30] iter = 14160, loss = 1.4826
[2023-10-04 22:28:31] iter = 14170, loss = 1.5302
[2023-10-04 22:28:32] iter = 14180, loss = 1.4733
[2023-10-04 22:28:32] iter = 14190, loss = 1.4822
[2023-10-04 22:28:33] iter = 14200, loss = 1.4802
[2023-10-04 22:28:34] iter = 14210, loss = 1.5111
[2023-10-04 22:28:35] iter = 14220, loss = 1.7047
[2023-10-04 22:28:36] iter = 14230, loss = 1.5575
[2023-10-04 22:28:37] iter = 14240, loss = 1.4727
[2023-10-04 22:28:38] iter = 14250, loss = 1.5768
[2023-10-04 22:28:39] iter = 14260, loss = 1.3836
[2023-10-04 22:28:40] iter = 14270, loss = 1.5402
[2023-10-04 22:28:40] iter = 14280, loss = 1.5325
[2023-10-04 22:28:41] iter = 14290, loss = 1.6146
[2023-10-04 22:28:42] iter = 14300, loss = 1.5360
[2023-10-04 22:28:43] iter = 14310, loss = 1.3595
[2023-10-04 22:28:44] iter = 14320, loss = 1.4672
[2023-10-04 22:28:45] iter = 14330, loss = 1.4190
[2023-10-04 22:28:46] iter = 14340, loss = 1.5175
[2023-10-04 22:28:47] iter = 14350, loss = 1.6135
[2023-10-04 22:28:48] iter = 14360, loss = 1.4488
[2023-10-04 22:28:49] iter = 14370, loss = 1.5685
[2023-10-04 22:28:50] iter = 14380, loss = 1.5557
[2023-10-04 22:28:51] iter = 14390, loss = 1.6205
[2023-10-04 22:28:52] iter = 14400, loss = 1.4735
[2023-10-04 22:28:52] iter = 14410, loss = 1.4120
[2023-10-04 22:28:54] iter = 14420, loss = 1.5267
[2023-10-04 22:28:54] iter = 14430, loss = 1.4534
[2023-10-04 22:28:55] iter = 14440, loss = 1.5875
[2023-10-04 22:28:56] iter = 14450, loss = 1.5406
[2023-10-04 22:28:57] iter = 14460, loss = 1.4177
[2023-10-04 22:28:58] iter = 14470, loss = 1.6630
[2023-10-04 22:28:59] iter = 14480, loss = 1.6043
[2023-10-04 22:29:00] iter = 14490, loss = 1.7456
[2023-10-04 22:29:01] iter = 14500, loss = 1.4955
[2023-10-04 22:29:02] iter = 14510, loss = 1.5459
[2023-10-04 22:29:03] iter = 14520, loss = 1.4612
[2023-10-04 22:29:04] iter = 14530, loss = 1.4455
[2023-10-04 22:29:05] iter = 14540, loss = 1.5405
[2023-10-04 22:29:05] iter = 14550, loss = 1.5109
[2023-10-04 22:29:06] iter = 14560, loss = 1.5463
[2023-10-04 22:29:07] iter = 14570, loss = 1.5398
[2023-10-04 22:29:08] iter = 14580, loss = 1.3618
[2023-10-04 22:29:09] iter = 14590, loss = 1.4311
[2023-10-04 22:29:10] iter = 14600, loss = 1.5704
[2023-10-04 22:29:11] iter = 14610, loss = 1.5516
[2023-10-04 22:29:12] iter = 14620, loss = 1.5280
[2023-10-04 22:29:13] iter = 14630, loss = 1.5515
[2023-10-04 22:29:14] iter = 14640, loss = 1.5293
[2023-10-04 22:29:15] iter = 14650, loss = 1.6696
[2023-10-04 22:29:15] iter = 14660, loss = 1.4318
[2023-10-04 22:29:16] iter = 14670, loss = 1.5303
[2023-10-04 22:29:17] iter = 14680, loss = 1.4779
[2023-10-04 22:29:18] iter = 14690, loss = 1.6406
[2023-10-04 22:29:19] iter = 14700, loss = 1.4208
[2023-10-04 22:29:20] iter = 14710, loss = 1.4021
[2023-10-04 22:29:21] iter = 14720, loss = 1.4865
[2023-10-04 22:29:22] iter = 14730, loss = 1.5602
[2023-10-04 22:29:23] iter = 14740, loss = 1.5293
[2023-10-04 22:29:24] iter = 14750, loss = 1.5871
[2023-10-04 22:29:25] iter = 14760, loss = 1.4887
[2023-10-04 22:29:25] iter = 14770, loss = 1.4736
[2023-10-04 22:29:26] iter = 14780, loss = 1.5341
[2023-10-04 22:29:27] iter = 14790, loss = 1.3918
[2023-10-04 22:29:28] iter = 14800, loss = 1.6081
[2023-10-04 22:29:29] iter = 14810, loss = 1.4889
[2023-10-04 22:29:30] iter = 14820, loss = 1.6154
[2023-10-04 22:29:31] iter = 14830, loss = 1.4739
[2023-10-04 22:29:32] iter = 14840, loss = 1.3624
[2023-10-04 22:29:33] iter = 14850, loss = 1.4875
[2023-10-04 22:29:34] iter = 14860, loss = 1.4559
[2023-10-04 22:29:35] iter = 14870, loss = 1.4936
[2023-10-04 22:29:36] iter = 14880, loss = 1.6053
[2023-10-04 22:29:36] iter = 14890, loss = 1.5783
[2023-10-04 22:29:37] iter = 14900, loss = 1.5714
[2023-10-04 22:29:38] iter = 14910, loss = 1.4793
[2023-10-04 22:29:39] iter = 14920, loss = 1.4148
[2023-10-04 22:29:40] iter = 14930, loss = 1.4098
[2023-10-04 22:29:41] iter = 14940, loss = 1.4532
[2023-10-04 22:29:42] iter = 14950, loss = 1.4843
[2023-10-04 22:29:43] iter = 14960, loss = 1.4780
[2023-10-04 22:29:44] iter = 14970, loss = 1.4697
[2023-10-04 22:29:45] iter = 14980, loss = 1.4049
[2023-10-04 22:29:46] iter = 14990, loss = 1.4974
[2023-10-04 22:29:47] iter = 15000, loss = 1.4686
[2023-10-04 22:29:48] iter = 15010, loss = 1.3555
[2023-10-04 22:29:49] iter = 15020, loss = 1.4498
[2023-10-04 22:29:50] iter = 15030, loss = 1.4999
[2023-10-04 22:29:50] iter = 15040, loss = 1.5377
[2023-10-04 22:29:51] iter = 15050, loss = 1.6551
[2023-10-04 22:29:52] iter = 15060, loss = 1.4271
[2023-10-04 22:29:53] iter = 15070, loss = 1.6584
[2023-10-04 22:29:54] iter = 15080, loss = 1.4515
[2023-10-04 22:29:55] iter = 15090, loss = 1.4885
[2023-10-04 22:29:56] iter = 15100, loss = 1.5788
[2023-10-04 22:29:57] iter = 15110, loss = 1.4907
[2023-10-04 22:29:58] iter = 15120, loss = 1.5305
[2023-10-04 22:29:58] iter = 15130, loss = 1.4813
[2023-10-04 22:29:59] iter = 15140, loss = 1.4692
[2023-10-04 22:30:00] iter = 15150, loss = 1.3557
[2023-10-04 22:30:01] iter = 15160, loss = 1.5044
[2023-10-04 22:30:02] iter = 15170, loss = 1.5502
[2023-10-04 22:30:03] iter = 15180, loss = 1.5610
[2023-10-04 22:30:04] iter = 15190, loss = 1.4792
[2023-10-04 22:30:05] iter = 15200, loss = 1.5295
[2023-10-04 22:30:06] iter = 15210, loss = 1.5457
[2023-10-04 22:30:07] iter = 15220, loss = 1.5692
[2023-10-04 22:30:08] iter = 15230, loss = 1.4729
[2023-10-04 22:30:09] iter = 15240, loss = 1.4278
[2023-10-04 22:30:10] iter = 15250, loss = 1.4850
[2023-10-04 22:30:10] iter = 15260, loss = 1.4675
[2023-10-04 22:30:11] iter = 15270, loss = 1.5434
[2023-10-04 22:30:12] iter = 15280, loss = 1.5584
[2023-10-04 22:30:13] iter = 15290, loss = 1.5311
[2023-10-04 22:30:14] iter = 15300, loss = 1.5667
[2023-10-04 22:30:15] iter = 15310, loss = 1.6880
[2023-10-04 22:30:16] iter = 15320, loss = 1.5706
[2023-10-04 22:30:17] iter = 15330, loss = 1.4990
[2023-10-04 22:30:18] iter = 15340, loss = 1.5743
[2023-10-04 22:30:19] iter = 15350, loss = 1.4824
[2023-10-04 22:30:20] iter = 15360, loss = 1.4356
[2023-10-04 22:30:21] iter = 15370, loss = 1.4915
[2023-10-04 22:30:22] iter = 15380, loss = 1.4877
[2023-10-04 22:30:23] iter = 15390, loss = 1.3676
[2023-10-04 22:30:24] iter = 15400, loss = 1.4695
[2023-10-04 22:30:24] iter = 15410, loss = 1.5340
[2023-10-04 22:30:25] iter = 15420, loss = 1.6073
[2023-10-04 22:30:26] iter = 15430, loss = 1.3936
[2023-10-04 22:30:27] iter = 15440, loss = 1.5221
[2023-10-04 22:30:28] iter = 15450, loss = 1.5684
[2023-10-04 22:30:29] iter = 15460, loss = 1.5709
[2023-10-04 22:30:30] iter = 15470, loss = 1.4882
[2023-10-04 22:30:31] iter = 15480, loss = 1.4604
[2023-10-04 22:30:32] iter = 15490, loss = 1.4079
[2023-10-04 22:30:33] iter = 15500, loss = 1.5853
[2023-10-04 22:30:34] iter = 15510, loss = 1.5495
[2023-10-04 22:30:35] iter = 15520, loss = 1.3803
[2023-10-04 22:30:36] iter = 15530, loss = 1.4864
[2023-10-04 22:30:36] iter = 15540, loss = 1.5344
[2023-10-04 22:30:37] iter = 15550, loss = 1.3977
[2023-10-04 22:30:38] iter = 15560, loss = 1.5726
[2023-10-04 22:30:39] iter = 15570, loss = 1.4974
[2023-10-04 22:30:40] iter = 15580, loss = 1.5563
[2023-10-04 22:30:41] iter = 15590, loss = 1.5667
[2023-10-04 22:30:42] iter = 15600, loss = 1.4768
[2023-10-04 22:30:43] iter = 15610, loss = 1.4560
[2023-10-04 22:30:44] iter = 15620, loss = 1.5428
[2023-10-04 22:30:45] iter = 15630, loss = 1.3321
[2023-10-04 22:30:46] iter = 15640, loss = 1.5454
[2023-10-04 22:30:46] iter = 15650, loss = 1.5666
[2023-10-04 22:30:47] iter = 15660, loss = 1.4402
[2023-10-04 22:30:48] iter = 15670, loss = 1.3921
[2023-10-04 22:30:49] iter = 15680, loss = 1.5811
[2023-10-04 22:30:50] iter = 15690, loss = 1.4640
[2023-10-04 22:30:51] iter = 15700, loss = 1.4798
[2023-10-04 22:30:52] iter = 15710, loss = 1.5963
[2023-10-04 22:30:53] iter = 15720, loss = 1.5208
[2023-10-04 22:30:54] iter = 15730, loss = 1.5026
[2023-10-04 22:30:55] iter = 15740, loss = 1.6242
[2023-10-04 22:30:55] iter = 15750, loss = 1.4801
[2023-10-04 22:30:56] iter = 15760, loss = 1.5108
[2023-10-04 22:30:57] iter = 15770, loss = 1.5270
[2023-10-04 22:30:58] iter = 15780, loss = 1.6910
[2023-10-04 22:30:59] iter = 15790, loss = 1.5638
[2023-10-04 22:31:00] iter = 15800, loss = 1.5511
[2023-10-04 22:31:01] iter = 15810, loss = 1.4301
[2023-10-04 22:31:02] iter = 15820, loss = 1.4303
[2023-10-04 22:31:03] iter = 15830, loss = 1.4558
[2023-10-04 22:31:04] iter = 15840, loss = 1.4470
[2023-10-04 22:31:04] iter = 15850, loss = 1.4524
[2023-10-04 22:31:05] iter = 15860, loss = 1.5545
[2023-10-04 22:31:06] iter = 15870, loss = 1.4254
[2023-10-04 22:31:07] iter = 15880, loss = 1.5723
[2023-10-04 22:31:08] iter = 15890, loss = 1.5894
[2023-10-04 22:31:09] iter = 15900, loss = 1.5441
[2023-10-04 22:31:10] iter = 15910, loss = 1.4617
[2023-10-04 22:31:11] iter = 15920, loss = 1.5393
[2023-10-04 22:31:12] iter = 15930, loss = 1.4568
[2023-10-04 22:31:13] iter = 15940, loss = 1.4944
[2023-10-04 22:31:13] iter = 15950, loss = 1.4099
[2023-10-04 22:31:14] iter = 15960, loss = 1.5490
[2023-10-04 22:31:15] iter = 15970, loss = 1.3339
[2023-10-04 22:31:16] iter = 15980, loss = 1.4320
[2023-10-04 22:31:17] iter = 15990, loss = 1.5109
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 78608}
[2023-10-04 22:31:43] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004041 train acc = 1.0000, test acc = 0.6308
[2023-10-04 22:32:07] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004024 train acc = 1.0000, test acc = 0.6286
[2023-10-04 22:32:32] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.007958 train acc = 1.0000, test acc = 0.6236
[2023-10-04 22:32:56] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.013200 train acc = 1.0000, test acc = 0.6244
[2023-10-04 22:33:21] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.012907 train acc = 1.0000, test acc = 0.6301
[2023-10-04 22:33:45] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003082 train acc = 1.0000, test acc = 0.6325
[2023-10-04 22:34:10] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.008698 train acc = 0.9980, test acc = 0.6302
[2023-10-04 22:34:34] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.006430 train acc = 1.0000, test acc = 0.6267
[2023-10-04 22:34:59] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.004869 train acc = 1.0000, test acc = 0.6278
[2023-10-04 22:35:23] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.014658 train acc = 1.0000, test acc = 0.6310
[2023-10-04 22:35:48] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.005568 train acc = 1.0000, test acc = 0.6243
[2023-10-04 22:36:13] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004174 train acc = 1.0000, test acc = 0.6276
[2023-10-04 22:36:37] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.016488 train acc = 1.0000, test acc = 0.6275
[2023-10-04 22:37:02] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002018 train acc = 1.0000, test acc = 0.6328
[2023-10-04 22:37:26] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.024038 train acc = 1.0000, test acc = 0.6290
[2023-10-04 22:37:51] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.005290 train acc = 1.0000, test acc = 0.6281
[2023-10-04 22:38:15] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004753 train acc = 1.0000, test acc = 0.6248
[2023-10-04 22:38:40] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.014989 train acc = 0.9980, test acc = 0.6309
[2023-10-04 22:39:05] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.011632 train acc = 0.9980, test acc = 0.6283
[2023-10-04 22:39:29] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003311 train acc = 1.0000, test acc = 0.6279
Evaluate 20 random ConvNet, mean = 0.6283 std = 0.0026
-------------------------
[2023-10-04 22:39:29] iter = 16000, loss = 1.4477
[2023-10-04 22:39:30] iter = 16010, loss = 1.4013
[2023-10-04 22:39:31] iter = 16020, loss = 1.5804
[2023-10-04 22:39:32] iter = 16030, loss = 1.5281
[2023-10-04 22:39:33] iter = 16040, loss = 1.4454
[2023-10-04 22:39:34] iter = 16050, loss = 1.4066
[2023-10-04 22:39:35] iter = 16060, loss = 1.4481
[2023-10-04 22:39:36] iter = 16070, loss = 1.4306
[2023-10-04 22:39:37] iter = 16080, loss = 1.4445
[2023-10-04 22:39:38] iter = 16090, loss = 1.4973
[2023-10-04 22:39:39] iter = 16100, loss = 1.5019
[2023-10-04 22:39:40] iter = 16110, loss = 1.4190
[2023-10-04 22:39:40] iter = 16120, loss = 1.4123
[2023-10-04 22:39:41] iter = 16130, loss = 1.5538
[2023-10-04 22:39:42] iter = 16140, loss = 1.4910
[2023-10-04 22:39:43] iter = 16150, loss = 1.4578
[2023-10-04 22:39:44] iter = 16160, loss = 1.5577
[2023-10-04 22:39:45] iter = 16170, loss = 1.5547
[2023-10-04 22:39:46] iter = 16180, loss = 1.4458
[2023-10-04 22:39:47] iter = 16190, loss = 1.6030
[2023-10-04 22:39:48] iter = 16200, loss = 1.5075
[2023-10-04 22:39:49] iter = 16210, loss = 1.3541
[2023-10-04 22:39:50] iter = 16220, loss = 1.6596
[2023-10-04 22:39:51] iter = 16230, loss = 1.5948
[2023-10-04 22:39:52] iter = 16240, loss = 1.4959
[2023-10-04 22:39:52] iter = 16250, loss = 1.5740
[2023-10-04 22:39:53] iter = 16260, loss = 1.5587
[2023-10-04 22:39:54] iter = 16270, loss = 1.5085
[2023-10-04 22:39:55] iter = 16280, loss = 1.4224
[2023-10-04 22:39:56] iter = 16290, loss = 1.4186
[2023-10-04 22:39:57] iter = 16300, loss = 1.5220
[2023-10-04 22:39:58] iter = 16310, loss = 1.5818
[2023-10-04 22:39:59] iter = 16320, loss = 1.5045
[2023-10-04 22:40:00] iter = 16330, loss = 1.4094
[2023-10-04 22:40:01] iter = 16340, loss = 1.4782
[2023-10-04 22:40:02] iter = 16350, loss = 1.6013
[2023-10-04 22:40:02] iter = 16360, loss = 1.6145
[2023-10-04 22:40:03] iter = 16370, loss = 1.5252
[2023-10-04 22:40:04] iter = 16380, loss = 1.4406
[2023-10-04 22:40:05] iter = 16390, loss = 1.5548
[2023-10-04 22:40:06] iter = 16400, loss = 1.4095
[2023-10-04 22:40:07] iter = 16410, loss = 1.5061
[2023-10-04 22:40:08] iter = 16420, loss = 1.4150
[2023-10-04 22:40:09] iter = 16430, loss = 1.5527
[2023-10-04 22:40:10] iter = 16440, loss = 1.5928
[2023-10-04 22:40:11] iter = 16450, loss = 1.3865
[2023-10-04 22:40:12] iter = 16460, loss = 1.4901
[2023-10-04 22:40:13] iter = 16470, loss = 1.7143
[2023-10-04 22:40:13] iter = 16480, loss = 1.4694
[2023-10-04 22:40:14] iter = 16490, loss = 1.4833
[2023-10-04 22:40:15] iter = 16500, loss = 1.6485
[2023-10-04 22:40:16] iter = 16510, loss = 1.5849
[2023-10-04 22:40:17] iter = 16520, loss = 1.4433
[2023-10-04 22:40:18] iter = 16530, loss = 1.5229
[2023-10-04 22:40:19] iter = 16540, loss = 1.5362
[2023-10-04 22:40:20] iter = 16550, loss = 1.4768
[2023-10-04 22:40:21] iter = 16560, loss = 1.4524
[2023-10-04 22:40:22] iter = 16570, loss = 1.4400
[2023-10-04 22:40:23] iter = 16580, loss = 1.5733
[2023-10-04 22:40:24] iter = 16590, loss = 1.4765
[2023-10-04 22:40:25] iter = 16600, loss = 1.4616
[2023-10-04 22:40:25] iter = 16610, loss = 1.4797
[2023-10-04 22:40:26] iter = 16620, loss = 1.6361
[2023-10-04 22:40:27] iter = 16630, loss = 1.3927
[2023-10-04 22:40:28] iter = 16640, loss = 1.4575
[2023-10-04 22:40:29] iter = 16650, loss = 1.4347
[2023-10-04 22:40:30] iter = 16660, loss = 1.5013
[2023-10-04 22:40:31] iter = 16670, loss = 1.4788
[2023-10-04 22:40:32] iter = 16680, loss = 1.6477
[2023-10-04 22:40:33] iter = 16690, loss = 1.3999
[2023-10-04 22:40:34] iter = 16700, loss = 1.5707
[2023-10-04 22:40:35] iter = 16710, loss = 1.4991
[2023-10-04 22:40:36] iter = 16720, loss = 1.4794
[2023-10-04 22:40:37] iter = 16730, loss = 1.5843
[2023-10-04 22:40:38] iter = 16740, loss = 1.5574
[2023-10-04 22:40:39] iter = 16750, loss = 1.5651
[2023-10-04 22:40:40] iter = 16760, loss = 1.5108
[2023-10-04 22:40:41] iter = 16770, loss = 1.5454
[2023-10-04 22:40:42] iter = 16780, loss = 1.4598
[2023-10-04 22:40:42] iter = 16790, loss = 1.4193
[2023-10-04 22:40:43] iter = 16800, loss = 1.5242
[2023-10-04 22:40:44] iter = 16810, loss = 1.4421
[2023-10-04 22:40:45] iter = 16820, loss = 1.5517
[2023-10-04 22:40:46] iter = 16830, loss = 1.5115
[2023-10-04 22:40:47] iter = 16840, loss = 1.4530
[2023-10-04 22:40:48] iter = 16850, loss = 1.5478
[2023-10-04 22:40:49] iter = 16860, loss = 1.5443
[2023-10-04 22:40:50] iter = 16870, loss = 1.4634
[2023-10-04 22:40:51] iter = 16880, loss = 1.4731
[2023-10-04 22:40:52] iter = 16890, loss = 1.7660
[2023-10-04 22:40:53] iter = 16900, loss = 1.4668
[2023-10-04 22:40:54] iter = 16910, loss = 1.3729
[2023-10-04 22:40:55] iter = 16920, loss = 1.5454
[2023-10-04 22:40:55] iter = 16930, loss = 1.5090
[2023-10-04 22:40:56] iter = 16940, loss = 1.4472
[2023-10-04 22:40:57] iter = 16950, loss = 1.5573
[2023-10-04 22:40:58] iter = 16960, loss = 1.3872
[2023-10-04 22:40:59] iter = 16970, loss = 1.3822
[2023-10-04 22:41:00] iter = 16980, loss = 1.4516
[2023-10-04 22:41:01] iter = 16990, loss = 1.5445
[2023-10-04 22:41:02] iter = 17000, loss = 1.5757
[2023-10-04 22:41:03] iter = 17010, loss = 1.5513
[2023-10-04 22:41:04] iter = 17020, loss = 1.4577
[2023-10-04 22:41:05] iter = 17030, loss = 1.4747
[2023-10-04 22:41:06] iter = 17040, loss = 1.6184
[2023-10-04 22:41:06] iter = 17050, loss = 1.4618
[2023-10-04 22:41:07] iter = 17060, loss = 1.4755
[2023-10-04 22:41:08] iter = 17070, loss = 1.3947
[2023-10-04 22:41:09] iter = 17080, loss = 1.4909
[2023-10-04 22:41:10] iter = 17090, loss = 1.4065
[2023-10-04 22:41:11] iter = 17100, loss = 1.4784
[2023-10-04 22:41:12] iter = 17110, loss = 1.3650
[2023-10-04 22:41:13] iter = 17120, loss = 1.4925
[2023-10-04 22:41:14] iter = 17130, loss = 1.4212
[2023-10-04 22:41:15] iter = 17140, loss = 1.4252
[2023-10-04 22:41:16] iter = 17150, loss = 1.3860
[2023-10-04 22:41:17] iter = 17160, loss = 1.5155
[2023-10-04 22:41:18] iter = 17170, loss = 1.4934
[2023-10-04 22:41:18] iter = 17180, loss = 1.4278
[2023-10-04 22:41:20] iter = 17190, loss = 1.6332
[2023-10-04 22:41:21] iter = 17200, loss = 1.4915
[2023-10-04 22:41:22] iter = 17210, loss = 1.4998
[2023-10-04 22:41:22] iter = 17220, loss = 1.4908
[2023-10-04 22:41:23] iter = 17230, loss = 1.5947
[2023-10-04 22:41:24] iter = 17240, loss = 1.5224
[2023-10-04 22:41:25] iter = 17250, loss = 1.5733
[2023-10-04 22:41:26] iter = 17260, loss = 1.4442
[2023-10-04 22:41:27] iter = 17270, loss = 1.4553
[2023-10-04 22:41:28] iter = 17280, loss = 1.4789
[2023-10-04 22:41:29] iter = 17290, loss = 1.4285
[2023-10-04 22:41:30] iter = 17300, loss = 1.4602
[2023-10-04 22:41:31] iter = 17310, loss = 1.4341
[2023-10-04 22:41:32] iter = 17320, loss = 1.5201
[2023-10-04 22:41:33] iter = 17330, loss = 1.5672
[2023-10-04 22:41:34] iter = 17340, loss = 1.4870
[2023-10-04 22:41:35] iter = 17350, loss = 1.6033
[2023-10-04 22:41:35] iter = 17360, loss = 1.6846
[2023-10-04 22:41:37] iter = 17370, loss = 1.5267
[2023-10-04 22:41:37] iter = 17380, loss = 1.4891
[2023-10-04 22:41:38] iter = 17390, loss = 1.5235
[2023-10-04 22:41:39] iter = 17400, loss = 1.5089
[2023-10-04 22:41:40] iter = 17410, loss = 1.4648
[2023-10-04 22:41:41] iter = 17420, loss = 1.3895
[2023-10-04 22:41:42] iter = 17430, loss = 1.4860
[2023-10-04 22:41:43] iter = 17440, loss = 1.5042
[2023-10-04 22:41:44] iter = 17450, loss = 1.4203
[2023-10-04 22:41:45] iter = 17460, loss = 1.5814
[2023-10-04 22:41:46] iter = 17470, loss = 1.5235
[2023-10-04 22:41:47] iter = 17480, loss = 1.3638
[2023-10-04 22:41:47] iter = 17490, loss = 1.5185
[2023-10-04 22:41:48] iter = 17500, loss = 1.4968
[2023-10-04 22:41:49] iter = 17510, loss = 1.4438
[2023-10-04 22:41:50] iter = 17520, loss = 1.4552
[2023-10-04 22:41:51] iter = 17530, loss = 1.5164
[2023-10-04 22:41:52] iter = 17540, loss = 1.4367
[2023-10-04 22:41:53] iter = 17550, loss = 1.5975
[2023-10-04 22:41:54] iter = 17560, loss = 1.3765
[2023-10-04 22:41:55] iter = 17570, loss = 1.4564
[2023-10-04 22:41:56] iter = 17580, loss = 1.5198
[2023-10-04 22:41:57] iter = 17590, loss = 1.4330
[2023-10-04 22:41:58] iter = 17600, loss = 1.4803
[2023-10-04 22:41:59] iter = 17610, loss = 1.3544
[2023-10-04 22:41:59] iter = 17620, loss = 1.5740
[2023-10-04 22:42:00] iter = 17630, loss = 1.4886
[2023-10-04 22:42:01] iter = 17640, loss = 1.4607
[2023-10-04 22:42:02] iter = 17650, loss = 1.5904
[2023-10-04 22:42:03] iter = 17660, loss = 1.4611
[2023-10-04 22:42:04] iter = 17670, loss = 1.5694
[2023-10-04 22:42:05] iter = 17680, loss = 1.4619
[2023-10-04 22:42:06] iter = 17690, loss = 1.5634
[2023-10-04 22:42:07] iter = 17700, loss = 1.4573
[2023-10-04 22:42:08] iter = 17710, loss = 1.3715
[2023-10-04 22:42:09] iter = 17720, loss = 1.4428
[2023-10-04 22:42:09] iter = 17730, loss = 1.5245
[2023-10-04 22:42:10] iter = 17740, loss = 1.4066
[2023-10-04 22:42:11] iter = 17750, loss = 1.4365
[2023-10-04 22:42:12] iter = 17760, loss = 1.5228
[2023-10-04 22:42:13] iter = 17770, loss = 1.5496
[2023-10-04 22:42:14] iter = 17780, loss = 1.5184
[2023-10-04 22:42:15] iter = 17790, loss = 1.7127
[2023-10-04 22:42:16] iter = 17800, loss = 1.4367
[2023-10-04 22:42:17] iter = 17810, loss = 1.4112
[2023-10-04 22:42:18] iter = 17820, loss = 1.3651
[2023-10-04 22:42:19] iter = 17830, loss = 1.5253
[2023-10-04 22:42:19] iter = 17840, loss = 1.4377
[2023-10-04 22:42:20] iter = 17850, loss = 1.4196
[2023-10-04 22:42:21] iter = 17860, loss = 1.5749
[2023-10-04 22:42:22] iter = 17870, loss = 1.5450
[2023-10-04 22:42:23] iter = 17880, loss = 1.4648
[2023-10-04 22:42:24] iter = 17890, loss = 1.4854
[2023-10-04 22:42:25] iter = 17900, loss = 1.4332
[2023-10-04 22:42:26] iter = 17910, loss = 1.3989
[2023-10-04 22:42:27] iter = 17920, loss = 1.4200
[2023-10-04 22:42:28] iter = 17930, loss = 1.4944
[2023-10-04 22:42:28] iter = 17940, loss = 1.4844
[2023-10-04 22:42:29] iter = 17950, loss = 1.6006
[2023-10-04 22:42:30] iter = 17960, loss = 1.4913
[2023-10-04 22:42:31] iter = 17970, loss = 1.4647
[2023-10-04 22:42:32] iter = 17980, loss = 1.4057
[2023-10-04 22:42:33] iter = 17990, loss = 1.5242
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 54611}
[2023-10-04 22:42:59] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.007063 train acc = 1.0000, test acc = 0.6292
[2023-10-04 22:43:23] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006160 train acc = 1.0000, test acc = 0.6259
[2023-10-04 22:43:48] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.014496 train acc = 1.0000, test acc = 0.6317
[2023-10-04 22:44:12] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004881 train acc = 1.0000, test acc = 0.6336
[2023-10-04 22:44:37] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004503 train acc = 1.0000, test acc = 0.6313
[2023-10-04 22:45:01] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.027264 train acc = 1.0000, test acc = 0.6322
[2023-10-04 22:45:26] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.018878 train acc = 1.0000, test acc = 0.6261
[2023-10-04 22:45:50] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012472 train acc = 0.9980, test acc = 0.6356
[2023-10-04 22:46:15] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.012513 train acc = 1.0000, test acc = 0.6326
[2023-10-04 22:46:39] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.017798 train acc = 0.9960, test acc = 0.6327
[2023-10-04 22:47:04] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002133 train acc = 1.0000, test acc = 0.6290
[2023-10-04 22:47:29] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.012357 train acc = 1.0000, test acc = 0.6288
[2023-10-04 22:47:53] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004262 train acc = 1.0000, test acc = 0.6302
[2023-10-04 22:48:18] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.010771 train acc = 1.0000, test acc = 0.6227
[2023-10-04 22:48:43] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004077 train acc = 1.0000, test acc = 0.6219
[2023-10-04 22:49:07] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015734 train acc = 1.0000, test acc = 0.6314
[2023-10-04 22:49:32] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.016451 train acc = 1.0000, test acc = 0.6299
[2023-10-04 22:49:56] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.006075 train acc = 1.0000, test acc = 0.6383
[2023-10-04 22:50:21] Evaluate_18: epoch = 1000 train time = 23 s train loss = 0.010294 train acc = 1.0000, test acc = 0.6258
[2023-10-04 22:50:46] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.002765 train acc = 1.0000, test acc = 0.6327
Evaluate 20 random ConvNet, mean = 0.6301 std = 0.0040
-------------------------
[2023-10-04 22:50:46] iter = 18000, loss = 1.5421
[2023-10-04 22:50:47] iter = 18010, loss = 1.3168
[2023-10-04 22:50:48] iter = 18020, loss = 1.5306
[2023-10-04 22:50:49] iter = 18030, loss = 1.6125
[2023-10-04 22:50:50] iter = 18040, loss = 1.5372
[2023-10-04 22:50:51] iter = 18050, loss = 1.5145
[2023-10-04 22:50:52] iter = 18060, loss = 1.4002
[2023-10-04 22:50:53] iter = 18070, loss = 1.4128
[2023-10-04 22:50:54] iter = 18080, loss = 1.3770
[2023-10-04 22:50:55] iter = 18090, loss = 1.4566
[2023-10-04 22:50:55] iter = 18100, loss = 1.6073
[2023-10-04 22:50:56] iter = 18110, loss = 1.4690
[2023-10-04 22:50:57] iter = 18120, loss = 1.4676
[2023-10-04 22:50:58] iter = 18130, loss = 1.5325
[2023-10-04 22:50:59] iter = 18140, loss = 1.4233
[2023-10-04 22:51:00] iter = 18150, loss = 1.5678
[2023-10-04 22:51:01] iter = 18160, loss = 1.4544
[2023-10-04 22:51:02] iter = 18170, loss = 1.3787
[2023-10-04 22:51:03] iter = 18180, loss = 1.4323
[2023-10-04 22:51:04] iter = 18190, loss = 1.3338
[2023-10-04 22:51:05] iter = 18200, loss = 1.3414
[2023-10-04 22:51:06] iter = 18210, loss = 1.4419
[2023-10-04 22:51:07] iter = 18220, loss = 1.3991
[2023-10-04 22:51:07] iter = 18230, loss = 1.5078
[2023-10-04 22:51:08] iter = 18240, loss = 1.3365
[2023-10-04 22:51:09] iter = 18250, loss = 1.5091
[2023-10-04 22:51:10] iter = 18260, loss = 1.4934
[2023-10-04 22:51:11] iter = 18270, loss = 1.4346
[2023-10-04 22:51:12] iter = 18280, loss = 1.4271
[2023-10-04 22:51:13] iter = 18290, loss = 1.4595
[2023-10-04 22:51:14] iter = 18300, loss = 1.4278
[2023-10-04 22:51:15] iter = 18310, loss = 1.6974
[2023-10-04 22:51:16] iter = 18320, loss = 1.6143
[2023-10-04 22:51:16] iter = 18330, loss = 1.4982
[2023-10-04 22:51:17] iter = 18340, loss = 1.5249
[2023-10-04 22:51:18] iter = 18350, loss = 1.4964
[2023-10-04 22:51:19] iter = 18360, loss = 1.4387
[2023-10-04 22:51:20] iter = 18370, loss = 1.6533
[2023-10-04 22:51:21] iter = 18380, loss = 1.4824
[2023-10-04 22:51:22] iter = 18390, loss = 1.3077
[2023-10-04 22:51:23] iter = 18400, loss = 1.4012
[2023-10-04 22:51:24] iter = 18410, loss = 1.5159
[2023-10-04 22:51:25] iter = 18420, loss = 1.7287
[2023-10-04 22:51:26] iter = 18430, loss = 1.4817
[2023-10-04 22:51:26] iter = 18440, loss = 1.4631
[2023-10-04 22:51:28] iter = 18450, loss = 1.4927
[2023-10-04 22:51:29] iter = 18460, loss = 1.3737
[2023-10-04 22:51:29] iter = 18470, loss = 1.5682
[2023-10-04 22:51:30] iter = 18480, loss = 1.5313
[2023-10-04 22:51:31] iter = 18490, loss = 1.5939
[2023-10-04 22:51:32] iter = 18500, loss = 1.5432
[2023-10-04 22:51:33] iter = 18510, loss = 1.4877
[2023-10-04 22:51:34] iter = 18520, loss = 1.4789
[2023-10-04 22:51:35] iter = 18530, loss = 1.4314
[2023-10-04 22:51:36] iter = 18540, loss = 1.5360
[2023-10-04 22:51:37] iter = 18550, loss = 1.5017
[2023-10-04 22:51:38] iter = 18560, loss = 1.6305
[2023-10-04 22:51:39] iter = 18570, loss = 1.5402
[2023-10-04 22:51:40] iter = 18580, loss = 1.4357
[2023-10-04 22:51:40] iter = 18590, loss = 1.6172
[2023-10-04 22:51:41] iter = 18600, loss = 1.4014
[2023-10-04 22:51:42] iter = 18610, loss = 1.4729
[2023-10-04 22:51:43] iter = 18620, loss = 1.3752
[2023-10-04 22:51:44] iter = 18630, loss = 1.3651
[2023-10-04 22:51:45] iter = 18640, loss = 1.5802
[2023-10-04 22:51:46] iter = 18650, loss = 1.5039
[2023-10-04 22:51:47] iter = 18660, loss = 1.4452
[2023-10-04 22:51:48] iter = 18670, loss = 1.5103
[2023-10-04 22:51:49] iter = 18680, loss = 1.4839
[2023-10-04 22:51:50] iter = 18690, loss = 1.5031
[2023-10-04 22:51:50] iter = 18700, loss = 1.4846
[2023-10-04 22:51:51] iter = 18710, loss = 1.5671
[2023-10-04 22:51:52] iter = 18720, loss = 1.5081
[2023-10-04 22:51:53] iter = 18730, loss = 1.5071
[2023-10-04 22:51:54] iter = 18740, loss = 1.5668
[2023-10-04 22:51:55] iter = 18750, loss = 1.3451
[2023-10-04 22:51:56] iter = 18760, loss = 1.5123
[2023-10-04 22:51:57] iter = 18770, loss = 1.5847
[2023-10-04 22:51:58] iter = 18780, loss = 1.5675
[2023-10-04 22:51:59] iter = 18790, loss = 1.4141
[2023-10-04 22:52:00] iter = 18800, loss = 1.4737
[2023-10-04 22:52:01] iter = 18810, loss = 1.3560
[2023-10-04 22:52:02] iter = 18820, loss = 1.4192
[2023-10-04 22:52:02] iter = 18830, loss = 1.4723
[2023-10-04 22:52:04] iter = 18840, loss = 1.4983
[2023-10-04 22:52:04] iter = 18850, loss = 1.4186
[2023-10-04 22:52:05] iter = 18860, loss = 1.4735
[2023-10-04 22:52:06] iter = 18870, loss = 1.4467
[2023-10-04 22:52:07] iter = 18880, loss = 1.5995
[2023-10-04 22:52:08] iter = 18890, loss = 1.5377
[2023-10-04 22:52:09] iter = 18900, loss = 1.3452
[2023-10-04 22:52:10] iter = 18910, loss = 1.4490
[2023-10-04 22:52:11] iter = 18920, loss = 1.4919
[2023-10-04 22:52:12] iter = 18930, loss = 1.3798
[2023-10-04 22:52:13] iter = 18940, loss = 1.5118
[2023-10-04 22:52:14] iter = 18950, loss = 1.4087
[2023-10-04 22:52:15] iter = 18960, loss = 1.4756
[2023-10-04 22:52:15] iter = 18970, loss = 1.5348
[2023-10-04 22:52:16] iter = 18980, loss = 1.5453
[2023-10-04 22:52:17] iter = 18990, loss = 1.4483
[2023-10-04 22:52:18] iter = 19000, loss = 1.4101
[2023-10-04 22:52:19] iter = 19010, loss = 1.6540
[2023-10-04 22:52:20] iter = 19020, loss = 1.6071
[2023-10-04 22:52:21] iter = 19030, loss = 1.5116
[2023-10-04 22:52:22] iter = 19040, loss = 1.4411
[2023-10-04 22:52:23] iter = 19050, loss = 1.4564
[2023-10-04 22:52:24] iter = 19060, loss = 1.4876
[2023-10-04 22:52:25] iter = 19070, loss = 1.5429
[2023-10-04 22:52:26] iter = 19080, loss = 1.6865
[2023-10-04 22:52:27] iter = 19090, loss = 1.5585
[2023-10-04 22:52:27] iter = 19100, loss = 1.5207
[2023-10-04 22:52:28] iter = 19110, loss = 1.7157
[2023-10-04 22:52:29] iter = 19120, loss = 1.6462
[2023-10-04 22:52:30] iter = 19130, loss = 1.4596
[2023-10-04 22:52:31] iter = 19140, loss = 1.5053
[2023-10-04 22:52:32] iter = 19150, loss = 1.3757
[2023-10-04 22:52:33] iter = 19160, loss = 1.5278
[2023-10-04 22:52:34] iter = 19170, loss = 1.4932
[2023-10-04 22:52:35] iter = 19180, loss = 1.5094
[2023-10-04 22:52:36] iter = 19190, loss = 1.3745
[2023-10-04 22:52:37] iter = 19200, loss = 1.4280
[2023-10-04 22:52:38] iter = 19210, loss = 1.4975
[2023-10-04 22:52:39] iter = 19220, loss = 1.5016
[2023-10-04 22:52:40] iter = 19230, loss = 1.5116
[2023-10-04 22:52:41] iter = 19240, loss = 1.5506
[2023-10-04 22:52:41] iter = 19250, loss = 1.3968
[2023-10-04 22:52:42] iter = 19260, loss = 1.5045
[2023-10-04 22:52:43] iter = 19270, loss = 1.4247
[2023-10-04 22:52:44] iter = 19280, loss = 1.5474
[2023-10-04 22:52:45] iter = 19290, loss = 1.5261
[2023-10-04 22:52:46] iter = 19300, loss = 1.7801
[2023-10-04 22:52:47] iter = 19310, loss = 1.5518
[2023-10-04 22:52:48] iter = 19320, loss = 1.3920
[2023-10-04 22:52:49] iter = 19330, loss = 1.4201
[2023-10-04 22:52:50] iter = 19340, loss = 1.4413
[2023-10-04 22:52:51] iter = 19350, loss = 1.5877
[2023-10-04 22:52:51] iter = 19360, loss = 1.5266
[2023-10-04 22:52:52] iter = 19370, loss = 1.5770
[2023-10-04 22:52:53] iter = 19380, loss = 1.4821
[2023-10-04 22:52:54] iter = 19390, loss = 1.5221
[2023-10-04 22:52:55] iter = 19400, loss = 1.5553
[2023-10-04 22:52:56] iter = 19410, loss = 1.4657
[2023-10-04 22:52:57] iter = 19420, loss = 1.3414
[2023-10-04 22:52:58] iter = 19430, loss = 1.4124
[2023-10-04 22:52:59] iter = 19440, loss = 1.4989
[2023-10-04 22:53:00] iter = 19450, loss = 1.3962
[2023-10-04 22:53:01] iter = 19460, loss = 1.5503
[2023-10-04 22:53:02] iter = 19470, loss = 1.4198
[2023-10-04 22:53:03] iter = 19480, loss = 1.4700
[2023-10-04 22:53:04] iter = 19490, loss = 1.4421
[2023-10-04 22:53:05] iter = 19500, loss = 1.4472
[2023-10-04 22:53:06] iter = 19510, loss = 1.4273
[2023-10-04 22:53:07] iter = 19520, loss = 1.4699
[2023-10-04 22:53:07] iter = 19530, loss = 1.5079
[2023-10-04 22:53:08] iter = 19540, loss = 1.5944
[2023-10-04 22:53:09] iter = 19550, loss = 1.6375
[2023-10-04 22:53:10] iter = 19560, loss = 1.5565
[2023-10-04 22:53:11] iter = 19570, loss = 1.4552
[2023-10-04 22:53:12] iter = 19580, loss = 1.5750
[2023-10-04 22:53:13] iter = 19590, loss = 1.6574
[2023-10-04 22:53:14] iter = 19600, loss = 1.5993
[2023-10-04 22:53:15] iter = 19610, loss = 1.5066
[2023-10-04 22:53:16] iter = 19620, loss = 1.3822
[2023-10-04 22:53:17] iter = 19630, loss = 1.4056
[2023-10-04 22:53:17] iter = 19640, loss = 1.5326
[2023-10-04 22:53:18] iter = 19650, loss = 1.4909
[2023-10-04 22:53:19] iter = 19660, loss = 1.3710
[2023-10-04 22:53:20] iter = 19670, loss = 1.4574
[2023-10-04 22:53:21] iter = 19680, loss = 1.5889
[2023-10-04 22:53:22] iter = 19690, loss = 1.5073
[2023-10-04 22:53:23] iter = 19700, loss = 1.4737
[2023-10-04 22:53:24] iter = 19710, loss = 1.4497
[2023-10-04 22:53:25] iter = 19720, loss = 1.4943
[2023-10-04 22:53:26] iter = 19730, loss = 1.3892
[2023-10-04 22:53:27] iter = 19740, loss = 1.4478
[2023-10-04 22:53:28] iter = 19750, loss = 1.6138
[2023-10-04 22:53:29] iter = 19760, loss = 1.4476
[2023-10-04 22:53:30] iter = 19770, loss = 1.4196
[2023-10-04 22:53:31] iter = 19780, loss = 1.5816
[2023-10-04 22:53:31] iter = 19790, loss = 1.5040
[2023-10-04 22:53:32] iter = 19800, loss = 1.4501
[2023-10-04 22:53:33] iter = 19810, loss = 1.4125
[2023-10-04 22:53:34] iter = 19820, loss = 1.4710
[2023-10-04 22:53:35] iter = 19830, loss = 1.6841
[2023-10-04 22:53:36] iter = 19840, loss = 1.4852
[2023-10-04 22:53:37] iter = 19850, loss = 1.4494
[2023-10-04 22:53:38] iter = 19860, loss = 1.5124
[2023-10-04 22:53:39] iter = 19870, loss = 1.4873
[2023-10-04 22:53:40] iter = 19880, loss = 1.5399
[2023-10-04 22:53:41] iter = 19890, loss = 1.4422
[2023-10-04 22:53:42] iter = 19900, loss = 1.6719
[2023-10-04 22:53:43] iter = 19910, loss = 1.5114
[2023-10-04 22:53:44] iter = 19920, loss = 1.5128
[2023-10-04 22:53:44] iter = 19930, loss = 1.4654
[2023-10-04 22:53:45] iter = 19940, loss = 1.5373
[2023-10-04 22:53:46] iter = 19950, loss = 1.4236
[2023-10-04 22:53:47] iter = 19960, loss = 1.4994
[2023-10-04 22:53:48] iter = 19970, loss = 1.4543
[2023-10-04 22:53:49] iter = 19980, loss = 1.6241
[2023-10-04 22:53:50] iter = 19990, loss = 1.4339
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 31140}
[2023-10-04 22:54:15] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.022391 train acc = 0.9980, test acc = 0.6361
[2023-10-04 22:54:40] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.012337 train acc = 0.9980, test acc = 0.6268
[2023-10-04 22:55:05] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.012390 train acc = 1.0000, test acc = 0.6312
[2023-10-04 22:55:29] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.012992 train acc = 1.0000, test acc = 0.6301
[2023-10-04 22:55:54] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.013136 train acc = 1.0000, test acc = 0.6334
[2023-10-04 22:56:18] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.028429 train acc = 0.9980, test acc = 0.6322
[2023-10-04 22:56:42] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016646 train acc = 1.0000, test acc = 0.6242
[2023-10-04 22:57:07] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012962 train acc = 1.0000, test acc = 0.6297
[2023-10-04 22:57:31] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.017054 train acc = 1.0000, test acc = 0.6332
[2023-10-04 22:57:56] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013888 train acc = 1.0000, test acc = 0.6268
[2023-10-04 22:58:20] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.009451 train acc = 1.0000, test acc = 0.6380
[2023-10-04 22:58:45] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.020386 train acc = 1.0000, test acc = 0.6264
[2023-10-04 22:59:09] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004641 train acc = 1.0000, test acc = 0.6347
[2023-10-04 22:59:33] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004618 train acc = 1.0000, test acc = 0.6322
[2023-10-04 22:59:58] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.009283 train acc = 0.9980, test acc = 0.6366
[2023-10-04 23:00:22] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.007594 train acc = 0.9980, test acc = 0.6299
[2023-10-04 23:00:47] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.019062 train acc = 1.0000, test acc = 0.6328
[2023-10-04 23:01:12] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.010523 train acc = 1.0000, test acc = 0.6284
[2023-10-04 23:01:36] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013786 train acc = 1.0000, test acc = 0.6303
[2023-10-04 23:02:01] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.005940 train acc = 1.0000, test acc = 0.6282
Evaluate 20 random ConvNet, mean = 0.6311 std = 0.0036
-------------------------
[2023-10-04 23:02:01] iter = 20000, loss = 1.4728

==================== Final Results ====================

Run 5 experiments, train on ConvNet, evaluate 100 random ConvNet, mean  = 62.72%  std = 0.42%
