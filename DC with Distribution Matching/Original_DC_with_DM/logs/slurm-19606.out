/data/happythgus/repos/DatasetCondensation
/data/opt/anaconda3/bin/python
moana-r1
eval_it_pool:  [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000, 20000]
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz

  0%|          | 0/170498071 [00:00<?, ?it/s]
  0%|          | 32768/170498071 [00:00<35:28, 80085.44it/s]
  0%|          | 65536/170498071 [00:00<23:41, 119927.44it/s]
  0%|          | 98304/170498071 [00:00<24:28, 116019.17it/s]
  0%|          | 196608/170498071 [00:01<12:06, 234450.46it/s]
  0%|          | 294912/170498071 [00:01<07:39, 370627.51it/s]
  0%|          | 393216/170498071 [00:01<05:55, 478888.70it/s]
  0%|          | 458752/170498071 [00:01<06:20, 446519.25it/s]
  0%|          | 655360/170498071 [00:01<05:30, 513131.45it/s]
  1%|          | 1343488/170498071 [00:02<02:14, 1259539.10it/s]
  1%|          | 1703936/170498071 [00:02<01:47, 1573435.47it/s]
  1%|          | 1900544/170498071 [00:02<01:58, 1424726.44it/s]
  1%|▏         | 2195456/170498071 [00:02<01:41, 1665193.39it/s]
  1%|▏         | 2392064/170498071 [00:02<01:54, 1469682.38it/s]
  2%|▏         | 2686976/170498071 [00:02<01:36, 1731672.53it/s]
  2%|▏         | 2883584/170498071 [00:02<01:51, 1507844.51it/s]
  2%|▏         | 3178496/170498071 [00:03<01:34, 1772181.36it/s]
  2%|▏         | 3407872/170498071 [00:03<01:45, 1583917.23it/s]
  2%|▏         | 3735552/170498071 [00:03<01:28, 1893999.95it/s]
  2%|▏         | 3964928/170498071 [00:03<01:40, 1661176.42it/s]
  2%|▏         | 4259840/170498071 [00:03<01:27, 1910040.79it/s]
  3%|▎         | 4489216/170498071 [00:03<01:39, 1666302.41it/s]
  3%|▎         | 4816896/170498071 [00:03<01:23, 1974350.27it/s]
  3%|▎         | 5046272/170498071 [00:04<01:36, 1713222.31it/s]
  3%|▎         | 5406720/170498071 [00:04<01:20, 2061076.66it/s]
  3%|▎         | 5636096/170498071 [00:04<01:33, 1758457.20it/s]
  4%|▎         | 5996544/170498071 [00:04<01:17, 2116239.12it/s]
  4%|▎         | 6258688/170498071 [00:04<01:28, 1847731.84it/s]
  4%|▍         | 6586368/170498071 [00:04<01:16, 2130821.92it/s]
  4%|▍         | 6848512/170498071 [00:05<01:28, 1855873.40it/s]
  4%|▍         | 7176192/170498071 [00:05<01:16, 2146117.34it/s]
  4%|▍         | 7438336/170498071 [00:05<01:27, 1854659.30it/s]
  5%|▍         | 7798784/170498071 [00:05<01:13, 2216413.78it/s]
  5%|▍         | 8060928/170498071 [00:05<01:25, 1896194.00it/s]
  5%|▍         | 8421376/170498071 [00:05<01:12, 2242734.61it/s]
  5%|▌         | 8683520/170498071 [00:05<01:24, 1920799.35it/s]
  5%|▌         | 9043968/170498071 [00:06<01:11, 2266266.26it/s]
  5%|▌         | 9306112/170498071 [00:06<01:23, 1929446.79it/s]
  6%|▌         | 9666560/170498071 [00:06<01:10, 2280445.54it/s]
  6%|▌         | 9928704/170498071 [00:06<01:22, 1937462.29it/s]
  6%|▌         | 10289152/170498071 [00:06<01:09, 2295386.55it/s]
  6%|▌         | 10584064/170498071 [00:06<01:20, 1992179.16it/s]
  6%|▋         | 10944512/170498071 [00:06<01:08, 2327602.95it/s]
  7%|▋         | 11239424/170498071 [00:07<01:18, 2022001.30it/s]
  7%|▋         | 11599872/170498071 [00:07<01:07, 2355099.49it/s]
  7%|▋         | 11894784/170498071 [00:07<01:17, 2036123.63it/s]
  7%|▋         | 12255232/170498071 [00:07<01:06, 2366606.30it/s]
  7%|▋         | 12550144/170498071 [00:07<01:17, 2040973.70it/s]
  8%|▊         | 12910592/170498071 [00:07<01:12, 2181892.26it/s]
  8%|▊         | 13172736/170498071 [00:08<01:16, 2043690.04it/s]
  8%|▊         | 13533184/170498071 [00:08<01:05, 2382604.35it/s]
  8%|▊         | 13795328/170498071 [00:08<01:18, 1994955.07it/s]
  8%|▊         | 14155776/170498071 [00:08<01:06, 2341670.05it/s]
  8%|▊         | 14450688/170498071 [00:08<01:17, 2023492.78it/s]
  9%|▊         | 14811136/170498071 [00:08<01:05, 2360895.69it/s]
  9%|▉         | 15106048/170498071 [00:08<01:16, 2035628.23it/s]
  9%|▉         | 15466496/170498071 [00:09<01:06, 2337036.18it/s]
  9%|▉         | 15761408/170498071 [00:09<01:15, 2048472.77it/s]
  9%|▉         | 16121856/170498071 [00:09<01:13, 2091840.66it/s]
 10%|▉         | 16351232/170498071 [00:09<01:15, 2029974.05it/s]
 10%|▉         | 16711680/170498071 [00:09<01:04, 2371921.77it/s]
 10%|▉         | 16973824/170498071 [00:09<01:17, 1972254.31it/s]
 10%|█         | 17367040/170498071 [00:09<01:03, 2397815.59it/s]
 10%|█         | 17661952/170498071 [00:10<01:14, 2046527.71it/s]
 11%|█         | 18055168/170498071 [00:10<01:14, 2058608.71it/s]
 11%|█         | 18284544/170498071 [00:10<01:13, 2083605.07it/s]
 11%|█         | 18677760/170498071 [00:10<01:06, 2292226.98it/s]
 11%|█         | 18939904/170498071 [00:10<01:12, 2097134.76it/s]
 11%|█▏        | 19333120/170498071 [00:10<01:10, 2131826.10it/s]
 11%|█▏        | 19562496/170498071 [00:10<01:11, 2098462.85it/s]
 12%|█▏        | 19955712/170498071 [00:11<01:00, 2476427.97it/s]
 12%|█▏        | 20217856/170498071 [00:11<01:12, 2064402.07it/s]
 12%|█▏        | 20611072/170498071 [00:11<01:06, 2261866.68it/s]
 12%|█▏        | 20873216/170498071 [00:11<01:11, 2090132.89it/s]
 12%|█▏        | 21266432/170498071 [00:11<01:08, 2164178.36it/s]
 13%|█▎        | 21495808/170498071 [00:11<01:11, 2093959.94it/s]
 13%|█▎        | 21889024/170498071 [00:11<00:59, 2503696.89it/s]
 13%|█▎        | 22183936/170498071 [00:12<01:10, 2104942.76it/s]
 13%|█▎        | 22577152/170498071 [00:12<01:06, 2210470.88it/s]
 13%|█▎        | 22839296/170498071 [00:12<01:08, 2148792.46it/s]
 14%|█▎        | 23232512/170498071 [00:12<01:02, 2343681.29it/s]
 14%|█▍        | 23494656/170498071 [00:12<01:08, 2138204.07it/s]
 14%|█▍        | 23887872/170498071 [00:12<00:57, 2538589.14it/s]
 14%|█▍        | 24182784/170498071 [00:13<01:08, 2138103.91it/s]
 14%|█▍        | 24576000/170498071 [00:13<01:02, 2332733.54it/s]
 15%|█▍        | 24838144/170498071 [00:13<01:08, 2135432.47it/s]
 15%|█▍        | 25264128/170498071 [00:13<00:59, 2460437.81it/s]
 15%|█▍        | 25526272/170498071 [00:13<01:07, 2157504.30it/s]
 15%|█▌        | 25952256/170498071 [00:13<00:55, 2610034.61it/s]
 15%|█▌        | 26247168/170498071 [00:13<01:05, 2190682.56it/s]
 16%|█▌        | 26673152/170498071 [00:14<00:59, 2419144.84it/s]
 16%|█▌        | 26935296/170498071 [00:14<01:04, 2217843.36it/s]
 16%|█▌        | 27361280/170498071 [00:14<00:53, 2666918.29it/s]
 16%|█▌        | 27656192/170498071 [00:14<01:04, 2227854.32it/s]
 16%|█▋        | 28114944/170498071 [00:14<00:52, 2728293.89it/s]
 17%|█▋        | 28442624/170498071 [00:14<01:01, 2323846.51it/s]
 17%|█▋        | 28901376/170498071 [00:14<00:55, 2539026.58it/s]
 17%|█▋        | 29196288/170498071 [00:15<00:58, 2395884.81it/s]
 17%|█▋        | 29655040/170498071 [00:15<00:48, 2875128.22it/s]
 18%|█▊        | 29982720/170498071 [00:15<00:58, 2420208.81it/s]
 18%|█▊        | 30441472/170498071 [00:15<00:48, 2887291.39it/s]
 18%|█▊        | 30769152/170498071 [00:15<00:57, 2437411.27it/s]
 18%|█▊        | 31260672/170498071 [00:15<00:47, 2932977.68it/s]
 19%|█▊        | 31621120/170498071 [00:15<00:54, 2548276.90it/s]
 19%|█▉        | 32112640/170498071 [00:16<00:50, 2766449.07it/s]
 19%|█▉        | 32440320/170498071 [00:16<00:52, 2624631.05it/s]
 19%|█▉        | 32964608/170498071 [00:16<00:43, 3157316.09it/s]
 20%|█▉        | 33325056/170498071 [00:16<00:50, 2693517.53it/s]
 20%|█▉        | 33849344/170498071 [00:16<00:42, 3242505.33it/s]
 20%|██        | 34242560/170498071 [00:16<00:49, 2776669.52it/s]
 20%|██        | 34799616/170498071 [00:17<00:44, 3056532.34it/s]
 21%|██        | 35160064/170498071 [00:17<00:46, 2893936.41it/s]
 21%|██        | 35717120/170498071 [00:17<00:38, 3483675.60it/s]
 21%|██        | 36110336/170498071 [00:17<00:45, 2939344.48it/s]
 22%|██▏       | 36700160/170498071 [00:17<00:37, 3571356.70it/s]
 22%|██▏       | 37126144/170498071 [00:17<00:43, 3053553.28it/s]
 22%|██▏       | 37715968/170498071 [00:17<00:36, 3669692.52it/s]
 22%|██▏       | 38141952/170498071 [00:18<00:42, 3123732.92it/s]
 23%|██▎       | 38764544/170498071 [00:18<00:34, 3791694.10it/s]
 23%|██▎       | 39223296/170498071 [00:18<00:40, 3253251.57it/s]
 23%|██▎       | 39878656/170498071 [00:18<00:33, 3954011.98it/s]
 24%|██▎       | 40337408/170498071 [00:18<00:38, 3361161.13it/s]
 24%|██▍       | 41058304/170498071 [00:18<00:31, 4158333.50it/s]
 24%|██▍       | 41549824/170498071 [00:18<00:36, 3548441.94it/s]
 25%|██▍       | 42270720/170498071 [00:19<00:29, 4328055.20it/s]
 25%|██▌       | 42795008/170498071 [00:19<00:34, 3701327.03it/s]
 26%|██▌       | 43548672/170498071 [00:19<00:28, 4517155.91it/s]
 26%|██▌       | 44105728/170498071 [00:19<00:32, 3872303.60it/s]
 26%|██▋       | 44892160/170498071 [00:19<00:26, 4699047.54it/s]
 27%|██▋       | 45449216/170498071 [00:19<00:31, 4021459.55it/s]
 27%|██▋       | 46301184/170498071 [00:19<00:27, 4551701.20it/s]
 27%|██▋       | 46825472/170498071 [00:20<00:29, 4235819.68it/s]
 28%|██▊       | 47710208/170498071 [00:20<00:23, 5221958.42it/s]
 28%|██▊       | 48300032/170498071 [00:20<00:27, 4393696.12it/s]
 29%|██▉       | 49217536/170498071 [00:20<00:22, 5410797.76it/s]
 29%|██▉       | 49840128/170498071 [00:20<00:26, 4567706.20it/s]
 30%|██▉       | 50823168/170498071 [00:20<00:22, 5213077.71it/s]
 30%|███       | 51412992/170498071 [00:21<00:24, 4813956.48it/s]
 31%|███       | 52461568/170498071 [00:21<00:19, 6020600.08it/s]
 31%|███       | 53149696/170498071 [00:21<00:23, 5038421.05it/s]
 32%|███▏      | 54198272/170498071 [00:21<00:20, 5748277.00it/s]
 32%|███▏      | 54853632/170498071 [00:21<00:21, 5271745.68it/s]
 33%|███▎      | 56000512/170498071 [00:21<00:18, 6105742.15it/s]
 33%|███▎      | 56655872/170498071 [00:21<00:20, 5518270.96it/s]
 34%|███▍      | 57835520/170498071 [00:22<00:16, 6902856.06it/s]
 34%|███▍      | 58621952/170498071 [00:22<00:19, 5746344.74it/s]
 35%|███▌      | 59801600/170498071 [00:22<00:15, 7030559.59it/s]
 36%|███▌      | 60620800/170498071 [00:22<00:18, 5966072.58it/s]
 36%|███▌      | 61800448/170498071 [00:22<00:15, 7207721.42it/s]
 37%|███▋      | 62652416/170498071 [00:22<00:17, 6196349.58it/s]
 37%|███▋      | 63832064/170498071 [00:22<00:14, 7394357.85it/s]
 38%|███▊      | 64684032/170498071 [00:23<00:16, 6413744.27it/s]
 39%|███▊      | 65863680/170498071 [00:23<00:13, 7591240.36it/s]
 39%|███▉      | 66748416/170498071 [00:23<00:15, 6668187.04it/s]
 40%|███▉      | 67928064/170498071 [00:23<00:13, 7796881.40it/s]
 40%|████      | 68812800/170498071 [00:23<00:12, 8008649.57it/s]
 41%|████      | 69697536/170498071 [00:23<00:13, 7450368.99it/s]
 42%|████▏     | 70877184/170498071 [00:23<00:11, 8522770.63it/s]
 42%|████▏     | 71794688/170498071 [00:24<00:22, 4446982.11it/s]
 43%|████▎     | 73072640/170498071 [00:24<00:17, 5698322.92it/s]
 44%|████▎     | 74350592/170498071 [00:24<00:15, 6094816.74it/s]
 44%|████▍     | 75169792/170498071 [00:24<00:23, 4140511.21it/s]
 45%|████▍     | 76021760/170498071 [00:25<00:22, 4238102.45it/s]
 45%|████▌     | 76906496/170498071 [00:25<00:27, 3421319.50it/s]
 46%|████▌     | 78610432/170498071 [00:25<00:21, 4292786.72it/s]
 47%|████▋     | 79527936/170498071 [00:26<00:23, 3909886.51it/s]
 47%|████▋     | 80445440/170498071 [00:26<00:22, 3916154.06it/s]
 47%|████▋     | 80904192/170498071 [00:26<00:22, 4003082.18it/s]
 48%|████▊     | 81395712/170498071 [00:26<00:24, 3596597.86it/s]
 48%|████▊     | 81854464/170498071 [00:26<00:23, 3762231.11it/s]
 48%|████▊     | 82378752/170498071 [00:26<00:28, 3111525.08it/s]
 49%|████▉     | 83361792/170498071 [00:27<00:25, 3412694.48it/s]
 49%|████▉     | 83951616/170498071 [00:27<00:22, 3820002.70it/s]
 50%|████▉     | 84410368/170498071 [00:27<00:27, 3087993.23it/s]
 50%|█████     | 85393408/170498071 [00:27<00:26, 3202552.29it/s]
 51%|█████     | 86409216/170498071 [00:28<00:20, 4050980.45it/s]
 51%|█████     | 86900736/170498071 [00:28<00:23, 3577803.47it/s]
 51%|█████▏    | 87457792/170498071 [00:28<00:24, 3376260.69it/s]
 52%|█████▏    | 87916544/170498071 [00:28<00:22, 3590772.08it/s]
 52%|█████▏    | 88539136/170498071 [00:28<00:26, 3128669.80it/s]
 53%|█████▎    | 89554944/170498071 [00:28<00:18, 4357055.05it/s]
 53%|█████▎    | 90112000/170498071 [00:29<00:22, 3605358.63it/s]
 53%|█████▎    | 90669056/170498071 [00:29<00:25, 3106965.75it/s]
 54%|█████▍    | 91717632/170498071 [00:29<00:20, 3797526.42it/s]
 54%|█████▍    | 92176384/170498071 [00:29<00:21, 3640810.60it/s]
 54%|█████▍    | 92798976/170498071 [00:29<00:20, 3777432.61it/s]
 55%|█████▍    | 93224960/170498071 [00:29<00:21, 3563444.67it/s]
 55%|█████▌    | 93880320/170498071 [00:30<00:22, 3336526.82it/s]
 56%|█████▌    | 94633984/170498071 [00:30<00:18, 4119011.36it/s]
 56%|█████▌    | 95125504/170498071 [00:30<00:22, 3290151.47it/s]
 56%|█████▋    | 96043008/170498071 [00:30<00:17, 4221384.52it/s]
 57%|█████▋    | 96567296/170498071 [00:30<00:20, 3652678.19it/s]
 57%|█████▋    | 97157120/170498071 [00:31<00:22, 3309112.43it/s]
 57%|█████▋    | 97976320/170498071 [00:31<00:17, 4175629.29it/s]
 58%|█████▊    | 98500608/170498071 [00:31<00:21, 3385051.06it/s]
 58%|█████▊    | 99319808/170498071 [00:31<00:17, 4178131.27it/s]
 59%|█████▊    | 99844096/170498071 [00:31<00:19, 3612718.06it/s]
 59%|█████▉    | 100433920/170498071 [00:31<00:21, 3268087.30it/s]
 59%|█████▉    | 101285888/170498071 [00:32<00:16, 4207496.67it/s]
 60%|█████▉    | 101810176/170498071 [00:32<00:20, 3403581.75it/s]
 60%|██████    | 102596608/170498071 [00:32<00:16, 4159271.27it/s]
 60%|██████    | 103120896/170498071 [00:32<00:18, 3594707.67it/s]
 61%|██████    | 103710720/170498071 [00:32<00:20, 3298738.06it/s]
 61%|██████▏   | 104464384/170498071 [00:32<00:16, 4064988.35it/s]
 62%|██████▏   | 104988672/170498071 [00:33<00:19, 3313803.68it/s]
 62%|██████▏   | 105873408/170498071 [00:33<00:14, 4336006.73it/s]
 62%|██████▏   | 106430464/170498071 [00:33<00:17, 3633433.91it/s]
 63%|██████▎   | 106987520/170498071 [00:33<00:19, 3290837.42it/s]
 63%|██████▎   | 107741184/170498071 [00:33<00:15, 4058146.57it/s]
 63%|██████▎   | 108265472/170498071 [00:34<00:18, 3309436.53it/s]
 64%|██████▍   | 109182976/170498071 [00:34<00:17, 3473526.22it/s]
 64%|██████▍   | 109903872/170498071 [00:34<00:14, 4116773.41it/s]
 65%|██████▍   | 110428160/170498071 [00:34<00:17, 3375373.95it/s]
 65%|██████▌   | 111345664/170498071 [00:34<00:13, 4392138.04it/s]
 66%|██████▌   | 111902720/170498071 [00:35<00:15, 3685982.43it/s]
 66%|██████▌   | 112459776/170498071 [00:35<00:14, 3906079.93it/s]
 66%|██████▌   | 112951296/170498071 [00:35<00:16, 3580909.36it/s]
 67%|██████▋   | 113573888/170498071 [00:35<00:15, 3789856.13it/s]
 67%|██████▋   | 113999872/170498071 [00:35<00:15, 3531856.52it/s]
 67%|██████▋   | 114688000/170498071 [00:35<00:16, 3367096.99it/s]
 68%|██████▊   | 115408896/170498071 [00:35<00:13, 4113926.49it/s]
 68%|██████▊   | 115900416/170498071 [00:36<00:16, 3290717.72it/s]
 69%|██████▊   | 116916224/170498071 [00:36<00:13, 3987403.77it/s]
 69%|██████▉   | 117374976/170498071 [00:36<00:14, 3767198.62it/s]
 69%|██████▉   | 118030336/170498071 [00:36<00:12, 4124352.10it/s]
 69%|██████▉   | 118489088/170498071 [00:36<00:14, 3668009.15it/s]
 70%|██████▉   | 119177216/170498071 [00:36<00:14, 3586549.50it/s]
 70%|███████   | 119701504/170498071 [00:37<00:13, 3904842.41it/s]
 71%|███████   | 120324096/170498071 [00:37<00:13, 3656651.42it/s]
 71%|███████   | 120848384/170498071 [00:37<00:12, 3969520.00it/s]
 71%|███████   | 121470976/170498071 [00:37<00:11, 4091040.80it/s]
 72%|███████▏  | 121929728/170498071 [00:37<00:12, 3800629.46it/s]
 72%|███████▏  | 122617856/170498071 [00:37<00:10, 4497321.35it/s]
 72%|███████▏  | 123109376/170498071 [00:37<00:12, 3725442.39it/s]
 73%|███████▎  | 123830272/170498071 [00:38<00:12, 3680727.58it/s]
 73%|███████▎  | 124354560/170498071 [00:38<00:11, 3996089.30it/s]
 73%|███████▎  | 125009920/170498071 [00:38<00:10, 4170975.35it/s]
 74%|███████▎  | 125468672/170498071 [00:38<00:11, 3867898.41it/s]
 74%|███████▍  | 126222336/170498071 [00:38<00:10, 4307479.13it/s]
 74%|███████▍  | 126681088/170498071 [00:38<00:11, 3881168.34it/s]
 75%|███████▍  | 127434752/170498071 [00:38<00:09, 4691078.20it/s]
 75%|███████▌  | 127959040/170498071 [00:39<00:10, 3895626.74it/s]
 75%|███████▌  | 128712704/170498071 [00:39<00:10, 4137035.22it/s]
 76%|███████▌  | 129171456/170498071 [00:39<00:10, 3975175.46it/s]
 76%|███████▌  | 129990656/170498071 [00:39<00:09, 4397788.71it/s]
 77%|███████▋  | 130449408/170498071 [00:39<00:09, 4055473.73it/s]
 77%|███████▋  | 131268608/170498071 [00:39<00:08, 4749772.61it/s]
 77%|███████▋  | 131760128/170498071 [00:40<00:09, 4080556.95it/s]
 78%|███████▊  | 132612096/170498071 [00:40<00:08, 4608806.48it/s]
 78%|███████▊  | 133103616/170498071 [00:40<00:08, 4194909.94it/s]
 79%|███████▊  | 133955584/170498071 [00:40<00:07, 4892924.18it/s]
 79%|███████▉  | 134479872/170498071 [00:40<00:08, 4257448.42it/s]
 79%|███████▉  | 135331840/170498071 [00:40<00:07, 5001710.74it/s]
 80%|███████▉  | 135888896/170498071 [00:40<00:07, 4328757.10it/s]
 80%|████████  | 136773632/170498071 [00:41<00:06, 4944172.59it/s]
 81%|████████  | 137297920/170498071 [00:41<00:07, 4419610.73it/s]
 81%|████████  | 138215424/170498071 [00:41<00:06, 5275402.32it/s]
 81%|████████▏ | 138805248/170498071 [00:41<00:06, 4546833.76it/s]
 82%|████████▏ | 139722752/170498071 [00:41<00:05, 5333716.71it/s]
 82%|████████▏ | 140312576/170498071 [00:41<00:06, 4628943.01it/s]
 83%|████████▎ | 141262848/170498071 [00:41<00:05, 5491770.23it/s]
 83%|████████▎ | 141885440/170498071 [00:42<00:05, 4773115.83it/s]
 84%|████████▍ | 142835712/170498071 [00:42<00:04, 5682821.61it/s]
 84%|████████▍ | 143491072/170498071 [00:42<00:05, 4889975.93it/s]
 85%|████████▍ | 144474112/170498071 [00:42<00:04, 5789685.58it/s]
 85%|████████▌ | 145129472/170498071 [00:42<00:05, 4579096.41it/s]
 85%|████████▌ | 145719296/170498071 [00:42<00:05, 4780484.93it/s]
 86%|████████▌ | 146276352/170498071 [00:42<00:04, 4921177.08it/s]
 86%|████████▌ | 146833408/170498071 [00:43<00:04, 5013262.06it/s]
 86%|████████▋ | 147390464/170498071 [00:43<00:04, 5107424.05it/s]
 87%|████████▋ | 147947520/170498071 [00:43<00:04, 5228405.32it/s]
 87%|████████▋ | 148537344/170498071 [00:43<00:04, 5341095.76it/s]
 87%|████████▋ | 149127168/170498071 [00:43<00:03, 5411099.58it/s]
 88%|████████▊ | 149684224/170498071 [00:43<00:03, 5395473.76it/s]
 88%|████████▊ | 150274048/170498071 [00:43<00:03, 5444393.25it/s]
 88%|████████▊ | 150831104/170498071 [00:43<00:03, 5468836.27it/s]
 89%|████████▉ | 151388160/170498071 [00:43<00:03, 5490106.75it/s]
 89%|████████▉ | 151945216/170498071 [00:43<00:03, 5507145.37it/s]
 89%|████████▉ | 152502272/170498071 [00:44<00:03, 5494086.96it/s]
 90%|████████▉ | 153092096/170498071 [00:44<00:03, 5546950.09it/s]
 90%|█████████ | 153845760/170498071 [00:44<00:02, 6118873.99it/s]
 91%|█████████ | 154468352/170498071 [00:44<00:02, 6063790.87it/s]
 91%|█████████ | 155090944/170498071 [00:44<00:02, 6087548.50it/s]
 91%|█████████▏| 155910144/170498071 [00:44<00:02, 6681160.61it/s]
 92%|█████████▏| 156598272/170498071 [00:44<00:02, 6585844.31it/s]
 92%|█████████▏| 157319168/170498071 [00:44<00:01, 6745311.39it/s]
 93%|█████████▎| 158105600/170498071 [00:44<00:01, 7039787.07it/s]
 93%|█████████▎| 158826496/170498071 [00:45<00:01, 6937519.82it/s]
 94%|█████████▎| 159645696/170498071 [00:45<00:01, 7291209.32it/s]
 94%|█████████▍| 160399360/170498071 [00:45<00:01, 7341322.71it/s]
 95%|█████████▍| 161153024/170498071 [00:45<00:01, 7232363.82it/s]
 95%|█████████▌| 162070528/170498071 [00:45<00:01, 7702595.99it/s]
 96%|█████████▌| 162856960/170498071 [00:45<00:01, 7552333.40it/s]
 96%|█████████▌| 163643392/170498071 [00:45<00:00, 7497893.75it/s]
 97%|█████████▋| 164593664/170498071 [00:45<00:00, 7991078.21it/s]
 97%|█████████▋| 165412864/170498071 [00:45<00:00, 7856488.86it/s]
 97%|█████████▋| 166232064/170498071 [00:45<00:00, 7766452.85it/s]
 98%|█████████▊| 167215104/170498071 [00:46<00:00, 8346144.02it/s]
 99%|█████████▊| 168067072/170498071 [00:46<00:00, 8137627.23it/s]
 99%|█████████▉| 168886272/170498071 [00:46<00:00, 8108050.15it/s]
100%|█████████▉| 169705472/170498071 [00:46<00:00, 4754436.74it/s]
100%|██████████| 170498071/170498071 [00:46<00:00, 3655773.90it/s]
/data/happythgus/repos/DatasetCondensation/main_DM.py:91: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
/data/happythgus/repos/DatasetCondensation/main_DM.py:91: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
/home/happythgus/.local/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/happythgus/.local/lib/python3.9/site-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/home/happythgus/.local/lib/python3.9/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
Extracting data/cifar-10-python.tar.gz to data
Files already downloaded and verified

================== Exp 0 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f7bf1f7b400>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-01 21:08:14] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}
[2023-10-01 21:08:39] Evaluate_00: epoch = 1000 train time = 23 s train loss = 0.013603 train acc = 0.9980, test acc = 0.5141
[2023-10-01 21:09:03] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006844 train acc = 1.0000, test acc = 0.4967
[2023-10-01 21:09:28] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002475 train acc = 1.0000, test acc = 0.5005
[2023-10-01 21:09:52] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.019767 train acc = 1.0000, test acc = 0.5055
[2023-10-01 21:10:16] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002555 train acc = 1.0000, test acc = 0.5060
[2023-10-01 21:10:40] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013294 train acc = 1.0000, test acc = 0.5032
[2023-10-01 21:11:05] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.001711 train acc = 1.0000, test acc = 0.5083
[2023-10-01 21:11:29] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012613 train acc = 1.0000, test acc = 0.5121
[2023-10-01 21:11:53] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.008091 train acc = 1.0000, test acc = 0.5007
[2023-10-01 21:12:18] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.007179 train acc = 1.0000, test acc = 0.5025
[2023-10-01 21:12:42] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.000928 train acc = 1.0000, test acc = 0.5068
[2023-10-01 21:13:06] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015353 train acc = 1.0000, test acc = 0.5055
[2023-10-01 21:13:30] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.007397 train acc = 1.0000, test acc = 0.5046
[2023-10-01 21:13:55] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.006886 train acc = 1.0000, test acc = 0.5122
[2023-10-01 21:14:19] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002780 train acc = 1.0000, test acc = 0.5122
[2023-10-01 21:14:44] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003362 train acc = 1.0000, test acc = 0.5068
[2023-10-01 21:15:08] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.000967 train acc = 1.0000, test acc = 0.4967
[2023-10-01 21:15:32] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.009367 train acc = 1.0000, test acc = 0.5133
[2023-10-01 21:15:56] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004435 train acc = 1.0000, test acc = 0.5075
[2023-10-01 21:16:21] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.005204 train acc = 1.0000, test acc = 0.5070
Evaluate 20 random ConvNet, mean = 0.5061 std = 0.0050
-------------------------
[2023-10-01 21:16:21] iter = 00000, loss = 6.0278
[2023-10-01 21:16:22] iter = 00010, loss = 5.4176
[2023-10-01 21:16:23] iter = 00020, loss = 5.1850
[2023-10-01 21:16:24] iter = 00030, loss = 4.2700
[2023-10-01 21:16:25] iter = 00040, loss = 4.0581
[2023-10-01 21:16:26] iter = 00050, loss = 3.9457
[2023-10-01 21:16:26] iter = 00060, loss = 3.6217
[2023-10-01 21:16:27] iter = 00070, loss = 3.3468
[2023-10-01 21:16:28] iter = 00080, loss = 3.6251
[2023-10-01 21:16:29] iter = 00090, loss = 3.2184
[2023-10-01 21:16:30] iter = 00100, loss = 3.3035
[2023-10-01 21:16:31] iter = 00110, loss = 3.3048
[2023-10-01 21:16:32] iter = 00120, loss = 3.3551
[2023-10-01 21:16:33] iter = 00130, loss = 3.1481
[2023-10-01 21:16:34] iter = 00140, loss = 2.9357
[2023-10-01 21:16:34] iter = 00150, loss = 3.1082
[2023-10-01 21:16:35] iter = 00160, loss = 2.9488
[2023-10-01 21:16:36] iter = 00170, loss = 2.9086
[2023-10-01 21:16:37] iter = 00180, loss = 3.0383
[2023-10-01 21:16:38] iter = 00190, loss = 2.7440
[2023-10-01 21:16:39] iter = 00200, loss = 2.6648
[2023-10-01 21:16:40] iter = 00210, loss = 3.0665
[2023-10-01 21:16:41] iter = 00220, loss = 2.7461
[2023-10-01 21:16:41] iter = 00230, loss = 2.8641
[2023-10-01 21:16:42] iter = 00240, loss = 2.5066
[2023-10-01 21:16:43] iter = 00250, loss = 2.6575
[2023-10-01 21:16:44] iter = 00260, loss = 2.4654
[2023-10-01 21:16:45] iter = 00270, loss = 2.7573
[2023-10-01 21:16:46] iter = 00280, loss = 2.6140
[2023-10-01 21:16:47] iter = 00290, loss = 2.6353
[2023-10-01 21:16:48] iter = 00300, loss = 2.3899
[2023-10-01 21:16:49] iter = 00310, loss = 2.5940
[2023-10-01 21:16:50] iter = 00320, loss = 2.4501
[2023-10-01 21:16:51] iter = 00330, loss = 2.6114
[2023-10-01 21:16:52] iter = 00340, loss = 2.5298
[2023-10-01 21:16:53] iter = 00350, loss = 2.5520
[2023-10-01 21:16:53] iter = 00360, loss = 2.6408
[2023-10-01 21:16:54] iter = 00370, loss = 2.5083
[2023-10-01 21:16:55] iter = 00380, loss = 2.3569
[2023-10-01 21:16:56] iter = 00390, loss = 2.3966
[2023-10-01 21:16:57] iter = 00400, loss = 2.1654
[2023-10-01 21:16:58] iter = 00410, loss = 2.2669
[2023-10-01 21:16:59] iter = 00420, loss = 2.3443
[2023-10-01 21:17:00] iter = 00430, loss = 2.1658
[2023-10-01 21:17:01] iter = 00440, loss = 2.5037
[2023-10-01 21:17:01] iter = 00450, loss = 2.2940
[2023-10-01 21:17:02] iter = 00460, loss = 2.3505
[2023-10-01 21:17:03] iter = 00470, loss = 2.3982
[2023-10-01 21:17:04] iter = 00480, loss = 2.1766
[2023-10-01 21:17:05] iter = 00490, loss = 2.1712
[2023-10-01 21:17:06] iter = 00500, loss = 2.1061
[2023-10-01 21:17:07] iter = 00510, loss = 2.2786
[2023-10-01 21:17:08] iter = 00520, loss = 2.2852
[2023-10-01 21:17:09] iter = 00530, loss = 2.4800
[2023-10-01 21:17:10] iter = 00540, loss = 2.1875
[2023-10-01 21:17:11] iter = 00550, loss = 2.2654
[2023-10-01 21:17:12] iter = 00560, loss = 2.3672
[2023-10-01 21:17:12] iter = 00570, loss = 2.2065
[2023-10-01 21:17:13] iter = 00580, loss = 2.2521
[2023-10-01 21:17:14] iter = 00590, loss = 2.4112
[2023-10-01 21:17:15] iter = 00600, loss = 2.2250
[2023-10-01 21:17:16] iter = 00610, loss = 2.3873
[2023-10-01 21:17:17] iter = 00620, loss = 2.1967
[2023-10-01 21:17:18] iter = 00630, loss = 2.1008
[2023-10-01 21:17:19] iter = 00640, loss = 2.2677
[2023-10-01 21:17:20] iter = 00650, loss = 2.2341
[2023-10-01 21:17:21] iter = 00660, loss = 2.0832
[2023-10-01 21:17:22] iter = 00670, loss = 2.1598
[2023-10-01 21:17:23] iter = 00680, loss = 2.4129
[2023-10-01 21:17:23] iter = 00690, loss = 2.2631
[2023-10-01 21:17:24] iter = 00700, loss = 2.1571
[2023-10-01 21:17:25] iter = 00710, loss = 2.5982
[2023-10-01 21:17:26] iter = 00720, loss = 2.2063
[2023-10-01 21:17:27] iter = 00730, loss = 2.1906
[2023-10-01 21:17:28] iter = 00740, loss = 2.2463
[2023-10-01 21:17:29] iter = 00750, loss = 2.2505
[2023-10-01 21:17:30] iter = 00760, loss = 2.1504
[2023-10-01 21:17:30] iter = 00770, loss = 2.0195
[2023-10-01 21:17:31] iter = 00780, loss = 2.1346
[2023-10-01 21:17:32] iter = 00790, loss = 2.1107
[2023-10-01 21:17:33] iter = 00800, loss = 2.1601
[2023-10-01 21:17:34] iter = 00810, loss = 2.2163
[2023-10-01 21:17:35] iter = 00820, loss = 2.1051
[2023-10-01 21:17:36] iter = 00830, loss = 2.0945
[2023-10-01 21:17:37] iter = 00840, loss = 2.1536
[2023-10-01 21:17:38] iter = 00850, loss = 2.0878
[2023-10-01 21:17:39] iter = 00860, loss = 2.0388
[2023-10-01 21:17:40] iter = 00870, loss = 2.0321
[2023-10-01 21:17:40] iter = 00880, loss = 2.3017
[2023-10-01 21:17:41] iter = 00890, loss = 2.1788
[2023-10-01 21:17:42] iter = 00900, loss = 1.9793
[2023-10-01 21:17:43] iter = 00910, loss = 2.0973
[2023-10-01 21:17:44] iter = 00920, loss = 2.1015
[2023-10-01 21:17:45] iter = 00930, loss = 2.1224
[2023-10-01 21:17:46] iter = 00940, loss = 2.0452
[2023-10-01 21:17:47] iter = 00950, loss = 1.9036
[2023-10-01 21:17:48] iter = 00960, loss = 1.9080
[2023-10-01 21:17:49] iter = 00970, loss = 2.0517
[2023-10-01 21:17:49] iter = 00980, loss = 1.9938
[2023-10-01 21:17:50] iter = 00990, loss = 1.8557
[2023-10-01 21:17:51] iter = 01000, loss = 1.9306
[2023-10-01 21:17:52] iter = 01010, loss = 2.1398
[2023-10-01 21:17:53] iter = 01020, loss = 2.0831
[2023-10-01 21:17:54] iter = 01030, loss = 2.1408
[2023-10-01 21:17:55] iter = 01040, loss = 1.8844
[2023-10-01 21:17:56] iter = 01050, loss = 1.8998
[2023-10-01 21:17:57] iter = 01060, loss = 2.0263
[2023-10-01 21:17:58] iter = 01070, loss = 1.9782
[2023-10-01 21:17:59] iter = 01080, loss = 1.9953
[2023-10-01 21:17:59] iter = 01090, loss = 1.8605
[2023-10-01 21:18:00] iter = 01100, loss = 1.9639
[2023-10-01 21:18:01] iter = 01110, loss = 2.1108
[2023-10-01 21:18:02] iter = 01120, loss = 1.9784
[2023-10-01 21:18:03] iter = 01130, loss = 2.0567
[2023-10-01 21:18:04] iter = 01140, loss = 1.9758
[2023-10-01 21:18:05] iter = 01150, loss = 1.8730
[2023-10-01 21:18:06] iter = 01160, loss = 1.9884
[2023-10-01 21:18:07] iter = 01170, loss = 2.0928
[2023-10-01 21:18:07] iter = 01180, loss = 2.0872
[2023-10-01 21:18:08] iter = 01190, loss = 2.0697
[2023-10-01 21:18:09] iter = 01200, loss = 1.9885
[2023-10-01 21:18:10] iter = 01210, loss = 2.0530
[2023-10-01 21:18:11] iter = 01220, loss = 2.0542
[2023-10-01 21:18:12] iter = 01230, loss = 2.1358
[2023-10-01 21:18:13] iter = 01240, loss = 1.9872
[2023-10-01 21:18:14] iter = 01250, loss = 1.9818
[2023-10-01 21:18:15] iter = 01260, loss = 1.9997
[2023-10-01 21:18:16] iter = 01270, loss = 1.9665
[2023-10-01 21:18:17] iter = 01280, loss = 1.9646
[2023-10-01 21:18:17] iter = 01290, loss = 2.1877
[2023-10-01 21:18:18] iter = 01300, loss = 1.9042
[2023-10-01 21:18:19] iter = 01310, loss = 2.0493
[2023-10-01 21:18:20] iter = 01320, loss = 1.9895
[2023-10-01 21:18:21] iter = 01330, loss = 2.1004
[2023-10-01 21:18:22] iter = 01340, loss = 1.8661
[2023-10-01 21:18:23] iter = 01350, loss = 1.7400
[2023-10-01 21:18:24] iter = 01360, loss = 1.8170
[2023-10-01 21:18:25] iter = 01370, loss = 2.0438
[2023-10-01 21:18:26] iter = 01380, loss = 2.0579
[2023-10-01 21:18:27] iter = 01390, loss = 1.9573
[2023-10-01 21:18:27] iter = 01400, loss = 2.0572
[2023-10-01 21:18:28] iter = 01410, loss = 1.9828
[2023-10-01 21:18:29] iter = 01420, loss = 1.8880
[2023-10-01 21:18:30] iter = 01430, loss = 1.9889
[2023-10-01 21:18:31] iter = 01440, loss = 1.8627
[2023-10-01 21:18:32] iter = 01450, loss = 1.8858
[2023-10-01 21:18:33] iter = 01460, loss = 1.9164
[2023-10-01 21:18:34] iter = 01470, loss = 1.8583
[2023-10-01 21:18:35] iter = 01480, loss = 2.0347
[2023-10-01 21:18:36] iter = 01490, loss = 1.9034
[2023-10-01 21:18:36] iter = 01500, loss = 1.7979
[2023-10-01 21:18:37] iter = 01510, loss = 1.9235
[2023-10-01 21:18:38] iter = 01520, loss = 1.9559
[2023-10-01 21:18:39] iter = 01530, loss = 1.7874
[2023-10-01 21:18:40] iter = 01540, loss = 1.9351
[2023-10-01 21:18:41] iter = 01550, loss = 1.8970
[2023-10-01 21:18:42] iter = 01560, loss = 1.8670
[2023-10-01 21:18:43] iter = 01570, loss = 1.9034
[2023-10-01 21:18:44] iter = 01580, loss = 1.9700
[2023-10-01 21:18:45] iter = 01590, loss = 1.8604
[2023-10-01 21:18:46] iter = 01600, loss = 2.1103
[2023-10-01 21:18:47] iter = 01610, loss = 1.7026
[2023-10-01 21:18:47] iter = 01620, loss = 1.9384
[2023-10-01 21:18:48] iter = 01630, loss = 1.8212
[2023-10-01 21:18:49] iter = 01640, loss = 1.9185
[2023-10-01 21:18:50] iter = 01650, loss = 1.8836
[2023-10-01 21:18:51] iter = 01660, loss = 1.8636
[2023-10-01 21:18:52] iter = 01670, loss = 2.0082
[2023-10-01 21:18:53] iter = 01680, loss = 1.8508
[2023-10-01 21:18:54] iter = 01690, loss = 1.9491
[2023-10-01 21:18:55] iter = 01700, loss = 2.0087
[2023-10-01 21:18:56] iter = 01710, loss = 1.9619
[2023-10-01 21:18:57] iter = 01720, loss = 1.9040
[2023-10-01 21:18:57] iter = 01730, loss = 1.9077
[2023-10-01 21:18:58] iter = 01740, loss = 2.0177
[2023-10-01 21:18:59] iter = 01750, loss = 1.9965
[2023-10-01 21:19:00] iter = 01760, loss = 1.8409
[2023-10-01 21:19:01] iter = 01770, loss = 1.7828
[2023-10-01 21:19:02] iter = 01780, loss = 1.8379
[2023-10-01 21:19:03] iter = 01790, loss = 1.8662
[2023-10-01 21:19:04] iter = 01800, loss = 1.8795
[2023-10-01 21:19:05] iter = 01810, loss = 1.8217
[2023-10-01 21:19:06] iter = 01820, loss = 2.0439
[2023-10-01 21:19:07] iter = 01830, loss = 1.9512
[2023-10-01 21:19:07] iter = 01840, loss = 1.7921
[2023-10-01 21:19:08] iter = 01850, loss = 2.0058
[2023-10-01 21:19:09] iter = 01860, loss = 1.7493
[2023-10-01 21:19:10] iter = 01870, loss = 1.9616
[2023-10-01 21:19:11] iter = 01880, loss = 1.8001
[2023-10-01 21:19:12] iter = 01890, loss = 1.8379
[2023-10-01 21:19:13] iter = 01900, loss = 1.7147
[2023-10-01 21:19:14] iter = 01910, loss = 1.7849
[2023-10-01 21:19:15] iter = 01920, loss = 1.8546
[2023-10-01 21:19:16] iter = 01930, loss = 1.8374
[2023-10-01 21:19:17] iter = 01940, loss = 1.9146
[2023-10-01 21:19:17] iter = 01950, loss = 1.6988
[2023-10-01 21:19:18] iter = 01960, loss = 1.8059
[2023-10-01 21:19:19] iter = 01970, loss = 1.8362
[2023-10-01 21:19:20] iter = 01980, loss = 1.7890
[2023-10-01 21:19:21] iter = 01990, loss = 1.7302
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 62217}
[2023-10-01 21:19:46] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002823 train acc = 1.0000, test acc = 0.5890
[2023-10-01 21:20:10] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006204 train acc = 1.0000, test acc = 0.5803
[2023-10-01 21:20:35] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002148 train acc = 1.0000, test acc = 0.5900
[2023-10-01 21:20:59] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.007202 train acc = 1.0000, test acc = 0.5905
[2023-10-01 21:21:23] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.001201 train acc = 1.0000, test acc = 0.5811
[2023-10-01 21:21:47] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001185 train acc = 1.0000, test acc = 0.5904
[2023-10-01 21:22:12] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.001305 train acc = 1.0000, test acc = 0.5860
[2023-10-01 21:22:36] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.017859 train acc = 1.0000, test acc = 0.5850
[2023-10-01 21:23:00] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.009395 train acc = 1.0000, test acc = 0.5848
[2023-10-01 21:23:24] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.016648 train acc = 1.0000, test acc = 0.5868
[2023-10-01 21:23:49] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.017758 train acc = 1.0000, test acc = 0.5846
[2023-10-01 21:24:13] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.020750 train acc = 0.9980, test acc = 0.5838
[2023-10-01 21:24:37] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011940 train acc = 1.0000, test acc = 0.5906
[2023-10-01 21:25:01] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.016809 train acc = 1.0000, test acc = 0.5931
[2023-10-01 21:25:26] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.007931 train acc = 1.0000, test acc = 0.5927
[2023-10-01 21:25:50] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004843 train acc = 1.0000, test acc = 0.5802
[2023-10-01 21:26:14] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002966 train acc = 1.0000, test acc = 0.5874
[2023-10-01 21:26:39] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.011853 train acc = 1.0000, test acc = 0.5917
[2023-10-01 21:27:03] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.005604 train acc = 1.0000, test acc = 0.5880
[2023-10-01 21:27:27] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.015563 train acc = 1.0000, test acc = 0.5871
Evaluate 20 random ConvNet, mean = 0.5872 std = 0.0038
-------------------------
[2023-10-01 21:27:27] iter = 02000, loss = 1.8464
[2023-10-01 21:27:28] iter = 02010, loss = 1.8982
[2023-10-01 21:27:29] iter = 02020, loss = 1.7374
[2023-10-01 21:27:30] iter = 02030, loss = 1.9352
[2023-10-01 21:27:31] iter = 02040, loss = 1.8063
[2023-10-01 21:27:32] iter = 02050, loss = 1.7646
[2023-10-01 21:27:33] iter = 02060, loss = 1.7414
[2023-10-01 21:27:34] iter = 02070, loss = 1.8329
[2023-10-01 21:27:34] iter = 02080, loss = 1.8925
[2023-10-01 21:27:35] iter = 02090, loss = 1.8081
[2023-10-01 21:27:36] iter = 02100, loss = 1.9696
[2023-10-01 21:27:37] iter = 02110, loss = 1.6678
[2023-10-01 21:27:38] iter = 02120, loss = 1.8765
[2023-10-01 21:27:39] iter = 02130, loss = 1.7977
[2023-10-01 21:27:40] iter = 02140, loss = 1.9367
[2023-10-01 21:27:41] iter = 02150, loss = 1.7395
[2023-10-01 21:27:42] iter = 02160, loss = 2.0130
[2023-10-01 21:27:43] iter = 02170, loss = 1.8808
[2023-10-01 21:27:44] iter = 02180, loss = 1.7313
[2023-10-01 21:27:44] iter = 02190, loss = 1.8821
[2023-10-01 21:27:45] iter = 02200, loss = 1.8552
[2023-10-01 21:27:46] iter = 02210, loss = 1.8182
[2023-10-01 21:27:47] iter = 02220, loss = 2.0745
[2023-10-01 21:27:48] iter = 02230, loss = 1.8880
[2023-10-01 21:27:49] iter = 02240, loss = 1.8497
[2023-10-01 21:27:50] iter = 02250, loss = 1.8441
[2023-10-01 21:27:51] iter = 02260, loss = 1.6964
[2023-10-01 21:27:52] iter = 02270, loss = 1.7269
[2023-10-01 21:27:53] iter = 02280, loss = 1.8256
[2023-10-01 21:27:53] iter = 02290, loss = 1.7514
[2023-10-01 21:27:54] iter = 02300, loss = 1.8764
[2023-10-01 21:27:55] iter = 02310, loss = 1.7448
[2023-10-01 21:27:56] iter = 02320, loss = 1.7288
[2023-10-01 21:27:57] iter = 02330, loss = 1.7270
[2023-10-01 21:27:58] iter = 02340, loss = 2.0016
[2023-10-01 21:27:59] iter = 02350, loss = 1.7032
[2023-10-01 21:28:00] iter = 02360, loss = 1.8971
[2023-10-01 21:28:01] iter = 02370, loss = 1.8241
[2023-10-01 21:28:02] iter = 02380, loss = 1.7989
[2023-10-01 21:28:03] iter = 02390, loss = 2.0187
[2023-10-01 21:28:04] iter = 02400, loss = 1.9549
[2023-10-01 21:28:04] iter = 02410, loss = 1.8142
[2023-10-01 21:28:05] iter = 02420, loss = 1.7167
[2023-10-01 21:28:06] iter = 02430, loss = 1.8206
[2023-10-01 21:28:07] iter = 02440, loss = 1.8217
[2023-10-01 21:28:08] iter = 02450, loss = 1.9178
[2023-10-01 21:28:09] iter = 02460, loss = 1.7515
[2023-10-01 21:28:10] iter = 02470, loss = 1.9209
[2023-10-01 21:28:11] iter = 02480, loss = 1.8363
[2023-10-01 21:28:12] iter = 02490, loss = 1.7837
[2023-10-01 21:28:12] iter = 02500, loss = 1.7865
[2023-10-01 21:28:13] iter = 02510, loss = 1.8128
[2023-10-01 21:28:14] iter = 02520, loss = 1.7379
[2023-10-01 21:28:15] iter = 02530, loss = 1.7888
[2023-10-01 21:28:16] iter = 02540, loss = 1.7160
[2023-10-01 21:28:17] iter = 02550, loss = 1.8468
[2023-10-01 21:28:18] iter = 02560, loss = 1.8579
[2023-10-01 21:28:19] iter = 02570, loss = 1.9054
[2023-10-01 21:28:20] iter = 02580, loss = 1.7737
[2023-10-01 21:28:21] iter = 02590, loss = 1.7614
[2023-10-01 21:28:21] iter = 02600, loss = 1.7378
[2023-10-01 21:28:22] iter = 02610, loss = 1.7781
[2023-10-01 21:28:23] iter = 02620, loss = 1.8093
[2023-10-01 21:28:24] iter = 02630, loss = 1.7514
[2023-10-01 21:28:25] iter = 02640, loss = 2.0502
[2023-10-01 21:28:26] iter = 02650, loss = 1.9134
[2023-10-01 21:28:27] iter = 02660, loss = 1.8372
[2023-10-01 21:28:28] iter = 02670, loss = 1.7245
[2023-10-01 21:28:29] iter = 02680, loss = 1.8592
[2023-10-01 21:28:30] iter = 02690, loss = 1.9114
[2023-10-01 21:28:31] iter = 02700, loss = 1.8492
[2023-10-01 21:28:32] iter = 02710, loss = 1.8145
[2023-10-01 21:28:32] iter = 02720, loss = 1.8305
[2023-10-01 21:28:33] iter = 02730, loss = 1.6912
[2023-10-01 21:28:34] iter = 02740, loss = 1.7389
[2023-10-01 21:28:35] iter = 02750, loss = 1.7524
[2023-10-01 21:28:36] iter = 02760, loss = 1.8053
[2023-10-01 21:28:37] iter = 02770, loss = 1.6949
[2023-10-01 21:28:38] iter = 02780, loss = 1.8350
[2023-10-01 21:28:39] iter = 02790, loss = 1.7196
[2023-10-01 21:28:40] iter = 02800, loss = 1.7856
[2023-10-01 21:28:41] iter = 02810, loss = 1.7669
[2023-10-01 21:28:41] iter = 02820, loss = 1.6051
[2023-10-01 21:28:42] iter = 02830, loss = 1.7918
[2023-10-01 21:28:43] iter = 02840, loss = 1.7376
[2023-10-01 21:28:44] iter = 02850, loss = 1.7991
[2023-10-01 21:28:45] iter = 02860, loss = 1.7910
[2023-10-01 21:28:46] iter = 02870, loss = 1.7070
[2023-10-01 21:28:47] iter = 02880, loss = 1.7109
[2023-10-01 21:28:48] iter = 02890, loss = 1.6931
[2023-10-01 21:28:49] iter = 02900, loss = 1.7209
[2023-10-01 21:28:50] iter = 02910, loss = 1.7531
[2023-10-01 21:28:51] iter = 02920, loss = 1.8303
[2023-10-01 21:28:52] iter = 02930, loss = 1.7376
[2023-10-01 21:28:53] iter = 02940, loss = 1.8981
[2023-10-01 21:28:53] iter = 02950, loss = 1.7750
[2023-10-01 21:28:54] iter = 02960, loss = 1.7060
[2023-10-01 21:28:55] iter = 02970, loss = 1.8284
[2023-10-01 21:28:56] iter = 02980, loss = 1.8104
[2023-10-01 21:28:57] iter = 02990, loss = 1.8859
[2023-10-01 21:28:58] iter = 03000, loss = 1.9327
[2023-10-01 21:28:59] iter = 03010, loss = 1.7739
[2023-10-01 21:29:00] iter = 03020, loss = 1.7248
[2023-10-01 21:29:01] iter = 03030, loss = 1.6584
[2023-10-01 21:29:02] iter = 03040, loss = 1.8885
[2023-10-01 21:29:03] iter = 03050, loss = 1.9182
[2023-10-01 21:29:04] iter = 03060, loss = 1.7972
[2023-10-01 21:29:05] iter = 03070, loss = 1.6665
[2023-10-01 21:29:06] iter = 03080, loss = 1.7868
[2023-10-01 21:29:07] iter = 03090, loss = 1.7747
[2023-10-01 21:29:07] iter = 03100, loss = 1.6868
[2023-10-01 21:29:08] iter = 03110, loss = 1.6771
[2023-10-01 21:29:09] iter = 03120, loss = 1.8481
[2023-10-01 21:29:10] iter = 03130, loss = 1.7566
[2023-10-01 21:29:11] iter = 03140, loss = 1.7408
[2023-10-01 21:29:12] iter = 03150, loss = 1.7238
[2023-10-01 21:29:13] iter = 03160, loss = 1.7889
[2023-10-01 21:29:14] iter = 03170, loss = 1.6423
[2023-10-01 21:29:15] iter = 03180, loss = 1.6575
[2023-10-01 21:29:16] iter = 03190, loss = 1.7796
[2023-10-01 21:29:17] iter = 03200, loss = 1.9369
[2023-10-01 21:29:18] iter = 03210, loss = 1.7321
[2023-10-01 21:29:18] iter = 03220, loss = 1.6887
[2023-10-01 21:29:19] iter = 03230, loss = 1.7601
[2023-10-01 21:29:20] iter = 03240, loss = 1.7618
[2023-10-01 21:29:21] iter = 03250, loss = 1.7290
[2023-10-01 21:29:22] iter = 03260, loss = 1.8103
[2023-10-01 21:29:23] iter = 03270, loss = 1.7932
[2023-10-01 21:29:24] iter = 03280, loss = 1.5801
[2023-10-01 21:29:25] iter = 03290, loss = 1.6587
[2023-10-01 21:29:26] iter = 03300, loss = 1.6434
[2023-10-01 21:29:26] iter = 03310, loss = 1.7132
[2023-10-01 21:29:27] iter = 03320, loss = 1.7809
[2023-10-01 21:29:28] iter = 03330, loss = 1.7486
[2023-10-01 21:29:29] iter = 03340, loss = 1.8680
[2023-10-01 21:29:30] iter = 03350, loss = 1.9814
[2023-10-01 21:29:31] iter = 03360, loss = 1.6270
[2023-10-01 21:29:32] iter = 03370, loss = 1.7820
[2023-10-01 21:29:33] iter = 03380, loss = 1.8128
[2023-10-01 21:29:34] iter = 03390, loss = 1.6209
[2023-10-01 21:29:34] iter = 03400, loss = 1.6946
[2023-10-01 21:29:35] iter = 03410, loss = 1.8297
[2023-10-01 21:29:36] iter = 03420, loss = 1.6966
[2023-10-01 21:29:37] iter = 03430, loss = 1.7357
[2023-10-01 21:29:38] iter = 03440, loss = 1.7677
[2023-10-01 21:29:39] iter = 03450, loss = 1.6811
[2023-10-01 21:29:40] iter = 03460, loss = 1.8437
[2023-10-01 21:29:41] iter = 03470, loss = 1.7353
[2023-10-01 21:29:42] iter = 03480, loss = 1.7260
[2023-10-01 21:29:43] iter = 03490, loss = 1.7625
[2023-10-01 21:29:43] iter = 03500, loss = 1.7544
[2023-10-01 21:29:44] iter = 03510, loss = 1.7616
[2023-10-01 21:29:45] iter = 03520, loss = 1.5960
[2023-10-01 21:29:46] iter = 03530, loss = 1.7852
[2023-10-01 21:29:47] iter = 03540, loss = 1.6227
[2023-10-01 21:29:48] iter = 03550, loss = 1.6706
[2023-10-01 21:29:49] iter = 03560, loss = 1.7337
[2023-10-01 21:29:50] iter = 03570, loss = 1.6667
[2023-10-01 21:29:51] iter = 03580, loss = 1.7208
[2023-10-01 21:29:52] iter = 03590, loss = 1.7994
[2023-10-01 21:29:52] iter = 03600, loss = 1.8380
[2023-10-01 21:29:53] iter = 03610, loss = 1.8043
[2023-10-01 21:29:54] iter = 03620, loss = 1.6341
[2023-10-01 21:29:55] iter = 03630, loss = 1.7180
[2023-10-01 21:29:56] iter = 03640, loss = 1.6262
[2023-10-01 21:29:57] iter = 03650, loss = 1.6522
[2023-10-01 21:29:58] iter = 03660, loss = 1.7077
[2023-10-01 21:29:59] iter = 03670, loss = 1.8445
[2023-10-01 21:30:00] iter = 03680, loss = 1.7830
[2023-10-01 21:30:01] iter = 03690, loss = 1.6445
[2023-10-01 21:30:01] iter = 03700, loss = 1.6972
[2023-10-01 21:30:02] iter = 03710, loss = 1.8141
[2023-10-01 21:30:03] iter = 03720, loss = 1.6777
[2023-10-01 21:30:04] iter = 03730, loss = 1.7365
[2023-10-01 21:30:05] iter = 03740, loss = 1.6736
[2023-10-01 21:30:06] iter = 03750, loss = 1.7023
[2023-10-01 21:30:07] iter = 03760, loss = 1.6947
[2023-10-01 21:30:08] iter = 03770, loss = 1.7624
[2023-10-01 21:30:09] iter = 03780, loss = 1.6125
[2023-10-01 21:30:10] iter = 03790, loss = 1.7240
[2023-10-01 21:30:11] iter = 03800, loss = 1.5806
[2023-10-01 21:30:11] iter = 03810, loss = 1.6360
[2023-10-01 21:30:12] iter = 03820, loss = 1.6446
[2023-10-01 21:30:13] iter = 03830, loss = 1.8768
[2023-10-01 21:30:14] iter = 03840, loss = 1.7022
[2023-10-01 21:30:15] iter = 03850, loss = 1.7920
[2023-10-01 21:30:16] iter = 03860, loss = 1.6569
[2023-10-01 21:30:17] iter = 03870, loss = 1.6960
[2023-10-01 21:30:18] iter = 03880, loss = 1.5462
[2023-10-01 21:30:19] iter = 03890, loss = 1.7326
[2023-10-01 21:30:20] iter = 03900, loss = 1.5472
[2023-10-01 21:30:20] iter = 03910, loss = 1.6751
[2023-10-01 21:30:21] iter = 03920, loss = 1.7423
[2023-10-01 21:30:22] iter = 03930, loss = 1.6485
[2023-10-01 21:30:23] iter = 03940, loss = 1.6429
[2023-10-01 21:30:24] iter = 03950, loss = 1.6247
[2023-10-01 21:30:25] iter = 03960, loss = 1.7433
[2023-10-01 21:30:26] iter = 03970, loss = 1.7369
[2023-10-01 21:30:27] iter = 03980, loss = 1.7670
[2023-10-01 21:30:28] iter = 03990, loss = 1.6972
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 29095}
[2023-10-01 21:30:53] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.003773 train acc = 1.0000, test acc = 0.6095
[2023-10-01 21:31:17] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001529 train acc = 1.0000, test acc = 0.6002
[2023-10-01 21:31:41] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003480 train acc = 1.0000, test acc = 0.5958
[2023-10-01 21:32:06] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003267 train acc = 1.0000, test acc = 0.6016
[2023-10-01 21:32:30] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.009811 train acc = 1.0000, test acc = 0.6078
[2023-10-01 21:32:54] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.016567 train acc = 0.9980, test acc = 0.6055
[2023-10-01 21:33:18] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002301 train acc = 1.0000, test acc = 0.6055
[2023-10-01 21:33:42] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.005695 train acc = 1.0000, test acc = 0.5954
[2023-10-01 21:34:07] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.009916 train acc = 1.0000, test acc = 0.6044
[2023-10-01 21:34:31] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003076 train acc = 1.0000, test acc = 0.6069
[2023-10-01 21:34:55] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003574 train acc = 1.0000, test acc = 0.6036
[2023-10-01 21:35:20] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.013624 train acc = 1.0000, test acc = 0.6013
[2023-10-01 21:35:44] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011548 train acc = 1.0000, test acc = 0.6037
[2023-10-01 21:36:09] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.007562 train acc = 1.0000, test acc = 0.6027
[2023-10-01 21:36:33] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003198 train acc = 1.0000, test acc = 0.6084
[2023-10-01 21:36:57] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.016969 train acc = 0.9980, test acc = 0.6097
[2023-10-01 21:37:21] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003240 train acc = 1.0000, test acc = 0.6060
[2023-10-01 21:37:46] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.018319 train acc = 1.0000, test acc = 0.5965
[2023-10-01 21:38:10] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.010840 train acc = 1.0000, test acc = 0.5978
[2023-10-01 21:38:34] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.011327 train acc = 1.0000, test acc = 0.6045
Evaluate 20 random ConvNet, mean = 0.6033 std = 0.0043
-------------------------
[2023-10-01 21:38:34] iter = 04000, loss = 1.6809
[2023-10-01 21:38:35] iter = 04010, loss = 1.7246
[2023-10-01 21:38:36] iter = 04020, loss = 1.9084
[2023-10-01 21:38:37] iter = 04030, loss = 1.6787
[2023-10-01 21:38:38] iter = 04040, loss = 1.6433
[2023-10-01 21:38:39] iter = 04050, loss = 1.8094
[2023-10-01 21:38:40] iter = 04060, loss = 1.5847
[2023-10-01 21:38:41] iter = 04070, loss = 1.7594
[2023-10-01 21:38:42] iter = 04080, loss = 1.6518
[2023-10-01 21:38:43] iter = 04090, loss = 1.6662
[2023-10-01 21:38:44] iter = 04100, loss = 1.7620
[2023-10-01 21:38:45] iter = 04110, loss = 1.6913
[2023-10-01 21:38:45] iter = 04120, loss = 1.9169
[2023-10-01 21:38:46] iter = 04130, loss = 1.5959
[2023-10-01 21:38:47] iter = 04140, loss = 1.9943
[2023-10-01 21:38:48] iter = 04150, loss = 1.7451
[2023-10-01 21:38:49] iter = 04160, loss = 1.7059
[2023-10-01 21:38:50] iter = 04170, loss = 1.7048
[2023-10-01 21:38:51] iter = 04180, loss = 1.6893
[2023-10-01 21:38:52] iter = 04190, loss = 1.8102
[2023-10-01 21:38:53] iter = 04200, loss = 1.6446
[2023-10-01 21:38:54] iter = 04210, loss = 1.8912
[2023-10-01 21:38:54] iter = 04220, loss = 1.6933
[2023-10-01 21:38:55] iter = 04230, loss = 1.7073
[2023-10-01 21:38:56] iter = 04240, loss = 1.8071
[2023-10-01 21:38:57] iter = 04250, loss = 1.6915
[2023-10-01 21:38:58] iter = 04260, loss = 1.7097
[2023-10-01 21:38:59] iter = 04270, loss = 1.8659
[2023-10-01 21:39:00] iter = 04280, loss = 1.7218
[2023-10-01 21:39:01] iter = 04290, loss = 1.6691
[2023-10-01 21:39:02] iter = 04300, loss = 1.6095
[2023-10-01 21:39:03] iter = 04310, loss = 1.7196
[2023-10-01 21:39:03] iter = 04320, loss = 1.7778
[2023-10-01 21:39:04] iter = 04330, loss = 1.7857
[2023-10-01 21:39:05] iter = 04340, loss = 1.6516
[2023-10-01 21:39:06] iter = 04350, loss = 1.5657
[2023-10-01 21:39:07] iter = 04360, loss = 1.7026
[2023-10-01 21:39:08] iter = 04370, loss = 1.6091
[2023-10-01 21:39:09] iter = 04380, loss = 1.6005
[2023-10-01 21:39:10] iter = 04390, loss = 1.6120
[2023-10-01 21:39:11] iter = 04400, loss = 1.5798
[2023-10-01 21:39:12] iter = 04410, loss = 1.8304
[2023-10-01 21:39:13] iter = 04420, loss = 1.6840
[2023-10-01 21:39:14] iter = 04430, loss = 1.6976
[2023-10-01 21:39:14] iter = 04440, loss = 1.7061
[2023-10-01 21:39:15] iter = 04450, loss = 1.7192
[2023-10-01 21:39:16] iter = 04460, loss = 1.7186
[2023-10-01 21:39:17] iter = 04470, loss = 1.6376
[2023-10-01 21:39:18] iter = 04480, loss = 1.6771
[2023-10-01 21:39:19] iter = 04490, loss = 1.7314
[2023-10-01 21:39:20] iter = 04500, loss = 1.5689
[2023-10-01 21:39:21] iter = 04510, loss = 1.7171
[2023-10-01 21:39:22] iter = 04520, loss = 1.6870
[2023-10-01 21:39:23] iter = 04530, loss = 1.7345
[2023-10-01 21:39:24] iter = 04540, loss = 1.7759
[2023-10-01 21:39:24] iter = 04550, loss = 1.6938
[2023-10-01 21:39:25] iter = 04560, loss = 1.5930
[2023-10-01 21:39:26] iter = 04570, loss = 1.5891
[2023-10-01 21:39:27] iter = 04580, loss = 1.6394
[2023-10-01 21:39:28] iter = 04590, loss = 1.6259
[2023-10-01 21:39:29] iter = 04600, loss = 1.7509
[2023-10-01 21:39:30] iter = 04610, loss = 1.6072
[2023-10-01 21:39:31] iter = 04620, loss = 1.5475
[2023-10-01 21:39:32] iter = 04630, loss = 1.7992
[2023-10-01 21:39:33] iter = 04640, loss = 1.6739
[2023-10-01 21:39:33] iter = 04650, loss = 1.7522
[2023-10-01 21:39:34] iter = 04660, loss = 1.7362
[2023-10-01 21:39:35] iter = 04670, loss = 1.5687
[2023-10-01 21:39:36] iter = 04680, loss = 1.7046
[2023-10-01 21:39:37] iter = 04690, loss = 1.6217
[2023-10-01 21:39:38] iter = 04700, loss = 1.5705
[2023-10-01 21:39:39] iter = 04710, loss = 1.6313
[2023-10-01 21:39:40] iter = 04720, loss = 1.7372
[2023-10-01 21:39:41] iter = 04730, loss = 1.6884
[2023-10-01 21:39:42] iter = 04740, loss = 1.6587
[2023-10-01 21:39:42] iter = 04750, loss = 1.7231
[2023-10-01 21:39:43] iter = 04760, loss = 1.8278
[2023-10-01 21:39:44] iter = 04770, loss = 1.6982
[2023-10-01 21:39:45] iter = 04780, loss = 1.6808
[2023-10-01 21:39:46] iter = 04790, loss = 1.6382
[2023-10-01 21:39:47] iter = 04800, loss = 1.6226
[2023-10-01 21:39:48] iter = 04810, loss = 1.6941
[2023-10-01 21:39:49] iter = 04820, loss = 1.5233
[2023-10-01 21:39:50] iter = 04830, loss = 1.6185
[2023-10-01 21:39:51] iter = 04840, loss = 1.7276
[2023-10-01 21:39:52] iter = 04850, loss = 1.8204
[2023-10-01 21:39:53] iter = 04860, loss = 1.6427
[2023-10-01 21:39:53] iter = 04870, loss = 1.7382
[2023-10-01 21:39:54] iter = 04880, loss = 1.6613
[2023-10-01 21:39:55] iter = 04890, loss = 1.6414
[2023-10-01 21:39:56] iter = 04900, loss = 1.7683
[2023-10-01 21:39:57] iter = 04910, loss = 1.6793
[2023-10-01 21:39:58] iter = 04920, loss = 1.7961
[2023-10-01 21:39:59] iter = 04930, loss = 1.7580
[2023-10-01 21:40:00] iter = 04940, loss = 1.5965
[2023-10-01 21:40:01] iter = 04950, loss = 1.7564
[2023-10-01 21:40:02] iter = 04960, loss = 1.6584
[2023-10-01 21:40:03] iter = 04970, loss = 1.5907
[2023-10-01 21:40:03] iter = 04980, loss = 1.6206
[2023-10-01 21:40:04] iter = 04990, loss = 1.7375
[2023-10-01 21:40:05] iter = 05000, loss = 1.7651
[2023-10-01 21:40:06] iter = 05010, loss = 1.5594
[2023-10-01 21:40:07] iter = 05020, loss = 1.8906
[2023-10-01 21:40:08] iter = 05030, loss = 1.7024
[2023-10-01 21:40:09] iter = 05040, loss = 1.8428
[2023-10-01 21:40:10] iter = 05050, loss = 1.8472
[2023-10-01 21:40:11] iter = 05060, loss = 1.7292
[2023-10-01 21:40:12] iter = 05070, loss = 1.8494
[2023-10-01 21:40:13] iter = 05080, loss = 1.6946
[2023-10-01 21:40:14] iter = 05090, loss = 1.6057
[2023-10-01 21:40:15] iter = 05100, loss = 1.5506
[2023-10-01 21:40:16] iter = 05110, loss = 1.6581
[2023-10-01 21:40:17] iter = 05120, loss = 1.6780
[2023-10-01 21:40:18] iter = 05130, loss = 1.7957
[2023-10-01 21:40:18] iter = 05140, loss = 1.5830
[2023-10-01 21:40:19] iter = 05150, loss = 1.7301
[2023-10-01 21:40:20] iter = 05160, loss = 1.7081
[2023-10-01 21:40:21] iter = 05170, loss = 1.7798
[2023-10-01 21:40:22] iter = 05180, loss = 1.6046
[2023-10-01 21:40:23] iter = 05190, loss = 1.5632
[2023-10-01 21:40:24] iter = 05200, loss = 1.7154
[2023-10-01 21:40:25] iter = 05210, loss = 1.5654
[2023-10-01 21:40:26] iter = 05220, loss = 1.5900
[2023-10-01 21:40:26] iter = 05230, loss = 1.6142
[2023-10-01 21:40:27] iter = 05240, loss = 1.4948
[2023-10-01 21:40:28] iter = 05250, loss = 1.6014
[2023-10-01 21:40:29] iter = 05260, loss = 1.6754
[2023-10-01 21:40:30] iter = 05270, loss = 1.4905
[2023-10-01 21:40:31] iter = 05280, loss = 1.5850
[2023-10-01 21:40:32] iter = 05290, loss = 1.6296
[2023-10-01 21:40:33] iter = 05300, loss = 1.6822
[2023-10-01 21:40:34] iter = 05310, loss = 1.6370
[2023-10-01 21:40:35] iter = 05320, loss = 1.6173
[2023-10-01 21:40:35] iter = 05330, loss = 1.5972
[2023-10-01 21:40:36] iter = 05340, loss = 1.6867
[2023-10-01 21:40:37] iter = 05350, loss = 1.5310
[2023-10-01 21:40:38] iter = 05360, loss = 1.7777
[2023-10-01 21:40:39] iter = 05370, loss = 1.5548
[2023-10-01 21:40:40] iter = 05380, loss = 1.6546
[2023-10-01 21:40:41] iter = 05390, loss = 1.6150
[2023-10-01 21:40:42] iter = 05400, loss = 1.6170
[2023-10-01 21:40:42] iter = 05410, loss = 1.6116
[2023-10-01 21:40:43] iter = 05420, loss = 1.6290
[2023-10-01 21:40:44] iter = 05430, loss = 1.5675
[2023-10-01 21:40:45] iter = 05440, loss = 1.8792
[2023-10-01 21:40:46] iter = 05450, loss = 1.6823
[2023-10-01 21:40:47] iter = 05460, loss = 1.5778
[2023-10-01 21:40:48] iter = 05470, loss = 1.7024
[2023-10-01 21:40:49] iter = 05480, loss = 1.6158
[2023-10-01 21:40:50] iter = 05490, loss = 1.5803
[2023-10-01 21:40:51] iter = 05500, loss = 1.6115
[2023-10-01 21:40:52] iter = 05510, loss = 1.6472
[2023-10-01 21:40:53] iter = 05520, loss = 1.7128
[2023-10-01 21:40:54] iter = 05530, loss = 1.4884
[2023-10-01 21:40:54] iter = 05540, loss = 1.6556
[2023-10-01 21:40:55] iter = 05550, loss = 1.5984
[2023-10-01 21:40:56] iter = 05560, loss = 1.7296
[2023-10-01 21:40:57] iter = 05570, loss = 1.5518
[2023-10-01 21:40:58] iter = 05580, loss = 1.6835
[2023-10-01 21:40:59] iter = 05590, loss = 1.6695
[2023-10-01 21:41:00] iter = 05600, loss = 1.6789
[2023-10-01 21:41:01] iter = 05610, loss = 1.6224
[2023-10-01 21:41:02] iter = 05620, loss = 1.7818
[2023-10-01 21:41:03] iter = 05630, loss = 1.6439
[2023-10-01 21:41:03] iter = 05640, loss = 1.6553
[2023-10-01 21:41:04] iter = 05650, loss = 1.6938
[2023-10-01 21:41:05] iter = 05660, loss = 1.7673
[2023-10-01 21:41:06] iter = 05670, loss = 1.6328
[2023-10-01 21:41:07] iter = 05680, loss = 1.5553
[2023-10-01 21:41:08] iter = 05690, loss = 1.6202
[2023-10-01 21:41:09] iter = 05700, loss = 1.6830
[2023-10-01 21:41:10] iter = 05710, loss = 1.5351
[2023-10-01 21:41:11] iter = 05720, loss = 1.8419
[2023-10-01 21:41:12] iter = 05730, loss = 1.5998
[2023-10-01 21:41:13] iter = 05740, loss = 1.6933
[2023-10-01 21:41:14] iter = 05750, loss = 1.6482
[2023-10-01 21:41:14] iter = 05760, loss = 1.5297
[2023-10-01 21:41:15] iter = 05770, loss = 1.7508
[2023-10-01 21:41:16] iter = 05780, loss = 1.6198
[2023-10-01 21:41:17] iter = 05790, loss = 1.5994
[2023-10-01 21:41:18] iter = 05800, loss = 1.5677
[2023-10-01 21:41:19] iter = 05810, loss = 1.6202
[2023-10-01 21:41:20] iter = 05820, loss = 1.5939
[2023-10-01 21:41:21] iter = 05830, loss = 1.5851
[2023-10-01 21:41:22] iter = 05840, loss = 1.6211
[2023-10-01 21:41:23] iter = 05850, loss = 1.6511
[2023-10-01 21:41:23] iter = 05860, loss = 1.5556
[2023-10-01 21:41:24] iter = 05870, loss = 1.6303
[2023-10-01 21:41:25] iter = 05880, loss = 1.7767
[2023-10-01 21:41:26] iter = 05890, loss = 1.5646
[2023-10-01 21:41:27] iter = 05900, loss = 1.6736
[2023-10-01 21:41:28] iter = 05910, loss = 1.6155
[2023-10-01 21:41:29] iter = 05920, loss = 1.7561
[2023-10-01 21:41:30] iter = 05930, loss = 1.6200
[2023-10-01 21:41:31] iter = 05940, loss = 1.5929
[2023-10-01 21:41:32] iter = 05950, loss = 1.6960
[2023-10-01 21:41:33] iter = 05960, loss = 1.5811
[2023-10-01 21:41:34] iter = 05970, loss = 1.6546
[2023-10-01 21:41:35] iter = 05980, loss = 1.6155
[2023-10-01 21:41:35] iter = 05990, loss = 1.5824
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 96687}
[2023-10-01 21:42:01] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.014454 train acc = 1.0000, test acc = 0.6098
[2023-10-01 21:42:25] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.002370 train acc = 1.0000, test acc = 0.6142
[2023-10-01 21:42:49] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.004574 train acc = 1.0000, test acc = 0.6175
[2023-10-01 21:43:13] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003653 train acc = 1.0000, test acc = 0.6147
[2023-10-01 21:43:38] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004625 train acc = 1.0000, test acc = 0.6196
[2023-10-01 21:44:02] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011502 train acc = 1.0000, test acc = 0.6118
[2023-10-01 21:44:27] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004315 train acc = 1.0000, test acc = 0.6182
[2023-10-01 21:44:51] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001446 train acc = 1.0000, test acc = 0.6091
[2023-10-01 21:45:15] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.026691 train acc = 1.0000, test acc = 0.6179
[2023-10-01 21:45:39] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.016973 train acc = 1.0000, test acc = 0.6105
[2023-10-01 21:46:04] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.014904 train acc = 1.0000, test acc = 0.6190
[2023-10-01 21:46:28] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001935 train acc = 1.0000, test acc = 0.6205
[2023-10-01 21:46:53] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004040 train acc = 1.0000, test acc = 0.6138
[2023-10-01 21:47:17] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.001582 train acc = 1.0000, test acc = 0.6149
[2023-10-01 21:47:41] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.014481 train acc = 0.9980, test acc = 0.6086
[2023-10-01 21:48:06] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.018268 train acc = 1.0000, test acc = 0.6086
[2023-10-01 21:48:30] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.001415 train acc = 1.0000, test acc = 0.6114
[2023-10-01 21:48:55] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.010527 train acc = 1.0000, test acc = 0.6121
[2023-10-01 21:49:19] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.022111 train acc = 1.0000, test acc = 0.6056
[2023-10-01 21:49:43] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.027689 train acc = 0.9960, test acc = 0.6095
Evaluate 20 random ConvNet, mean = 0.6134 std = 0.0042
-------------------------
[2023-10-01 21:49:44] iter = 06000, loss = 1.5213
[2023-10-01 21:49:44] iter = 06010, loss = 1.7504
[2023-10-01 21:49:45] iter = 06020, loss = 1.6654
[2023-10-01 21:49:46] iter = 06030, loss = 1.5682
[2023-10-01 21:49:47] iter = 06040, loss = 1.7393
[2023-10-01 21:49:48] iter = 06050, loss = 1.6877
[2023-10-01 21:49:49] iter = 06060, loss = 1.6204
[2023-10-01 21:49:50] iter = 06070, loss = 1.6330
[2023-10-01 21:49:51] iter = 06080, loss = 1.6754
[2023-10-01 21:49:52] iter = 06090, loss = 1.7553
[2023-10-01 21:49:52] iter = 06100, loss = 1.5758
[2023-10-01 21:49:53] iter = 06110, loss = 1.5395
[2023-10-01 21:49:54] iter = 06120, loss = 1.5612
[2023-10-01 21:49:55] iter = 06130, loss = 1.6619
[2023-10-01 21:49:56] iter = 06140, loss = 1.6142
[2023-10-01 21:49:57] iter = 06150, loss = 1.5500
[2023-10-01 21:49:58] iter = 06160, loss = 1.5633
[2023-10-01 21:49:59] iter = 06170, loss = 1.8640
[2023-10-01 21:50:00] iter = 06180, loss = 1.6288
[2023-10-01 21:50:01] iter = 06190, loss = 1.6226
[2023-10-01 21:50:02] iter = 06200, loss = 1.5785
[2023-10-01 21:50:03] iter = 06210, loss = 1.6312
[2023-10-01 21:50:03] iter = 06220, loss = 1.5895
[2023-10-01 21:50:04] iter = 06230, loss = 1.7538
[2023-10-01 21:50:05] iter = 06240, loss = 1.7195
[2023-10-01 21:50:06] iter = 06250, loss = 1.7774
[2023-10-01 21:50:07] iter = 06260, loss = 1.7308
[2023-10-01 21:50:08] iter = 06270, loss = 1.4999
[2023-10-01 21:50:09] iter = 06280, loss = 1.6608
[2023-10-01 21:50:10] iter = 06290, loss = 1.6018
[2023-10-01 21:50:11] iter = 06300, loss = 1.6904
[2023-10-01 21:50:12] iter = 06310, loss = 1.5907
[2023-10-01 21:50:13] iter = 06320, loss = 1.7993
[2023-10-01 21:50:14] iter = 06330, loss = 1.5797
[2023-10-01 21:50:14] iter = 06340, loss = 1.5447
[2023-10-01 21:50:15] iter = 06350, loss = 1.6228
[2023-10-01 21:50:16] iter = 06360, loss = 1.6078
[2023-10-01 21:50:17] iter = 06370, loss = 1.6308
[2023-10-01 21:50:18] iter = 06380, loss = 1.6687
[2023-10-01 21:50:19] iter = 06390, loss = 1.4831
[2023-10-01 21:50:20] iter = 06400, loss = 1.5418
[2023-10-01 21:50:21] iter = 06410, loss = 1.6000
[2023-10-01 21:50:22] iter = 06420, loss = 1.5861
[2023-10-01 21:50:22] iter = 06430, loss = 1.6428
[2023-10-01 21:50:23] iter = 06440, loss = 1.5538
[2023-10-01 21:50:24] iter = 06450, loss = 1.5863
[2023-10-01 21:50:25] iter = 06460, loss = 1.6826
[2023-10-01 21:50:26] iter = 06470, loss = 1.5279
[2023-10-01 21:50:27] iter = 06480, loss = 1.5615
[2023-10-01 21:50:28] iter = 06490, loss = 1.5294
[2023-10-01 21:50:29] iter = 06500, loss = 1.6440
[2023-10-01 21:50:30] iter = 06510, loss = 1.4789
[2023-10-01 21:50:31] iter = 06520, loss = 1.5579
[2023-10-01 21:50:32] iter = 06530, loss = 1.5902
[2023-10-01 21:50:32] iter = 06540, loss = 1.5955
[2023-10-01 21:50:33] iter = 06550, loss = 1.6344
[2023-10-01 21:50:34] iter = 06560, loss = 1.7128
[2023-10-01 21:50:35] iter = 06570, loss = 1.6566
[2023-10-01 21:50:36] iter = 06580, loss = 1.5897
[2023-10-01 21:50:37] iter = 06590, loss = 1.8520
[2023-10-01 21:50:38] iter = 06600, loss = 1.5927
[2023-10-01 21:50:39] iter = 06610, loss = 1.6531
[2023-10-01 21:50:40] iter = 06620, loss = 1.5196
[2023-10-01 21:50:41] iter = 06630, loss = 1.6011
[2023-10-01 21:50:41] iter = 06640, loss = 1.6843
[2023-10-01 21:50:42] iter = 06650, loss = 1.5119
[2023-10-01 21:50:43] iter = 06660, loss = 1.5682
[2023-10-01 21:50:44] iter = 06670, loss = 1.7152
[2023-10-01 21:50:45] iter = 06680, loss = 1.6998
[2023-10-01 21:50:46] iter = 06690, loss = 1.6125
[2023-10-01 21:50:47] iter = 06700, loss = 1.6988
[2023-10-01 21:50:48] iter = 06710, loss = 1.5566
[2023-10-01 21:50:49] iter = 06720, loss = 1.4899
[2023-10-01 21:50:50] iter = 06730, loss = 1.6904
[2023-10-01 21:50:51] iter = 06740, loss = 1.6949
[2023-10-01 21:50:52] iter = 06750, loss = 1.7571
[2023-10-01 21:50:52] iter = 06760, loss = 1.7061
[2023-10-01 21:50:53] iter = 06770, loss = 1.7408
[2023-10-01 21:50:54] iter = 06780, loss = 1.7752
[2023-10-01 21:50:55] iter = 06790, loss = 1.7901
[2023-10-01 21:50:56] iter = 06800, loss = 1.6646
[2023-10-01 21:50:57] iter = 06810, loss = 1.6489
[2023-10-01 21:50:58] iter = 06820, loss = 1.5051
[2023-10-01 21:50:59] iter = 06830, loss = 1.6542
[2023-10-01 21:51:00] iter = 06840, loss = 1.6174
[2023-10-01 21:51:01] iter = 06850, loss = 1.6287
[2023-10-01 21:51:01] iter = 06860, loss = 1.6156
[2023-10-01 21:51:02] iter = 06870, loss = 1.7956
[2023-10-01 21:51:03] iter = 06880, loss = 1.6676
[2023-10-01 21:51:04] iter = 06890, loss = 1.6619
[2023-10-01 21:51:05] iter = 06900, loss = 1.6897
[2023-10-01 21:51:06] iter = 06910, loss = 1.6084
[2023-10-01 21:51:07] iter = 06920, loss = 1.5720
[2023-10-01 21:51:08] iter = 06930, loss = 1.7041
[2023-10-01 21:51:09] iter = 06940, loss = 1.4734
[2023-10-01 21:51:10] iter = 06950, loss = 1.5916
[2023-10-01 21:51:11] iter = 06960, loss = 1.5337
[2023-10-01 21:51:11] iter = 06970, loss = 1.7556
[2023-10-01 21:51:12] iter = 06980, loss = 1.5132
[2023-10-01 21:51:13] iter = 06990, loss = 1.6653
[2023-10-01 21:51:14] iter = 07000, loss = 1.4999
[2023-10-01 21:51:15] iter = 07010, loss = 1.7467
[2023-10-01 21:51:16] iter = 07020, loss = 1.5856
[2023-10-01 21:51:17] iter = 07030, loss = 1.5800
[2023-10-01 21:51:18] iter = 07040, loss = 1.6860
[2023-10-01 21:51:19] iter = 07050, loss = 1.6000
[2023-10-01 21:51:20] iter = 07060, loss = 1.6183
[2023-10-01 21:51:20] iter = 07070, loss = 1.5675
[2023-10-01 21:51:21] iter = 07080, loss = 1.6847
[2023-10-01 21:51:22] iter = 07090, loss = 1.5963
[2023-10-01 21:51:23] iter = 07100, loss = 1.6369
[2023-10-01 21:51:24] iter = 07110, loss = 1.5690
[2023-10-01 21:51:25] iter = 07120, loss = 1.6653
[2023-10-01 21:51:26] iter = 07130, loss = 1.4514
[2023-10-01 21:51:27] iter = 07140, loss = 1.6715
[2023-10-01 21:51:28] iter = 07150, loss = 1.5231
[2023-10-01 21:51:29] iter = 07160, loss = 1.8309
[2023-10-01 21:51:29] iter = 07170, loss = 1.7100
[2023-10-01 21:51:30] iter = 07180, loss = 1.5515
[2023-10-01 21:51:31] iter = 07190, loss = 1.6671
[2023-10-01 21:51:32] iter = 07200, loss = 1.4741
[2023-10-01 21:51:33] iter = 07210, loss = 1.7088
[2023-10-01 21:51:34] iter = 07220, loss = 1.6396
[2023-10-01 21:51:35] iter = 07230, loss = 1.4370
[2023-10-01 21:51:36] iter = 07240, loss = 1.6729
[2023-10-01 21:51:37] iter = 07250, loss = 1.5652
[2023-10-01 21:51:38] iter = 07260, loss = 1.6076
[2023-10-01 21:51:39] iter = 07270, loss = 1.6162
[2023-10-01 21:51:40] iter = 07280, loss = 1.5210
[2023-10-01 21:51:40] iter = 07290, loss = 1.7749
[2023-10-01 21:51:41] iter = 07300, loss = 1.5717
[2023-10-01 21:51:42] iter = 07310, loss = 1.5310
[2023-10-01 21:51:43] iter = 07320, loss = 1.6284
[2023-10-01 21:51:44] iter = 07330, loss = 1.5389
[2023-10-01 21:51:45] iter = 07340, loss = 1.5658
[2023-10-01 21:51:46] iter = 07350, loss = 1.5992
[2023-10-01 21:51:47] iter = 07360, loss = 1.6557
[2023-10-01 21:51:47] iter = 07370, loss = 1.6111
[2023-10-01 21:51:48] iter = 07380, loss = 1.5890
[2023-10-01 21:51:49] iter = 07390, loss = 1.6120
[2023-10-01 21:51:50] iter = 07400, loss = 1.6480
[2023-10-01 21:51:51] iter = 07410, loss = 1.5157
[2023-10-01 21:51:52] iter = 07420, loss = 1.5978
[2023-10-01 21:51:53] iter = 07430, loss = 1.5652
[2023-10-01 21:51:54] iter = 07440, loss = 1.5396
[2023-10-01 21:51:55] iter = 07450, loss = 1.5668
[2023-10-01 21:51:56] iter = 07460, loss = 1.7625
[2023-10-01 21:51:57] iter = 07470, loss = 1.4678
[2023-10-01 21:51:58] iter = 07480, loss = 1.4886
[2023-10-01 21:51:58] iter = 07490, loss = 1.5755
[2023-10-01 21:51:59] iter = 07500, loss = 1.7379
[2023-10-01 21:52:00] iter = 07510, loss = 1.5304
[2023-10-01 21:52:01] iter = 07520, loss = 1.7486
[2023-10-01 21:52:02] iter = 07530, loss = 1.5423
[2023-10-01 21:52:03] iter = 07540, loss = 1.5390
[2023-10-01 21:52:04] iter = 07550, loss = 1.5258
[2023-10-01 21:52:05] iter = 07560, loss = 1.5015
[2023-10-01 21:52:06] iter = 07570, loss = 1.6071
[2023-10-01 21:52:07] iter = 07580, loss = 1.5878
[2023-10-01 21:52:08] iter = 07590, loss = 1.4025
[2023-10-01 21:52:09] iter = 07600, loss = 1.5704
[2023-10-01 21:52:10] iter = 07610, loss = 1.5836
[2023-10-01 21:52:10] iter = 07620, loss = 1.5069
[2023-10-01 21:52:11] iter = 07630, loss = 1.5812
[2023-10-01 21:52:12] iter = 07640, loss = 1.4204
[2023-10-01 21:52:13] iter = 07650, loss = 1.5510
[2023-10-01 21:52:14] iter = 07660, loss = 1.7635
[2023-10-01 21:52:15] iter = 07670, loss = 1.5317
[2023-10-01 21:52:16] iter = 07680, loss = 1.6328
[2023-10-01 21:52:17] iter = 07690, loss = 1.5648
[2023-10-01 21:52:18] iter = 07700, loss = 1.6133
[2023-10-01 21:52:19] iter = 07710, loss = 1.5560
[2023-10-01 21:52:20] iter = 07720, loss = 1.7161
[2023-10-01 21:52:21] iter = 07730, loss = 1.6043
[2023-10-01 21:52:21] iter = 07740, loss = 1.5765
[2023-10-01 21:52:22] iter = 07750, loss = 1.5535
[2023-10-01 21:52:23] iter = 07760, loss = 1.4720
[2023-10-01 21:52:24] iter = 07770, loss = 1.6987
[2023-10-01 21:52:25] iter = 07780, loss = 1.5472
[2023-10-01 21:52:26] iter = 07790, loss = 1.6091
[2023-10-01 21:52:27] iter = 07800, loss = 1.6134
[2023-10-01 21:52:28] iter = 07810, loss = 1.6674
[2023-10-01 21:52:29] iter = 07820, loss = 1.4990
[2023-10-01 21:52:30] iter = 07830, loss = 1.5448
[2023-10-01 21:52:31] iter = 07840, loss = 1.7007
[2023-10-01 21:52:32] iter = 07850, loss = 1.6048
[2023-10-01 21:52:33] iter = 07860, loss = 1.6873
[2023-10-01 21:52:33] iter = 07870, loss = 1.6038
[2023-10-01 21:52:34] iter = 07880, loss = 1.6103
[2023-10-01 21:52:35] iter = 07890, loss = 1.4999
[2023-10-01 21:52:36] iter = 07900, loss = 1.6169
[2023-10-01 21:52:37] iter = 07910, loss = 1.6180
[2023-10-01 21:52:38] iter = 07920, loss = 1.6973
[2023-10-01 21:52:39] iter = 07930, loss = 1.4952
[2023-10-01 21:52:40] iter = 07940, loss = 1.5474
[2023-10-01 21:52:41] iter = 07950, loss = 1.6670
[2023-10-01 21:52:42] iter = 07960, loss = 1.4822
[2023-10-01 21:52:43] iter = 07970, loss = 1.6152
[2023-10-01 21:52:43] iter = 07980, loss = 1.6239
[2023-10-01 21:52:44] iter = 07990, loss = 1.5706
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 65687}
[2023-10-01 21:53:09] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.008561 train acc = 1.0000, test acc = 0.6135
[2023-10-01 21:53:34] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.018027 train acc = 1.0000, test acc = 0.6255
[2023-10-01 21:53:58] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002660 train acc = 1.0000, test acc = 0.6185
[2023-10-01 21:54:22] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004292 train acc = 1.0000, test acc = 0.6151
[2023-10-01 21:54:47] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004427 train acc = 1.0000, test acc = 0.6116
[2023-10-01 21:55:11] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004376 train acc = 1.0000, test acc = 0.6139
[2023-10-01 21:55:35] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004204 train acc = 1.0000, test acc = 0.6072
[2023-10-01 21:55:59] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004340 train acc = 1.0000, test acc = 0.6080
[2023-10-01 21:56:24] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.007624 train acc = 1.0000, test acc = 0.6118
[2023-10-01 21:56:48] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013593 train acc = 0.9980, test acc = 0.6142
[2023-10-01 21:57:13] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.009156 train acc = 1.0000, test acc = 0.6115
[2023-10-01 21:57:37] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.012172 train acc = 1.0000, test acc = 0.6147
[2023-10-01 21:58:01] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.008797 train acc = 1.0000, test acc = 0.6191
[2023-10-01 21:58:25] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.019772 train acc = 1.0000, test acc = 0.6218
[2023-10-01 21:58:50] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.012820 train acc = 1.0000, test acc = 0.6174
[2023-10-01 21:59:14] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.013467 train acc = 0.9980, test acc = 0.6077
[2023-10-01 21:59:38] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.015632 train acc = 0.9980, test acc = 0.6178
[2023-10-01 22:00:02] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.011681 train acc = 1.0000, test acc = 0.6144
[2023-10-01 22:00:27] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004150 train acc = 1.0000, test acc = 0.6143
[2023-10-01 22:00:52] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.007021 train acc = 1.0000, test acc = 0.6166
Evaluate 20 random ConvNet, mean = 0.6147 std = 0.0045
-------------------------
[2023-10-01 22:00:52] iter = 08000, loss = 1.6834
[2023-10-01 22:00:53] iter = 08010, loss = 1.4732
[2023-10-01 22:00:54] iter = 08020, loss = 1.6058
[2023-10-01 22:00:55] iter = 08030, loss = 1.6550
[2023-10-01 22:00:55] iter = 08040, loss = 1.6027
[2023-10-01 22:00:56] iter = 08050, loss = 1.6020
[2023-10-01 22:00:57] iter = 08060, loss = 1.6305
[2023-10-01 22:00:58] iter = 08070, loss = 1.4983
[2023-10-01 22:00:59] iter = 08080, loss = 1.6400
[2023-10-01 22:01:00] iter = 08090, loss = 1.5706
[2023-10-01 22:01:01] iter = 08100, loss = 1.5132
[2023-10-01 22:01:02] iter = 08110, loss = 1.6711
[2023-10-01 22:01:03] iter = 08120, loss = 1.6874
[2023-10-01 22:01:04] iter = 08130, loss = 1.4279
[2023-10-01 22:01:04] iter = 08140, loss = 1.5322
[2023-10-01 22:01:05] iter = 08150, loss = 1.6897
[2023-10-01 22:01:06] iter = 08160, loss = 1.4890
[2023-10-01 22:01:07] iter = 08170, loss = 1.5333
[2023-10-01 22:01:08] iter = 08180, loss = 1.7671
[2023-10-01 22:01:09] iter = 08190, loss = 1.5436
[2023-10-01 22:01:10] iter = 08200, loss = 1.4753
[2023-10-01 22:01:11] iter = 08210, loss = 1.6375
[2023-10-01 22:01:12] iter = 08220, loss = 1.5659
[2023-10-01 22:01:13] iter = 08230, loss = 1.5375
[2023-10-01 22:01:13] iter = 08240, loss = 1.5901
[2023-10-01 22:01:14] iter = 08250, loss = 1.7144
[2023-10-01 22:01:15] iter = 08260, loss = 1.4585
[2023-10-01 22:01:16] iter = 08270, loss = 1.6045
[2023-10-01 22:01:17] iter = 08280, loss = 1.5201
[2023-10-01 22:01:18] iter = 08290, loss = 1.4332
[2023-10-01 22:01:19] iter = 08300, loss = 1.8147
[2023-10-01 22:01:20] iter = 08310, loss = 1.6640
[2023-10-01 22:01:21] iter = 08320, loss = 1.5952
[2023-10-01 22:01:22] iter = 08330, loss = 1.6309
[2023-10-01 22:01:22] iter = 08340, loss = 1.7552
[2023-10-01 22:01:23] iter = 08350, loss = 1.5022
[2023-10-01 22:01:24] iter = 08360, loss = 1.4643
[2023-10-01 22:01:25] iter = 08370, loss = 1.6125
[2023-10-01 22:01:26] iter = 08380, loss = 1.5605
[2023-10-01 22:01:27] iter = 08390, loss = 1.4924
[2023-10-01 22:01:28] iter = 08400, loss = 1.5753
[2023-10-01 22:01:29] iter = 08410, loss = 1.4550
[2023-10-01 22:01:30] iter = 08420, loss = 1.4892
[2023-10-01 22:01:31] iter = 08430, loss = 1.6611
[2023-10-01 22:01:32] iter = 08440, loss = 1.6276
[2023-10-01 22:01:33] iter = 08450, loss = 1.5968
[2023-10-01 22:01:34] iter = 08460, loss = 1.5316
[2023-10-01 22:01:34] iter = 08470, loss = 1.4973
[2023-10-01 22:01:35] iter = 08480, loss = 1.5586
[2023-10-01 22:01:36] iter = 08490, loss = 1.8566
[2023-10-01 22:01:37] iter = 08500, loss = 1.6115
[2023-10-01 22:01:38] iter = 08510, loss = 1.5589
[2023-10-01 22:01:39] iter = 08520, loss = 1.5068
[2023-10-01 22:01:40] iter = 08530, loss = 1.6342
[2023-10-01 22:01:41] iter = 08540, loss = 1.6601
[2023-10-01 22:01:42] iter = 08550, loss = 1.6763
[2023-10-01 22:01:43] iter = 08560, loss = 1.6918
[2023-10-01 22:01:43] iter = 08570, loss = 1.5633
[2023-10-01 22:01:44] iter = 08580, loss = 1.5429
[2023-10-01 22:01:45] iter = 08590, loss = 1.6034
[2023-10-01 22:01:46] iter = 08600, loss = 1.5146
[2023-10-01 22:01:47] iter = 08610, loss = 1.4739
[2023-10-01 22:01:48] iter = 08620, loss = 1.5351
[2023-10-01 22:01:49] iter = 08630, loss = 1.7321
[2023-10-01 22:01:50] iter = 08640, loss = 1.6443
[2023-10-01 22:01:51] iter = 08650, loss = 1.4854
[2023-10-01 22:01:52] iter = 08660, loss = 1.6099
[2023-10-01 22:01:53] iter = 08670, loss = 1.6113
[2023-10-01 22:01:53] iter = 08680, loss = 1.5562
[2023-10-01 22:01:54] iter = 08690, loss = 1.4742
[2023-10-01 22:01:55] iter = 08700, loss = 1.4797
[2023-10-01 22:01:56] iter = 08710, loss = 1.5124
[2023-10-01 22:01:57] iter = 08720, loss = 1.5770
[2023-10-01 22:01:58] iter = 08730, loss = 1.5906
[2023-10-01 22:01:59] iter = 08740, loss = 1.5208
[2023-10-01 22:02:00] iter = 08750, loss = 1.4612
[2023-10-01 22:02:01] iter = 08760, loss = 1.5744
[2023-10-01 22:02:02] iter = 08770, loss = 1.5940
[2023-10-01 22:02:03] iter = 08780, loss = 1.4967
[2023-10-01 22:02:04] iter = 08790, loss = 1.5030
[2023-10-01 22:02:05] iter = 08800, loss = 1.4634
[2023-10-01 22:02:05] iter = 08810, loss = 1.5618
[2023-10-01 22:02:06] iter = 08820, loss = 1.6098
[2023-10-01 22:02:07] iter = 08830, loss = 1.7322
[2023-10-01 22:02:08] iter = 08840, loss = 1.5245
[2023-10-01 22:02:09] iter = 08850, loss = 1.5382
[2023-10-01 22:02:10] iter = 08860, loss = 1.5068
[2023-10-01 22:02:11] iter = 08870, loss = 1.6230
[2023-10-01 22:02:12] iter = 08880, loss = 1.5484
[2023-10-01 22:02:13] iter = 08890, loss = 1.5602
[2023-10-01 22:02:14] iter = 08900, loss = 1.5232
[2023-10-01 22:02:15] iter = 08910, loss = 1.4562
[2023-10-01 22:02:15] iter = 08920, loss = 1.7169
[2023-10-01 22:02:16] iter = 08930, loss = 1.4193
[2023-10-01 22:02:17] iter = 08940, loss = 1.7460
[2023-10-01 22:02:18] iter = 08950, loss = 1.6017
[2023-10-01 22:02:19] iter = 08960, loss = 1.5229
[2023-10-01 22:02:20] iter = 08970, loss = 1.6301
[2023-10-01 22:02:21] iter = 08980, loss = 1.5394
[2023-10-01 22:02:22] iter = 08990, loss = 1.5112
[2023-10-01 22:02:23] iter = 09000, loss = 1.4971
[2023-10-01 22:02:24] iter = 09010, loss = 1.5663
[2023-10-01 22:02:24] iter = 09020, loss = 1.5644
[2023-10-01 22:02:25] iter = 09030, loss = 1.5827
[2023-10-01 22:02:26] iter = 09040, loss = 1.6471
[2023-10-01 22:02:27] iter = 09050, loss = 1.6216
[2023-10-01 22:02:28] iter = 09060, loss = 1.6224
[2023-10-01 22:02:29] iter = 09070, loss = 1.4880
[2023-10-01 22:02:30] iter = 09080, loss = 1.5504
[2023-10-01 22:02:31] iter = 09090, loss = 1.6306
[2023-10-01 22:02:32] iter = 09100, loss = 1.5458
[2023-10-01 22:02:33] iter = 09110, loss = 1.5471
[2023-10-01 22:02:34] iter = 09120, loss = 1.6665
[2023-10-01 22:02:34] iter = 09130, loss = 1.4835
[2023-10-01 22:02:35] iter = 09140, loss = 1.5522
[2023-10-01 22:02:36] iter = 09150, loss = 1.5216
[2023-10-01 22:02:37] iter = 09160, loss = 1.4783
[2023-10-01 22:02:38] iter = 09170, loss = 1.5985
[2023-10-01 22:02:39] iter = 09180, loss = 1.6426
[2023-10-01 22:02:40] iter = 09190, loss = 1.6211
[2023-10-01 22:02:41] iter = 09200, loss = 1.4718
[2023-10-01 22:02:42] iter = 09210, loss = 1.7629
[2023-10-01 22:02:43] iter = 09220, loss = 1.3959
[2023-10-01 22:02:43] iter = 09230, loss = 1.6052
[2023-10-01 22:02:44] iter = 09240, loss = 1.8034
[2023-10-01 22:02:45] iter = 09250, loss = 1.5922
[2023-10-01 22:02:46] iter = 09260, loss = 1.7157
[2023-10-01 22:02:47] iter = 09270, loss = 1.6156
[2023-10-01 22:02:48] iter = 09280, loss = 1.5244
[2023-10-01 22:02:49] iter = 09290, loss = 1.5026
[2023-10-01 22:02:50] iter = 09300, loss = 1.4924
[2023-10-01 22:02:50] iter = 09310, loss = 1.6038
[2023-10-01 22:02:51] iter = 09320, loss = 1.6778
[2023-10-01 22:02:52] iter = 09330, loss = 1.4151
[2023-10-01 22:02:53] iter = 09340, loss = 1.5684
[2023-10-01 22:02:54] iter = 09350, loss = 1.5125
[2023-10-01 22:02:55] iter = 09360, loss = 1.5720
[2023-10-01 22:02:56] iter = 09370, loss = 1.5638
[2023-10-01 22:02:57] iter = 09380, loss = 1.6038
[2023-10-01 22:02:58] iter = 09390, loss = 1.5011
[2023-10-01 22:02:59] iter = 09400, loss = 1.6825
[2023-10-01 22:03:00] iter = 09410, loss = 1.5172
[2023-10-01 22:03:00] iter = 09420, loss = 1.5299
[2023-10-01 22:03:01] iter = 09430, loss = 1.6090
[2023-10-01 22:03:02] iter = 09440, loss = 1.5051
[2023-10-01 22:03:03] iter = 09450, loss = 1.6206
[2023-10-01 22:03:04] iter = 09460, loss = 1.6993
[2023-10-01 22:03:05] iter = 09470, loss = 1.5601
[2023-10-01 22:03:06] iter = 09480, loss = 1.6719
[2023-10-01 22:03:07] iter = 09490, loss = 1.4555
[2023-10-01 22:03:08] iter = 09500, loss = 1.4580
[2023-10-01 22:03:09] iter = 09510, loss = 1.4968
[2023-10-01 22:03:10] iter = 09520, loss = 1.6844
[2023-10-01 22:03:11] iter = 09530, loss = 1.5519
[2023-10-01 22:03:12] iter = 09540, loss = 1.6361
[2023-10-01 22:03:12] iter = 09550, loss = 1.5526
[2023-10-01 22:03:13] iter = 09560, loss = 1.4938
[2023-10-01 22:03:14] iter = 09570, loss = 1.5827
[2023-10-01 22:03:15] iter = 09580, loss = 1.4372
[2023-10-01 22:03:16] iter = 09590, loss = 1.4956
[2023-10-01 22:03:17] iter = 09600, loss = 1.6063
[2023-10-01 22:03:18] iter = 09610, loss = 1.4794
[2023-10-01 22:03:19] iter = 09620, loss = 1.4583
[2023-10-01 22:03:20] iter = 09630, loss = 1.6962
[2023-10-01 22:03:21] iter = 09640, loss = 1.4558
[2023-10-01 22:03:22] iter = 09650, loss = 1.5936
[2023-10-01 22:03:22] iter = 09660, loss = 1.4759
[2023-10-01 22:03:23] iter = 09670, loss = 1.5136
[2023-10-01 22:03:24] iter = 09680, loss = 1.6162
[2023-10-01 22:03:25] iter = 09690, loss = 1.5827
[2023-10-01 22:03:26] iter = 09700, loss = 1.6007
[2023-10-01 22:03:27] iter = 09710, loss = 1.5099
[2023-10-01 22:03:28] iter = 09720, loss = 1.4754
[2023-10-01 22:03:29] iter = 09730, loss = 1.5839
[2023-10-01 22:03:30] iter = 09740, loss = 1.6379
[2023-10-01 22:03:31] iter = 09750, loss = 1.5039
[2023-10-01 22:03:32] iter = 09760, loss = 1.4887
[2023-10-01 22:03:33] iter = 09770, loss = 1.5981
[2023-10-01 22:03:33] iter = 09780, loss = 1.6225
[2023-10-01 22:03:34] iter = 09790, loss = 1.6342
[2023-10-01 22:03:35] iter = 09800, loss = 1.4416
[2023-10-01 22:03:36] iter = 09810, loss = 1.7538
[2023-10-01 22:03:37] iter = 09820, loss = 1.5430
[2023-10-01 22:03:38] iter = 09830, loss = 1.5126
[2023-10-01 22:03:39] iter = 09840, loss = 1.4611
[2023-10-01 22:03:40] iter = 09850, loss = 1.4774
[2023-10-01 22:03:41] iter = 09860, loss = 1.5599
[2023-10-01 22:03:42] iter = 09870, loss = 1.5349
[2023-10-01 22:03:43] iter = 09880, loss = 1.6413
[2023-10-01 22:03:44] iter = 09890, loss = 1.5596
[2023-10-01 22:03:44] iter = 09900, loss = 1.4839
[2023-10-01 22:03:45] iter = 09910, loss = 1.5958
[2023-10-01 22:03:46] iter = 09920, loss = 1.6135
[2023-10-01 22:03:47] iter = 09930, loss = 1.4322
[2023-10-01 22:03:48] iter = 09940, loss = 1.5499
[2023-10-01 22:03:49] iter = 09950, loss = 1.4542
[2023-10-01 22:03:50] iter = 09960, loss = 1.4408
[2023-10-01 22:03:51] iter = 09970, loss = 1.6018
[2023-10-01 22:03:52] iter = 09980, loss = 1.5117
[2023-10-01 22:03:53] iter = 09990, loss = 1.5744
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 33998}
[2023-10-01 22:04:18] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.006369 train acc = 1.0000, test acc = 0.6244
[2023-10-01 22:04:42] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006332 train acc = 1.0000, test acc = 0.6213
[2023-10-01 22:05:06] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.018750 train acc = 1.0000, test acc = 0.6239
[2023-10-01 22:05:31] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004012 train acc = 1.0000, test acc = 0.6315
[2023-10-01 22:05:55] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.015465 train acc = 1.0000, test acc = 0.6181
[2023-10-01 22:06:19] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001735 train acc = 1.0000, test acc = 0.6207
[2023-10-01 22:06:44] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016953 train acc = 0.9980, test acc = 0.6191
[2023-10-01 22:07:08] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.023414 train acc = 0.9960, test acc = 0.6190
[2023-10-01 22:07:32] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.010281 train acc = 1.0000, test acc = 0.6254
[2023-10-01 22:07:56] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.010873 train acc = 1.0000, test acc = 0.6181
[2023-10-01 22:08:21] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.017851 train acc = 1.0000, test acc = 0.6227
[2023-10-01 22:08:45] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.007358 train acc = 1.0000, test acc = 0.6206
[2023-10-01 22:09:09] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.010731 train acc = 0.9980, test acc = 0.6267
[2023-10-01 22:09:34] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.012111 train acc = 1.0000, test acc = 0.6143
[2023-10-01 22:09:58] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002153 train acc = 1.0000, test acc = 0.6288
[2023-10-01 22:10:22] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.010211 train acc = 1.0000, test acc = 0.6149
[2023-10-01 22:10:46] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.011233 train acc = 1.0000, test acc = 0.6216
[2023-10-01 22:11:11] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003835 train acc = 1.0000, test acc = 0.6136
[2023-10-01 22:11:35] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002454 train acc = 1.0000, test acc = 0.6254
[2023-10-01 22:11:59] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.006593 train acc = 1.0000, test acc = 0.6177
Evaluate 20 random ConvNet, mean = 0.6214 std = 0.0047
-------------------------
[2023-10-01 22:12:00] iter = 10000, loss = 1.7306
[2023-10-01 22:12:00] iter = 10010, loss = 1.5201
[2023-10-01 22:12:01] iter = 10020, loss = 1.4924
[2023-10-01 22:12:02] iter = 10030, loss = 1.5052
[2023-10-01 22:12:03] iter = 10040, loss = 1.5625
[2023-10-01 22:12:04] iter = 10050, loss = 1.4731
[2023-10-01 22:12:05] iter = 10060, loss = 1.5402
[2023-10-01 22:12:06] iter = 10070, loss = 1.5729
[2023-10-01 22:12:07] iter = 10080, loss = 1.4531
[2023-10-01 22:12:08] iter = 10090, loss = 1.5317
[2023-10-01 22:12:09] iter = 10100, loss = 1.6870
[2023-10-01 22:12:09] iter = 10110, loss = 1.5387
[2023-10-01 22:12:10] iter = 10120, loss = 1.5605
[2023-10-01 22:12:11] iter = 10130, loss = 1.5982
[2023-10-01 22:12:12] iter = 10140, loss = 1.5763
[2023-10-01 22:12:13] iter = 10150, loss = 1.4320
[2023-10-01 22:12:14] iter = 10160, loss = 1.6822
[2023-10-01 22:12:15] iter = 10170, loss = 1.5433
[2023-10-01 22:12:16] iter = 10180, loss = 1.5183
[2023-10-01 22:12:17] iter = 10190, loss = 1.6400
[2023-10-01 22:12:18] iter = 10200, loss = 1.4339
[2023-10-01 22:12:18] iter = 10210, loss = 1.5421
[2023-10-01 22:12:19] iter = 10220, loss = 1.4697
[2023-10-01 22:12:21] iter = 10230, loss = 1.5740
[2023-10-01 22:12:21] iter = 10240, loss = 1.7028
[2023-10-01 22:12:22] iter = 10250, loss = 1.5016
[2023-10-01 22:12:23] iter = 10260, loss = 1.5099
[2023-10-01 22:12:24] iter = 10270, loss = 1.5416
[2023-10-01 22:12:25] iter = 10280, loss = 1.4961
[2023-10-01 22:12:26] iter = 10290, loss = 1.6484
[2023-10-01 22:12:27] iter = 10300, loss = 1.5773
[2023-10-01 22:12:28] iter = 10310, loss = 1.5447
[2023-10-01 22:12:29] iter = 10320, loss = 1.6682
[2023-10-01 22:12:29] iter = 10330, loss = 1.4320
[2023-10-01 22:12:30] iter = 10340, loss = 1.4357
[2023-10-01 22:12:31] iter = 10350, loss = 1.6227
[2023-10-01 22:12:32] iter = 10360, loss = 1.5058
[2023-10-01 22:12:33] iter = 10370, loss = 1.5222
[2023-10-01 22:12:34] iter = 10380, loss = 1.7115
[2023-10-01 22:12:35] iter = 10390, loss = 1.5385
[2023-10-01 22:12:36] iter = 10400, loss = 1.5027
[2023-10-01 22:12:37] iter = 10410, loss = 1.5481
[2023-10-01 22:12:38] iter = 10420, loss = 1.5217
[2023-10-01 22:12:38] iter = 10430, loss = 1.5687
[2023-10-01 22:12:39] iter = 10440, loss = 1.5810
[2023-10-01 22:12:40] iter = 10450, loss = 1.3986
[2023-10-01 22:12:41] iter = 10460, loss = 1.6001
[2023-10-01 22:12:42] iter = 10470, loss = 1.5656
[2023-10-01 22:12:43] iter = 10480, loss = 1.7061
[2023-10-01 22:12:44] iter = 10490, loss = 1.6348
[2023-10-01 22:12:45] iter = 10500, loss = 1.6317
[2023-10-01 22:12:46] iter = 10510, loss = 1.6189
[2023-10-01 22:12:47] iter = 10520, loss = 1.4519
[2023-10-01 22:12:47] iter = 10530, loss = 1.4145
[2023-10-01 22:12:48] iter = 10540, loss = 1.5306
[2023-10-01 22:12:49] iter = 10550, loss = 1.4668
[2023-10-01 22:12:50] iter = 10560, loss = 1.6014
[2023-10-01 22:12:51] iter = 10570, loss = 1.4376
[2023-10-01 22:12:52] iter = 10580, loss = 1.6198
[2023-10-01 22:12:53] iter = 10590, loss = 1.6406
[2023-10-01 22:12:54] iter = 10600, loss = 1.4843
[2023-10-01 22:12:55] iter = 10610, loss = 1.6549
[2023-10-01 22:12:56] iter = 10620, loss = 1.5268
[2023-10-01 22:12:57] iter = 10630, loss = 1.5365
[2023-10-01 22:12:57] iter = 10640, loss = 1.5199
[2023-10-01 22:12:58] iter = 10650, loss = 1.6524
[2023-10-01 22:12:59] iter = 10660, loss = 1.4637
[2023-10-01 22:13:00] iter = 10670, loss = 1.5999
[2023-10-01 22:13:01] iter = 10680, loss = 1.5612
[2023-10-01 22:13:02] iter = 10690, loss = 1.5139
[2023-10-01 22:13:03] iter = 10700, loss = 1.4867
[2023-10-01 22:13:04] iter = 10710, loss = 1.4834
[2023-10-01 22:13:05] iter = 10720, loss = 1.5804
[2023-10-01 22:13:06] iter = 10730, loss = 1.6142
[2023-10-01 22:13:07] iter = 10740, loss = 1.4669
[2023-10-01 22:13:07] iter = 10750, loss = 1.3626
[2023-10-01 22:13:08] iter = 10760, loss = 1.4842
[2023-10-01 22:13:09] iter = 10770, loss = 1.6229
[2023-10-01 22:13:10] iter = 10780, loss = 1.4715
[2023-10-01 22:13:11] iter = 10790, loss = 1.5833
[2023-10-01 22:13:12] iter = 10800, loss = 1.6147
[2023-10-01 22:13:13] iter = 10810, loss = 1.5062
[2023-10-01 22:13:14] iter = 10820, loss = 1.5343
[2023-10-01 22:13:15] iter = 10830, loss = 1.5865
[2023-10-01 22:13:15] iter = 10840, loss = 1.6047
[2023-10-01 22:13:16] iter = 10850, loss = 1.5031
[2023-10-01 22:13:17] iter = 10860, loss = 1.6028
[2023-10-01 22:13:18] iter = 10870, loss = 1.4493
[2023-10-01 22:13:19] iter = 10880, loss = 1.7244
[2023-10-01 22:13:20] iter = 10890, loss = 1.5223
[2023-10-01 22:13:21] iter = 10900, loss = 1.6450
[2023-10-01 22:13:22] iter = 10910, loss = 1.5085
[2023-10-01 22:13:23] iter = 10920, loss = 1.5186
[2023-10-01 22:13:24] iter = 10930, loss = 1.5039
[2023-10-01 22:13:24] iter = 10940, loss = 1.5690
[2023-10-01 22:13:25] iter = 10950, loss = 1.6510
[2023-10-01 22:13:26] iter = 10960, loss = 1.4845
[2023-10-01 22:13:27] iter = 10970, loss = 1.6973
[2023-10-01 22:13:28] iter = 10980, loss = 1.6731
[2023-10-01 22:13:29] iter = 10990, loss = 1.6252
[2023-10-01 22:13:30] iter = 11000, loss = 1.5964
[2023-10-01 22:13:31] iter = 11010, loss = 1.4430
[2023-10-01 22:13:32] iter = 11020, loss = 1.6007
[2023-10-01 22:13:33] iter = 11030, loss = 1.5743
[2023-10-01 22:13:33] iter = 11040, loss = 1.4920
[2023-10-01 22:13:34] iter = 11050, loss = 1.5028
[2023-10-01 22:13:35] iter = 11060, loss = 1.5595
[2023-10-01 22:13:36] iter = 11070, loss = 1.6152
[2023-10-01 22:13:37] iter = 11080, loss = 1.4257
[2023-10-01 22:13:38] iter = 11090, loss = 1.5018
[2023-10-01 22:13:39] iter = 11100, loss = 1.6666
[2023-10-01 22:13:40] iter = 11110, loss = 1.5113
[2023-10-01 22:13:41] iter = 11120, loss = 1.3701
[2023-10-01 22:13:42] iter = 11130, loss = 1.6101
[2023-10-01 22:13:43] iter = 11140, loss = 1.5700
[2023-10-01 22:13:43] iter = 11150, loss = 1.4167
[2023-10-01 22:13:44] iter = 11160, loss = 1.5194
[2023-10-01 22:13:45] iter = 11170, loss = 1.4415
[2023-10-01 22:13:46] iter = 11180, loss = 1.3757
[2023-10-01 22:13:47] iter = 11190, loss = 1.4722
[2023-10-01 22:13:48] iter = 11200, loss = 1.6089
[2023-10-01 22:13:49] iter = 11210, loss = 1.6320
[2023-10-01 22:13:50] iter = 11220, loss = 1.5167
[2023-10-01 22:13:51] iter = 11230, loss = 1.5582
[2023-10-01 22:13:52] iter = 11240, loss = 1.6010
[2023-10-01 22:13:52] iter = 11250, loss = 1.6239
[2023-10-01 22:13:53] iter = 11260, loss = 1.4832
[2023-10-01 22:13:54] iter = 11270, loss = 1.3453
[2023-10-01 22:13:55] iter = 11280, loss = 1.4554
[2023-10-01 22:13:56] iter = 11290, loss = 1.4222
[2023-10-01 22:13:57] iter = 11300, loss = 1.5133
[2023-10-01 22:13:58] iter = 11310, loss = 1.5087
[2023-10-01 22:13:59] iter = 11320, loss = 1.6254
[2023-10-01 22:14:00] iter = 11330, loss = 1.5726
[2023-10-01 22:14:01] iter = 11340, loss = 1.5956
[2023-10-01 22:14:02] iter = 11350, loss = 1.5585
[2023-10-01 22:14:03] iter = 11360, loss = 1.5648
[2023-10-01 22:14:03] iter = 11370, loss = 1.4510
[2023-10-01 22:14:04] iter = 11380, loss = 1.5952
[2023-10-01 22:14:05] iter = 11390, loss = 1.5421
[2023-10-01 22:14:06] iter = 11400, loss = 1.8554
[2023-10-01 22:14:07] iter = 11410, loss = 1.5575
[2023-10-01 22:14:08] iter = 11420, loss = 1.5556
[2023-10-01 22:14:09] iter = 11430, loss = 1.3676
[2023-10-01 22:14:10] iter = 11440, loss = 1.4524
[2023-10-01 22:14:11] iter = 11450, loss = 1.6493
[2023-10-01 22:14:12] iter = 11460, loss = 1.4412
[2023-10-01 22:14:12] iter = 11470, loss = 1.6028
[2023-10-01 22:14:13] iter = 11480, loss = 1.4895
[2023-10-01 22:14:14] iter = 11490, loss = 1.6111
[2023-10-01 22:14:15] iter = 11500, loss = 1.7099
[2023-10-01 22:14:16] iter = 11510, loss = 1.5311
[2023-10-01 22:14:17] iter = 11520, loss = 1.6836
[2023-10-01 22:14:18] iter = 11530, loss = 1.5374
[2023-10-01 22:14:19] iter = 11540, loss = 1.4951
[2023-10-01 22:14:20] iter = 11550, loss = 1.6108
[2023-10-01 22:14:20] iter = 11560, loss = 1.6919
[2023-10-01 22:14:21] iter = 11570, loss = 1.6804
[2023-10-01 22:14:22] iter = 11580, loss = 1.5917
[2023-10-01 22:14:23] iter = 11590, loss = 1.5371
[2023-10-01 22:14:24] iter = 11600, loss = 1.6568
[2023-10-01 22:14:25] iter = 11610, loss = 1.5218
[2023-10-01 22:14:26] iter = 11620, loss = 1.4310
[2023-10-01 22:14:27] iter = 11630, loss = 1.5032
[2023-10-01 22:14:28] iter = 11640, loss = 1.5034
[2023-10-01 22:14:28] iter = 11650, loss = 1.7308
[2023-10-01 22:14:29] iter = 11660, loss = 1.6645
[2023-10-01 22:14:30] iter = 11670, loss = 1.5903
[2023-10-01 22:14:31] iter = 11680, loss = 1.6142
[2023-10-01 22:14:32] iter = 11690, loss = 1.3204
[2023-10-01 22:14:33] iter = 11700, loss = 1.6836
[2023-10-01 22:14:34] iter = 11710, loss = 1.6501
[2023-10-01 22:14:35] iter = 11720, loss = 1.5310
[2023-10-01 22:14:36] iter = 11730, loss = 1.5790
[2023-10-01 22:14:37] iter = 11740, loss = 1.4817
[2023-10-01 22:14:38] iter = 11750, loss = 1.5457
[2023-10-01 22:14:39] iter = 11760, loss = 1.5395
[2023-10-01 22:14:39] iter = 11770, loss = 1.4682
[2023-10-01 22:14:40] iter = 11780, loss = 1.5933
[2023-10-01 22:14:41] iter = 11790, loss = 1.5034
[2023-10-01 22:14:42] iter = 11800, loss = 1.4885
[2023-10-01 22:14:43] iter = 11810, loss = 1.6073
[2023-10-01 22:14:44] iter = 11820, loss = 1.5163
[2023-10-01 22:14:45] iter = 11830, loss = 1.5359
[2023-10-01 22:14:46] iter = 11840, loss = 1.5158
[2023-10-01 22:14:47] iter = 11850, loss = 1.6051
[2023-10-01 22:14:48] iter = 11860, loss = 1.4926
[2023-10-01 22:14:49] iter = 11870, loss = 1.5257
[2023-10-01 22:14:50] iter = 11880, loss = 1.6180
[2023-10-01 22:14:51] iter = 11890, loss = 1.5414
[2023-10-01 22:14:51] iter = 11900, loss = 1.5027
[2023-10-01 22:14:52] iter = 11910, loss = 1.4862
[2023-10-01 22:14:53] iter = 11920, loss = 1.5851
[2023-10-01 22:14:54] iter = 11930, loss = 1.5238
[2023-10-01 22:14:55] iter = 11940, loss = 1.4089
[2023-10-01 22:14:56] iter = 11950, loss = 1.4020
[2023-10-01 22:14:57] iter = 11960, loss = 1.4466
[2023-10-01 22:14:58] iter = 11970, loss = 1.5100
[2023-10-01 22:14:59] iter = 11980, loss = 1.6224
[2023-10-01 22:15:00] iter = 11990, loss = 1.5363
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 1106}
[2023-10-01 22:15:25] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004134 train acc = 1.0000, test acc = 0.6217
[2023-10-01 22:15:49] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.012218 train acc = 1.0000, test acc = 0.6188
[2023-10-01 22:16:14] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.034892 train acc = 0.9980, test acc = 0.6247
[2023-10-01 22:16:38] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004423 train acc = 1.0000, test acc = 0.6182
[2023-10-01 22:17:02] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002122 train acc = 1.0000, test acc = 0.6180
[2023-10-01 22:17:26] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003435 train acc = 1.0000, test acc = 0.6160
[2023-10-01 22:17:51] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.032306 train acc = 0.9980, test acc = 0.6137
[2023-10-01 22:18:15] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.010829 train acc = 1.0000, test acc = 0.6221
[2023-10-01 22:18:39] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.023131 train acc = 1.0000, test acc = 0.6183
[2023-10-01 22:19:03] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.022333 train acc = 1.0000, test acc = 0.6150
[2023-10-01 22:19:28] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.018490 train acc = 1.0000, test acc = 0.6201
[2023-10-01 22:19:52] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001791 train acc = 1.0000, test acc = 0.6283
[2023-10-01 22:20:17] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003746 train acc = 1.0000, test acc = 0.6241
[2023-10-01 22:20:41] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.001675 train acc = 1.0000, test acc = 0.6255
[2023-10-01 22:21:05] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013166 train acc = 1.0000, test acc = 0.6225
[2023-10-01 22:21:30] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.013691 train acc = 1.0000, test acc = 0.6239
[2023-10-01 22:21:54] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002630 train acc = 1.0000, test acc = 0.6233
[2023-10-01 22:22:18] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.018678 train acc = 0.9980, test acc = 0.6301
[2023-10-01 22:22:42] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003904 train acc = 1.0000, test acc = 0.6258
[2023-10-01 22:23:07] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.018010 train acc = 0.9980, test acc = 0.6281
Evaluate 20 random ConvNet, mean = 0.6219 std = 0.0045
-------------------------
[2023-10-01 22:23:07] iter = 12000, loss = 1.4469
[2023-10-01 22:23:08] iter = 12010, loss = 1.6332
[2023-10-01 22:23:09] iter = 12020, loss = 1.5815
[2023-10-01 22:23:10] iter = 12030, loss = 1.6400
[2023-10-01 22:23:11] iter = 12040, loss = 1.4860
[2023-10-01 22:23:11] iter = 12050, loss = 1.5634
[2023-10-01 22:23:12] iter = 12060, loss = 1.5408
[2023-10-01 22:23:13] iter = 12070, loss = 1.6216
[2023-10-01 22:23:14] iter = 12080, loss = 1.4931
[2023-10-01 22:23:15] iter = 12090, loss = 1.4667
[2023-10-01 22:23:16] iter = 12100, loss = 1.4747
[2023-10-01 22:23:17] iter = 12110, loss = 1.6158
[2023-10-01 22:23:18] iter = 12120, loss = 1.5464
[2023-10-01 22:23:19] iter = 12130, loss = 1.4485
[2023-10-01 22:23:20] iter = 12140, loss = 1.7150
[2023-10-01 22:23:20] iter = 12150, loss = 1.4718
[2023-10-01 22:23:21] iter = 12160, loss = 1.5312
[2023-10-01 22:23:22] iter = 12170, loss = 1.3968
[2023-10-01 22:23:23] iter = 12180, loss = 1.5415
[2023-10-01 22:23:24] iter = 12190, loss = 1.5567
[2023-10-01 22:23:25] iter = 12200, loss = 1.5752
[2023-10-01 22:23:26] iter = 12210, loss = 1.5013
[2023-10-01 22:23:27] iter = 12220, loss = 1.6230
[2023-10-01 22:23:28] iter = 12230, loss = 1.5999
[2023-10-01 22:23:29] iter = 12240, loss = 1.6277
[2023-10-01 22:23:30] iter = 12250, loss = 1.5197
[2023-10-01 22:23:31] iter = 12260, loss = 1.4949
[2023-10-01 22:23:32] iter = 12270, loss = 1.5658
[2023-10-01 22:23:32] iter = 12280, loss = 1.4832
[2023-10-01 22:23:33] iter = 12290, loss = 1.5548
[2023-10-01 22:23:34] iter = 12300, loss = 1.5275
[2023-10-01 22:23:35] iter = 12310, loss = 1.6624
[2023-10-01 22:23:36] iter = 12320, loss = 1.4836
[2023-10-01 22:23:37] iter = 12330, loss = 1.7258
[2023-10-01 22:23:38] iter = 12340, loss = 1.6370
[2023-10-01 22:23:39] iter = 12350, loss = 1.5929
[2023-10-01 22:23:40] iter = 12360, loss = 1.5057
[2023-10-01 22:23:41] iter = 12370, loss = 1.4732
[2023-10-01 22:23:42] iter = 12380, loss = 1.6471
[2023-10-01 22:23:43] iter = 12390, loss = 1.5027
[2023-10-01 22:23:44] iter = 12400, loss = 1.4927
[2023-10-01 22:23:45] iter = 12410, loss = 1.6177
[2023-10-01 22:23:45] iter = 12420, loss = 1.4741
[2023-10-01 22:23:46] iter = 12430, loss = 1.5675
[2023-10-01 22:23:47] iter = 12440, loss = 1.4968
[2023-10-01 22:23:48] iter = 12450, loss = 1.5713
[2023-10-01 22:23:49] iter = 12460, loss = 1.6350
[2023-10-01 22:23:50] iter = 12470, loss = 1.5600
[2023-10-01 22:23:51] iter = 12480, loss = 1.5336
[2023-10-01 22:23:52] iter = 12490, loss = 1.5009
[2023-10-01 22:23:53] iter = 12500, loss = 1.4802
[2023-10-01 22:23:54] iter = 12510, loss = 1.6500
[2023-10-01 22:23:55] iter = 12520, loss = 1.5633
[2023-10-01 22:23:55] iter = 12530, loss = 1.6322
[2023-10-01 22:23:56] iter = 12540, loss = 1.4374
[2023-10-01 22:23:57] iter = 12550, loss = 1.6038
[2023-10-01 22:23:58] iter = 12560, loss = 1.6321
[2023-10-01 22:23:59] iter = 12570, loss = 1.4796
[2023-10-01 22:24:00] iter = 12580, loss = 1.5409
[2023-10-01 22:24:01] iter = 12590, loss = 1.7252
[2023-10-01 22:24:02] iter = 12600, loss = 1.5090
[2023-10-01 22:24:03] iter = 12610, loss = 1.6312
[2023-10-01 22:24:04] iter = 12620, loss = 1.5301
[2023-10-01 22:24:05] iter = 12630, loss = 1.4723
[2023-10-01 22:24:06] iter = 12640, loss = 1.5376
[2023-10-01 22:24:07] iter = 12650, loss = 1.5637
[2023-10-01 22:24:07] iter = 12660, loss = 1.6233
[2023-10-01 22:24:08] iter = 12670, loss = 1.5364
[2023-10-01 22:24:09] iter = 12680, loss = 1.5772
[2023-10-01 22:24:10] iter = 12690, loss = 1.5415
[2023-10-01 22:24:11] iter = 12700, loss = 1.3840
[2023-10-01 22:24:12] iter = 12710, loss = 1.4900
[2023-10-01 22:24:13] iter = 12720, loss = 1.6048
[2023-10-01 22:24:14] iter = 12730, loss = 1.5361
[2023-10-01 22:24:14] iter = 12740, loss = 1.5536
[2023-10-01 22:24:15] iter = 12750, loss = 1.6583
[2023-10-01 22:24:16] iter = 12760, loss = 1.5074
[2023-10-01 22:24:17] iter = 12770, loss = 1.4738
[2023-10-01 22:24:18] iter = 12780, loss = 1.3621
[2023-10-01 22:24:19] iter = 12790, loss = 1.5579
[2023-10-01 22:24:20] iter = 12800, loss = 1.5044
[2023-10-01 22:24:21] iter = 12810, loss = 1.6573
[2023-10-01 22:24:22] iter = 12820, loss = 1.5657
[2023-10-01 22:24:23] iter = 12830, loss = 1.4337
[2023-10-01 22:24:23] iter = 12840, loss = 1.5087
[2023-10-01 22:24:24] iter = 12850, loss = 1.5752
[2023-10-01 22:24:25] iter = 12860, loss = 1.5954
[2023-10-01 22:24:26] iter = 12870, loss = 1.4650
[2023-10-01 22:24:27] iter = 12880, loss = 1.6259
[2023-10-01 22:24:28] iter = 12890, loss = 1.5124
[2023-10-01 22:24:29] iter = 12900, loss = 1.5526
[2023-10-01 22:24:30] iter = 12910, loss = 1.4429
[2023-10-01 22:24:31] iter = 12920, loss = 1.5831
[2023-10-01 22:24:31] iter = 12930, loss = 1.5270
[2023-10-01 22:24:32] iter = 12940, loss = 1.6090
[2023-10-01 22:24:33] iter = 12950, loss = 1.5209
[2023-10-01 22:24:34] iter = 12960, loss = 1.4290
[2023-10-01 22:24:35] iter = 12970, loss = 1.5809
[2023-10-01 22:24:36] iter = 12980, loss = 1.6449
[2023-10-01 22:24:37] iter = 12990, loss = 1.5021
[2023-10-01 22:24:38] iter = 13000, loss = 1.5329
[2023-10-01 22:24:39] iter = 13010, loss = 1.5565
[2023-10-01 22:24:40] iter = 13020, loss = 1.6117
[2023-10-01 22:24:40] iter = 13030, loss = 1.5182
[2023-10-01 22:24:41] iter = 13040, loss = 1.5756
[2023-10-01 22:24:42] iter = 13050, loss = 1.4468
[2023-10-01 22:24:43] iter = 13060, loss = 1.5760
[2023-10-01 22:24:44] iter = 13070, loss = 1.5727
[2023-10-01 22:24:45] iter = 13080, loss = 1.5867
[2023-10-01 22:24:46] iter = 13090, loss = 1.7223
[2023-10-01 22:24:47] iter = 13100, loss = 1.5106
[2023-10-01 22:24:48] iter = 13110, loss = 1.4635
[2023-10-01 22:24:49] iter = 13120, loss = 1.4154
[2023-10-01 22:24:50] iter = 13130, loss = 1.4729
[2023-10-01 22:24:51] iter = 13140, loss = 1.4990
[2023-10-01 22:24:51] iter = 13150, loss = 1.4348
[2023-10-01 22:24:52] iter = 13160, loss = 1.4855
[2023-10-01 22:24:53] iter = 13170, loss = 1.5142
[2023-10-01 22:24:54] iter = 13180, loss = 1.5684
[2023-10-01 22:24:55] iter = 13190, loss = 1.4107
[2023-10-01 22:24:56] iter = 13200, loss = 1.4235
[2023-10-01 22:24:57] iter = 13210, loss = 1.3901
[2023-10-01 22:24:58] iter = 13220, loss = 1.7615
[2023-10-01 22:24:59] iter = 13230, loss = 1.4958
[2023-10-01 22:25:00] iter = 13240, loss = 1.6042
[2023-10-01 22:25:00] iter = 13250, loss = 1.5575
[2023-10-01 22:25:01] iter = 13260, loss = 1.5497
[2023-10-01 22:25:02] iter = 13270, loss = 1.5838
[2023-10-01 22:25:03] iter = 13280, loss = 1.5672
[2023-10-01 22:25:04] iter = 13290, loss = 1.4777
[2023-10-01 22:25:05] iter = 13300, loss = 1.4486
[2023-10-01 22:25:06] iter = 13310, loss = 1.6121
[2023-10-01 22:25:07] iter = 13320, loss = 1.5328
[2023-10-01 22:25:08] iter = 13330, loss = 1.5268
[2023-10-01 22:25:09] iter = 13340, loss = 1.5036
[2023-10-01 22:25:10] iter = 13350, loss = 1.5298
[2023-10-01 22:25:10] iter = 13360, loss = 1.5264
[2023-10-01 22:25:11] iter = 13370, loss = 1.5125
[2023-10-01 22:25:12] iter = 13380, loss = 1.5755
[2023-10-01 22:25:13] iter = 13390, loss = 1.5235
[2023-10-01 22:25:14] iter = 13400, loss = 1.3659
[2023-10-01 22:25:15] iter = 13410, loss = 1.4385
[2023-10-01 22:25:16] iter = 13420, loss = 1.5667
[2023-10-01 22:25:17] iter = 13430, loss = 1.7031
[2023-10-01 22:25:18] iter = 13440, loss = 1.5435
[2023-10-01 22:25:19] iter = 13450, loss = 1.5961
[2023-10-01 22:25:19] iter = 13460, loss = 1.6340
[2023-10-01 22:25:20] iter = 13470, loss = 1.5172
[2023-10-01 22:25:21] iter = 13480, loss = 1.4641
[2023-10-01 22:25:22] iter = 13490, loss = 1.5093
[2023-10-01 22:25:23] iter = 13500, loss = 1.6937
[2023-10-01 22:25:24] iter = 13510, loss = 1.4795
[2023-10-01 22:25:25] iter = 13520, loss = 1.5253
[2023-10-01 22:25:26] iter = 13530, loss = 1.3903
[2023-10-01 22:25:26] iter = 13540, loss = 1.6133
[2023-10-01 22:25:27] iter = 13550, loss = 1.4981
[2023-10-01 22:25:28] iter = 13560, loss = 1.5826
[2023-10-01 22:25:29] iter = 13570, loss = 1.7198
[2023-10-01 22:25:30] iter = 13580, loss = 1.5767
[2023-10-01 22:25:31] iter = 13590, loss = 1.4328
[2023-10-01 22:25:32] iter = 13600, loss = 1.5871
[2023-10-01 22:25:33] iter = 13610, loss = 1.6259
[2023-10-01 22:25:34] iter = 13620, loss = 1.5915
[2023-10-01 22:25:35] iter = 13630, loss = 1.6120
[2023-10-01 22:25:36] iter = 13640, loss = 1.4870
[2023-10-01 22:25:37] iter = 13650, loss = 1.4773
[2023-10-01 22:25:38] iter = 13660, loss = 1.5253
[2023-10-01 22:25:39] iter = 13670, loss = 1.4066
[2023-10-01 22:25:40] iter = 13680, loss = 1.5224
[2023-10-01 22:25:40] iter = 13690, loss = 1.6263
[2023-10-01 22:25:41] iter = 13700, loss = 1.5735
[2023-10-01 22:25:42] iter = 13710, loss = 1.5187
[2023-10-01 22:25:43] iter = 13720, loss = 1.4230
[2023-10-01 22:25:44] iter = 13730, loss = 1.4323
[2023-10-01 22:25:45] iter = 13740, loss = 1.4476
[2023-10-01 22:25:46] iter = 13750, loss = 1.4514
[2023-10-01 22:25:47] iter = 13760, loss = 1.4610
[2023-10-01 22:25:48] iter = 13770, loss = 1.4884
[2023-10-01 22:25:48] iter = 13780, loss = 1.4548
[2023-10-01 22:25:49] iter = 13790, loss = 1.5370
[2023-10-01 22:25:50] iter = 13800, loss = 1.4867
[2023-10-01 22:25:51] iter = 13810, loss = 1.6082
[2023-10-01 22:25:52] iter = 13820, loss = 1.5688
[2023-10-01 22:25:53] iter = 13830, loss = 1.4538
[2023-10-01 22:25:54] iter = 13840, loss = 1.5339
[2023-10-01 22:25:55] iter = 13850, loss = 1.4787
[2023-10-01 22:25:56] iter = 13860, loss = 1.4545
[2023-10-01 22:25:57] iter = 13870, loss = 1.7780
[2023-10-01 22:25:57] iter = 13880, loss = 1.6121
[2023-10-01 22:25:58] iter = 13890, loss = 1.4795
[2023-10-01 22:25:59] iter = 13900, loss = 1.6791
[2023-10-01 22:26:00] iter = 13910, loss = 1.4581
[2023-10-01 22:26:01] iter = 13920, loss = 1.6234
[2023-10-01 22:26:02] iter = 13930, loss = 1.4758
[2023-10-01 22:26:03] iter = 13940, loss = 1.5522
[2023-10-01 22:26:04] iter = 13950, loss = 1.4915
[2023-10-01 22:26:04] iter = 13960, loss = 1.5460
[2023-10-01 22:26:05] iter = 13970, loss = 1.5472
[2023-10-01 22:26:06] iter = 13980, loss = 1.4961
[2023-10-01 22:26:07] iter = 13990, loss = 1.7162
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 68390}
[2023-10-01 22:26:32] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.016266 train acc = 1.0000, test acc = 0.6240
[2023-10-01 22:26:56] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.014228 train acc = 0.9960, test acc = 0.6299
[2023-10-01 22:27:21] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010193 train acc = 1.0000, test acc = 0.6298
[2023-10-01 22:27:45] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003556 train acc = 1.0000, test acc = 0.6241
[2023-10-01 22:28:10] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002282 train acc = 1.0000, test acc = 0.6254
[2023-10-01 22:28:34] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013101 train acc = 1.0000, test acc = 0.6248
[2023-10-01 22:28:58] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003218 train acc = 1.0000, test acc = 0.6186
[2023-10-01 22:29:22] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002964 train acc = 1.0000, test acc = 0.6238
[2023-10-01 22:29:47] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.031979 train acc = 1.0000, test acc = 0.6208
[2023-10-01 22:30:11] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013121 train acc = 1.0000, test acc = 0.6237
[2023-10-01 22:30:35] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.006806 train acc = 0.9980, test acc = 0.6205
[2023-10-01 22:30:59] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003515 train acc = 1.0000, test acc = 0.6255
[2023-10-01 22:31:24] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.014305 train acc = 1.0000, test acc = 0.6221
[2023-10-01 22:31:48] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002301 train acc = 1.0000, test acc = 0.6256
[2023-10-01 22:32:12] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003324 train acc = 1.0000, test acc = 0.6221
[2023-10-01 22:32:37] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004090 train acc = 1.0000, test acc = 0.6271
[2023-10-01 22:33:01] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003860 train acc = 1.0000, test acc = 0.6207
[2023-10-01 22:33:25] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.002167 train acc = 1.0000, test acc = 0.6288
[2023-10-01 22:33:49] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.016547 train acc = 1.0000, test acc = 0.6226
[2023-10-01 22:34:14] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.013859 train acc = 0.9980, test acc = 0.6173
Evaluate 20 random ConvNet, mean = 0.6239 std = 0.0034
-------------------------
[2023-10-01 22:34:14] iter = 14000, loss = 1.4594
[2023-10-01 22:34:15] iter = 14010, loss = 1.5663
[2023-10-01 22:34:16] iter = 14020, loss = 1.5882
[2023-10-01 22:34:17] iter = 14030, loss = 1.5251
[2023-10-01 22:34:18] iter = 14040, loss = 1.5474
[2023-10-01 22:34:18] iter = 14050, loss = 1.5668
[2023-10-01 22:34:19] iter = 14060, loss = 1.4524
[2023-10-01 22:34:20] iter = 14070, loss = 1.6650
[2023-10-01 22:34:21] iter = 14080, loss = 1.4209
[2023-10-01 22:34:22] iter = 14090, loss = 1.4053
[2023-10-01 22:34:23] iter = 14100, loss = 1.5907
[2023-10-01 22:34:24] iter = 14110, loss = 1.4686
[2023-10-01 22:34:25] iter = 14120, loss = 1.4003
[2023-10-01 22:34:26] iter = 14130, loss = 1.4536
[2023-10-01 22:34:27] iter = 14140, loss = 1.4522
[2023-10-01 22:34:27] iter = 14150, loss = 1.5156
[2023-10-01 22:34:28] iter = 14160, loss = 1.5874
[2023-10-01 22:34:29] iter = 14170, loss = 1.5327
[2023-10-01 22:34:30] iter = 14180, loss = 1.5047
[2023-10-01 22:34:31] iter = 14190, loss = 1.4346
[2023-10-01 22:34:32] iter = 14200, loss = 1.5176
[2023-10-01 22:34:33] iter = 14210, loss = 1.6148
[2023-10-01 22:34:34] iter = 14220, loss = 1.4732
[2023-10-01 22:34:35] iter = 14230, loss = 1.4353
[2023-10-01 22:34:36] iter = 14240, loss = 1.4638
[2023-10-01 22:34:36] iter = 14250, loss = 1.5674
[2023-10-01 22:34:37] iter = 14260, loss = 1.6744
[2023-10-01 22:34:38] iter = 14270, loss = 1.5048
[2023-10-01 22:34:39] iter = 14280, loss = 1.5925
[2023-10-01 22:34:40] iter = 14290, loss = 1.5907
[2023-10-01 22:34:41] iter = 14300, loss = 1.4957
[2023-10-01 22:34:42] iter = 14310, loss = 1.5843
[2023-10-01 22:34:43] iter = 14320, loss = 1.5817
[2023-10-01 22:34:44] iter = 14330, loss = 1.5074
[2023-10-01 22:34:45] iter = 14340, loss = 1.4188
[2023-10-01 22:34:46] iter = 14350, loss = 1.4779
[2023-10-01 22:34:46] iter = 14360, loss = 1.4986
[2023-10-01 22:34:47] iter = 14370, loss = 1.5567
[2023-10-01 22:34:48] iter = 14380, loss = 1.7007
[2023-10-01 22:34:49] iter = 14390, loss = 1.5302
[2023-10-01 22:34:50] iter = 14400, loss = 1.6583
[2023-10-01 22:34:51] iter = 14410, loss = 1.5279
[2023-10-01 22:34:52] iter = 14420, loss = 1.5911
[2023-10-01 22:34:53] iter = 14430, loss = 1.5862
[2023-10-01 22:34:54] iter = 14440, loss = 1.6088
[2023-10-01 22:34:55] iter = 14450, loss = 1.4617
[2023-10-01 22:34:56] iter = 14460, loss = 1.5716
[2023-10-01 22:34:57] iter = 14470, loss = 1.6765
[2023-10-01 22:34:57] iter = 14480, loss = 1.6005
[2023-10-01 22:34:58] iter = 14490, loss = 1.5002
[2023-10-01 22:34:59] iter = 14500, loss = 1.3503
[2023-10-01 22:35:00] iter = 14510, loss = 1.4046
[2023-10-01 22:35:01] iter = 14520, loss = 1.3823
[2023-10-01 22:35:02] iter = 14530, loss = 1.5869
[2023-10-01 22:35:03] iter = 14540, loss = 1.5600
[2023-10-01 22:35:04] iter = 14550, loss = 1.6190
[2023-10-01 22:35:04] iter = 14560, loss = 1.5666
[2023-10-01 22:35:05] iter = 14570, loss = 1.5560
[2023-10-01 22:35:06] iter = 14580, loss = 1.5722
[2023-10-01 22:35:07] iter = 14590, loss = 1.4492
[2023-10-01 22:35:08] iter = 14600, loss = 1.5337
[2023-10-01 22:35:09] iter = 14610, loss = 1.3678
[2023-10-01 22:35:10] iter = 14620, loss = 1.7396
[2023-10-01 22:35:11] iter = 14630, loss = 1.4418
[2023-10-01 22:35:12] iter = 14640, loss = 1.4983
[2023-10-01 22:35:13] iter = 14650, loss = 1.4702
[2023-10-01 22:35:13] iter = 14660, loss = 1.5095
[2023-10-01 22:35:14] iter = 14670, loss = 1.5226
[2023-10-01 22:35:15] iter = 14680, loss = 1.4299
[2023-10-01 22:35:16] iter = 14690, loss = 1.3884
[2023-10-01 22:35:17] iter = 14700, loss = 1.6306
[2023-10-01 22:35:18] iter = 14710, loss = 1.4951
[2023-10-01 22:35:19] iter = 14720, loss = 1.4249
[2023-10-01 22:35:20] iter = 14730, loss = 1.4440
[2023-10-01 22:35:21] iter = 14740, loss = 1.4830
[2023-10-01 22:35:21] iter = 14750, loss = 1.5038
[2023-10-01 22:35:22] iter = 14760, loss = 1.4089
[2023-10-01 22:35:23] iter = 14770, loss = 1.5521
[2023-10-01 22:35:24] iter = 14780, loss = 1.7391
[2023-10-01 22:35:25] iter = 14790, loss = 1.7459
[2023-10-01 22:35:26] iter = 14800, loss = 1.3887
[2023-10-01 22:35:27] iter = 14810, loss = 1.5224
[2023-10-01 22:35:28] iter = 14820, loss = 1.5265
[2023-10-01 22:35:29] iter = 14830, loss = 1.5187
[2023-10-01 22:35:30] iter = 14840, loss = 1.5411
[2023-10-01 22:35:30] iter = 14850, loss = 1.5037
[2023-10-01 22:35:31] iter = 14860, loss = 1.4973
[2023-10-01 22:35:32] iter = 14870, loss = 1.5022
[2023-10-01 22:35:33] iter = 14880, loss = 1.3704
[2023-10-01 22:35:34] iter = 14890, loss = 1.4863
[2023-10-01 22:35:35] iter = 14900, loss = 1.5315
[2023-10-01 22:35:36] iter = 14910, loss = 1.5330
[2023-10-01 22:35:37] iter = 14920, loss = 1.3985
[2023-10-01 22:35:38] iter = 14930, loss = 1.4772
[2023-10-01 22:35:39] iter = 14940, loss = 1.5476
[2023-10-01 22:35:40] iter = 14950, loss = 1.5816
[2023-10-01 22:35:40] iter = 14960, loss = 1.4625
[2023-10-01 22:35:41] iter = 14970, loss = 1.4723
[2023-10-01 22:35:42] iter = 14980, loss = 1.3664
[2023-10-01 22:35:43] iter = 14990, loss = 1.3841
[2023-10-01 22:35:44] iter = 15000, loss = 1.4526
[2023-10-01 22:35:45] iter = 15010, loss = 1.3853
[2023-10-01 22:35:46] iter = 15020, loss = 1.5175
[2023-10-01 22:35:47] iter = 15030, loss = 1.4240
[2023-10-01 22:35:48] iter = 15040, loss = 1.3976
[2023-10-01 22:35:49] iter = 15050, loss = 1.4970
[2023-10-01 22:35:50] iter = 15060, loss = 1.5494
[2023-10-01 22:35:51] iter = 15070, loss = 1.5331
[2023-10-01 22:35:52] iter = 15080, loss = 1.5166
[2023-10-01 22:35:52] iter = 15090, loss = 1.5313
[2023-10-01 22:35:53] iter = 15100, loss = 1.5072
[2023-10-01 22:35:54] iter = 15110, loss = 1.5857
[2023-10-01 22:35:55] iter = 15120, loss = 1.5564
[2023-10-01 22:35:56] iter = 15130, loss = 1.6074
[2023-10-01 22:35:57] iter = 15140, loss = 1.4397
[2023-10-01 22:35:58] iter = 15150, loss = 1.5041
[2023-10-01 22:35:59] iter = 15160, loss = 1.5855
[2023-10-01 22:36:00] iter = 15170, loss = 1.4746
[2023-10-01 22:36:00] iter = 15180, loss = 1.5113
[2023-10-01 22:36:01] iter = 15190, loss = 1.3755
[2023-10-01 22:36:02] iter = 15200, loss = 1.5686
[2023-10-01 22:36:03] iter = 15210, loss = 1.4154
[2023-10-01 22:36:04] iter = 15220, loss = 1.6388
[2023-10-01 22:36:05] iter = 15230, loss = 1.5331
[2023-10-01 22:36:06] iter = 15240, loss = 1.5044
[2023-10-01 22:36:07] iter = 15250, loss = 1.6033
[2023-10-01 22:36:08] iter = 15260, loss = 1.5865
[2023-10-01 22:36:09] iter = 15270, loss = 1.5060
[2023-10-01 22:36:09] iter = 15280, loss = 1.5468
[2023-10-01 22:36:10] iter = 15290, loss = 1.5725
[2023-10-01 22:36:11] iter = 15300, loss = 1.4865
[2023-10-01 22:36:12] iter = 15310, loss = 1.4674
[2023-10-01 22:36:13] iter = 15320, loss = 1.4159
[2023-10-01 22:36:14] iter = 15330, loss = 1.5799
[2023-10-01 22:36:15] iter = 15340, loss = 1.5857
[2023-10-01 22:36:16] iter = 15350, loss = 1.7222
[2023-10-01 22:36:17] iter = 15360, loss = 1.3684
[2023-10-01 22:36:18] iter = 15370, loss = 1.5222
[2023-10-01 22:36:18] iter = 15380, loss = 1.5247
[2023-10-01 22:36:19] iter = 15390, loss = 1.6136
[2023-10-01 22:36:20] iter = 15400, loss = 1.5184
[2023-10-01 22:36:21] iter = 15410, loss = 1.3296
[2023-10-01 22:36:22] iter = 15420, loss = 1.4736
[2023-10-01 22:36:23] iter = 15430, loss = 1.4730
[2023-10-01 22:36:24] iter = 15440, loss = 1.5297
[2023-10-01 22:36:25] iter = 15450, loss = 1.3963
[2023-10-01 22:36:26] iter = 15460, loss = 1.4824
[2023-10-01 22:36:27] iter = 15470, loss = 1.5456
[2023-10-01 22:36:28] iter = 15480, loss = 1.4877
[2023-10-01 22:36:29] iter = 15490, loss = 1.5985
[2023-10-01 22:36:30] iter = 15500, loss = 1.4818
[2023-10-01 22:36:30] iter = 15510, loss = 1.6109
[2023-10-01 22:36:31] iter = 15520, loss = 1.4613
[2023-10-01 22:36:32] iter = 15530, loss = 1.7200
[2023-10-01 22:36:33] iter = 15540, loss = 1.5279
[2023-10-01 22:36:34] iter = 15550, loss = 1.4831
[2023-10-01 22:36:35] iter = 15560, loss = 1.3548
[2023-10-01 22:36:36] iter = 15570, loss = 1.5259
[2023-10-01 22:36:37] iter = 15580, loss = 1.5460
[2023-10-01 22:36:38] iter = 15590, loss = 1.5537
[2023-10-01 22:36:39] iter = 15600, loss = 1.5240
[2023-10-01 22:36:40] iter = 15610, loss = 1.5068
[2023-10-01 22:36:41] iter = 15620, loss = 1.4794
[2023-10-01 22:36:42] iter = 15630, loss = 1.7068
[2023-10-01 22:36:43] iter = 15640, loss = 1.5673
[2023-10-01 22:36:44] iter = 15650, loss = 1.5507
[2023-10-01 22:36:44] iter = 15660, loss = 1.3407
[2023-10-01 22:36:45] iter = 15670, loss = 1.4772
[2023-10-01 22:36:46] iter = 15680, loss = 1.5145
[2023-10-01 22:36:47] iter = 15690, loss = 1.5509
[2023-10-01 22:36:48] iter = 15700, loss = 1.4905
[2023-10-01 22:36:49] iter = 15710, loss = 1.4807
[2023-10-01 22:36:50] iter = 15720, loss = 1.5197
[2023-10-01 22:36:51] iter = 15730, loss = 1.4450
[2023-10-01 22:36:52] iter = 15740, loss = 1.4783
[2023-10-01 22:36:53] iter = 15750, loss = 1.5377
[2023-10-01 22:36:54] iter = 15760, loss = 1.4525
[2023-10-01 22:36:54] iter = 15770, loss = 1.4921
[2023-10-01 22:36:55] iter = 15780, loss = 1.3789
[2023-10-01 22:36:56] iter = 15790, loss = 1.6333
[2023-10-01 22:36:57] iter = 15800, loss = 1.4260
[2023-10-01 22:36:58] iter = 15810, loss = 1.5462
[2023-10-01 22:36:59] iter = 15820, loss = 1.5258
[2023-10-01 22:37:00] iter = 15830, loss = 1.5447
[2023-10-01 22:37:01] iter = 15840, loss = 1.7503
[2023-10-01 22:37:02] iter = 15850, loss = 1.5552
[2023-10-01 22:37:03] iter = 15860, loss = 1.4919
[2023-10-01 22:37:04] iter = 15870, loss = 1.4681
[2023-10-01 22:37:04] iter = 15880, loss = 1.4592
[2023-10-01 22:37:05] iter = 15890, loss = 1.6091
[2023-10-01 22:37:06] iter = 15900, loss = 1.5083
[2023-10-01 22:37:07] iter = 15910, loss = 1.3995
[2023-10-01 22:37:08] iter = 15920, loss = 1.6827
[2023-10-01 22:37:09] iter = 15930, loss = 1.5053
[2023-10-01 22:37:10] iter = 15940, loss = 1.3531
[2023-10-01 22:37:11] iter = 15950, loss = 1.6256
[2023-10-01 22:37:12] iter = 15960, loss = 1.5075
[2023-10-01 22:37:13] iter = 15970, loss = 1.4265
[2023-10-01 22:37:13] iter = 15980, loss = 1.5135
[2023-10-01 22:37:14] iter = 15990, loss = 1.3829
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 35765}
[2023-10-01 22:37:40] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002663 train acc = 1.0000, test acc = 0.6250
[2023-10-01 22:38:04] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.015173 train acc = 1.0000, test acc = 0.6259
[2023-10-01 22:38:28] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.004406 train acc = 1.0000, test acc = 0.6253
[2023-10-01 22:38:52] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.012516 train acc = 1.0000, test acc = 0.6259
[2023-10-01 22:39:17] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.016712 train acc = 1.0000, test acc = 0.6238
[2023-10-01 22:39:41] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.009393 train acc = 1.0000, test acc = 0.6318
[2023-10-01 22:40:05] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.005578 train acc = 1.0000, test acc = 0.6252
[2023-10-01 22:40:30] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.011389 train acc = 1.0000, test acc = 0.6289
[2023-10-01 22:40:54] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003305 train acc = 1.0000, test acc = 0.6283
[2023-10-01 22:41:18] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013531 train acc = 1.0000, test acc = 0.6277
[2023-10-01 22:41:42] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004490 train acc = 1.0000, test acc = 0.6255
[2023-10-01 22:42:06] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015374 train acc = 1.0000, test acc = 0.6294
[2023-10-01 22:42:31] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.025361 train acc = 1.0000, test acc = 0.6198
[2023-10-01 22:42:55] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004373 train acc = 1.0000, test acc = 0.6275
[2023-10-01 22:43:19] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.006200 train acc = 1.0000, test acc = 0.6350
[2023-10-01 22:43:44] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002344 train acc = 1.0000, test acc = 0.6306
[2023-10-01 22:44:08] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.005744 train acc = 0.9980, test acc = 0.6232
[2023-10-01 22:44:32] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003528 train acc = 1.0000, test acc = 0.6245
[2023-10-01 22:44:56] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.008424 train acc = 0.9980, test acc = 0.6255
[2023-10-01 22:45:21] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.015938 train acc = 1.0000, test acc = 0.6230
Evaluate 20 random ConvNet, mean = 0.6266 std = 0.0033
-------------------------
[2023-10-01 22:45:21] iter = 16000, loss = 1.4619
[2023-10-01 22:45:22] iter = 16010, loss = 1.6078
[2023-10-01 22:45:23] iter = 16020, loss = 1.4050
[2023-10-01 22:45:24] iter = 16030, loss = 1.6302
[2023-10-01 22:45:25] iter = 16040, loss = 1.4010
[2023-10-01 22:45:26] iter = 16050, loss = 1.4551
[2023-10-01 22:45:26] iter = 16060, loss = 1.5103
[2023-10-01 22:45:27] iter = 16070, loss = 1.4174
[2023-10-01 22:45:28] iter = 16080, loss = 1.4211
[2023-10-01 22:45:29] iter = 16090, loss = 1.3580
[2023-10-01 22:45:30] iter = 16100, loss = 1.5237
[2023-10-01 22:45:31] iter = 16110, loss = 1.4230
[2023-10-01 22:45:32] iter = 16120, loss = 1.4997
[2023-10-01 22:45:33] iter = 16130, loss = 1.5786
[2023-10-01 22:45:34] iter = 16140, loss = 1.5554
[2023-10-01 22:45:35] iter = 16150, loss = 1.7096
[2023-10-01 22:45:35] iter = 16160, loss = 1.5021
[2023-10-01 22:45:36] iter = 16170, loss = 1.5528
[2023-10-01 22:45:37] iter = 16180, loss = 1.5426
[2023-10-01 22:45:38] iter = 16190, loss = 1.5330
[2023-10-01 22:45:39] iter = 16200, loss = 1.5029
[2023-10-01 22:45:40] iter = 16210, loss = 1.4559
[2023-10-01 22:45:41] iter = 16220, loss = 1.4156
[2023-10-01 22:45:42] iter = 16230, loss = 1.3885
[2023-10-01 22:45:43] iter = 16240, loss = 1.5265
[2023-10-01 22:45:44] iter = 16250, loss = 1.4600
[2023-10-01 22:45:45] iter = 16260, loss = 1.4791
[2023-10-01 22:45:45] iter = 16270, loss = 1.4672
[2023-10-01 22:45:46] iter = 16280, loss = 1.4620
[2023-10-01 22:45:47] iter = 16290, loss = 1.5836
[2023-10-01 22:45:48] iter = 16300, loss = 1.6600
[2023-10-01 22:45:49] iter = 16310, loss = 1.5105
[2023-10-01 22:45:50] iter = 16320, loss = 1.4720
[2023-10-01 22:45:51] iter = 16330, loss = 1.5850
[2023-10-01 22:45:52] iter = 16340, loss = 1.7162
[2023-10-01 22:45:53] iter = 16350, loss = 1.5014
[2023-10-01 22:45:54] iter = 16360, loss = 1.4934
[2023-10-01 22:45:55] iter = 16370, loss = 1.4792
[2023-10-01 22:45:55] iter = 16380, loss = 1.3745
[2023-10-01 22:45:56] iter = 16390, loss = 1.4625
[2023-10-01 22:45:57] iter = 16400, loss = 1.6221
[2023-10-01 22:45:58] iter = 16410, loss = 1.4944
[2023-10-01 22:45:59] iter = 16420, loss = 1.4233
[2023-10-01 22:46:00] iter = 16430, loss = 1.4720
[2023-10-01 22:46:01] iter = 16440, loss = 1.5578
[2023-10-01 22:46:02] iter = 16450, loss = 1.3694
[2023-10-01 22:46:03] iter = 16460, loss = 1.6799
[2023-10-01 22:46:04] iter = 16470, loss = 1.3899
[2023-10-01 22:46:05] iter = 16480, loss = 1.3831
[2023-10-01 22:46:05] iter = 16490, loss = 1.5792
[2023-10-01 22:46:06] iter = 16500, loss = 1.6844
[2023-10-01 22:46:07] iter = 16510, loss = 1.3904
[2023-10-01 22:46:08] iter = 16520, loss = 1.5618
[2023-10-01 22:46:09] iter = 16530, loss = 1.4364
[2023-10-01 22:46:10] iter = 16540, loss = 1.4381
[2023-10-01 22:46:11] iter = 16550, loss = 1.4810
[2023-10-01 22:46:12] iter = 16560, loss = 1.5403
[2023-10-01 22:46:13] iter = 16570, loss = 1.5718
[2023-10-01 22:46:14] iter = 16580, loss = 1.5856
[2023-10-01 22:46:14] iter = 16590, loss = 1.6440
[2023-10-01 22:46:15] iter = 16600, loss = 1.5239
[2023-10-01 22:46:16] iter = 16610, loss = 1.4057
[2023-10-01 22:46:17] iter = 16620, loss = 1.5052
[2023-10-01 22:46:18] iter = 16630, loss = 1.3225
[2023-10-01 22:46:19] iter = 16640, loss = 1.6060
[2023-10-01 22:46:20] iter = 16650, loss = 1.5792
[2023-10-01 22:46:21] iter = 16660, loss = 1.4749
[2023-10-01 22:46:22] iter = 16670, loss = 1.6001
[2023-10-01 22:46:23] iter = 16680, loss = 1.6294
[2023-10-01 22:46:24] iter = 16690, loss = 1.6361
[2023-10-01 22:46:24] iter = 16700, loss = 1.4228
[2023-10-01 22:46:26] iter = 16710, loss = 1.4974
[2023-10-01 22:46:26] iter = 16720, loss = 1.4747
[2023-10-01 22:46:27] iter = 16730, loss = 1.4940
[2023-10-01 22:46:28] iter = 16740, loss = 1.6035
[2023-10-01 22:46:29] iter = 16750, loss = 1.5595
[2023-10-01 22:46:30] iter = 16760, loss = 1.4324
[2023-10-01 22:46:31] iter = 16770, loss = 1.5923
[2023-10-01 22:46:32] iter = 16780, loss = 1.5112
[2023-10-01 22:46:33] iter = 16790, loss = 1.4520
[2023-10-01 22:46:34] iter = 16800, loss = 1.6616
[2023-10-01 22:46:35] iter = 16810, loss = 1.5182
[2023-10-01 22:46:36] iter = 16820, loss = 1.6063
[2023-10-01 22:46:36] iter = 16830, loss = 1.5001
[2023-10-01 22:46:37] iter = 16840, loss = 1.7158
[2023-10-01 22:46:38] iter = 16850, loss = 1.6430
[2023-10-01 22:46:39] iter = 16860, loss = 1.4796
[2023-10-01 22:46:40] iter = 16870, loss = 1.5341
[2023-10-01 22:46:41] iter = 16880, loss = 1.4878
[2023-10-01 22:46:42] iter = 16890, loss = 1.3779
[2023-10-01 22:46:43] iter = 16900, loss = 1.5440
[2023-10-01 22:46:43] iter = 16910, loss = 1.3832
[2023-10-01 22:46:44] iter = 16920, loss = 1.6032
[2023-10-01 22:46:45] iter = 16930, loss = 1.4376
[2023-10-01 22:46:46] iter = 16940, loss = 1.3768
[2023-10-01 22:46:47] iter = 16950, loss = 1.5680
[2023-10-01 22:46:48] iter = 16960, loss = 1.4148
[2023-10-01 22:46:49] iter = 16970, loss = 1.5270
[2023-10-01 22:46:50] iter = 16980, loss = 1.5727
[2023-10-01 22:46:51] iter = 16990, loss = 1.4690
[2023-10-01 22:46:52] iter = 17000, loss = 1.5438
[2023-10-01 22:46:52] iter = 17010, loss = 1.4866
[2023-10-01 22:46:53] iter = 17020, loss = 1.4161
[2023-10-01 22:46:54] iter = 17030, loss = 1.6165
[2023-10-01 22:46:55] iter = 17040, loss = 1.4935
[2023-10-01 22:46:56] iter = 17050, loss = 1.5251
[2023-10-01 22:46:57] iter = 17060, loss = 1.4707
[2023-10-01 22:46:58] iter = 17070, loss = 1.4390
[2023-10-01 22:46:59] iter = 17080, loss = 1.3945
[2023-10-01 22:47:00] iter = 17090, loss = 1.5028
[2023-10-01 22:47:01] iter = 17100, loss = 1.3885
[2023-10-01 22:47:02] iter = 17110, loss = 1.5130
[2023-10-01 22:47:03] iter = 17120, loss = 1.2817
[2023-10-01 22:47:04] iter = 17130, loss = 1.4902
[2023-10-01 22:47:04] iter = 17140, loss = 1.5345
[2023-10-01 22:47:05] iter = 17150, loss = 1.3901
[2023-10-01 22:47:06] iter = 17160, loss = 1.4073
[2023-10-01 22:47:07] iter = 17170, loss = 1.6048
[2023-10-01 22:47:08] iter = 17180, loss = 1.4842
[2023-10-01 22:47:09] iter = 17190, loss = 1.6640
[2023-10-01 22:47:10] iter = 17200, loss = 1.4889
[2023-10-01 22:47:11] iter = 17210, loss = 1.4973
[2023-10-01 22:47:12] iter = 17220, loss = 1.6249
[2023-10-01 22:47:13] iter = 17230, loss = 1.5198
[2023-10-01 22:47:14] iter = 17240, loss = 1.5194
[2023-10-01 22:47:15] iter = 17250, loss = 1.4827
[2023-10-01 22:47:15] iter = 17260, loss = 1.5397
[2023-10-01 22:47:16] iter = 17270, loss = 1.3793
[2023-10-01 22:47:17] iter = 17280, loss = 1.4578
[2023-10-01 22:47:18] iter = 17290, loss = 1.4782
[2023-10-01 22:47:19] iter = 17300, loss = 1.5050
[2023-10-01 22:47:20] iter = 17310, loss = 1.4873
[2023-10-01 22:47:21] iter = 17320, loss = 1.6153
[2023-10-01 22:47:22] iter = 17330, loss = 1.4597
[2023-10-01 22:47:23] iter = 17340, loss = 1.5177
[2023-10-01 22:47:24] iter = 17350, loss = 1.5780
[2023-10-01 22:47:24] iter = 17360, loss = 1.4142
[2023-10-01 22:47:25] iter = 17370, loss = 1.4604
[2023-10-01 22:47:26] iter = 17380, loss = 1.7144
[2023-10-01 22:47:27] iter = 17390, loss = 1.3379
[2023-10-01 22:47:28] iter = 17400, loss = 1.5637
[2023-10-01 22:47:29] iter = 17410, loss = 1.4437
[2023-10-01 22:47:30] iter = 17420, loss = 1.4912
[2023-10-01 22:47:31] iter = 17430, loss = 1.5011
[2023-10-01 22:47:31] iter = 17440, loss = 1.5063
[2023-10-01 22:47:33] iter = 17450, loss = 1.3949
[2023-10-01 22:47:33] iter = 17460, loss = 1.4844
[2023-10-01 22:47:34] iter = 17470, loss = 1.4881
[2023-10-01 22:47:35] iter = 17480, loss = 1.4042
[2023-10-01 22:47:36] iter = 17490, loss = 1.5599
[2023-10-01 22:47:37] iter = 17500, loss = 1.5457
[2023-10-01 22:47:38] iter = 17510, loss = 1.3857
[2023-10-01 22:47:39] iter = 17520, loss = 1.5628
[2023-10-01 22:47:40] iter = 17530, loss = 1.4567
[2023-10-01 22:47:40] iter = 17540, loss = 1.4024
[2023-10-01 22:47:41] iter = 17550, loss = 1.7130
[2023-10-01 22:47:42] iter = 17560, loss = 1.4160
[2023-10-01 22:47:43] iter = 17570, loss = 1.5064
[2023-10-01 22:47:44] iter = 17580, loss = 1.5450
[2023-10-01 22:47:45] iter = 17590, loss = 1.5147
[2023-10-01 22:47:46] iter = 17600, loss = 1.3897
[2023-10-01 22:47:47] iter = 17610, loss = 1.3857
[2023-10-01 22:47:48] iter = 17620, loss = 1.4786
[2023-10-01 22:47:49] iter = 17630, loss = 1.4358
[2023-10-01 22:47:49] iter = 17640, loss = 1.4259
[2023-10-01 22:47:50] iter = 17650, loss = 1.4939
[2023-10-01 22:47:51] iter = 17660, loss = 1.5550
[2023-10-01 22:47:52] iter = 17670, loss = 1.5785
[2023-10-01 22:47:53] iter = 17680, loss = 1.4603
[2023-10-01 22:47:54] iter = 17690, loss = 1.4303
[2023-10-01 22:47:55] iter = 17700, loss = 1.5046
[2023-10-01 22:47:56] iter = 17710, loss = 1.6197
[2023-10-01 22:47:57] iter = 17720, loss = 1.3761
[2023-10-01 22:47:58] iter = 17730, loss = 1.4541
[2023-10-01 22:47:59] iter = 17740, loss = 1.4189
[2023-10-01 22:48:00] iter = 17750, loss = 1.6552
[2023-10-01 22:48:01] iter = 17760, loss = 1.5274
[2023-10-01 22:48:02] iter = 17770, loss = 1.4998
[2023-10-01 22:48:02] iter = 17780, loss = 1.4729
[2023-10-01 22:48:03] iter = 17790, loss = 1.3690
[2023-10-01 22:48:04] iter = 17800, loss = 1.4120
[2023-10-01 22:48:05] iter = 17810, loss = 1.4966
[2023-10-01 22:48:06] iter = 17820, loss = 1.5136
[2023-10-01 22:48:07] iter = 17830, loss = 1.5077
[2023-10-01 22:48:08] iter = 17840, loss = 1.4317
[2023-10-01 22:48:09] iter = 17850, loss = 1.3807
[2023-10-01 22:48:10] iter = 17860, loss = 1.5725
[2023-10-01 22:48:11] iter = 17870, loss = 1.5218
[2023-10-01 22:48:12] iter = 17880, loss = 1.4577
[2023-10-01 22:48:12] iter = 17890, loss = 1.3921
[2023-10-01 22:48:13] iter = 17900, loss = 1.5356
[2023-10-01 22:48:14] iter = 17910, loss = 1.6066
[2023-10-01 22:48:15] iter = 17920, loss = 1.5025
[2023-10-01 22:48:16] iter = 17930, loss = 1.5428
[2023-10-01 22:48:17] iter = 17940, loss = 1.5184
[2023-10-01 22:48:18] iter = 17950, loss = 1.5136
[2023-10-01 22:48:19] iter = 17960, loss = 1.6578
[2023-10-01 22:48:20] iter = 17970, loss = 1.5027
[2023-10-01 22:48:20] iter = 17980, loss = 1.5077
[2023-10-01 22:48:21] iter = 17990, loss = 1.4556
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 2624}
[2023-10-01 22:48:46] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.011655 train acc = 1.0000, test acc = 0.6233
[2023-10-01 22:49:11] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.002540 train acc = 1.0000, test acc = 0.6277
[2023-10-01 22:49:35] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.016011 train acc = 1.0000, test acc = 0.6320
[2023-10-01 22:49:59] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.014089 train acc = 1.0000, test acc = 0.6288
[2023-10-01 22:50:23] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003523 train acc = 1.0000, test acc = 0.6324
[2023-10-01 22:50:47] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004927 train acc = 1.0000, test acc = 0.6192
[2023-10-01 22:51:12] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002551 train acc = 1.0000, test acc = 0.6227
[2023-10-01 22:51:36] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012907 train acc = 0.9980, test acc = 0.6313
[2023-10-01 22:52:01] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003611 train acc = 1.0000, test acc = 0.6287
[2023-10-01 22:52:25] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003251 train acc = 1.0000, test acc = 0.6226
[2023-10-01 22:52:49] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.018970 train acc = 0.9980, test acc = 0.6263
[2023-10-01 22:53:13] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.017910 train acc = 0.9980, test acc = 0.6273
[2023-10-01 22:53:37] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.009799 train acc = 1.0000, test acc = 0.6332
[2023-10-01 22:54:02] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.016987 train acc = 0.9980, test acc = 0.6389
[2023-10-01 22:54:26] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013755 train acc = 1.0000, test acc = 0.6274
[2023-10-01 22:54:50] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.014816 train acc = 0.9980, test acc = 0.6314
[2023-10-01 22:55:15] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002501 train acc = 1.0000, test acc = 0.6309
[2023-10-01 22:55:39] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.002379 train acc = 1.0000, test acc = 0.6293
[2023-10-01 22:56:03] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.024100 train acc = 1.0000, test acc = 0.6301
[2023-10-01 22:56:27] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.005148 train acc = 1.0000, test acc = 0.6216
Evaluate 20 random ConvNet, mean = 0.6283 std = 0.0046
-------------------------
[2023-10-01 22:56:27] iter = 18000, loss = 1.3012
[2023-10-01 22:56:28] iter = 18010, loss = 1.4622
[2023-10-01 22:56:29] iter = 18020, loss = 1.5581
[2023-10-01 22:56:30] iter = 18030, loss = 1.5126
[2023-10-01 22:56:31] iter = 18040, loss = 1.4314
[2023-10-01 22:56:32] iter = 18050, loss = 1.5264
[2023-10-01 22:56:33] iter = 18060, loss = 1.5057
[2023-10-01 22:56:34] iter = 18070, loss = 1.5549
[2023-10-01 22:56:35] iter = 18080, loss = 1.4782
[2023-10-01 22:56:36] iter = 18090, loss = 1.5770
[2023-10-01 22:56:37] iter = 18100, loss = 1.4503
[2023-10-01 22:56:37] iter = 18110, loss = 1.6207
[2023-10-01 22:56:38] iter = 18120, loss = 1.4506
[2023-10-01 22:56:39] iter = 18130, loss = 1.6469
[2023-10-01 22:56:40] iter = 18140, loss = 1.3865
[2023-10-01 22:56:41] iter = 18150, loss = 1.4532
[2023-10-01 22:56:42] iter = 18160, loss = 1.5563
[2023-10-01 22:56:43] iter = 18170, loss = 1.4443
[2023-10-01 22:56:44] iter = 18180, loss = 1.5742
[2023-10-01 22:56:45] iter = 18190, loss = 1.5738
[2023-10-01 22:56:46] iter = 18200, loss = 1.5139
[2023-10-01 22:56:47] iter = 18210, loss = 1.6114
[2023-10-01 22:56:47] iter = 18220, loss = 1.5166
[2023-10-01 22:56:48] iter = 18230, loss = 1.5035
[2023-10-01 22:56:49] iter = 18240, loss = 1.5941
[2023-10-01 22:56:50] iter = 18250, loss = 1.4985
[2023-10-01 22:56:51] iter = 18260, loss = 1.4634
[2023-10-01 22:56:52] iter = 18270, loss = 1.3636
[2023-10-01 22:56:53] iter = 18280, loss = 1.4550
[2023-10-01 22:56:54] iter = 18290, loss = 1.4381
[2023-10-01 22:56:55] iter = 18300, loss = 1.5910
[2023-10-01 22:56:55] iter = 18310, loss = 1.5536
[2023-10-01 22:56:56] iter = 18320, loss = 1.5408
[2023-10-01 22:56:57] iter = 18330, loss = 1.5298
[2023-10-01 22:56:58] iter = 18340, loss = 1.5075
[2023-10-01 22:56:59] iter = 18350, loss = 1.6922
[2023-10-01 22:57:00] iter = 18360, loss = 1.4776
[2023-10-01 22:57:01] iter = 18370, loss = 1.5605
[2023-10-01 22:57:02] iter = 18380, loss = 1.4665
[2023-10-01 22:57:03] iter = 18390, loss = 1.6451
[2023-10-01 22:57:04] iter = 18400, loss = 1.5456
[2023-10-01 22:57:05] iter = 18410, loss = 1.6395
[2023-10-01 22:57:05] iter = 18420, loss = 1.3931
[2023-10-01 22:57:06] iter = 18430, loss = 1.4643
[2023-10-01 22:57:07] iter = 18440, loss = 1.4291
[2023-10-01 22:57:08] iter = 18450, loss = 1.5264
[2023-10-01 22:57:09] iter = 18460, loss = 1.5243
[2023-10-01 22:57:10] iter = 18470, loss = 1.4649
[2023-10-01 22:57:11] iter = 18480, loss = 1.4110
[2023-10-01 22:57:12] iter = 18490, loss = 1.4158
[2023-10-01 22:57:13] iter = 18500, loss = 1.5563
[2023-10-01 22:57:14] iter = 18510, loss = 1.3675
[2023-10-01 22:57:14] iter = 18520, loss = 1.3429
[2023-10-01 22:57:15] iter = 18530, loss = 1.3611
[2023-10-01 22:57:16] iter = 18540, loss = 1.6328
[2023-10-01 22:57:17] iter = 18550, loss = 1.6399
[2023-10-01 22:57:18] iter = 18560, loss = 1.3960
[2023-10-01 22:57:19] iter = 18570, loss = 1.4745
[2023-10-01 22:57:20] iter = 18580, loss = 1.4675
[2023-10-01 22:57:21] iter = 18590, loss = 1.4535
[2023-10-01 22:57:22] iter = 18600, loss = 1.4076
[2023-10-01 22:57:23] iter = 18610, loss = 1.5419
[2023-10-01 22:57:23] iter = 18620, loss = 1.4385
[2023-10-01 22:57:24] iter = 18630, loss = 1.5129
[2023-10-01 22:57:25] iter = 18640, loss = 1.6923
[2023-10-01 22:57:26] iter = 18650, loss = 1.6038
[2023-10-01 22:57:27] iter = 18660, loss = 1.4453
[2023-10-01 22:57:28] iter = 18670, loss = 1.3467
[2023-10-01 22:57:29] iter = 18680, loss = 1.5736
[2023-10-01 22:57:30] iter = 18690, loss = 1.5290
[2023-10-01 22:57:31] iter = 18700, loss = 1.5723
[2023-10-01 22:57:32] iter = 18710, loss = 1.5001
[2023-10-01 22:57:33] iter = 18720, loss = 1.4378
[2023-10-01 22:57:34] iter = 18730, loss = 1.3914
[2023-10-01 22:57:35] iter = 18740, loss = 1.5198
[2023-10-01 22:57:35] iter = 18750, loss = 1.5087
[2023-10-01 22:57:36] iter = 18760, loss = 1.4625
[2023-10-01 22:57:37] iter = 18770, loss = 1.5605
[2023-10-01 22:57:38] iter = 18780, loss = 1.4893
[2023-10-01 22:57:39] iter = 18790, loss = 1.5309
[2023-10-01 22:57:40] iter = 18800, loss = 1.4296
[2023-10-01 22:57:41] iter = 18810, loss = 1.4415
[2023-10-01 22:57:42] iter = 18820, loss = 1.4272
[2023-10-01 22:57:43] iter = 18830, loss = 1.4005
[2023-10-01 22:57:44] iter = 18840, loss = 1.4413
[2023-10-01 22:57:45] iter = 18850, loss = 1.4333
[2023-10-01 22:57:46] iter = 18860, loss = 1.5184
[2023-10-01 22:57:46] iter = 18870, loss = 1.5835
[2023-10-01 22:57:47] iter = 18880, loss = 1.6575
[2023-10-01 22:57:48] iter = 18890, loss = 1.5000
[2023-10-01 22:57:49] iter = 18900, loss = 1.4773
[2023-10-01 22:57:50] iter = 18910, loss = 1.4166
[2023-10-01 22:57:51] iter = 18920, loss = 1.5463
[2023-10-01 22:57:52] iter = 18930, loss = 1.4911
[2023-10-01 22:57:53] iter = 18940, loss = 1.3843
[2023-10-01 22:57:54] iter = 18950, loss = 1.4153
[2023-10-01 22:57:55] iter = 18960, loss = 1.3894
[2023-10-01 22:57:56] iter = 18970, loss = 1.4786
[2023-10-01 22:57:57] iter = 18980, loss = 1.3599
[2023-10-01 22:57:57] iter = 18990, loss = 1.4154
[2023-10-01 22:57:58] iter = 19000, loss = 1.4958
[2023-10-01 22:57:59] iter = 19010, loss = 1.5756
[2023-10-01 22:58:00] iter = 19020, loss = 1.4957
[2023-10-01 22:58:01] iter = 19030, loss = 1.5552
[2023-10-01 22:58:02] iter = 19040, loss = 1.3842
[2023-10-01 22:58:03] iter = 19050, loss = 1.4872
[2023-10-01 22:58:04] iter = 19060, loss = 1.3767
[2023-10-01 22:58:05] iter = 19070, loss = 1.4703
[2023-10-01 22:58:06] iter = 19080, loss = 1.4356
[2023-10-01 22:58:07] iter = 19090, loss = 1.5625
[2023-10-01 22:58:07] iter = 19100, loss = 1.4476
[2023-10-01 22:58:08] iter = 19110, loss = 1.5032
[2023-10-01 22:58:09] iter = 19120, loss = 1.4984
[2023-10-01 22:58:10] iter = 19130, loss = 1.5287
[2023-10-01 22:58:11] iter = 19140, loss = 1.4849
[2023-10-01 22:58:12] iter = 19150, loss = 1.4834
[2023-10-01 22:58:13] iter = 19160, loss = 1.4786
[2023-10-01 22:58:14] iter = 19170, loss = 1.4691
[2023-10-01 22:58:15] iter = 19180, loss = 1.5585
[2023-10-01 22:58:16] iter = 19190, loss = 1.4898
[2023-10-01 22:58:17] iter = 19200, loss = 1.5972
[2023-10-01 22:58:17] iter = 19210, loss = 1.4434
[2023-10-01 22:58:18] iter = 19220, loss = 1.4985
[2023-10-01 22:58:19] iter = 19230, loss = 1.5638
[2023-10-01 22:58:20] iter = 19240, loss = 1.3881
[2023-10-01 22:58:21] iter = 19250, loss = 1.4660
[2023-10-01 22:58:22] iter = 19260, loss = 1.3969
[2023-10-01 22:58:23] iter = 19270, loss = 1.5229
[2023-10-01 22:58:24] iter = 19280, loss = 1.5118
[2023-10-01 22:58:25] iter = 19290, loss = 1.4404
[2023-10-01 22:58:25] iter = 19300, loss = 1.3490
[2023-10-01 22:58:26] iter = 19310, loss = 1.5264
[2023-10-01 22:58:27] iter = 19320, loss = 1.4041
[2023-10-01 22:58:28] iter = 19330, loss = 1.5219
[2023-10-01 22:58:29] iter = 19340, loss = 1.4009
[2023-10-01 22:58:30] iter = 19350, loss = 1.4915
[2023-10-01 22:58:31] iter = 19360, loss = 1.3722
[2023-10-01 22:58:32] iter = 19370, loss = 1.4415
[2023-10-01 22:58:33] iter = 19380, loss = 1.6446
[2023-10-01 22:58:34] iter = 19390, loss = 1.6158
[2023-10-01 22:58:35] iter = 19400, loss = 1.5223
[2023-10-01 22:58:36] iter = 19410, loss = 1.6305
[2023-10-01 22:58:37] iter = 19420, loss = 1.5170
[2023-10-01 22:58:38] iter = 19430, loss = 1.6127
[2023-10-01 22:58:39] iter = 19440, loss = 1.3404
[2023-10-01 22:58:39] iter = 19450, loss = 1.5301
[2023-10-01 22:58:40] iter = 19460, loss = 1.4910
[2023-10-01 22:58:41] iter = 19470, loss = 1.4967
[2023-10-01 22:58:42] iter = 19480, loss = 1.4252
[2023-10-01 22:58:43] iter = 19490, loss = 1.6163
[2023-10-01 22:58:44] iter = 19500, loss = 1.5649
[2023-10-01 22:58:45] iter = 19510, loss = 1.4528
[2023-10-01 22:58:46] iter = 19520, loss = 1.6757
[2023-10-01 22:58:47] iter = 19530, loss = 1.4439
[2023-10-01 22:58:48] iter = 19540, loss = 1.5851
[2023-10-01 22:58:49] iter = 19550, loss = 1.5894
[2023-10-01 22:58:50] iter = 19560, loss = 1.4500
[2023-10-01 22:58:51] iter = 19570, loss = 1.3915
[2023-10-01 22:58:51] iter = 19580, loss = 1.6321
[2023-10-01 22:58:52] iter = 19590, loss = 1.3936
[2023-10-01 22:58:53] iter = 19600, loss = 1.6839
[2023-10-01 22:58:54] iter = 19610, loss = 1.4686
[2023-10-01 22:58:55] iter = 19620, loss = 1.6464
[2023-10-01 22:58:56] iter = 19630, loss = 1.5725
[2023-10-01 22:58:57] iter = 19640, loss = 1.5052
[2023-10-01 22:58:58] iter = 19650, loss = 1.3901
[2023-10-01 22:58:59] iter = 19660, loss = 1.3911
[2023-10-01 22:59:00] iter = 19670, loss = 1.4646
[2023-10-01 22:59:01] iter = 19680, loss = 1.5554
[2023-10-01 22:59:01] iter = 19690, loss = 1.3784
[2023-10-01 22:59:02] iter = 19700, loss = 1.5645
[2023-10-01 22:59:03] iter = 19710, loss = 1.3882
[2023-10-01 22:59:04] iter = 19720, loss = 1.3356
[2023-10-01 22:59:05] iter = 19730, loss = 1.5033
[2023-10-01 22:59:06] iter = 19740, loss = 1.4106
[2023-10-01 22:59:07] iter = 19750, loss = 1.4002
[2023-10-01 22:59:08] iter = 19760, loss = 1.3948
[2023-10-01 22:59:09] iter = 19770, loss = 1.6431
[2023-10-01 22:59:10] iter = 19780, loss = 1.5388
[2023-10-01 22:59:11] iter = 19790, loss = 1.4433
[2023-10-01 22:59:12] iter = 19800, loss = 1.3656
[2023-10-01 22:59:12] iter = 19810, loss = 1.3727
[2023-10-01 22:59:13] iter = 19820, loss = 1.4375
[2023-10-01 22:59:14] iter = 19830, loss = 1.3730
[2023-10-01 22:59:15] iter = 19840, loss = 1.5732
[2023-10-01 22:59:16] iter = 19850, loss = 1.5207
[2023-10-01 22:59:17] iter = 19860, loss = 1.3982
[2023-10-01 22:59:18] iter = 19870, loss = 1.4872
[2023-10-01 22:59:19] iter = 19880, loss = 1.4840
[2023-10-01 22:59:20] iter = 19890, loss = 1.3740
[2023-10-01 22:59:21] iter = 19900, loss = 1.5998
[2023-10-01 22:59:22] iter = 19910, loss = 1.4395
[2023-10-01 22:59:22] iter = 19920, loss = 1.5004
[2023-10-01 22:59:23] iter = 19930, loss = 1.5552
[2023-10-01 22:59:24] iter = 19940, loss = 1.4945
[2023-10-01 22:59:25] iter = 19950, loss = 1.4433
[2023-10-01 22:59:26] iter = 19960, loss = 1.3943
[2023-10-01 22:59:27] iter = 19970, loss = 1.5855
[2023-10-01 22:59:28] iter = 19980, loss = 1.5853
[2023-10-01 22:59:29] iter = 19990, loss = 1.4343
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 70214}
[2023-10-01 22:59:54] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002356 train acc = 1.0000, test acc = 0.6283
[2023-10-01 23:00:18] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004408 train acc = 1.0000, test acc = 0.6236
[2023-10-01 23:00:43] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002795 train acc = 1.0000, test acc = 0.6239
[2023-10-01 23:01:07] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002619 train acc = 1.0000, test acc = 0.6337
[2023-10-01 23:01:31] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.013880 train acc = 1.0000, test acc = 0.6240
[2023-10-01 23:01:56] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.015235 train acc = 1.0000, test acc = 0.6308
[2023-10-01 23:02:20] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.010441 train acc = 1.0000, test acc = 0.6234
[2023-10-01 23:02:45] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.014641 train acc = 1.0000, test acc = 0.6265
[2023-10-01 23:03:09] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002578 train acc = 1.0000, test acc = 0.6280
[2023-10-01 23:03:33] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.020731 train acc = 1.0000, test acc = 0.6332
[2023-10-01 23:03:57] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003491 train acc = 1.0000, test acc = 0.6372
[2023-10-01 23:04:21] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003164 train acc = 1.0000, test acc = 0.6267
[2023-10-01 23:04:46] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.008138 train acc = 0.9980, test acc = 0.6341
[2023-10-01 23:05:10] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.006446 train acc = 1.0000, test acc = 0.6328
[2023-10-01 23:05:34] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.017129 train acc = 0.9980, test acc = 0.6238
[2023-10-01 23:05:58] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.009784 train acc = 1.0000, test acc = 0.6283
[2023-10-01 23:06:23] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.020945 train acc = 0.9980, test acc = 0.6283
[2023-10-01 23:06:47] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.026649 train acc = 1.0000, test acc = 0.6312
[2023-10-01 23:07:11] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003845 train acc = 1.0000, test acc = 0.6293
[2023-10-01 23:07:35] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.002341 train acc = 1.0000, test acc = 0.6252
Evaluate 20 random ConvNet, mean = 0.6286 std = 0.0040
-------------------------
[2023-10-01 23:07:36] iter = 20000, loss = 1.4600

================== Exp 1 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f7bf1f7b400>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-01 23:07:53] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 56187}
[2023-10-01 23:08:17] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.018462 train acc = 1.0000, test acc = 0.4918
[2023-10-01 23:08:42] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003003 train acc = 1.0000, test acc = 0.5005
[2023-10-01 23:09:06] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.000839 train acc = 1.0000, test acc = 0.4997
[2023-10-01 23:09:30] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.012759 train acc = 1.0000, test acc = 0.5009
[2023-10-01 23:09:55] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011094 train acc = 1.0000, test acc = 0.4978
[2023-10-01 23:10:19] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.007232 train acc = 1.0000, test acc = 0.4989
[2023-10-01 23:10:43] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.015511 train acc = 1.0000, test acc = 0.4892
[2023-10-01 23:11:08] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.013556 train acc = 1.0000, test acc = 0.4958
[2023-10-01 23:11:32] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.007745 train acc = 1.0000, test acc = 0.4972
[2023-10-01 23:11:56] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.008840 train acc = 1.0000, test acc = 0.4988
[2023-10-01 23:12:20] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013218 train acc = 0.9980, test acc = 0.4861
[2023-10-01 23:12:45] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002697 train acc = 1.0000, test acc = 0.4947
[2023-10-01 23:13:10] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.012812 train acc = 1.0000, test acc = 0.4961
[2023-10-01 23:13:34] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.013467 train acc = 1.0000, test acc = 0.4966
[2023-10-01 23:13:58] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.009144 train acc = 1.0000, test acc = 0.4872
[2023-10-01 23:14:22] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.006148 train acc = 1.0000, test acc = 0.4986
[2023-10-01 23:14:47] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002632 train acc = 1.0000, test acc = 0.4946
[2023-10-01 23:15:11] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.000884 train acc = 1.0000, test acc = 0.4943
[2023-10-01 23:15:35] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.000883 train acc = 1.0000, test acc = 0.4855
[2023-10-01 23:15:59] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.000785 train acc = 1.0000, test acc = 0.4907
Evaluate 20 random ConvNet, mean = 0.4948 std = 0.0047
-------------------------
[2023-10-01 23:16:00] iter = 00000, loss = 6.2742
[2023-10-01 23:16:01] iter = 00010, loss = 5.1954
[2023-10-01 23:16:02] iter = 00020, loss = 5.4566
[2023-10-01 23:16:02] iter = 00030, loss = 4.8245
[2023-10-01 23:16:03] iter = 00040, loss = 4.2301
[2023-10-01 23:16:04] iter = 00050, loss = 4.2973
[2023-10-01 23:16:05] iter = 00060, loss = 4.4324
[2023-10-01 23:16:06] iter = 00070, loss = 4.0016
[2023-10-01 23:16:07] iter = 00080, loss = 3.3984
[2023-10-01 23:16:08] iter = 00090, loss = 3.5704
[2023-10-01 23:16:09] iter = 00100, loss = 3.4603
[2023-10-01 23:16:10] iter = 00110, loss = 3.3757
[2023-10-01 23:16:11] iter = 00120, loss = 3.3937
[2023-10-01 23:16:11] iter = 00130, loss = 3.2595
[2023-10-01 23:16:12] iter = 00140, loss = 3.1104
[2023-10-01 23:16:13] iter = 00150, loss = 3.1068
[2023-10-01 23:16:14] iter = 00160, loss = 3.2070
[2023-10-01 23:16:15] iter = 00170, loss = 3.0556
[2023-10-01 23:16:16] iter = 00180, loss = 3.0108
[2023-10-01 23:16:17] iter = 00190, loss = 3.1786
[2023-10-01 23:16:18] iter = 00200, loss = 2.5912
[2023-10-01 23:16:19] iter = 00210, loss = 2.7528
[2023-10-01 23:16:20] iter = 00220, loss = 2.7999
[2023-10-01 23:16:20] iter = 00230, loss = 2.7616
[2023-10-01 23:16:21] iter = 00240, loss = 2.6432
[2023-10-01 23:16:22] iter = 00250, loss = 3.3224
[2023-10-01 23:16:23] iter = 00260, loss = 2.6927
[2023-10-01 23:16:24] iter = 00270, loss = 2.6877
[2023-10-01 23:16:25] iter = 00280, loss = 2.6681
[2023-10-01 23:16:26] iter = 00290, loss = 2.7064
[2023-10-01 23:16:27] iter = 00300, loss = 2.6475
[2023-10-01 23:16:28] iter = 00310, loss = 2.4660
[2023-10-01 23:16:29] iter = 00320, loss = 2.9288
[2023-10-01 23:16:30] iter = 00330, loss = 2.5530
[2023-10-01 23:16:31] iter = 00340, loss = 2.6288
[2023-10-01 23:16:32] iter = 00350, loss = 2.4875
[2023-10-01 23:16:33] iter = 00360, loss = 2.3989
[2023-10-01 23:16:34] iter = 00370, loss = 2.6329
[2023-10-01 23:16:34] iter = 00380, loss = 2.4028
[2023-10-01 23:16:35] iter = 00390, loss = 2.4873
[2023-10-01 23:16:36] iter = 00400, loss = 2.7167
[2023-10-01 23:16:37] iter = 00410, loss = 2.3966
[2023-10-01 23:16:38] iter = 00420, loss = 2.3836
[2023-10-01 23:16:39] iter = 00430, loss = 2.4979
[2023-10-01 23:16:40] iter = 00440, loss = 2.3509
[2023-10-01 23:16:41] iter = 00450, loss = 2.3255
[2023-10-01 23:16:42] iter = 00460, loss = 2.4877
[2023-10-01 23:16:43] iter = 00470, loss = 2.2320
[2023-10-01 23:16:43] iter = 00480, loss = 2.4253
[2023-10-01 23:16:44] iter = 00490, loss = 2.6495
[2023-10-01 23:16:45] iter = 00500, loss = 2.4518
[2023-10-01 23:16:46] iter = 00510, loss = 2.7580
[2023-10-01 23:16:47] iter = 00520, loss = 2.2238
[2023-10-01 23:16:48] iter = 00530, loss = 2.5712
[2023-10-01 23:16:49] iter = 00540, loss = 2.1680
[2023-10-01 23:16:50] iter = 00550, loss = 2.4336
[2023-10-01 23:16:51] iter = 00560, loss = 2.1374
[2023-10-01 23:16:52] iter = 00570, loss = 2.2387
[2023-10-01 23:16:53] iter = 00580, loss = 2.4057
[2023-10-01 23:16:54] iter = 00590, loss = 2.1266
[2023-10-01 23:16:54] iter = 00600, loss = 2.1596
[2023-10-01 23:16:55] iter = 00610, loss = 2.2999
[2023-10-01 23:16:56] iter = 00620, loss = 2.2774
[2023-10-01 23:16:57] iter = 00630, loss = 2.3159
[2023-10-01 23:16:58] iter = 00640, loss = 2.1323
[2023-10-01 23:16:59] iter = 00650, loss = 2.2651
[2023-10-01 23:17:00] iter = 00660, loss = 2.3401
[2023-10-01 23:17:01] iter = 00670, loss = 2.2301
[2023-10-01 23:17:02] iter = 00680, loss = 2.1352
[2023-10-01 23:17:03] iter = 00690, loss = 2.1182
[2023-10-01 23:17:04] iter = 00700, loss = 2.2651
[2023-10-01 23:17:04] iter = 00710, loss = 2.1122
[2023-10-01 23:17:05] iter = 00720, loss = 2.2841
[2023-10-01 23:17:06] iter = 00730, loss = 2.1466
[2023-10-01 23:17:07] iter = 00740, loss = 2.0009
[2023-10-01 23:17:08] iter = 00750, loss = 2.1273
[2023-10-01 23:17:09] iter = 00760, loss = 2.0658
[2023-10-01 23:17:10] iter = 00770, loss = 1.9880
[2023-10-01 23:17:11] iter = 00780, loss = 2.2324
[2023-10-01 23:17:12] iter = 00790, loss = 2.4421
[2023-10-01 23:17:13] iter = 00800, loss = 2.2605
[2023-10-01 23:17:14] iter = 00810, loss = 2.0442
[2023-10-01 23:17:14] iter = 00820, loss = 2.1130
[2023-10-01 23:17:15] iter = 00830, loss = 2.0696
[2023-10-01 23:17:16] iter = 00840, loss = 2.0736
[2023-10-01 23:17:17] iter = 00850, loss = 2.1992
[2023-10-01 23:17:18] iter = 00860, loss = 2.0904
[2023-10-01 23:17:19] iter = 00870, loss = 2.0690
[2023-10-01 23:17:20] iter = 00880, loss = 2.2397
[2023-10-01 23:17:21] iter = 00890, loss = 2.1265
[2023-10-01 23:17:22] iter = 00900, loss = 2.0149
[2023-10-01 23:17:23] iter = 00910, loss = 1.9796
[2023-10-01 23:17:24] iter = 00920, loss = 2.1053
[2023-10-01 23:17:24] iter = 00930, loss = 2.1117
[2023-10-01 23:17:25] iter = 00940, loss = 1.9610
[2023-10-01 23:17:26] iter = 00950, loss = 2.0202
[2023-10-01 23:17:27] iter = 00960, loss = 2.2623
[2023-10-01 23:17:28] iter = 00970, loss = 1.8682
[2023-10-01 23:17:29] iter = 00980, loss = 1.9555
[2023-10-01 23:17:30] iter = 00990, loss = 2.1798
[2023-10-01 23:17:31] iter = 01000, loss = 2.1775
[2023-10-01 23:17:32] iter = 01010, loss = 2.4202
[2023-10-01 23:17:32] iter = 01020, loss = 2.1460
[2023-10-01 23:17:33] iter = 01030, loss = 2.1191
[2023-10-01 23:17:34] iter = 01040, loss = 2.0814
[2023-10-01 23:17:35] iter = 01050, loss = 2.2126
[2023-10-01 23:17:36] iter = 01060, loss = 1.9329
[2023-10-01 23:17:37] iter = 01070, loss = 2.0946
[2023-10-01 23:17:38] iter = 01080, loss = 2.0479
[2023-10-01 23:17:39] iter = 01090, loss = 1.9769
[2023-10-01 23:17:40] iter = 01100, loss = 2.1860
[2023-10-01 23:17:41] iter = 01110, loss = 2.0737
[2023-10-01 23:17:41] iter = 01120, loss = 1.9269
[2023-10-01 23:17:42] iter = 01130, loss = 2.0070
[2023-10-01 23:17:43] iter = 01140, loss = 1.7811
[2023-10-01 23:17:44] iter = 01150, loss = 2.0711
[2023-10-01 23:17:45] iter = 01160, loss = 1.9410
[2023-10-01 23:17:46] iter = 01170, loss = 1.9197
[2023-10-01 23:17:47] iter = 01180, loss = 1.9645
[2023-10-01 23:17:48] iter = 01190, loss = 1.9313
[2023-10-01 23:17:49] iter = 01200, loss = 2.0369
[2023-10-01 23:17:49] iter = 01210, loss = 2.0872
[2023-10-01 23:17:50] iter = 01220, loss = 1.9367
[2023-10-01 23:17:51] iter = 01230, loss = 2.0361
[2023-10-01 23:17:52] iter = 01240, loss = 1.8774
[2023-10-01 23:17:53] iter = 01250, loss = 1.8944
[2023-10-01 23:17:54] iter = 01260, loss = 1.9779
[2023-10-01 23:17:55] iter = 01270, loss = 1.8782
[2023-10-01 23:17:56] iter = 01280, loss = 1.9756
[2023-10-01 23:17:57] iter = 01290, loss = 1.9895
[2023-10-01 23:17:58] iter = 01300, loss = 2.1581
[2023-10-01 23:17:58] iter = 01310, loss = 1.8391
[2023-10-01 23:17:59] iter = 01320, loss = 1.9774
[2023-10-01 23:18:00] iter = 01330, loss = 2.1682
[2023-10-01 23:18:01] iter = 01340, loss = 2.1010
[2023-10-01 23:18:02] iter = 01350, loss = 2.0314
[2023-10-01 23:18:03] iter = 01360, loss = 1.9277
[2023-10-01 23:18:04] iter = 01370, loss = 1.9571
[2023-10-01 23:18:05] iter = 01380, loss = 1.7763
[2023-10-01 23:18:06] iter = 01390, loss = 1.8038
[2023-10-01 23:18:07] iter = 01400, loss = 1.8704
[2023-10-01 23:18:08] iter = 01410, loss = 2.0145
[2023-10-01 23:18:09] iter = 01420, loss = 1.9499
[2023-10-01 23:18:10] iter = 01430, loss = 1.9180
[2023-10-01 23:18:11] iter = 01440, loss = 1.8212
[2023-10-01 23:18:11] iter = 01450, loss = 1.9020
[2023-10-01 23:18:12] iter = 01460, loss = 2.0749
[2023-10-01 23:18:13] iter = 01470, loss = 2.0942
[2023-10-01 23:18:14] iter = 01480, loss = 1.8568
[2023-10-01 23:18:15] iter = 01490, loss = 1.9020
[2023-10-01 23:18:16] iter = 01500, loss = 2.1073
[2023-10-01 23:18:17] iter = 01510, loss = 1.9152
[2023-10-01 23:18:18] iter = 01520, loss = 1.8204
[2023-10-01 23:18:18] iter = 01530, loss = 1.8401
[2023-10-01 23:18:19] iter = 01540, loss = 1.8749
[2023-10-01 23:18:20] iter = 01550, loss = 1.9341
[2023-10-01 23:18:21] iter = 01560, loss = 1.8305
[2023-10-01 23:18:22] iter = 01570, loss = 1.9281
[2023-10-01 23:18:23] iter = 01580, loss = 1.8105
[2023-10-01 23:18:24] iter = 01590, loss = 2.0603
[2023-10-01 23:18:25] iter = 01600, loss = 1.9026
[2023-10-01 23:18:26] iter = 01610, loss = 1.9928
[2023-10-01 23:18:27] iter = 01620, loss = 1.9139
[2023-10-01 23:18:28] iter = 01630, loss = 1.8641
[2023-10-01 23:18:29] iter = 01640, loss = 1.7641
[2023-10-01 23:18:29] iter = 01650, loss = 1.8208
[2023-10-01 23:18:30] iter = 01660, loss = 1.8904
[2023-10-01 23:18:31] iter = 01670, loss = 1.8808
[2023-10-01 23:18:32] iter = 01680, loss = 1.9100
[2023-10-01 23:18:33] iter = 01690, loss = 1.9066
[2023-10-01 23:18:34] iter = 01700, loss = 1.8831
[2023-10-01 23:18:35] iter = 01710, loss = 1.9609
[2023-10-01 23:18:36] iter = 01720, loss = 2.0762
[2023-10-01 23:18:37] iter = 01730, loss = 1.8046
[2023-10-01 23:18:38] iter = 01740, loss = 2.0354
[2023-10-01 23:18:39] iter = 01750, loss = 1.9809
[2023-10-01 23:18:39] iter = 01760, loss = 1.8403
[2023-10-01 23:18:40] iter = 01770, loss = 1.8754
[2023-10-01 23:18:41] iter = 01780, loss = 1.8638
[2023-10-01 23:18:42] iter = 01790, loss = 1.7885
[2023-10-01 23:18:43] iter = 01800, loss = 1.8627
[2023-10-01 23:18:44] iter = 01810, loss = 1.8508
[2023-10-01 23:18:45] iter = 01820, loss = 1.9001
[2023-10-01 23:18:46] iter = 01830, loss = 1.8231
[2023-10-01 23:18:47] iter = 01840, loss = 1.7972
[2023-10-01 23:18:47] iter = 01850, loss = 2.0228
[2023-10-01 23:18:48] iter = 01860, loss = 1.8575
[2023-10-01 23:18:49] iter = 01870, loss = 1.8088
[2023-10-01 23:18:50] iter = 01880, loss = 1.9293
[2023-10-01 23:18:51] iter = 01890, loss = 1.8119
[2023-10-01 23:18:52] iter = 01900, loss = 1.8668
[2023-10-01 23:18:53] iter = 01910, loss = 1.8569
[2023-10-01 23:18:54] iter = 01920, loss = 1.9429
[2023-10-01 23:18:55] iter = 01930, loss = 1.8765
[2023-10-01 23:18:55] iter = 01940, loss = 1.9695
[2023-10-01 23:18:56] iter = 01950, loss = 2.1462
[2023-10-01 23:18:57] iter = 01960, loss = 1.8633
[2023-10-01 23:18:58] iter = 01970, loss = 1.8370
[2023-10-01 23:18:59] iter = 01980, loss = 1.9361
[2023-10-01 23:19:00] iter = 01990, loss = 1.8987
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 41160}
[2023-10-01 23:19:25] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.000856 train acc = 1.0000, test acc = 0.5802
[2023-10-01 23:19:49] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001237 train acc = 1.0000, test acc = 0.5852
[2023-10-01 23:20:13] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010088 train acc = 1.0000, test acc = 0.5837
[2023-10-01 23:20:38] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.011239 train acc = 1.0000, test acc = 0.5742
[2023-10-01 23:21:02] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003349 train acc = 1.0000, test acc = 0.5911
[2023-10-01 23:21:26] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.012389 train acc = 1.0000, test acc = 0.5824
[2023-10-01 23:21:50] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.006312 train acc = 1.0000, test acc = 0.5793
[2023-10-01 23:22:15] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003148 train acc = 1.0000, test acc = 0.5775
[2023-10-01 23:22:39] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002352 train acc = 1.0000, test acc = 0.5870
[2023-10-01 23:23:03] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002515 train acc = 1.0000, test acc = 0.5776
[2023-10-01 23:23:28] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.007700 train acc = 1.0000, test acc = 0.5836
[2023-10-01 23:23:52] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.010561 train acc = 1.0000, test acc = 0.5822
[2023-10-01 23:24:16] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.014501 train acc = 1.0000, test acc = 0.5847
[2023-10-01 23:24:41] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.011689 train acc = 0.9980, test acc = 0.5805
[2023-10-01 23:25:05] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001410 train acc = 1.0000, test acc = 0.5814
[2023-10-01 23:25:29] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001076 train acc = 1.0000, test acc = 0.5851
[2023-10-01 23:25:54] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.010691 train acc = 1.0000, test acc = 0.5797
[2023-10-01 23:26:18] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.008501 train acc = 1.0000, test acc = 0.5785
[2023-10-01 23:26:42] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001443 train acc = 1.0000, test acc = 0.5817
[2023-10-01 23:27:06] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.006854 train acc = 1.0000, test acc = 0.5796
Evaluate 20 random ConvNet, mean = 0.5818 std = 0.0037
-------------------------
[2023-10-01 23:27:07] iter = 02000, loss = 1.7704
[2023-10-01 23:27:08] iter = 02010, loss = 1.8184
[2023-10-01 23:27:09] iter = 02020, loss = 1.8269
[2023-10-01 23:27:10] iter = 02030, loss = 1.7692
[2023-10-01 23:27:10] iter = 02040, loss = 1.9344
[2023-10-01 23:27:11] iter = 02050, loss = 2.0267
[2023-10-01 23:27:12] iter = 02060, loss = 1.8305
[2023-10-01 23:27:13] iter = 02070, loss = 1.8820
[2023-10-01 23:27:14] iter = 02080, loss = 1.7986
[2023-10-01 23:27:15] iter = 02090, loss = 1.9182
[2023-10-01 23:27:16] iter = 02100, loss = 1.8181
[2023-10-01 23:27:17] iter = 02110, loss = 1.8802
[2023-10-01 23:27:18] iter = 02120, loss = 1.8938
[2023-10-01 23:27:19] iter = 02130, loss = 1.7381
[2023-10-01 23:27:20] iter = 02140, loss = 1.8757
[2023-10-01 23:27:20] iter = 02150, loss = 1.8166
[2023-10-01 23:27:21] iter = 02160, loss = 1.8477
[2023-10-01 23:27:22] iter = 02170, loss = 1.8723
[2023-10-01 23:27:23] iter = 02180, loss = 1.8962
[2023-10-01 23:27:24] iter = 02190, loss = 1.8925
[2023-10-01 23:27:25] iter = 02200, loss = 1.8565
[2023-10-01 23:27:26] iter = 02210, loss = 1.8295
[2023-10-01 23:27:27] iter = 02220, loss = 1.7561
[2023-10-01 23:27:28] iter = 02230, loss = 1.8380
[2023-10-01 23:27:29] iter = 02240, loss = 2.0420
[2023-10-01 23:27:30] iter = 02250, loss = 1.9259
[2023-10-01 23:27:30] iter = 02260, loss = 1.8250
[2023-10-01 23:27:31] iter = 02270, loss = 1.7739
[2023-10-01 23:27:32] iter = 02280, loss = 1.7498
[2023-10-01 23:27:33] iter = 02290, loss = 1.7995
[2023-10-01 23:27:34] iter = 02300, loss = 2.0346
[2023-10-01 23:27:35] iter = 02310, loss = 1.8193
[2023-10-01 23:27:36] iter = 02320, loss = 1.7211
[2023-10-01 23:27:37] iter = 02330, loss = 1.7337
[2023-10-01 23:27:38] iter = 02340, loss = 1.7539
[2023-10-01 23:27:39] iter = 02350, loss = 1.8367
[2023-10-01 23:27:40] iter = 02360, loss = 1.7205
[2023-10-01 23:27:40] iter = 02370, loss = 1.6974
[2023-10-01 23:27:41] iter = 02380, loss = 1.8158
[2023-10-01 23:27:42] iter = 02390, loss = 1.8877
[2023-10-01 23:27:43] iter = 02400, loss = 1.7258
[2023-10-01 23:27:44] iter = 02410, loss = 1.8794
[2023-10-01 23:27:45] iter = 02420, loss = 1.8622
[2023-10-01 23:27:46] iter = 02430, loss = 1.8139
[2023-10-01 23:27:47] iter = 02440, loss = 1.8416
[2023-10-01 23:27:48] iter = 02450, loss = 1.7507
[2023-10-01 23:27:49] iter = 02460, loss = 1.7922
[2023-10-01 23:27:50] iter = 02470, loss = 1.7326
[2023-10-01 23:27:51] iter = 02480, loss = 1.6518
[2023-10-01 23:27:51] iter = 02490, loss = 1.8334
[2023-10-01 23:27:52] iter = 02500, loss = 1.8449
[2023-10-01 23:27:53] iter = 02510, loss = 1.6783
[2023-10-01 23:27:54] iter = 02520, loss = 1.7610
[2023-10-01 23:27:55] iter = 02530, loss = 1.9110
[2023-10-01 23:27:56] iter = 02540, loss = 1.8973
[2023-10-01 23:27:57] iter = 02550, loss = 1.8175
[2023-10-01 23:27:58] iter = 02560, loss = 1.9273
[2023-10-01 23:27:59] iter = 02570, loss = 1.8778
[2023-10-01 23:28:00] iter = 02580, loss = 1.8008
[2023-10-01 23:28:01] iter = 02590, loss = 1.8750
[2023-10-01 23:28:01] iter = 02600, loss = 1.8481
[2023-10-01 23:28:02] iter = 02610, loss = 1.7809
[2023-10-01 23:28:03] iter = 02620, loss = 1.6691
[2023-10-01 23:28:04] iter = 02630, loss = 1.8462
[2023-10-01 23:28:05] iter = 02640, loss = 1.8096
[2023-10-01 23:28:06] iter = 02650, loss = 1.9239
[2023-10-01 23:28:07] iter = 02660, loss = 1.8175
[2023-10-01 23:28:08] iter = 02670, loss = 1.7406
[2023-10-01 23:28:09] iter = 02680, loss = 1.7642
[2023-10-01 23:28:10] iter = 02690, loss = 1.7988
[2023-10-01 23:28:11] iter = 02700, loss = 1.7849
[2023-10-01 23:28:11] iter = 02710, loss = 1.8037
[2023-10-01 23:28:12] iter = 02720, loss = 1.7946
[2023-10-01 23:28:13] iter = 02730, loss = 1.7632
[2023-10-01 23:28:14] iter = 02740, loss = 1.7729
[2023-10-01 23:28:15] iter = 02750, loss = 1.7735
[2023-10-01 23:28:16] iter = 02760, loss = 1.7841
[2023-10-01 23:28:17] iter = 02770, loss = 1.9074
[2023-10-01 23:28:18] iter = 02780, loss = 1.6454
[2023-10-01 23:28:19] iter = 02790, loss = 1.6762
[2023-10-01 23:28:20] iter = 02800, loss = 1.8328
[2023-10-01 23:28:21] iter = 02810, loss = 1.7304
[2023-10-01 23:28:22] iter = 02820, loss = 1.6792
[2023-10-01 23:28:23] iter = 02830, loss = 1.7965
[2023-10-01 23:28:23] iter = 02840, loss = 1.8840
[2023-10-01 23:28:24] iter = 02850, loss = 1.7836
[2023-10-01 23:28:25] iter = 02860, loss = 1.7030
[2023-10-01 23:28:26] iter = 02870, loss = 1.9801
[2023-10-01 23:28:27] iter = 02880, loss = 1.7996
[2023-10-01 23:28:28] iter = 02890, loss = 1.8633
[2023-10-01 23:28:29] iter = 02900, loss = 1.7964
[2023-10-01 23:28:30] iter = 02910, loss = 1.7307
[2023-10-01 23:28:31] iter = 02920, loss = 1.8079
[2023-10-01 23:28:32] iter = 02930, loss = 1.7558
[2023-10-01 23:28:33] iter = 02940, loss = 1.6413
[2023-10-01 23:28:34] iter = 02950, loss = 1.8746
[2023-10-01 23:28:34] iter = 02960, loss = 1.6696
[2023-10-01 23:28:36] iter = 02970, loss = 1.7977
[2023-10-01 23:28:36] iter = 02980, loss = 2.0338
[2023-10-01 23:28:37] iter = 02990, loss = 1.8962
[2023-10-01 23:28:38] iter = 03000, loss = 1.8210
[2023-10-01 23:28:39] iter = 03010, loss = 1.8643
[2023-10-01 23:28:40] iter = 03020, loss = 1.7953
[2023-10-01 23:28:41] iter = 03030, loss = 1.6945
[2023-10-01 23:28:42] iter = 03040, loss = 1.7693
[2023-10-01 23:28:43] iter = 03050, loss = 1.6801
[2023-10-01 23:28:44] iter = 03060, loss = 1.7999
[2023-10-01 23:28:45] iter = 03070, loss = 1.8051
[2023-10-01 23:28:45] iter = 03080, loss = 1.7768
[2023-10-01 23:28:46] iter = 03090, loss = 1.8463
[2023-10-01 23:28:47] iter = 03100, loss = 1.6962
[2023-10-01 23:28:48] iter = 03110, loss = 1.6785
[2023-10-01 23:28:49] iter = 03120, loss = 1.7454
[2023-10-01 23:28:50] iter = 03130, loss = 1.6414
[2023-10-01 23:28:51] iter = 03140, loss = 1.7859
[2023-10-01 23:28:52] iter = 03150, loss = 1.7390
[2023-10-01 23:28:53] iter = 03160, loss = 1.8599
[2023-10-01 23:28:54] iter = 03170, loss = 1.8774
[2023-10-01 23:28:55] iter = 03180, loss = 1.8072
[2023-10-01 23:28:55] iter = 03190, loss = 1.7558
[2023-10-01 23:28:57] iter = 03200, loss = 1.7246
[2023-10-01 23:28:57] iter = 03210, loss = 1.8225
[2023-10-01 23:28:58] iter = 03220, loss = 1.7717
[2023-10-01 23:28:59] iter = 03230, loss = 1.6413
[2023-10-01 23:29:00] iter = 03240, loss = 1.8931
[2023-10-01 23:29:01] iter = 03250, loss = 1.8708
[2023-10-01 23:29:02] iter = 03260, loss = 1.7173
[2023-10-01 23:29:03] iter = 03270, loss = 1.7483
[2023-10-01 23:29:04] iter = 03280, loss = 1.8066
[2023-10-01 23:29:04] iter = 03290, loss = 1.8666
[2023-10-01 23:29:05] iter = 03300, loss = 1.6713
[2023-10-01 23:29:06] iter = 03310, loss = 1.7909
[2023-10-01 23:29:07] iter = 03320, loss = 1.7095
[2023-10-01 23:29:08] iter = 03330, loss = 1.6080
[2023-10-01 23:29:09] iter = 03340, loss = 1.8549
[2023-10-01 23:29:10] iter = 03350, loss = 1.7717
[2023-10-01 23:29:11] iter = 03360, loss = 1.6794
[2023-10-01 23:29:12] iter = 03370, loss = 1.7433
[2023-10-01 23:29:13] iter = 03380, loss = 1.6194
[2023-10-01 23:29:13] iter = 03390, loss = 1.7433
[2023-10-01 23:29:14] iter = 03400, loss = 1.8064
[2023-10-01 23:29:15] iter = 03410, loss = 1.6662
[2023-10-01 23:29:16] iter = 03420, loss = 1.7108
[2023-10-01 23:29:17] iter = 03430, loss = 1.8249
[2023-10-01 23:29:18] iter = 03440, loss = 1.7097
[2023-10-01 23:29:19] iter = 03450, loss = 1.7751
[2023-10-01 23:29:20] iter = 03460, loss = 1.9482
[2023-10-01 23:29:20] iter = 03470, loss = 1.7785
[2023-10-01 23:29:21] iter = 03480, loss = 1.7075
[2023-10-01 23:29:22] iter = 03490, loss = 1.8528
[2023-10-01 23:29:23] iter = 03500, loss = 1.7478
[2023-10-01 23:29:24] iter = 03510, loss = 1.6224
[2023-10-01 23:29:25] iter = 03520, loss = 1.6501
[2023-10-01 23:29:26] iter = 03530, loss = 1.7390
[2023-10-01 23:29:27] iter = 03540, loss = 1.6701
[2023-10-01 23:29:28] iter = 03550, loss = 1.6817
[2023-10-01 23:29:29] iter = 03560, loss = 1.6903
[2023-10-01 23:29:30] iter = 03570, loss = 1.7465
[2023-10-01 23:29:31] iter = 03580, loss = 1.7836
[2023-10-01 23:29:31] iter = 03590, loss = 1.7869
[2023-10-01 23:29:33] iter = 03600, loss = 1.7572
[2023-10-01 23:29:33] iter = 03610, loss = 1.6193
[2023-10-01 23:29:34] iter = 03620, loss = 1.6968
[2023-10-01 23:29:35] iter = 03630, loss = 1.6351
[2023-10-01 23:29:36] iter = 03640, loss = 1.6431
[2023-10-01 23:29:37] iter = 03650, loss = 1.8024
[2023-10-01 23:29:38] iter = 03660, loss = 1.7517
[2023-10-01 23:29:39] iter = 03670, loss = 1.7987
[2023-10-01 23:29:40] iter = 03680, loss = 1.7145
[2023-10-01 23:29:41] iter = 03690, loss = 1.7196
[2023-10-01 23:29:42] iter = 03700, loss = 1.8451
[2023-10-01 23:29:43] iter = 03710, loss = 1.7596
[2023-10-01 23:29:44] iter = 03720, loss = 1.7117
[2023-10-01 23:29:45] iter = 03730, loss = 1.6693
[2023-10-01 23:29:46] iter = 03740, loss = 1.7813
[2023-10-01 23:29:47] iter = 03750, loss = 1.6218
[2023-10-01 23:29:48] iter = 03760, loss = 1.7173
[2023-10-01 23:29:48] iter = 03770, loss = 1.5753
[2023-10-01 23:29:49] iter = 03780, loss = 1.5884
[2023-10-01 23:29:50] iter = 03790, loss = 1.5851
[2023-10-01 23:29:51] iter = 03800, loss = 1.7189
[2023-10-01 23:29:52] iter = 03810, loss = 1.7181
[2023-10-01 23:29:53] iter = 03820, loss = 1.6007
[2023-10-01 23:29:54] iter = 03830, loss = 1.7235
[2023-10-01 23:29:55] iter = 03840, loss = 1.9013
[2023-10-01 23:29:56] iter = 03850, loss = 1.7404
[2023-10-01 23:29:56] iter = 03860, loss = 1.6552
[2023-10-01 23:29:57] iter = 03870, loss = 1.7523
[2023-10-01 23:29:58] iter = 03880, loss = 1.7753
[2023-10-01 23:29:59] iter = 03890, loss = 1.7345
[2023-10-01 23:30:00] iter = 03900, loss = 1.8035
[2023-10-01 23:30:01] iter = 03910, loss = 1.6130
[2023-10-01 23:30:02] iter = 03920, loss = 1.5232
[2023-10-01 23:30:03] iter = 03930, loss = 1.5291
[2023-10-01 23:30:04] iter = 03940, loss = 1.7115
[2023-10-01 23:30:05] iter = 03950, loss = 1.6423
[2023-10-01 23:30:06] iter = 03960, loss = 1.8012
[2023-10-01 23:30:07] iter = 03970, loss = 1.6818
[2023-10-01 23:30:07] iter = 03980, loss = 1.7997
[2023-10-01 23:30:08] iter = 03990, loss = 1.8791
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 9769}
[2023-10-01 23:30:33] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.016712 train acc = 1.0000, test acc = 0.5971
[2023-10-01 23:30:58] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010781 train acc = 0.9980, test acc = 0.6070
[2023-10-01 23:31:22] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010151 train acc = 0.9980, test acc = 0.5976
[2023-10-01 23:31:46] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.012411 train acc = 1.0000, test acc = 0.5958
[2023-10-01 23:32:11] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.020693 train acc = 0.9980, test acc = 0.5937
[2023-10-01 23:32:35] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001324 train acc = 1.0000, test acc = 0.6020
[2023-10-01 23:32:59] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004868 train acc = 0.9980, test acc = 0.6039
[2023-10-01 23:33:24] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003346 train acc = 1.0000, test acc = 0.5974
[2023-10-01 23:33:48] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003238 train acc = 1.0000, test acc = 0.5947
[2023-10-01 23:34:13] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.004203 train acc = 1.0000, test acc = 0.5960
[2023-10-01 23:34:37] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.001408 train acc = 1.0000, test acc = 0.5961
[2023-10-01 23:35:01] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001482 train acc = 1.0000, test acc = 0.5984
[2023-10-01 23:35:26] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001700 train acc = 1.0000, test acc = 0.6035
[2023-10-01 23:35:50] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.001783 train acc = 1.0000, test acc = 0.6024
[2023-10-01 23:36:15] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.025806 train acc = 0.9960, test acc = 0.5893
[2023-10-01 23:36:39] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.012277 train acc = 1.0000, test acc = 0.6033
[2023-10-01 23:37:03] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.015759 train acc = 0.9980, test acc = 0.5969
[2023-10-01 23:37:28] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.015615 train acc = 1.0000, test acc = 0.5940
[2023-10-01 23:37:52] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.010136 train acc = 1.0000, test acc = 0.6035
[2023-10-01 23:38:17] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012311 train acc = 1.0000, test acc = 0.5995
Evaluate 20 random ConvNet, mean = 0.5986 std = 0.0043
-------------------------
[2023-10-01 23:38:17] iter = 04000, loss = 1.5148
[2023-10-01 23:38:18] iter = 04010, loss = 1.8197
[2023-10-01 23:38:18] iter = 04020, loss = 1.8460
[2023-10-01 23:38:19] iter = 04030, loss = 1.8711
[2023-10-01 23:38:20] iter = 04040, loss = 1.7597
[2023-10-01 23:38:21] iter = 04050, loss = 1.6670
[2023-10-01 23:38:22] iter = 04060, loss = 1.8041
[2023-10-01 23:38:23] iter = 04070, loss = 1.8124
[2023-10-01 23:38:24] iter = 04080, loss = 1.6210
[2023-10-01 23:38:25] iter = 04090, loss = 1.5848
[2023-10-01 23:38:26] iter = 04100, loss = 1.9096
[2023-10-01 23:38:27] iter = 04110, loss = 1.6605
[2023-10-01 23:38:27] iter = 04120, loss = 1.7099
[2023-10-01 23:38:28] iter = 04130, loss = 1.7089
[2023-10-01 23:38:29] iter = 04140, loss = 1.7098
[2023-10-01 23:38:30] iter = 04150, loss = 1.9104
[2023-10-01 23:38:31] iter = 04160, loss = 1.6627
[2023-10-01 23:38:32] iter = 04170, loss = 1.7029
[2023-10-01 23:38:33] iter = 04180, loss = 1.5911
[2023-10-01 23:38:34] iter = 04190, loss = 1.6279
[2023-10-01 23:38:35] iter = 04200, loss = 1.5800
[2023-10-01 23:38:36] iter = 04210, loss = 1.6818
[2023-10-01 23:38:37] iter = 04220, loss = 1.7144
[2023-10-01 23:38:38] iter = 04230, loss = 1.7807
[2023-10-01 23:38:39] iter = 04240, loss = 1.6406
[2023-10-01 23:38:39] iter = 04250, loss = 1.8112
[2023-10-01 23:38:40] iter = 04260, loss = 1.6353
[2023-10-01 23:38:41] iter = 04270, loss = 1.7247
[2023-10-01 23:38:42] iter = 04280, loss = 1.7420
[2023-10-01 23:38:43] iter = 04290, loss = 1.6712
[2023-10-01 23:38:44] iter = 04300, loss = 1.6530
[2023-10-01 23:38:45] iter = 04310, loss = 1.6950
[2023-10-01 23:38:46] iter = 04320, loss = 1.6390
[2023-10-01 23:38:47] iter = 04330, loss = 1.7681
[2023-10-01 23:38:48] iter = 04340, loss = 1.6082
[2023-10-01 23:38:49] iter = 04350, loss = 1.6276
[2023-10-01 23:38:50] iter = 04360, loss = 1.7952
[2023-10-01 23:38:50] iter = 04370, loss = 1.6467
[2023-10-01 23:38:51] iter = 04380, loss = 1.5639
[2023-10-01 23:38:52] iter = 04390, loss = 1.7929
[2023-10-01 23:38:53] iter = 04400, loss = 1.6444
[2023-10-01 23:38:54] iter = 04410, loss = 1.6979
[2023-10-01 23:38:55] iter = 04420, loss = 1.6508
[2023-10-01 23:38:56] iter = 04430, loss = 1.6605
[2023-10-01 23:38:57] iter = 04440, loss = 1.8050
[2023-10-01 23:38:58] iter = 04450, loss = 1.6591
[2023-10-01 23:38:59] iter = 04460, loss = 1.6584
[2023-10-01 23:38:59] iter = 04470, loss = 1.7524
[2023-10-01 23:39:01] iter = 04480, loss = 1.7158
[2023-10-01 23:39:01] iter = 04490, loss = 1.6115
[2023-10-01 23:39:02] iter = 04500, loss = 1.8199
[2023-10-01 23:39:03] iter = 04510, loss = 1.7840
[2023-10-01 23:39:04] iter = 04520, loss = 1.7007
[2023-10-01 23:39:05] iter = 04530, loss = 1.6786
[2023-10-01 23:39:06] iter = 04540, loss = 1.6573
[2023-10-01 23:39:07] iter = 04550, loss = 1.6307
[2023-10-01 23:39:08] iter = 04560, loss = 1.7273
[2023-10-01 23:39:09] iter = 04570, loss = 1.6665
[2023-10-01 23:39:10] iter = 04580, loss = 1.5674
[2023-10-01 23:39:11] iter = 04590, loss = 1.7871
[2023-10-01 23:39:11] iter = 04600, loss = 1.7005
[2023-10-01 23:39:12] iter = 04610, loss = 1.7565
[2023-10-01 23:39:13] iter = 04620, loss = 1.8966
[2023-10-01 23:39:14] iter = 04630, loss = 1.5857
[2023-10-01 23:39:15] iter = 04640, loss = 1.8051
[2023-10-01 23:39:16] iter = 04650, loss = 1.8422
[2023-10-01 23:39:17] iter = 04660, loss = 1.6820
[2023-10-01 23:39:18] iter = 04670, loss = 1.7642
[2023-10-01 23:39:19] iter = 04680, loss = 1.7793
[2023-10-01 23:39:20] iter = 04690, loss = 1.5762
[2023-10-01 23:39:21] iter = 04700, loss = 1.6308
[2023-10-01 23:39:21] iter = 04710, loss = 1.8003
[2023-10-01 23:39:22] iter = 04720, loss = 1.6523
[2023-10-01 23:39:23] iter = 04730, loss = 1.5846
[2023-10-01 23:39:24] iter = 04740, loss = 1.8776
[2023-10-01 23:39:25] iter = 04750, loss = 1.7181
[2023-10-01 23:39:26] iter = 04760, loss = 1.6704
[2023-10-01 23:39:27] iter = 04770, loss = 1.7798
[2023-10-01 23:39:28] iter = 04780, loss = 1.8137
[2023-10-01 23:39:29] iter = 04790, loss = 1.8546
[2023-10-01 23:39:30] iter = 04800, loss = 1.6642
[2023-10-01 23:39:31] iter = 04810, loss = 1.7061
[2023-10-01 23:39:32] iter = 04820, loss = 1.5460
[2023-10-01 23:39:33] iter = 04830, loss = 1.7608
[2023-10-01 23:39:34] iter = 04840, loss = 1.5752
[2023-10-01 23:39:34] iter = 04850, loss = 1.5161
[2023-10-01 23:39:35] iter = 04860, loss = 1.5313
[2023-10-01 23:39:36] iter = 04870, loss = 1.7865
[2023-10-01 23:39:37] iter = 04880, loss = 1.7950
[2023-10-01 23:39:38] iter = 04890, loss = 1.7786
[2023-10-01 23:39:39] iter = 04900, loss = 1.5887
[2023-10-01 23:39:40] iter = 04910, loss = 1.7742
[2023-10-01 23:39:41] iter = 04920, loss = 1.7242
[2023-10-01 23:39:42] iter = 04930, loss = 1.7004
[2023-10-01 23:39:43] iter = 04940, loss = 1.7983
[2023-10-01 23:39:43] iter = 04950, loss = 1.7182
[2023-10-01 23:39:44] iter = 04960, loss = 1.7032
[2023-10-01 23:39:45] iter = 04970, loss = 1.6584
[2023-10-01 23:39:46] iter = 04980, loss = 1.6125
[2023-10-01 23:39:47] iter = 04990, loss = 1.7473
[2023-10-01 23:39:48] iter = 05000, loss = 1.6057
[2023-10-01 23:39:49] iter = 05010, loss = 1.5774
[2023-10-01 23:39:50] iter = 05020, loss = 1.6335
[2023-10-01 23:39:51] iter = 05030, loss = 1.6216
[2023-10-01 23:39:52] iter = 05040, loss = 1.6336
[2023-10-01 23:39:52] iter = 05050, loss = 1.6299
[2023-10-01 23:39:53] iter = 05060, loss = 1.5661
[2023-10-01 23:39:54] iter = 05070, loss = 1.6743
[2023-10-01 23:39:55] iter = 05080, loss = 1.6160
[2023-10-01 23:39:56] iter = 05090, loss = 1.6838
[2023-10-01 23:39:57] iter = 05100, loss = 1.6066
[2023-10-01 23:39:58] iter = 05110, loss = 1.6915
[2023-10-01 23:39:59] iter = 05120, loss = 1.5632
[2023-10-01 23:40:00] iter = 05130, loss = 1.7425
[2023-10-01 23:40:01] iter = 05140, loss = 1.5919
[2023-10-01 23:40:02] iter = 05150, loss = 1.6118
[2023-10-01 23:40:03] iter = 05160, loss = 1.7771
[2023-10-01 23:40:04] iter = 05170, loss = 1.6406
[2023-10-01 23:40:04] iter = 05180, loss = 1.6615
[2023-10-01 23:40:05] iter = 05190, loss = 1.6781
[2023-10-01 23:40:06] iter = 05200, loss = 1.6746
[2023-10-01 23:40:07] iter = 05210, loss = 1.6367
[2023-10-01 23:40:08] iter = 05220, loss = 1.6223
[2023-10-01 23:40:09] iter = 05230, loss = 1.6707
[2023-10-01 23:40:10] iter = 05240, loss = 1.6282
[2023-10-01 23:40:11] iter = 05250, loss = 1.6939
[2023-10-01 23:40:12] iter = 05260, loss = 1.5873
[2023-10-01 23:40:13] iter = 05270, loss = 1.5963
[2023-10-01 23:40:14] iter = 05280, loss = 1.5778
[2023-10-01 23:40:15] iter = 05290, loss = 1.5241
[2023-10-01 23:40:15] iter = 05300, loss = 1.7276
[2023-10-01 23:40:16] iter = 05310, loss = 1.6345
[2023-10-01 23:40:17] iter = 05320, loss = 1.6082
[2023-10-01 23:40:18] iter = 05330, loss = 1.5813
[2023-10-01 23:40:19] iter = 05340, loss = 1.6630
[2023-10-01 23:40:20] iter = 05350, loss = 1.8365
[2023-10-01 23:40:21] iter = 05360, loss = 1.8238
[2023-10-01 23:40:22] iter = 05370, loss = 1.5624
[2023-10-01 23:40:23] iter = 05380, loss = 1.6033
[2023-10-01 23:40:24] iter = 05390, loss = 1.5791
[2023-10-01 23:40:24] iter = 05400, loss = 1.5974
[2023-10-01 23:40:25] iter = 05410, loss = 1.6253
[2023-10-01 23:40:26] iter = 05420, loss = 1.7016
[2023-10-01 23:40:27] iter = 05430, loss = 1.5962
[2023-10-01 23:40:28] iter = 05440, loss = 1.7126
[2023-10-01 23:40:29] iter = 05450, loss = 1.6274
[2023-10-01 23:40:30] iter = 05460, loss = 1.6349
[2023-10-01 23:40:31] iter = 05470, loss = 1.6483
[2023-10-01 23:40:32] iter = 05480, loss = 1.6299
[2023-10-01 23:40:33] iter = 05490, loss = 1.6210
[2023-10-01 23:40:34] iter = 05500, loss = 1.6634
[2023-10-01 23:40:35] iter = 05510, loss = 1.5830
[2023-10-01 23:40:36] iter = 05520, loss = 1.5867
[2023-10-01 23:40:36] iter = 05530, loss = 1.6755
[2023-10-01 23:40:37] iter = 05540, loss = 1.5117
[2023-10-01 23:40:38] iter = 05550, loss = 1.6095
[2023-10-01 23:40:39] iter = 05560, loss = 1.5340
[2023-10-01 23:40:40] iter = 05570, loss = 1.6258
[2023-10-01 23:40:41] iter = 05580, loss = 1.6327
[2023-10-01 23:40:42] iter = 05590, loss = 1.7043
[2023-10-01 23:40:43] iter = 05600, loss = 1.6318
[2023-10-01 23:40:44] iter = 05610, loss = 1.6372
[2023-10-01 23:40:45] iter = 05620, loss = 1.5240
[2023-10-01 23:40:45] iter = 05630, loss = 1.4977
[2023-10-01 23:40:46] iter = 05640, loss = 1.5691
[2023-10-01 23:40:47] iter = 05650, loss = 1.5324
[2023-10-01 23:40:48] iter = 05660, loss = 1.6363
[2023-10-01 23:40:49] iter = 05670, loss = 1.6357
[2023-10-01 23:40:50] iter = 05680, loss = 1.6185
[2023-10-01 23:40:51] iter = 05690, loss = 1.7053
[2023-10-01 23:40:52] iter = 05700, loss = 1.6333
[2023-10-01 23:40:53] iter = 05710, loss = 1.7346
[2023-10-01 23:40:54] iter = 05720, loss = 1.6879
[2023-10-01 23:40:55] iter = 05730, loss = 1.5985
[2023-10-01 23:40:55] iter = 05740, loss = 1.5645
[2023-10-01 23:40:56] iter = 05750, loss = 1.6136
[2023-10-01 23:40:57] iter = 05760, loss = 1.7350
[2023-10-01 23:40:58] iter = 05770, loss = 1.5387
[2023-10-01 23:40:59] iter = 05780, loss = 1.5648
[2023-10-01 23:41:00] iter = 05790, loss = 1.5676
[2023-10-01 23:41:01] iter = 05800, loss = 1.5780
[2023-10-01 23:41:02] iter = 05810, loss = 1.6664
[2023-10-01 23:41:03] iter = 05820, loss = 1.5821
[2023-10-01 23:41:04] iter = 05830, loss = 1.7693
[2023-10-01 23:41:05] iter = 05840, loss = 1.6276
[2023-10-01 23:41:06] iter = 05850, loss = 1.4896
[2023-10-01 23:41:06] iter = 05860, loss = 1.6396
[2023-10-01 23:41:07] iter = 05870, loss = 1.6157
[2023-10-01 23:41:08] iter = 05880, loss = 1.6311
[2023-10-01 23:41:09] iter = 05890, loss = 1.6075
[2023-10-01 23:41:10] iter = 05900, loss = 1.5862
[2023-10-01 23:41:11] iter = 05910, loss = 1.5284
[2023-10-01 23:41:12] iter = 05920, loss = 1.6415
[2023-10-01 23:41:13] iter = 05930, loss = 1.6419
[2023-10-01 23:41:14] iter = 05940, loss = 1.6638
[2023-10-01 23:41:15] iter = 05950, loss = 1.6350
[2023-10-01 23:41:16] iter = 05960, loss = 1.7140
[2023-10-01 23:41:17] iter = 05970, loss = 1.6748
[2023-10-01 23:41:17] iter = 05980, loss = 1.5766
[2023-10-01 23:41:18] iter = 05990, loss = 1.6413
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 79731}
[2023-10-01 23:41:44] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005249 train acc = 1.0000, test acc = 0.6107
[2023-10-01 23:42:08] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.014808 train acc = 1.0000, test acc = 0.6079
[2023-10-01 23:42:32] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.025971 train acc = 0.9980, test acc = 0.6133
[2023-10-01 23:42:57] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.005179 train acc = 1.0000, test acc = 0.6049
[2023-10-01 23:43:21] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.012575 train acc = 1.0000, test acc = 0.6099
[2023-10-01 23:43:45] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.014620 train acc = 1.0000, test acc = 0.6038
[2023-10-01 23:44:10] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.001417 train acc = 1.0000, test acc = 0.5994
[2023-10-01 23:44:34] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.015146 train acc = 1.0000, test acc = 0.6105
[2023-10-01 23:44:58] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003576 train acc = 1.0000, test acc = 0.6018
[2023-10-01 23:45:22] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.008561 train acc = 1.0000, test acc = 0.6050
[2023-10-01 23:45:47] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.001914 train acc = 1.0000, test acc = 0.6063
[2023-10-01 23:46:11] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.009682 train acc = 1.0000, test acc = 0.6104
[2023-10-01 23:46:35] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001619 train acc = 1.0000, test acc = 0.6057
[2023-10-01 23:46:59] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.019524 train acc = 0.9980, test acc = 0.6114
[2023-10-01 23:47:24] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002947 train acc = 1.0000, test acc = 0.6025
[2023-10-01 23:47:48] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.012579 train acc = 1.0000, test acc = 0.6104
[2023-10-01 23:48:12] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.011306 train acc = 1.0000, test acc = 0.6041
[2023-10-01 23:48:37] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.006308 train acc = 1.0000, test acc = 0.6107
[2023-10-01 23:49:01] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.015348 train acc = 0.9960, test acc = 0.6012
[2023-10-01 23:49:25] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.014988 train acc = 1.0000, test acc = 0.5999
Evaluate 20 random ConvNet, mean = 0.6065 std = 0.0041
-------------------------
[2023-10-01 23:49:26] iter = 06000, loss = 1.5415
[2023-10-01 23:49:27] iter = 06010, loss = 1.5750
[2023-10-01 23:49:27] iter = 06020, loss = 1.6712
[2023-10-01 23:49:28] iter = 06030, loss = 1.7925
[2023-10-01 23:49:29] iter = 06040, loss = 1.5483
[2023-10-01 23:49:30] iter = 06050, loss = 1.5835
[2023-10-01 23:49:31] iter = 06060, loss = 1.6199
[2023-10-01 23:49:32] iter = 06070, loss = 1.6713
[2023-10-01 23:49:33] iter = 06080, loss = 1.4388
[2023-10-01 23:49:34] iter = 06090, loss = 1.8027
[2023-10-01 23:49:35] iter = 06100, loss = 1.6864
[2023-10-01 23:49:36] iter = 06110, loss = 1.8651
[2023-10-01 23:49:37] iter = 06120, loss = 1.6858
[2023-10-01 23:49:38] iter = 06130, loss = 1.5594
[2023-10-01 23:49:39] iter = 06140, loss = 1.5894
[2023-10-01 23:49:40] iter = 06150, loss = 1.6403
[2023-10-01 23:49:40] iter = 06160, loss = 1.4888
[2023-10-01 23:49:41] iter = 06170, loss = 1.6698
[2023-10-01 23:49:42] iter = 06180, loss = 1.5693
[2023-10-01 23:49:43] iter = 06190, loss = 1.6817
[2023-10-01 23:49:44] iter = 06200, loss = 1.6352
[2023-10-01 23:49:45] iter = 06210, loss = 1.5192
[2023-10-01 23:49:46] iter = 06220, loss = 1.5896
[2023-10-01 23:49:47] iter = 06230, loss = 1.5495
[2023-10-01 23:49:48] iter = 06240, loss = 1.6765
[2023-10-01 23:49:49] iter = 06250, loss = 1.5741
[2023-10-01 23:49:50] iter = 06260, loss = 1.4984
[2023-10-01 23:49:51] iter = 06270, loss = 1.5391
[2023-10-01 23:49:51] iter = 06280, loss = 1.4839
[2023-10-01 23:49:52] iter = 06290, loss = 1.5548
[2023-10-01 23:49:53] iter = 06300, loss = 1.7235
[2023-10-01 23:49:54] iter = 06310, loss = 1.6650
[2023-10-01 23:49:55] iter = 06320, loss = 1.4939
[2023-10-01 23:49:56] iter = 06330, loss = 1.7699
[2023-10-01 23:49:57] iter = 06340, loss = 1.6012
[2023-10-01 23:49:58] iter = 06350, loss = 1.6854
[2023-10-01 23:49:59] iter = 06360, loss = 1.5237
[2023-10-01 23:49:59] iter = 06370, loss = 1.6619
[2023-10-01 23:50:00] iter = 06380, loss = 1.6230
[2023-10-01 23:50:01] iter = 06390, loss = 1.5276
[2023-10-01 23:50:02] iter = 06400, loss = 1.6661
[2023-10-01 23:50:03] iter = 06410, loss = 1.6110
[2023-10-01 23:50:04] iter = 06420, loss = 1.4938
[2023-10-01 23:50:05] iter = 06430, loss = 1.6210
[2023-10-01 23:50:06] iter = 06440, loss = 1.5800
[2023-10-01 23:50:07] iter = 06450, loss = 1.5806
[2023-10-01 23:50:08] iter = 06460, loss = 1.6344
[2023-10-01 23:50:09] iter = 06470, loss = 1.7910
[2023-10-01 23:50:10] iter = 06480, loss = 1.6949
[2023-10-01 23:50:10] iter = 06490, loss = 1.6943
[2023-10-01 23:50:11] iter = 06500, loss = 1.7912
[2023-10-01 23:50:12] iter = 06510, loss = 1.5797
[2023-10-01 23:50:13] iter = 06520, loss = 1.5749
[2023-10-01 23:50:14] iter = 06530, loss = 1.5818
[2023-10-01 23:50:15] iter = 06540, loss = 1.5574
[2023-10-01 23:50:16] iter = 06550, loss = 1.7277
[2023-10-01 23:50:17] iter = 06560, loss = 1.6853
[2023-10-01 23:50:18] iter = 06570, loss = 1.5684
[2023-10-01 23:50:19] iter = 06580, loss = 1.5119
[2023-10-01 23:50:20] iter = 06590, loss = 1.5655
[2023-10-01 23:50:20] iter = 06600, loss = 1.5813
[2023-10-01 23:50:21] iter = 06610, loss = 1.5634
[2023-10-01 23:50:22] iter = 06620, loss = 1.5903
[2023-10-01 23:50:23] iter = 06630, loss = 1.5398
[2023-10-01 23:50:24] iter = 06640, loss = 1.5493
[2023-10-01 23:50:25] iter = 06650, loss = 1.4828
[2023-10-01 23:50:26] iter = 06660, loss = 1.6361
[2023-10-01 23:50:27] iter = 06670, loss = 1.6353
[2023-10-01 23:50:28] iter = 06680, loss = 1.4850
[2023-10-01 23:50:29] iter = 06690, loss = 1.6469
[2023-10-01 23:50:30] iter = 06700, loss = 1.4745
[2023-10-01 23:50:30] iter = 06710, loss = 1.5910
[2023-10-01 23:50:31] iter = 06720, loss = 1.6741
[2023-10-01 23:50:32] iter = 06730, loss = 1.6015
[2023-10-01 23:50:33] iter = 06740, loss = 1.5987
[2023-10-01 23:50:34] iter = 06750, loss = 1.5597
[2023-10-01 23:50:35] iter = 06760, loss = 1.7235
[2023-10-01 23:50:36] iter = 06770, loss = 1.5757
[2023-10-01 23:50:37] iter = 06780, loss = 1.6710
[2023-10-01 23:50:38] iter = 06790, loss = 1.7205
[2023-10-01 23:50:39] iter = 06800, loss = 1.6886
[2023-10-01 23:50:40] iter = 06810, loss = 1.5950
[2023-10-01 23:50:41] iter = 06820, loss = 1.6116
[2023-10-01 23:50:41] iter = 06830, loss = 1.6276
[2023-10-01 23:50:42] iter = 06840, loss = 1.5632
[2023-10-01 23:50:43] iter = 06850, loss = 1.6861
[2023-10-01 23:50:44] iter = 06860, loss = 1.5102
[2023-10-01 23:50:45] iter = 06870, loss = 1.7701
[2023-10-01 23:50:46] iter = 06880, loss = 1.6681
[2023-10-01 23:50:47] iter = 06890, loss = 1.7386
[2023-10-01 23:50:48] iter = 06900, loss = 1.6356
[2023-10-01 23:50:48] iter = 06910, loss = 1.5440
[2023-10-01 23:50:49] iter = 06920, loss = 1.6243
[2023-10-01 23:50:50] iter = 06930, loss = 1.6829
[2023-10-01 23:50:51] iter = 06940, loss = 1.5255
[2023-10-01 23:50:52] iter = 06950, loss = 1.4743
[2023-10-01 23:50:53] iter = 06960, loss = 1.6346
[2023-10-01 23:50:54] iter = 06970, loss = 1.5556
[2023-10-01 23:50:55] iter = 06980, loss = 1.6260
[2023-10-01 23:50:56] iter = 06990, loss = 1.5638
[2023-10-01 23:50:57] iter = 07000, loss = 1.6140
[2023-10-01 23:50:57] iter = 07010, loss = 1.8123
[2023-10-01 23:50:58] iter = 07020, loss = 1.6027
[2023-10-01 23:50:59] iter = 07030, loss = 1.4546
[2023-10-01 23:51:00] iter = 07040, loss = 1.6990
[2023-10-01 23:51:01] iter = 07050, loss = 1.7257
[2023-10-01 23:51:02] iter = 07060, loss = 1.4348
[2023-10-01 23:51:03] iter = 07070, loss = 1.7624
[2023-10-01 23:51:04] iter = 07080, loss = 1.5205
[2023-10-01 23:51:04] iter = 07090, loss = 1.6113
[2023-10-01 23:51:05] iter = 07100, loss = 1.6124
[2023-10-01 23:51:06] iter = 07110, loss = 1.4947
[2023-10-01 23:51:07] iter = 07120, loss = 1.5157
[2023-10-01 23:51:08] iter = 07130, loss = 1.6928
[2023-10-01 23:51:09] iter = 07140, loss = 1.6179
[2023-10-01 23:51:10] iter = 07150, loss = 1.6222
[2023-10-01 23:51:11] iter = 07160, loss = 1.4511
[2023-10-01 23:51:12] iter = 07170, loss = 1.5436
[2023-10-01 23:51:13] iter = 07180, loss = 1.6415
[2023-10-01 23:51:14] iter = 07190, loss = 1.7514
[2023-10-01 23:51:15] iter = 07200, loss = 1.5920
[2023-10-01 23:51:16] iter = 07210, loss = 1.6573
[2023-10-01 23:51:17] iter = 07220, loss = 1.5954
[2023-10-01 23:51:17] iter = 07230, loss = 1.4482
[2023-10-01 23:51:18] iter = 07240, loss = 1.6307
[2023-10-01 23:51:19] iter = 07250, loss = 1.7299
[2023-10-01 23:51:20] iter = 07260, loss = 1.6081
[2023-10-01 23:51:21] iter = 07270, loss = 1.5754
[2023-10-01 23:51:22] iter = 07280, loss = 1.5649
[2023-10-01 23:51:23] iter = 07290, loss = 1.4783
[2023-10-01 23:51:24] iter = 07300, loss = 1.4788
[2023-10-01 23:51:25] iter = 07310, loss = 1.5317
[2023-10-01 23:51:25] iter = 07320, loss = 1.7167
[2023-10-01 23:51:26] iter = 07330, loss = 1.6134
[2023-10-01 23:51:27] iter = 07340, loss = 1.4989
[2023-10-01 23:51:28] iter = 07350, loss = 1.5982
[2023-10-01 23:51:29] iter = 07360, loss = 1.5836
[2023-10-01 23:51:30] iter = 07370, loss = 1.6497
[2023-10-01 23:51:31] iter = 07380, loss = 1.5702
[2023-10-01 23:51:32] iter = 07390, loss = 1.4329
[2023-10-01 23:51:33] iter = 07400, loss = 1.6314
[2023-10-01 23:51:34] iter = 07410, loss = 1.8123
[2023-10-01 23:51:35] iter = 07420, loss = 1.7522
[2023-10-01 23:51:36] iter = 07430, loss = 1.6311
[2023-10-01 23:51:37] iter = 07440, loss = 1.6969
[2023-10-01 23:51:37] iter = 07450, loss = 1.7460
[2023-10-01 23:51:38] iter = 07460, loss = 1.6672
[2023-10-01 23:51:39] iter = 07470, loss = 1.6006
[2023-10-01 23:51:40] iter = 07480, loss = 1.5510
[2023-10-01 23:51:41] iter = 07490, loss = 1.6595
[2023-10-01 23:51:42] iter = 07500, loss = 1.6039
[2023-10-01 23:51:43] iter = 07510, loss = 1.5906
[2023-10-01 23:51:44] iter = 07520, loss = 1.5187
[2023-10-01 23:51:45] iter = 07530, loss = 1.6837
[2023-10-01 23:51:45] iter = 07540, loss = 1.5466
[2023-10-01 23:51:46] iter = 07550, loss = 1.5274
[2023-10-01 23:51:47] iter = 07560, loss = 1.5546
[2023-10-01 23:51:48] iter = 07570, loss = 1.5077
[2023-10-01 23:51:49] iter = 07580, loss = 1.6365
[2023-10-01 23:51:50] iter = 07590, loss = 1.6684
[2023-10-01 23:51:51] iter = 07600, loss = 1.5569
[2023-10-01 23:51:52] iter = 07610, loss = 1.6032
[2023-10-01 23:51:53] iter = 07620, loss = 1.6835
[2023-10-01 23:51:54] iter = 07630, loss = 1.6510
[2023-10-01 23:51:55] iter = 07640, loss = 1.4864
[2023-10-01 23:51:56] iter = 07650, loss = 1.6605
[2023-10-01 23:51:57] iter = 07660, loss = 1.5168
[2023-10-01 23:51:58] iter = 07670, loss = 1.5146
[2023-10-01 23:51:59] iter = 07680, loss = 1.6118
[2023-10-01 23:51:59] iter = 07690, loss = 1.6364
[2023-10-01 23:52:00] iter = 07700, loss = 1.7072
[2023-10-01 23:52:01] iter = 07710, loss = 1.5994
[2023-10-01 23:52:02] iter = 07720, loss = 1.5253
[2023-10-01 23:52:03] iter = 07730, loss = 1.6779
[2023-10-01 23:52:04] iter = 07740, loss = 1.6426
[2023-10-01 23:52:05] iter = 07750, loss = 1.6354
[2023-10-01 23:52:06] iter = 07760, loss = 1.5258
[2023-10-01 23:52:06] iter = 07770, loss = 1.5254
[2023-10-01 23:52:07] iter = 07780, loss = 1.5602
[2023-10-01 23:52:08] iter = 07790, loss = 1.6011
[2023-10-01 23:52:09] iter = 07800, loss = 1.5963
[2023-10-01 23:52:10] iter = 07810, loss = 1.6076
[2023-10-01 23:52:11] iter = 07820, loss = 1.5461
[2023-10-01 23:52:12] iter = 07830, loss = 1.6723
[2023-10-01 23:52:13] iter = 07840, loss = 1.5156
[2023-10-01 23:52:14] iter = 07850, loss = 1.6222
[2023-10-01 23:52:15] iter = 07860, loss = 1.7193
[2023-10-01 23:52:16] iter = 07870, loss = 1.5373
[2023-10-01 23:52:17] iter = 07880, loss = 1.5511
[2023-10-01 23:52:17] iter = 07890, loss = 1.5958
[2023-10-01 23:52:18] iter = 07900, loss = 1.6027
[2023-10-01 23:52:19] iter = 07910, loss = 1.5455
[2023-10-01 23:52:20] iter = 07920, loss = 1.6070
[2023-10-01 23:52:21] iter = 07930, loss = 1.5705
[2023-10-01 23:52:22] iter = 07940, loss = 1.4961
[2023-10-01 23:52:23] iter = 07950, loss = 1.5699
[2023-10-01 23:52:24] iter = 07960, loss = 1.5534
[2023-10-01 23:52:25] iter = 07970, loss = 1.6684
[2023-10-01 23:52:26] iter = 07980, loss = 1.5892
[2023-10-01 23:52:27] iter = 07990, loss = 1.6024
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 47968}
[2023-10-01 23:52:52] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002527 train acc = 1.0000, test acc = 0.6172
[2023-10-01 23:53:16] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.013667 train acc = 1.0000, test acc = 0.6187
[2023-10-01 23:53:40] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003967 train acc = 1.0000, test acc = 0.6125
[2023-10-01 23:54:04] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.022235 train acc = 0.9980, test acc = 0.6135
[2023-10-01 23:54:28] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003283 train acc = 1.0000, test acc = 0.6226
[2023-10-01 23:54:53] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.021682 train acc = 1.0000, test acc = 0.6140
[2023-10-01 23:55:17] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.010281 train acc = 1.0000, test acc = 0.6148
[2023-10-01 23:55:41] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003321 train acc = 1.0000, test acc = 0.6076
[2023-10-01 23:56:05] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003091 train acc = 1.0000, test acc = 0.6090
[2023-10-01 23:56:29] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003518 train acc = 1.0000, test acc = 0.6062
[2023-10-01 23:56:53] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002121 train acc = 1.0000, test acc = 0.6122
[2023-10-01 23:57:17] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004283 train acc = 1.0000, test acc = 0.6127
[2023-10-01 23:57:41] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011999 train acc = 1.0000, test acc = 0.6124
[2023-10-01 23:58:06] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.013094 train acc = 1.0000, test acc = 0.6137
[2023-10-01 23:58:30] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.001784 train acc = 1.0000, test acc = 0.6129
[2023-10-01 23:58:54] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.005393 train acc = 1.0000, test acc = 0.6084
[2023-10-01 23:59:18] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.012898 train acc = 1.0000, test acc = 0.6166
[2023-10-01 23:59:43] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.028385 train acc = 0.9980, test acc = 0.6177
[2023-10-02 00:00:07] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002113 train acc = 1.0000, test acc = 0.6204
[2023-10-02 00:00:31] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.010968 train acc = 0.9980, test acc = 0.6132
Evaluate 20 random ConvNet, mean = 0.6138 std = 0.0041
-------------------------
[2023-10-02 00:00:31] iter = 08000, loss = 1.5878
[2023-10-02 00:00:32] iter = 08010, loss = 1.6036
[2023-10-02 00:00:33] iter = 08020, loss = 1.5999
[2023-10-02 00:00:34] iter = 08030, loss = 1.6961
[2023-10-02 00:00:35] iter = 08040, loss = 1.5430
[2023-10-02 00:00:36] iter = 08050, loss = 1.8377
[2023-10-02 00:00:37] iter = 08060, loss = 1.6167
[2023-10-02 00:00:38] iter = 08070, loss = 1.7221
[2023-10-02 00:00:38] iter = 08080, loss = 1.5583
[2023-10-02 00:00:39] iter = 08090, loss = 1.6775
[2023-10-02 00:00:40] iter = 08100, loss = 1.6255
[2023-10-02 00:00:41] iter = 08110, loss = 1.7219
[2023-10-02 00:00:42] iter = 08120, loss = 1.4757
[2023-10-02 00:00:43] iter = 08130, loss = 1.5880
[2023-10-02 00:00:44] iter = 08140, loss = 1.5592
[2023-10-02 00:00:45] iter = 08150, loss = 1.6733
[2023-10-02 00:00:46] iter = 08160, loss = 1.4781
[2023-10-02 00:00:47] iter = 08170, loss = 1.5250
[2023-10-02 00:00:47] iter = 08180, loss = 1.5139
[2023-10-02 00:00:48] iter = 08190, loss = 1.7889
[2023-10-02 00:00:49] iter = 08200, loss = 1.5948
[2023-10-02 00:00:50] iter = 08210, loss = 1.5178
[2023-10-02 00:00:51] iter = 08220, loss = 1.6363
[2023-10-02 00:00:52] iter = 08230, loss = 1.4259
[2023-10-02 00:00:53] iter = 08240, loss = 1.6505
[2023-10-02 00:00:54] iter = 08250, loss = 1.5526
[2023-10-02 00:00:55] iter = 08260, loss = 1.5325
[2023-10-02 00:00:56] iter = 08270, loss = 1.5466
[2023-10-02 00:00:57] iter = 08280, loss = 1.5100
[2023-10-02 00:00:58] iter = 08290, loss = 1.6002
[2023-10-02 00:00:59] iter = 08300, loss = 1.6319
[2023-10-02 00:01:00] iter = 08310, loss = 1.5246
[2023-10-02 00:01:00] iter = 08320, loss = 1.6998
[2023-10-02 00:01:01] iter = 08330, loss = 1.5866
[2023-10-02 00:01:02] iter = 08340, loss = 1.6423
[2023-10-02 00:01:03] iter = 08350, loss = 1.5848
[2023-10-02 00:01:04] iter = 08360, loss = 1.7262
[2023-10-02 00:01:05] iter = 08370, loss = 1.6010
[2023-10-02 00:01:06] iter = 08380, loss = 1.5114
[2023-10-02 00:01:07] iter = 08390, loss = 1.5327
[2023-10-02 00:01:08] iter = 08400, loss = 1.5906
[2023-10-02 00:01:09] iter = 08410, loss = 1.5604
[2023-10-02 00:01:10] iter = 08420, loss = 1.5989
[2023-10-02 00:01:10] iter = 08430, loss = 1.5624
[2023-10-02 00:01:11] iter = 08440, loss = 1.6523
[2023-10-02 00:01:12] iter = 08450, loss = 1.7086
[2023-10-02 00:01:13] iter = 08460, loss = 1.4360
[2023-10-02 00:01:14] iter = 08470, loss = 1.5690
[2023-10-02 00:01:15] iter = 08480, loss = 1.4866
[2023-10-02 00:01:16] iter = 08490, loss = 1.6924
[2023-10-02 00:01:17] iter = 08500, loss = 1.7574
[2023-10-02 00:01:18] iter = 08510, loss = 1.8354
[2023-10-02 00:01:19] iter = 08520, loss = 1.5277
[2023-10-02 00:01:20] iter = 08530, loss = 1.7028
[2023-10-02 00:01:21] iter = 08540, loss = 1.5822
[2023-10-02 00:01:22] iter = 08550, loss = 1.5969
[2023-10-02 00:01:22] iter = 08560, loss = 1.7079
[2023-10-02 00:01:23] iter = 08570, loss = 1.4376
[2023-10-02 00:01:24] iter = 08580, loss = 1.5150
[2023-10-02 00:01:25] iter = 08590, loss = 1.5861
[2023-10-02 00:01:26] iter = 08600, loss = 1.5958
[2023-10-02 00:01:27] iter = 08610, loss = 1.5005
[2023-10-02 00:01:28] iter = 08620, loss = 1.6484
[2023-10-02 00:01:29] iter = 08630, loss = 1.5951
[2023-10-02 00:01:30] iter = 08640, loss = 1.5589
[2023-10-02 00:01:31] iter = 08650, loss = 1.5361
[2023-10-02 00:01:32] iter = 08660, loss = 1.4454
[2023-10-02 00:01:32] iter = 08670, loss = 1.4933
[2023-10-02 00:01:33] iter = 08680, loss = 1.6636
[2023-10-02 00:01:34] iter = 08690, loss = 1.5648
[2023-10-02 00:01:35] iter = 08700, loss = 1.6786
[2023-10-02 00:01:36] iter = 08710, loss = 1.5464
[2023-10-02 00:01:37] iter = 08720, loss = 1.5075
[2023-10-02 00:01:38] iter = 08730, loss = 1.7842
[2023-10-02 00:01:39] iter = 08740, loss = 1.6106
[2023-10-02 00:01:40] iter = 08750, loss = 1.7020
[2023-10-02 00:01:40] iter = 08760, loss = 1.6126
[2023-10-02 00:01:41] iter = 08770, loss = 1.5810
[2023-10-02 00:01:42] iter = 08780, loss = 1.5687
[2023-10-02 00:01:43] iter = 08790, loss = 1.6879
[2023-10-02 00:01:44] iter = 08800, loss = 1.5428
[2023-10-02 00:01:45] iter = 08810, loss = 1.5721
[2023-10-02 00:01:46] iter = 08820, loss = 1.4674
[2023-10-02 00:01:47] iter = 08830, loss = 1.6650
[2023-10-02 00:01:48] iter = 08840, loss = 1.5605
[2023-10-02 00:01:49] iter = 08850, loss = 1.6470
[2023-10-02 00:01:50] iter = 08860, loss = 1.5879
[2023-10-02 00:01:51] iter = 08870, loss = 1.6135
[2023-10-02 00:01:52] iter = 08880, loss = 1.6383
[2023-10-02 00:01:53] iter = 08890, loss = 1.4836
[2023-10-02 00:01:53] iter = 08900, loss = 1.5819
[2023-10-02 00:01:54] iter = 08910, loss = 1.5890
[2023-10-02 00:01:55] iter = 08920, loss = 1.4716
[2023-10-02 00:01:56] iter = 08930, loss = 1.5678
[2023-10-02 00:01:57] iter = 08940, loss = 1.5265
[2023-10-02 00:01:58] iter = 08950, loss = 1.5321
[2023-10-02 00:01:59] iter = 08960, loss = 1.5373
[2023-10-02 00:02:00] iter = 08970, loss = 1.5512
[2023-10-02 00:02:00] iter = 08980, loss = 1.7359
[2023-10-02 00:02:01] iter = 08990, loss = 1.5033
[2023-10-02 00:02:02] iter = 09000, loss = 1.4444
[2023-10-02 00:02:03] iter = 09010, loss = 1.5826
[2023-10-02 00:02:04] iter = 09020, loss = 1.6422
[2023-10-02 00:02:05] iter = 09030, loss = 1.6210
[2023-10-02 00:02:06] iter = 09040, loss = 1.5330
[2023-10-02 00:02:07] iter = 09050, loss = 1.5195
[2023-10-02 00:02:07] iter = 09060, loss = 1.5637
[2023-10-02 00:02:08] iter = 09070, loss = 1.7175
[2023-10-02 00:02:09] iter = 09080, loss = 1.4902
[2023-10-02 00:02:10] iter = 09090, loss = 1.6318
[2023-10-02 00:02:11] iter = 09100, loss = 1.5707
[2023-10-02 00:02:12] iter = 09110, loss = 1.5193
[2023-10-02 00:02:13] iter = 09120, loss = 1.4771
[2023-10-02 00:02:14] iter = 09130, loss = 1.5211
[2023-10-02 00:02:15] iter = 09140, loss = 1.6781
[2023-10-02 00:02:16] iter = 09150, loss = 1.5245
[2023-10-02 00:02:16] iter = 09160, loss = 1.5875
[2023-10-02 00:02:17] iter = 09170, loss = 1.6433
[2023-10-02 00:02:18] iter = 09180, loss = 1.5627
[2023-10-02 00:02:19] iter = 09190, loss = 1.5481
[2023-10-02 00:02:20] iter = 09200, loss = 1.5119
[2023-10-02 00:02:21] iter = 09210, loss = 1.4680
[2023-10-02 00:02:22] iter = 09220, loss = 1.6059
[2023-10-02 00:02:23] iter = 09230, loss = 1.4704
[2023-10-02 00:02:24] iter = 09240, loss = 1.5006
[2023-10-02 00:02:24] iter = 09250, loss = 1.7049
[2023-10-02 00:02:25] iter = 09260, loss = 1.5905
[2023-10-02 00:02:26] iter = 09270, loss = 1.5070
[2023-10-02 00:02:27] iter = 09280, loss = 1.4916
[2023-10-02 00:02:28] iter = 09290, loss = 1.8083
[2023-10-02 00:02:29] iter = 09300, loss = 1.5949
[2023-10-02 00:02:30] iter = 09310, loss = 1.6054
[2023-10-02 00:02:31] iter = 09320, loss = 1.4460
[2023-10-02 00:02:32] iter = 09330, loss = 1.4951
[2023-10-02 00:02:33] iter = 09340, loss = 1.6588
[2023-10-02 00:02:33] iter = 09350, loss = 1.4443
[2023-10-02 00:02:34] iter = 09360, loss = 1.6672
[2023-10-02 00:02:35] iter = 09370, loss = 1.5219
[2023-10-02 00:02:36] iter = 09380, loss = 1.5107
[2023-10-02 00:02:37] iter = 09390, loss = 1.4330
[2023-10-02 00:02:38] iter = 09400, loss = 1.5903
[2023-10-02 00:02:39] iter = 09410, loss = 1.7009
[2023-10-02 00:02:40] iter = 09420, loss = 1.4952
[2023-10-02 00:02:41] iter = 09430, loss = 1.7357
[2023-10-02 00:02:42] iter = 09440, loss = 1.4356
[2023-10-02 00:02:42] iter = 09450, loss = 1.5733
[2023-10-02 00:02:43] iter = 09460, loss = 1.7491
[2023-10-02 00:02:44] iter = 09470, loss = 1.5979
[2023-10-02 00:02:45] iter = 09480, loss = 1.6075
[2023-10-02 00:02:46] iter = 09490, loss = 1.6210
[2023-10-02 00:02:47] iter = 09500, loss = 1.6776
[2023-10-02 00:02:48] iter = 09510, loss = 1.5783
[2023-10-02 00:02:48] iter = 09520, loss = 1.7155
[2023-10-02 00:02:49] iter = 09530, loss = 1.5701
[2023-10-02 00:02:50] iter = 09540, loss = 1.5958
[2023-10-02 00:02:51] iter = 09550, loss = 1.6476
[2023-10-02 00:02:52] iter = 09560, loss = 1.6534
[2023-10-02 00:02:53] iter = 09570, loss = 1.6741
[2023-10-02 00:02:54] iter = 09580, loss = 1.4819
[2023-10-02 00:02:55] iter = 09590, loss = 1.7617
[2023-10-02 00:02:56] iter = 09600, loss = 1.4466
[2023-10-02 00:02:57] iter = 09610, loss = 1.6239
[2023-10-02 00:02:57] iter = 09620, loss = 1.5752
[2023-10-02 00:02:58] iter = 09630, loss = 1.5799
[2023-10-02 00:02:59] iter = 09640, loss = 1.5792
[2023-10-02 00:03:00] iter = 09650, loss = 1.6250
[2023-10-02 00:03:01] iter = 09660, loss = 1.5151
[2023-10-02 00:03:02] iter = 09670, loss = 1.4816
[2023-10-02 00:03:03] iter = 09680, loss = 1.5353
[2023-10-02 00:03:04] iter = 09690, loss = 1.4245
[2023-10-02 00:03:05] iter = 09700, loss = 1.4921
[2023-10-02 00:03:06] iter = 09710, loss = 1.5661
[2023-10-02 00:03:07] iter = 09720, loss = 1.5957
[2023-10-02 00:03:07] iter = 09730, loss = 1.4595
[2023-10-02 00:03:08] iter = 09740, loss = 1.6259
[2023-10-02 00:03:09] iter = 09750, loss = 1.6508
[2023-10-02 00:03:10] iter = 09760, loss = 1.6364
[2023-10-02 00:03:11] iter = 09770, loss = 1.6042
[2023-10-02 00:03:12] iter = 09780, loss = 1.4998
[2023-10-02 00:03:13] iter = 09790, loss = 1.5404
[2023-10-02 00:03:14] iter = 09800, loss = 1.4570
[2023-10-02 00:03:15] iter = 09810, loss = 1.5827
[2023-10-02 00:03:16] iter = 09820, loss = 1.4984
[2023-10-02 00:03:17] iter = 09830, loss = 1.6313
[2023-10-02 00:03:17] iter = 09840, loss = 1.5681
[2023-10-02 00:03:18] iter = 09850, loss = 1.6475
[2023-10-02 00:03:19] iter = 09860, loss = 1.6703
[2023-10-02 00:03:20] iter = 09870, loss = 1.4547
[2023-10-02 00:03:21] iter = 09880, loss = 1.4897
[2023-10-02 00:03:22] iter = 09890, loss = 1.6101
[2023-10-02 00:03:23] iter = 09900, loss = 1.6604
[2023-10-02 00:03:24] iter = 09910, loss = 1.6006
[2023-10-02 00:03:25] iter = 09920, loss = 1.4733
[2023-10-02 00:03:26] iter = 09930, loss = 1.3780
[2023-10-02 00:03:27] iter = 09940, loss = 1.6225
[2023-10-02 00:03:28] iter = 09950, loss = 1.3121
[2023-10-02 00:03:28] iter = 09960, loss = 1.6289
[2023-10-02 00:03:29] iter = 09970, loss = 1.6784
[2023-10-02 00:03:30] iter = 09980, loss = 1.4414
[2023-10-02 00:03:31] iter = 09990, loss = 1.5292
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 12564}
[2023-10-02 00:03:56] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.012482 train acc = 1.0000, test acc = 0.6165
[2023-10-02 00:04:21] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.035235 train acc = 0.9980, test acc = 0.6173
[2023-10-02 00:04:45] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003440 train acc = 1.0000, test acc = 0.6198
[2023-10-02 00:05:09] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.014156 train acc = 1.0000, test acc = 0.6194
[2023-10-02 00:05:33] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004585 train acc = 1.0000, test acc = 0.6254
[2023-10-02 00:05:58] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.016582 train acc = 1.0000, test acc = 0.6202
[2023-10-02 00:06:22] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003870 train acc = 1.0000, test acc = 0.6185
[2023-10-02 00:06:46] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.006360 train acc = 1.0000, test acc = 0.6210
[2023-10-02 00:07:10] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.025386 train acc = 0.9980, test acc = 0.6119
[2023-10-02 00:07:34] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.011246 train acc = 1.0000, test acc = 0.6184
[2023-10-02 00:07:58] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004947 train acc = 0.9980, test acc = 0.6137
[2023-10-02 00:08:22] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003462 train acc = 1.0000, test acc = 0.6162
[2023-10-02 00:08:47] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.018078 train acc = 1.0000, test acc = 0.6178
[2023-10-02 00:09:11] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.022224 train acc = 1.0000, test acc = 0.6184
[2023-10-02 00:09:35] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004652 train acc = 1.0000, test acc = 0.6208
[2023-10-02 00:10:00] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015692 train acc = 0.9980, test acc = 0.6183
[2023-10-02 00:10:24] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.024017 train acc = 1.0000, test acc = 0.6100
[2023-10-02 00:10:48] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.017830 train acc = 1.0000, test acc = 0.6124
[2023-10-02 00:11:12] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003830 train acc = 1.0000, test acc = 0.6182
[2023-10-02 00:11:36] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012512 train acc = 1.0000, test acc = 0.6051
Evaluate 20 random ConvNet, mean = 0.6170 std = 0.0044
-------------------------
[2023-10-02 00:11:36] iter = 10000, loss = 1.6348
[2023-10-02 00:11:37] iter = 10010, loss = 1.5066
[2023-10-02 00:11:38] iter = 10020, loss = 1.5114
[2023-10-02 00:11:39] iter = 10030, loss = 1.4010
[2023-10-02 00:11:40] iter = 10040, loss = 1.5278
[2023-10-02 00:11:41] iter = 10050, loss = 1.5692
[2023-10-02 00:11:42] iter = 10060, loss = 1.6025
[2023-10-02 00:11:43] iter = 10070, loss = 1.4886
[2023-10-02 00:11:43] iter = 10080, loss = 1.4387
[2023-10-02 00:11:44] iter = 10090, loss = 1.5033
[2023-10-02 00:11:45] iter = 10100, loss = 1.7747
[2023-10-02 00:11:46] iter = 10110, loss = 1.5838
[2023-10-02 00:11:47] iter = 10120, loss = 1.5549
[2023-10-02 00:11:48] iter = 10130, loss = 1.6147
[2023-10-02 00:11:49] iter = 10140, loss = 1.6243
[2023-10-02 00:11:50] iter = 10150, loss = 1.6702
[2023-10-02 00:11:51] iter = 10160, loss = 1.5335
[2023-10-02 00:11:52] iter = 10170, loss = 1.7585
[2023-10-02 00:11:53] iter = 10180, loss = 1.5773
[2023-10-02 00:11:53] iter = 10190, loss = 1.6550
[2023-10-02 00:11:54] iter = 10200, loss = 1.4346
[2023-10-02 00:11:55] iter = 10210, loss = 1.6386
[2023-10-02 00:11:56] iter = 10220, loss = 1.5261
[2023-10-02 00:11:57] iter = 10230, loss = 1.7450
[2023-10-02 00:11:58] iter = 10240, loss = 1.7545
[2023-10-02 00:11:59] iter = 10250, loss = 1.6513
[2023-10-02 00:12:00] iter = 10260, loss = 1.5678
[2023-10-02 00:12:01] iter = 10270, loss = 1.5088
[2023-10-02 00:12:02] iter = 10280, loss = 1.6029
[2023-10-02 00:12:02] iter = 10290, loss = 1.5928
[2023-10-02 00:12:03] iter = 10300, loss = 1.5726
[2023-10-02 00:12:04] iter = 10310, loss = 1.5396
[2023-10-02 00:12:05] iter = 10320, loss = 1.4246
[2023-10-02 00:12:06] iter = 10330, loss = 1.5559
[2023-10-02 00:12:07] iter = 10340, loss = 1.5908
[2023-10-02 00:12:08] iter = 10350, loss = 1.5716
[2023-10-02 00:12:09] iter = 10360, loss = 1.5752
[2023-10-02 00:12:10] iter = 10370, loss = 1.5642
[2023-10-02 00:12:10] iter = 10380, loss = 1.5631
[2023-10-02 00:12:11] iter = 10390, loss = 1.5057
[2023-10-02 00:12:12] iter = 10400, loss = 1.7080
[2023-10-02 00:12:13] iter = 10410, loss = 1.5353
[2023-10-02 00:12:14] iter = 10420, loss = 1.5795
[2023-10-02 00:12:15] iter = 10430, loss = 1.5572
[2023-10-02 00:12:16] iter = 10440, loss = 1.5888
[2023-10-02 00:12:17] iter = 10450, loss = 1.4306
[2023-10-02 00:12:18] iter = 10460, loss = 1.4374
[2023-10-02 00:12:19] iter = 10470, loss = 1.5776
[2023-10-02 00:12:19] iter = 10480, loss = 1.5928
[2023-10-02 00:12:20] iter = 10490, loss = 1.6034
[2023-10-02 00:12:21] iter = 10500, loss = 1.4713
[2023-10-02 00:12:22] iter = 10510, loss = 1.4230
[2023-10-02 00:12:23] iter = 10520, loss = 1.6371
[2023-10-02 00:12:24] iter = 10530, loss = 1.5480
[2023-10-02 00:12:25] iter = 10540, loss = 1.7039
[2023-10-02 00:12:26] iter = 10550, loss = 1.5698
[2023-10-02 00:12:27] iter = 10560, loss = 1.5836
[2023-10-02 00:12:28] iter = 10570, loss = 1.6483
[2023-10-02 00:12:29] iter = 10580, loss = 1.6767
[2023-10-02 00:12:30] iter = 10590, loss = 1.5553
[2023-10-02 00:12:30] iter = 10600, loss = 1.5548
[2023-10-02 00:12:31] iter = 10610, loss = 1.6341
[2023-10-02 00:12:32] iter = 10620, loss = 1.4630
[2023-10-02 00:12:33] iter = 10630, loss = 1.5044
[2023-10-02 00:12:34] iter = 10640, loss = 1.5079
[2023-10-02 00:12:35] iter = 10650, loss = 1.5656
[2023-10-02 00:12:36] iter = 10660, loss = 1.4710
[2023-10-02 00:12:37] iter = 10670, loss = 1.5646
[2023-10-02 00:12:38] iter = 10680, loss = 1.4980
[2023-10-02 00:12:39] iter = 10690, loss = 1.5200
[2023-10-02 00:12:40] iter = 10700, loss = 1.7227
[2023-10-02 00:12:41] iter = 10710, loss = 1.5706
[2023-10-02 00:12:41] iter = 10720, loss = 1.4834
[2023-10-02 00:12:42] iter = 10730, loss = 1.6150
[2023-10-02 00:12:43] iter = 10740, loss = 1.5537
[2023-10-02 00:12:44] iter = 10750, loss = 1.5809
[2023-10-02 00:12:45] iter = 10760, loss = 1.5324
[2023-10-02 00:12:46] iter = 10770, loss = 1.5706
[2023-10-02 00:12:47] iter = 10780, loss = 1.5561
[2023-10-02 00:12:48] iter = 10790, loss = 1.3811
[2023-10-02 00:12:49] iter = 10800, loss = 1.7207
[2023-10-02 00:12:49] iter = 10810, loss = 1.4639
[2023-10-02 00:12:50] iter = 10820, loss = 1.4482
[2023-10-02 00:12:51] iter = 10830, loss = 1.6359
[2023-10-02 00:12:52] iter = 10840, loss = 1.5955
[2023-10-02 00:12:53] iter = 10850, loss = 1.6046
[2023-10-02 00:12:54] iter = 10860, loss = 1.5893
[2023-10-02 00:12:55] iter = 10870, loss = 1.5331
[2023-10-02 00:12:56] iter = 10880, loss = 1.5086
[2023-10-02 00:12:57] iter = 10890, loss = 1.5932
[2023-10-02 00:12:58] iter = 10900, loss = 1.6507
[2023-10-02 00:12:58] iter = 10910, loss = 1.6052
[2023-10-02 00:12:59] iter = 10920, loss = 1.5312
[2023-10-02 00:13:00] iter = 10930, loss = 1.4410
[2023-10-02 00:13:01] iter = 10940, loss = 1.6552
[2023-10-02 00:13:02] iter = 10950, loss = 1.6009
[2023-10-02 00:13:03] iter = 10960, loss = 1.6032
[2023-10-02 00:13:04] iter = 10970, loss = 1.4731
[2023-10-02 00:13:05] iter = 10980, loss = 1.4024
[2023-10-02 00:13:06] iter = 10990, loss = 1.7363
[2023-10-02 00:13:07] iter = 11000, loss = 1.4954
[2023-10-02 00:13:08] iter = 11010, loss = 1.6091
[2023-10-02 00:13:09] iter = 11020, loss = 1.4676
[2023-10-02 00:13:10] iter = 11030, loss = 1.5921
[2023-10-02 00:13:10] iter = 11040, loss = 1.4609
[2023-10-02 00:13:11] iter = 11050, loss = 1.5620
[2023-10-02 00:13:12] iter = 11060, loss = 1.4419
[2023-10-02 00:13:13] iter = 11070, loss = 1.5033
[2023-10-02 00:13:14] iter = 11080, loss = 1.5237
[2023-10-02 00:13:15] iter = 11090, loss = 1.7313
[2023-10-02 00:13:16] iter = 11100, loss = 1.4981
[2023-10-02 00:13:17] iter = 11110, loss = 1.4337
[2023-10-02 00:13:18] iter = 11120, loss = 1.5572
[2023-10-02 00:13:18] iter = 11130, loss = 1.5823
[2023-10-02 00:13:19] iter = 11140, loss = 1.4668
[2023-10-02 00:13:20] iter = 11150, loss = 1.4437
[2023-10-02 00:13:21] iter = 11160, loss = 1.5076
[2023-10-02 00:13:22] iter = 11170, loss = 1.5762
[2023-10-02 00:13:23] iter = 11180, loss = 1.6006
[2023-10-02 00:13:24] iter = 11190, loss = 1.2986
[2023-10-02 00:13:24] iter = 11200, loss = 1.6733
[2023-10-02 00:13:25] iter = 11210, loss = 1.5225
[2023-10-02 00:13:26] iter = 11220, loss = 1.6878
[2023-10-02 00:13:27] iter = 11230, loss = 1.5951
[2023-10-02 00:13:28] iter = 11240, loss = 1.3954
[2023-10-02 00:13:29] iter = 11250, loss = 1.6164
[2023-10-02 00:13:30] iter = 11260, loss = 1.5675
[2023-10-02 00:13:31] iter = 11270, loss = 1.5085
[2023-10-02 00:13:32] iter = 11280, loss = 1.5663
[2023-10-02 00:13:33] iter = 11290, loss = 1.4732
[2023-10-02 00:13:34] iter = 11300, loss = 1.4805
[2023-10-02 00:13:35] iter = 11310, loss = 1.5215
[2023-10-02 00:13:36] iter = 11320, loss = 1.5189
[2023-10-02 00:13:37] iter = 11330, loss = 1.5989
[2023-10-02 00:13:37] iter = 11340, loss = 1.4614
[2023-10-02 00:13:38] iter = 11350, loss = 1.4626
[2023-10-02 00:13:39] iter = 11360, loss = 1.5477
[2023-10-02 00:13:40] iter = 11370, loss = 1.6074
[2023-10-02 00:13:41] iter = 11380, loss = 1.6694
[2023-10-02 00:13:42] iter = 11390, loss = 1.6763
[2023-10-02 00:13:43] iter = 11400, loss = 1.4611
[2023-10-02 00:13:44] iter = 11410, loss = 1.6107
[2023-10-02 00:13:45] iter = 11420, loss = 1.6129
[2023-10-02 00:13:45] iter = 11430, loss = 1.5172
[2023-10-02 00:13:46] iter = 11440, loss = 1.5005
[2023-10-02 00:13:47] iter = 11450, loss = 1.5105
[2023-10-02 00:13:48] iter = 11460, loss = 1.5516
[2023-10-02 00:13:49] iter = 11470, loss = 1.4434
[2023-10-02 00:13:50] iter = 11480, loss = 1.5004
[2023-10-02 00:13:51] iter = 11490, loss = 1.7952
[2023-10-02 00:13:52] iter = 11500, loss = 1.6678
[2023-10-02 00:13:53] iter = 11510, loss = 1.6457
[2023-10-02 00:13:54] iter = 11520, loss = 1.4929
[2023-10-02 00:13:55] iter = 11530, loss = 1.5331
[2023-10-02 00:13:55] iter = 11540, loss = 1.4837
[2023-10-02 00:13:56] iter = 11550, loss = 1.4739
[2023-10-02 00:13:57] iter = 11560, loss = 1.5272
[2023-10-02 00:13:58] iter = 11570, loss = 1.5659
[2023-10-02 00:13:59] iter = 11580, loss = 1.5673
[2023-10-02 00:14:00] iter = 11590, loss = 1.6085
[2023-10-02 00:14:01] iter = 11600, loss = 1.3985
[2023-10-02 00:14:02] iter = 11610, loss = 1.5519
[2023-10-02 00:14:02] iter = 11620, loss = 1.6238
[2023-10-02 00:14:03] iter = 11630, loss = 1.6763
[2023-10-02 00:14:04] iter = 11640, loss = 1.3829
[2023-10-02 00:14:05] iter = 11650, loss = 1.4354
[2023-10-02 00:14:06] iter = 11660, loss = 1.6341
[2023-10-02 00:14:07] iter = 11670, loss = 1.7271
[2023-10-02 00:14:08] iter = 11680, loss = 1.5407
[2023-10-02 00:14:09] iter = 11690, loss = 1.4760
[2023-10-02 00:14:10] iter = 11700, loss = 1.6277
[2023-10-02 00:14:11] iter = 11710, loss = 1.4366
[2023-10-02 00:14:12] iter = 11720, loss = 1.5751
[2023-10-02 00:14:13] iter = 11730, loss = 1.4862
[2023-10-02 00:14:13] iter = 11740, loss = 1.4054
[2023-10-02 00:14:14] iter = 11750, loss = 1.4974
[2023-10-02 00:14:15] iter = 11760, loss = 1.4364
[2023-10-02 00:14:16] iter = 11770, loss = 1.5155
[2023-10-02 00:14:17] iter = 11780, loss = 1.5157
[2023-10-02 00:14:18] iter = 11790, loss = 1.6132
[2023-10-02 00:14:19] iter = 11800, loss = 1.5383
[2023-10-02 00:14:20] iter = 11810, loss = 1.6561
[2023-10-02 00:14:21] iter = 11820, loss = 1.4082
[2023-10-02 00:14:21] iter = 11830, loss = 1.5913
[2023-10-02 00:14:22] iter = 11840, loss = 1.4453
[2023-10-02 00:14:23] iter = 11850, loss = 1.4527
[2023-10-02 00:14:24] iter = 11860, loss = 1.5403
[2023-10-02 00:14:25] iter = 11870, loss = 1.6237
[2023-10-02 00:14:26] iter = 11880, loss = 1.5218
[2023-10-02 00:14:27] iter = 11890, loss = 1.5667
[2023-10-02 00:14:28] iter = 11900, loss = 1.5105
[2023-10-02 00:14:29] iter = 11910, loss = 1.5602
[2023-10-02 00:14:30] iter = 11920, loss = 1.4728
[2023-10-02 00:14:31] iter = 11930, loss = 1.5570
[2023-10-02 00:14:31] iter = 11940, loss = 1.4927
[2023-10-02 00:14:32] iter = 11950, loss = 1.3662
[2023-10-02 00:14:33] iter = 11960, loss = 1.5225
[2023-10-02 00:14:34] iter = 11970, loss = 1.4285
[2023-10-02 00:14:35] iter = 11980, loss = 1.5403
[2023-10-02 00:14:36] iter = 11990, loss = 1.4260
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 77205}
[2023-10-02 00:15:01] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.013793 train acc = 1.0000, test acc = 0.6270
[2023-10-02 00:15:25] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.009806 train acc = 1.0000, test acc = 0.6195
[2023-10-02 00:15:49] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.023418 train acc = 1.0000, test acc = 0.6296
[2023-10-02 00:16:14] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.008804 train acc = 1.0000, test acc = 0.6171
[2023-10-02 00:16:38] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002754 train acc = 1.0000, test acc = 0.6278
[2023-10-02 00:17:02] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013477 train acc = 1.0000, test acc = 0.6236
[2023-10-02 00:17:27] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.015099 train acc = 1.0000, test acc = 0.6246
[2023-10-02 00:17:51] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.012347 train acc = 1.0000, test acc = 0.6093
[2023-10-02 00:18:15] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014006 train acc = 0.9980, test acc = 0.6185
[2023-10-02 00:18:39] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003103 train acc = 1.0000, test acc = 0.6182
[2023-10-02 00:19:03] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.014971 train acc = 1.0000, test acc = 0.6291
[2023-10-02 00:19:27] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.023305 train acc = 1.0000, test acc = 0.6251
[2023-10-02 00:19:51] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.029507 train acc = 0.9940, test acc = 0.6223
[2023-10-02 00:20:16] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.022191 train acc = 1.0000, test acc = 0.6222
[2023-10-02 00:20:40] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013544 train acc = 1.0000, test acc = 0.6201
[2023-10-02 00:21:04] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.011512 train acc = 1.0000, test acc = 0.6234
[2023-10-02 00:21:28] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.023657 train acc = 0.9980, test acc = 0.6192
[2023-10-02 00:21:52] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004163 train acc = 1.0000, test acc = 0.6222
[2023-10-02 00:22:17] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002148 train acc = 1.0000, test acc = 0.6227
[2023-10-02 00:22:41] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.013544 train acc = 1.0000, test acc = 0.6216
Evaluate 20 random ConvNet, mean = 0.6222 std = 0.0046
-------------------------
[2023-10-02 00:22:41] iter = 12000, loss = 1.4228
[2023-10-02 00:22:42] iter = 12010, loss = 1.5511
[2023-10-02 00:22:43] iter = 12020, loss = 1.6669
[2023-10-02 00:22:44] iter = 12030, loss = 1.5457
[2023-10-02 00:22:45] iter = 12040, loss = 1.3740
[2023-10-02 00:22:45] iter = 12050, loss = 1.6898
[2023-10-02 00:22:46] iter = 12060, loss = 1.4537
[2023-10-02 00:22:47] iter = 12070, loss = 1.3553
[2023-10-02 00:22:48] iter = 12080, loss = 1.5175
[2023-10-02 00:22:49] iter = 12090, loss = 1.5356
[2023-10-02 00:22:50] iter = 12100, loss = 1.4481
[2023-10-02 00:22:51] iter = 12110, loss = 1.4459
[2023-10-02 00:22:52] iter = 12120, loss = 1.5629
[2023-10-02 00:22:53] iter = 12130, loss = 1.5138
[2023-10-02 00:22:54] iter = 12140, loss = 1.5517
[2023-10-02 00:22:55] iter = 12150, loss = 1.5889
[2023-10-02 00:22:56] iter = 12160, loss = 1.4138
[2023-10-02 00:22:56] iter = 12170, loss = 1.5281
[2023-10-02 00:22:57] iter = 12180, loss = 1.4524
[2023-10-02 00:22:58] iter = 12190, loss = 1.4994
[2023-10-02 00:22:59] iter = 12200, loss = 1.5518
[2023-10-02 00:23:00] iter = 12210, loss = 1.3827
[2023-10-02 00:23:01] iter = 12220, loss = 1.3865
[2023-10-02 00:23:02] iter = 12230, loss = 1.5494
[2023-10-02 00:23:03] iter = 12240, loss = 1.3387
[2023-10-02 00:23:04] iter = 12250, loss = 1.5374
[2023-10-02 00:23:05] iter = 12260, loss = 1.7317
[2023-10-02 00:23:05] iter = 12270, loss = 1.5532
[2023-10-02 00:23:06] iter = 12280, loss = 1.6106
[2023-10-02 00:23:07] iter = 12290, loss = 1.5773
[2023-10-02 00:23:08] iter = 12300, loss = 1.4236
[2023-10-02 00:23:09] iter = 12310, loss = 1.4853
[2023-10-02 00:23:10] iter = 12320, loss = 1.5979
[2023-10-02 00:23:11] iter = 12330, loss = 1.4984
[2023-10-02 00:23:12] iter = 12340, loss = 1.5141
[2023-10-02 00:23:13] iter = 12350, loss = 1.6218
[2023-10-02 00:23:14] iter = 12360, loss = 1.5223
[2023-10-02 00:23:15] iter = 12370, loss = 1.5785
[2023-10-02 00:23:15] iter = 12380, loss = 1.5856
[2023-10-02 00:23:16] iter = 12390, loss = 1.6090
[2023-10-02 00:23:17] iter = 12400, loss = 1.4892
[2023-10-02 00:23:18] iter = 12410, loss = 1.5349
[2023-10-02 00:23:19] iter = 12420, loss = 1.4190
[2023-10-02 00:23:20] iter = 12430, loss = 1.5943
[2023-10-02 00:23:21] iter = 12440, loss = 1.4925
[2023-10-02 00:23:22] iter = 12450, loss = 1.6645
[2023-10-02 00:23:23] iter = 12460, loss = 1.5246
[2023-10-02 00:23:23] iter = 12470, loss = 1.6035
[2023-10-02 00:23:24] iter = 12480, loss = 1.6210
[2023-10-02 00:23:25] iter = 12490, loss = 1.6820
[2023-10-02 00:23:26] iter = 12500, loss = 1.5411
[2023-10-02 00:23:27] iter = 12510, loss = 1.5783
[2023-10-02 00:23:28] iter = 12520, loss = 1.6543
[2023-10-02 00:23:29] iter = 12530, loss = 1.5018
[2023-10-02 00:23:30] iter = 12540, loss = 1.5548
[2023-10-02 00:23:31] iter = 12550, loss = 1.4850
[2023-10-02 00:23:32] iter = 12560, loss = 1.7272
[2023-10-02 00:23:33] iter = 12570, loss = 1.3721
[2023-10-02 00:23:33] iter = 12580, loss = 1.4980
[2023-10-02 00:23:34] iter = 12590, loss = 1.4304
[2023-10-02 00:23:35] iter = 12600, loss = 1.4853
[2023-10-02 00:23:36] iter = 12610, loss = 1.4588
[2023-10-02 00:23:37] iter = 12620, loss = 1.5455
[2023-10-02 00:23:38] iter = 12630, loss = 1.8267
[2023-10-02 00:23:39] iter = 12640, loss = 1.6158
[2023-10-02 00:23:40] iter = 12650, loss = 1.5263
[2023-10-02 00:23:41] iter = 12660, loss = 1.6873
[2023-10-02 00:23:41] iter = 12670, loss = 1.5773
[2023-10-02 00:23:42] iter = 12680, loss = 1.5334
[2023-10-02 00:23:43] iter = 12690, loss = 1.4297
[2023-10-02 00:23:44] iter = 12700, loss = 1.3671
[2023-10-02 00:23:45] iter = 12710, loss = 1.5860
[2023-10-02 00:23:46] iter = 12720, loss = 1.6483
[2023-10-02 00:23:47] iter = 12730, loss = 1.3821
[2023-10-02 00:23:48] iter = 12740, loss = 1.4483
[2023-10-02 00:23:49] iter = 12750, loss = 1.4486
[2023-10-02 00:23:50] iter = 12760, loss = 1.5317
[2023-10-02 00:23:50] iter = 12770, loss = 1.6773
[2023-10-02 00:23:51] iter = 12780, loss = 1.4886
[2023-10-02 00:23:52] iter = 12790, loss = 1.4826
[2023-10-02 00:23:53] iter = 12800, loss = 1.4443
[2023-10-02 00:23:54] iter = 12810, loss = 1.6135
[2023-10-02 00:23:55] iter = 12820, loss = 1.4743
[2023-10-02 00:23:56] iter = 12830, loss = 1.6300
[2023-10-02 00:23:57] iter = 12840, loss = 1.6794
[2023-10-02 00:23:58] iter = 12850, loss = 1.5889
[2023-10-02 00:23:58] iter = 12860, loss = 1.4967
[2023-10-02 00:23:59] iter = 12870, loss = 1.4681
[2023-10-02 00:24:00] iter = 12880, loss = 1.4092
[2023-10-02 00:24:01] iter = 12890, loss = 1.5181
[2023-10-02 00:24:02] iter = 12900, loss = 1.6608
[2023-10-02 00:24:03] iter = 12910, loss = 1.5069
[2023-10-02 00:24:04] iter = 12920, loss = 1.5483
[2023-10-02 00:24:05] iter = 12930, loss = 1.5577
[2023-10-02 00:24:06] iter = 12940, loss = 1.6043
[2023-10-02 00:24:07] iter = 12950, loss = 1.4305
[2023-10-02 00:24:07] iter = 12960, loss = 1.4236
[2023-10-02 00:24:08] iter = 12970, loss = 1.6693
[2023-10-02 00:24:09] iter = 12980, loss = 1.4129
[2023-10-02 00:24:10] iter = 12990, loss = 1.6316
[2023-10-02 00:24:11] iter = 13000, loss = 1.4659
[2023-10-02 00:24:12] iter = 13010, loss = 1.6121
[2023-10-02 00:24:13] iter = 13020, loss = 1.5061
[2023-10-02 00:24:14] iter = 13030, loss = 1.3404
[2023-10-02 00:24:15] iter = 13040, loss = 1.6736
[2023-10-02 00:24:16] iter = 13050, loss = 1.4578
[2023-10-02 00:24:16] iter = 13060, loss = 1.4554
[2023-10-02 00:24:17] iter = 13070, loss = 1.7034
[2023-10-02 00:24:18] iter = 13080, loss = 1.6122
[2023-10-02 00:24:19] iter = 13090, loss = 1.4995
[2023-10-02 00:24:20] iter = 13100, loss = 1.3611
[2023-10-02 00:24:21] iter = 13110, loss = 1.5739
[2023-10-02 00:24:22] iter = 13120, loss = 1.5473
[2023-10-02 00:24:23] iter = 13130, loss = 1.5166
[2023-10-02 00:24:24] iter = 13140, loss = 1.5512
[2023-10-02 00:24:24] iter = 13150, loss = 1.5235
[2023-10-02 00:24:25] iter = 13160, loss = 1.5214
[2023-10-02 00:24:26] iter = 13170, loss = 1.5261
[2023-10-02 00:24:27] iter = 13180, loss = 1.5666
[2023-10-02 00:24:28] iter = 13190, loss = 1.5640
[2023-10-02 00:24:29] iter = 13200, loss = 1.6980
[2023-10-02 00:24:30] iter = 13210, loss = 1.5796
[2023-10-02 00:24:31] iter = 13220, loss = 1.4833
[2023-10-02 00:24:32] iter = 13230, loss = 1.4993
[2023-10-02 00:24:33] iter = 13240, loss = 1.5602
[2023-10-02 00:24:34] iter = 13250, loss = 1.5408
[2023-10-02 00:24:34] iter = 13260, loss = 1.6118
[2023-10-02 00:24:35] iter = 13270, loss = 1.5440
[2023-10-02 00:24:36] iter = 13280, loss = 1.5477
[2023-10-02 00:24:37] iter = 13290, loss = 1.4919
[2023-10-02 00:24:38] iter = 13300, loss = 1.3290
[2023-10-02 00:24:39] iter = 13310, loss = 1.3995
[2023-10-02 00:24:40] iter = 13320, loss = 1.4220
[2023-10-02 00:24:41] iter = 13330, loss = 1.5025
[2023-10-02 00:24:42] iter = 13340, loss = 1.3305
[2023-10-02 00:24:43] iter = 13350, loss = 1.7152
[2023-10-02 00:24:44] iter = 13360, loss = 1.6339
[2023-10-02 00:24:45] iter = 13370, loss = 1.4642
[2023-10-02 00:24:45] iter = 13380, loss = 1.4535
[2023-10-02 00:24:46] iter = 13390, loss = 1.6071
[2023-10-02 00:24:47] iter = 13400, loss = 1.5811
[2023-10-02 00:24:48] iter = 13410, loss = 1.5511
[2023-10-02 00:24:49] iter = 13420, loss = 1.6104
[2023-10-02 00:24:50] iter = 13430, loss = 1.4961
[2023-10-02 00:24:51] iter = 13440, loss = 1.5059
[2023-10-02 00:24:52] iter = 13450, loss = 1.4895
[2023-10-02 00:24:53] iter = 13460, loss = 1.4937
[2023-10-02 00:24:54] iter = 13470, loss = 1.5206
[2023-10-02 00:24:55] iter = 13480, loss = 1.5417
[2023-10-02 00:24:56] iter = 13490, loss = 1.5223
[2023-10-02 00:24:57] iter = 13500, loss = 1.5247
[2023-10-02 00:24:57] iter = 13510, loss = 1.4293
[2023-10-02 00:24:58] iter = 13520, loss = 1.5287
[2023-10-02 00:24:59] iter = 13530, loss = 1.6027
[2023-10-02 00:25:00] iter = 13540, loss = 1.5903
[2023-10-02 00:25:01] iter = 13550, loss = 1.4976
[2023-10-02 00:25:02] iter = 13560, loss = 1.5312
[2023-10-02 00:25:03] iter = 13570, loss = 1.4064
[2023-10-02 00:25:04] iter = 13580, loss = 1.4586
[2023-10-02 00:25:05] iter = 13590, loss = 1.6656
[2023-10-02 00:25:06] iter = 13600, loss = 1.4673
[2023-10-02 00:25:07] iter = 13610, loss = 1.4587
[2023-10-02 00:25:08] iter = 13620, loss = 1.6200
[2023-10-02 00:25:09] iter = 13630, loss = 1.5400
[2023-10-02 00:25:09] iter = 13640, loss = 1.5427
[2023-10-02 00:25:10] iter = 13650, loss = 1.5274
[2023-10-02 00:25:11] iter = 13660, loss = 1.5713
[2023-10-02 00:25:12] iter = 13670, loss = 1.4913
[2023-10-02 00:25:13] iter = 13680, loss = 1.6357
[2023-10-02 00:25:14] iter = 13690, loss = 1.5482
[2023-10-02 00:25:15] iter = 13700, loss = 1.7262
[2023-10-02 00:25:16] iter = 13710, loss = 1.4406
[2023-10-02 00:25:17] iter = 13720, loss = 1.5207
[2023-10-02 00:25:18] iter = 13730, loss = 1.5984
[2023-10-02 00:25:19] iter = 13740, loss = 1.7259
[2023-10-02 00:25:19] iter = 13750, loss = 1.4977
[2023-10-02 00:25:20] iter = 13760, loss = 1.5730
[2023-10-02 00:25:21] iter = 13770, loss = 1.4692
[2023-10-02 00:25:22] iter = 13780, loss = 1.4482
[2023-10-02 00:25:23] iter = 13790, loss = 1.4077
[2023-10-02 00:25:24] iter = 13800, loss = 1.5189
[2023-10-02 00:25:25] iter = 13810, loss = 1.4634
[2023-10-02 00:25:26] iter = 13820, loss = 1.4427
[2023-10-02 00:25:27] iter = 13830, loss = 1.5259
[2023-10-02 00:25:28] iter = 13840, loss = 1.5338
[2023-10-02 00:25:29] iter = 13850, loss = 1.5317
[2023-10-02 00:25:30] iter = 13860, loss = 1.4294
[2023-10-02 00:25:30] iter = 13870, loss = 1.5428
[2023-10-02 00:25:31] iter = 13880, loss = 1.4618
[2023-10-02 00:25:32] iter = 13890, loss = 1.4937
[2023-10-02 00:25:33] iter = 13900, loss = 1.4517
[2023-10-02 00:25:34] iter = 13910, loss = 1.6041
[2023-10-02 00:25:35] iter = 13920, loss = 1.5047
[2023-10-02 00:25:36] iter = 13930, loss = 1.6244
[2023-10-02 00:25:37] iter = 13940, loss = 1.6320
[2023-10-02 00:25:38] iter = 13950, loss = 1.5377
[2023-10-02 00:25:39] iter = 13960, loss = 1.5840
[2023-10-02 00:25:39] iter = 13970, loss = 1.5060
[2023-10-02 00:25:40] iter = 13980, loss = 1.5002
[2023-10-02 00:25:41] iter = 13990, loss = 1.5376
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 42649}
[2023-10-02 00:26:06] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004227 train acc = 1.0000, test acc = 0.6322
[2023-10-02 00:26:30] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.005957 train acc = 1.0000, test acc = 0.6274
[2023-10-02 00:26:55] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.009345 train acc = 1.0000, test acc = 0.6270
[2023-10-02 00:27:19] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003790 train acc = 1.0000, test acc = 0.6257
[2023-10-02 00:27:43] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.027820 train acc = 1.0000, test acc = 0.6248
[2023-10-02 00:28:07] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.016800 train acc = 0.9980, test acc = 0.6272
[2023-10-02 00:28:31] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.035819 train acc = 0.9980, test acc = 0.6195
[2023-10-02 00:28:56] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.025122 train acc = 1.0000, test acc = 0.6188
[2023-10-02 00:29:20] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.012662 train acc = 1.0000, test acc = 0.6327
[2023-10-02 00:29:44] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.006398 train acc = 1.0000, test acc = 0.6176
[2023-10-02 00:30:08] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.007546 train acc = 1.0000, test acc = 0.6173
[2023-10-02 00:30:32] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003494 train acc = 1.0000, test acc = 0.6230
[2023-10-02 00:30:57] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.002465 train acc = 1.0000, test acc = 0.6203
[2023-10-02 00:31:21] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.011371 train acc = 1.0000, test acc = 0.6242
[2023-10-02 00:31:45] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004185 train acc = 1.0000, test acc = 0.6253
[2023-10-02 00:32:09] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.010298 train acc = 1.0000, test acc = 0.6248
[2023-10-02 00:32:33] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.016230 train acc = 1.0000, test acc = 0.6230
[2023-10-02 00:32:57] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.009816 train acc = 1.0000, test acc = 0.6290
[2023-10-02 00:33:21] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003372 train acc = 1.0000, test acc = 0.6302
[2023-10-02 00:33:46] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.005091 train acc = 1.0000, test acc = 0.6212
Evaluate 20 random ConvNet, mean = 0.6246 std = 0.0044
-------------------------
[2023-10-02 00:33:46] iter = 14000, loss = 1.5608
[2023-10-02 00:33:47] iter = 14010, loss = 1.4535
[2023-10-02 00:33:48] iter = 14020, loss = 1.6328
[2023-10-02 00:33:49] iter = 14030, loss = 1.6112
[2023-10-02 00:33:50] iter = 14040, loss = 1.3978
[2023-10-02 00:33:50] iter = 14050, loss = 1.4137
[2023-10-02 00:33:51] iter = 14060, loss = 1.4933
[2023-10-02 00:33:52] iter = 14070, loss = 1.6944
[2023-10-02 00:33:53] iter = 14080, loss = 1.4976
[2023-10-02 00:33:54] iter = 14090, loss = 1.6232
[2023-10-02 00:33:55] iter = 14100, loss = 1.4206
[2023-10-02 00:33:56] iter = 14110, loss = 1.4813
[2023-10-02 00:33:57] iter = 14120, loss = 1.5038
[2023-10-02 00:33:58] iter = 14130, loss = 1.5205
[2023-10-02 00:33:59] iter = 14140, loss = 1.4504
[2023-10-02 00:33:59] iter = 14150, loss = 1.5259
[2023-10-02 00:34:00] iter = 14160, loss = 1.5167
[2023-10-02 00:34:01] iter = 14170, loss = 1.5817
[2023-10-02 00:34:02] iter = 14180, loss = 1.3953
[2023-10-02 00:34:03] iter = 14190, loss = 1.4854
[2023-10-02 00:34:04] iter = 14200, loss = 1.3711
[2023-10-02 00:34:05] iter = 14210, loss = 1.6678
[2023-10-02 00:34:06] iter = 14220, loss = 1.4512
[2023-10-02 00:34:07] iter = 14230, loss = 1.3468
[2023-10-02 00:34:07] iter = 14240, loss = 1.4901
[2023-10-02 00:34:08] iter = 14250, loss = 1.4538
[2023-10-02 00:34:09] iter = 14260, loss = 1.5621
[2023-10-02 00:34:10] iter = 14270, loss = 1.4353
[2023-10-02 00:34:11] iter = 14280, loss = 1.4171
[2023-10-02 00:34:12] iter = 14290, loss = 1.5209
[2023-10-02 00:34:13] iter = 14300, loss = 1.4049
[2023-10-02 00:34:14] iter = 14310, loss = 1.5591
[2023-10-02 00:34:15] iter = 14320, loss = 1.3999
[2023-10-02 00:34:15] iter = 14330, loss = 1.4865
[2023-10-02 00:34:16] iter = 14340, loss = 1.4865
[2023-10-02 00:34:17] iter = 14350, loss = 1.5791
[2023-10-02 00:34:18] iter = 14360, loss = 1.6134
[2023-10-02 00:34:19] iter = 14370, loss = 1.6631
[2023-10-02 00:34:20] iter = 14380, loss = 1.6530
[2023-10-02 00:34:21] iter = 14390, loss = 1.5143
[2023-10-02 00:34:22] iter = 14400, loss = 1.4780
[2023-10-02 00:34:22] iter = 14410, loss = 1.4737
[2023-10-02 00:34:23] iter = 14420, loss = 1.6172
[2023-10-02 00:34:24] iter = 14430, loss = 1.5085
[2023-10-02 00:34:25] iter = 14440, loss = 1.5555
[2023-10-02 00:34:26] iter = 14450, loss = 1.4153
[2023-10-02 00:34:27] iter = 14460, loss = 1.6432
[2023-10-02 00:34:28] iter = 14470, loss = 1.4874
[2023-10-02 00:34:29] iter = 14480, loss = 1.5918
[2023-10-02 00:34:30] iter = 14490, loss = 1.5724
[2023-10-02 00:34:31] iter = 14500, loss = 1.4131
[2023-10-02 00:34:32] iter = 14510, loss = 1.5050
[2023-10-02 00:34:32] iter = 14520, loss = 1.4800
[2023-10-02 00:34:33] iter = 14530, loss = 1.5695
[2023-10-02 00:34:34] iter = 14540, loss = 1.5262
[2023-10-02 00:34:35] iter = 14550, loss = 1.3501
[2023-10-02 00:34:36] iter = 14560, loss = 1.5874
[2023-10-02 00:34:37] iter = 14570, loss = 1.5291
[2023-10-02 00:34:38] iter = 14580, loss = 1.4349
[2023-10-02 00:34:39] iter = 14590, loss = 1.7147
[2023-10-02 00:34:40] iter = 14600, loss = 1.5136
[2023-10-02 00:34:41] iter = 14610, loss = 1.5943
[2023-10-02 00:34:42] iter = 14620, loss = 1.5213
[2023-10-02 00:34:42] iter = 14630, loss = 1.4576
[2023-10-02 00:34:43] iter = 14640, loss = 1.6178
[2023-10-02 00:34:44] iter = 14650, loss = 1.5358
[2023-10-02 00:34:45] iter = 14660, loss = 1.5257
[2023-10-02 00:34:46] iter = 14670, loss = 1.6501
[2023-10-02 00:34:47] iter = 14680, loss = 1.5906
[2023-10-02 00:34:48] iter = 14690, loss = 1.4121
[2023-10-02 00:34:49] iter = 14700, loss = 1.4977
[2023-10-02 00:34:50] iter = 14710, loss = 1.4495
[2023-10-02 00:34:51] iter = 14720, loss = 1.5498
[2023-10-02 00:34:51] iter = 14730, loss = 1.4434
[2023-10-02 00:34:52] iter = 14740, loss = 1.4259
[2023-10-02 00:34:53] iter = 14750, loss = 1.3805
[2023-10-02 00:34:54] iter = 14760, loss = 1.5296
[2023-10-02 00:34:55] iter = 14770, loss = 1.3864
[2023-10-02 00:34:56] iter = 14780, loss = 1.4778
[2023-10-02 00:34:57] iter = 14790, loss = 1.5631
[2023-10-02 00:34:58] iter = 14800, loss = 1.4835
[2023-10-02 00:34:59] iter = 14810, loss = 1.4981
[2023-10-02 00:34:59] iter = 14820, loss = 1.6520
[2023-10-02 00:35:00] iter = 14830, loss = 1.4237
[2023-10-02 00:35:01] iter = 14840, loss = 1.4046
[2023-10-02 00:35:02] iter = 14850, loss = 1.5607
[2023-10-02 00:35:03] iter = 14860, loss = 1.3973
[2023-10-02 00:35:04] iter = 14870, loss = 1.4546
[2023-10-02 00:35:05] iter = 14880, loss = 1.4056
[2023-10-02 00:35:06] iter = 14890, loss = 1.4334
[2023-10-02 00:35:07] iter = 14900, loss = 1.5059
[2023-10-02 00:35:07] iter = 14910, loss = 1.5380
[2023-10-02 00:35:08] iter = 14920, loss = 1.4948
[2023-10-02 00:35:09] iter = 14930, loss = 1.4775
[2023-10-02 00:35:10] iter = 14940, loss = 1.3822
[2023-10-02 00:35:11] iter = 14950, loss = 1.5515
[2023-10-02 00:35:12] iter = 14960, loss = 1.5691
[2023-10-02 00:35:13] iter = 14970, loss = 1.5950
[2023-10-02 00:35:14] iter = 14980, loss = 1.7331
[2023-10-02 00:35:14] iter = 14990, loss = 1.5021
[2023-10-02 00:35:15] iter = 15000, loss = 1.4922
[2023-10-02 00:35:16] iter = 15010, loss = 1.4823
[2023-10-02 00:35:17] iter = 15020, loss = 1.6347
[2023-10-02 00:35:18] iter = 15030, loss = 1.5955
[2023-10-02 00:35:19] iter = 15040, loss = 1.5017
[2023-10-02 00:35:20] iter = 15050, loss = 1.4819
[2023-10-02 00:35:21] iter = 15060, loss = 1.5927
[2023-10-02 00:35:22] iter = 15070, loss = 1.6891
[2023-10-02 00:35:22] iter = 15080, loss = 1.5500
[2023-10-02 00:35:23] iter = 15090, loss = 1.6052
[2023-10-02 00:35:24] iter = 15100, loss = 1.5160
[2023-10-02 00:35:25] iter = 15110, loss = 1.4283
[2023-10-02 00:35:26] iter = 15120, loss = 1.7067
[2023-10-02 00:35:27] iter = 15130, loss = 1.6154
[2023-10-02 00:35:28] iter = 15140, loss = 1.5139
[2023-10-02 00:35:29] iter = 15150, loss = 1.5174
[2023-10-02 00:35:30] iter = 15160, loss = 1.3741
[2023-10-02 00:35:31] iter = 15170, loss = 1.5219
[2023-10-02 00:35:32] iter = 15180, loss = 1.4189
[2023-10-02 00:35:32] iter = 15190, loss = 1.4561
[2023-10-02 00:35:33] iter = 15200, loss = 1.3746
[2023-10-02 00:35:34] iter = 15210, loss = 1.4251
[2023-10-02 00:35:35] iter = 15220, loss = 1.4857
[2023-10-02 00:35:36] iter = 15230, loss = 1.4830
[2023-10-02 00:35:37] iter = 15240, loss = 1.5023
[2023-10-02 00:35:38] iter = 15250, loss = 1.5019
[2023-10-02 00:35:39] iter = 15260, loss = 1.4295
[2023-10-02 00:35:40] iter = 15270, loss = 1.4969
[2023-10-02 00:35:41] iter = 15280, loss = 1.4991
[2023-10-02 00:35:42] iter = 15290, loss = 1.4858
[2023-10-02 00:35:43] iter = 15300, loss = 1.5046
[2023-10-02 00:35:44] iter = 15310, loss = 1.6021
[2023-10-02 00:35:44] iter = 15320, loss = 1.5273
[2023-10-02 00:35:45] iter = 15330, loss = 1.4738
[2023-10-02 00:35:46] iter = 15340, loss = 1.5623
[2023-10-02 00:35:47] iter = 15350, loss = 1.5402
[2023-10-02 00:35:48] iter = 15360, loss = 1.4302
[2023-10-02 00:35:49] iter = 15370, loss = 1.5752
[2023-10-02 00:35:50] iter = 15380, loss = 1.4800
[2023-10-02 00:35:51] iter = 15390, loss = 1.4709
[2023-10-02 00:35:52] iter = 15400, loss = 1.5153
[2023-10-02 00:35:52] iter = 15410, loss = 1.4589
[2023-10-02 00:35:53] iter = 15420, loss = 1.6550
[2023-10-02 00:35:54] iter = 15430, loss = 1.7264
[2023-10-02 00:35:55] iter = 15440, loss = 1.5448
[2023-10-02 00:35:56] iter = 15450, loss = 1.4982
[2023-10-02 00:35:57] iter = 15460, loss = 1.6494
[2023-10-02 00:35:58] iter = 15470, loss = 1.5213
[2023-10-02 00:35:59] iter = 15480, loss = 1.4618
[2023-10-02 00:36:00] iter = 15490, loss = 1.4523
[2023-10-02 00:36:01] iter = 15500, loss = 1.5442
[2023-10-02 00:36:01] iter = 15510, loss = 1.5320
[2023-10-02 00:36:02] iter = 15520, loss = 1.5395
[2023-10-02 00:36:03] iter = 15530, loss = 1.3844
[2023-10-02 00:36:04] iter = 15540, loss = 1.5180
[2023-10-02 00:36:05] iter = 15550, loss = 1.5258
[2023-10-02 00:36:06] iter = 15560, loss = 1.5017
[2023-10-02 00:36:07] iter = 15570, loss = 1.5032
[2023-10-02 00:36:08] iter = 15580, loss = 1.4326
[2023-10-02 00:36:08] iter = 15590, loss = 1.3762
[2023-10-02 00:36:09] iter = 15600, loss = 1.4354
[2023-10-02 00:36:10] iter = 15610, loss = 1.4353
[2023-10-02 00:36:11] iter = 15620, loss = 1.4743
[2023-10-02 00:36:12] iter = 15630, loss = 1.5010
[2023-10-02 00:36:13] iter = 15640, loss = 1.6944
[2023-10-02 00:36:14] iter = 15650, loss = 1.6266
[2023-10-02 00:36:15] iter = 15660, loss = 1.4491
[2023-10-02 00:36:16] iter = 15670, loss = 1.5505
[2023-10-02 00:36:17] iter = 15680, loss = 1.5703
[2023-10-02 00:36:18] iter = 15690, loss = 1.5792
[2023-10-02 00:36:19] iter = 15700, loss = 1.7395
[2023-10-02 00:36:20] iter = 15710, loss = 1.5739
[2023-10-02 00:36:20] iter = 15720, loss = 1.4054
[2023-10-02 00:36:21] iter = 15730, loss = 1.5813
[2023-10-02 00:36:22] iter = 15740, loss = 1.7140
[2023-10-02 00:36:23] iter = 15750, loss = 1.3954
[2023-10-02 00:36:24] iter = 15760, loss = 1.4746
[2023-10-02 00:36:25] iter = 15770, loss = 1.5259
[2023-10-02 00:36:26] iter = 15780, loss = 1.5572
[2023-10-02 00:36:27] iter = 15790, loss = 1.5305
[2023-10-02 00:36:28] iter = 15800, loss = 1.5196
[2023-10-02 00:36:29] iter = 15810, loss = 1.4767
[2023-10-02 00:36:29] iter = 15820, loss = 1.5607
[2023-10-02 00:36:30] iter = 15830, loss = 1.3851
[2023-10-02 00:36:31] iter = 15840, loss = 1.4586
[2023-10-02 00:36:32] iter = 15850, loss = 1.4282
[2023-10-02 00:36:33] iter = 15860, loss = 1.5017
[2023-10-02 00:36:34] iter = 15870, loss = 1.6469
[2023-10-02 00:36:35] iter = 15880, loss = 1.5184
[2023-10-02 00:36:36] iter = 15890, loss = 1.7426
[2023-10-02 00:36:37] iter = 15900, loss = 1.4254
[2023-10-02 00:36:38] iter = 15910, loss = 1.4321
[2023-10-02 00:36:39] iter = 15920, loss = 1.4497
[2023-10-02 00:36:40] iter = 15930, loss = 1.4617
[2023-10-02 00:36:41] iter = 15940, loss = 1.5230
[2023-10-02 00:36:41] iter = 15950, loss = 1.4713
[2023-10-02 00:36:42] iter = 15960, loss = 1.5387
[2023-10-02 00:36:43] iter = 15970, loss = 1.3570
[2023-10-02 00:36:44] iter = 15980, loss = 1.4987
[2023-10-02 00:36:45] iter = 15990, loss = 1.5025
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 6227}
[2023-10-02 00:37:10] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.029006 train acc = 0.9980, test acc = 0.6169
[2023-10-02 00:37:34] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.005873 train acc = 1.0000, test acc = 0.6243
[2023-10-02 00:37:58] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.004516 train acc = 1.0000, test acc = 0.6195
[2023-10-02 00:38:22] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.025143 train acc = 0.9960, test acc = 0.6253
[2023-10-02 00:38:47] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.017995 train acc = 1.0000, test acc = 0.6270
[2023-10-02 00:39:11] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.023214 train acc = 1.0000, test acc = 0.6218
[2023-10-02 00:39:35] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.006039 train acc = 1.0000, test acc = 0.6229
[2023-10-02 00:39:59] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.010031 train acc = 1.0000, test acc = 0.6285
[2023-10-02 00:40:23] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.011762 train acc = 1.0000, test acc = 0.6210
[2023-10-02 00:40:47] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002149 train acc = 1.0000, test acc = 0.6234
[2023-10-02 00:41:12] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004288 train acc = 1.0000, test acc = 0.6234
[2023-10-02 00:41:36] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015987 train acc = 1.0000, test acc = 0.6218
[2023-10-02 00:42:00] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.015696 train acc = 1.0000, test acc = 0.6238
[2023-10-02 00:42:24] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.010178 train acc = 1.0000, test acc = 0.6265
[2023-10-02 00:42:48] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004091 train acc = 1.0000, test acc = 0.6278
[2023-10-02 00:43:12] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002275 train acc = 1.0000, test acc = 0.6235
[2023-10-02 00:43:36] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002378 train acc = 1.0000, test acc = 0.6192
[2023-10-02 00:44:01] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.001981 train acc = 1.0000, test acc = 0.6211
[2023-10-02 00:44:25] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013941 train acc = 1.0000, test acc = 0.6251
[2023-10-02 00:44:49] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003803 train acc = 1.0000, test acc = 0.6276
Evaluate 20 random ConvNet, mean = 0.6235 std = 0.0030
-------------------------
[2023-10-02 00:44:49] iter = 16000, loss = 1.3302
[2023-10-02 00:44:50] iter = 16010, loss = 1.6466
[2023-10-02 00:44:51] iter = 16020, loss = 1.3404
[2023-10-02 00:44:52] iter = 16030, loss = 1.5945
[2023-10-02 00:44:53] iter = 16040, loss = 1.5036
[2023-10-02 00:44:54] iter = 16050, loss = 1.5894
[2023-10-02 00:44:55] iter = 16060, loss = 1.5007
[2023-10-02 00:44:56] iter = 16070, loss = 1.4995
[2023-10-02 00:44:56] iter = 16080, loss = 1.5115
[2023-10-02 00:44:57] iter = 16090, loss = 1.4332
[2023-10-02 00:44:58] iter = 16100, loss = 1.6308
[2023-10-02 00:44:59] iter = 16110, loss = 1.7088
[2023-10-02 00:45:00] iter = 16120, loss = 1.5483
[2023-10-02 00:45:01] iter = 16130, loss = 1.5167
[2023-10-02 00:45:02] iter = 16140, loss = 1.4844
[2023-10-02 00:45:03] iter = 16150, loss = 1.3776
[2023-10-02 00:45:04] iter = 16160, loss = 1.4933
[2023-10-02 00:45:05] iter = 16170, loss = 1.4647
[2023-10-02 00:45:06] iter = 16180, loss = 1.6729
[2023-10-02 00:45:07] iter = 16190, loss = 1.5059
[2023-10-02 00:45:07] iter = 16200, loss = 1.6084
[2023-10-02 00:45:08] iter = 16210, loss = 1.6050
[2023-10-02 00:45:09] iter = 16220, loss = 1.4631
[2023-10-02 00:45:10] iter = 16230, loss = 1.6646
[2023-10-02 00:45:11] iter = 16240, loss = 1.5760
[2023-10-02 00:45:12] iter = 16250, loss = 1.5633
[2023-10-02 00:45:13] iter = 16260, loss = 1.4431
[2023-10-02 00:45:14] iter = 16270, loss = 1.4214
[2023-10-02 00:45:15] iter = 16280, loss = 1.5391
[2023-10-02 00:45:16] iter = 16290, loss = 1.4604
[2023-10-02 00:45:17] iter = 16300, loss = 1.3976
[2023-10-02 00:45:18] iter = 16310, loss = 1.4280
[2023-10-02 00:45:19] iter = 16320, loss = 1.5151
[2023-10-02 00:45:19] iter = 16330, loss = 1.4558
[2023-10-02 00:45:20] iter = 16340, loss = 1.5532
[2023-10-02 00:45:21] iter = 16350, loss = 1.5380
[2023-10-02 00:45:22] iter = 16360, loss = 1.5409
[2023-10-02 00:45:23] iter = 16370, loss = 1.5860
[2023-10-02 00:45:24] iter = 16380, loss = 1.5698
[2023-10-02 00:45:25] iter = 16390, loss = 1.7834
[2023-10-02 00:45:26] iter = 16400, loss = 1.4053
[2023-10-02 00:45:27] iter = 16410, loss = 1.4744
[2023-10-02 00:45:28] iter = 16420, loss = 1.4253
[2023-10-02 00:45:29] iter = 16430, loss = 1.4847
[2023-10-02 00:45:30] iter = 16440, loss = 1.3510
[2023-10-02 00:45:30] iter = 16450, loss = 1.4149
[2023-10-02 00:45:32] iter = 16460, loss = 1.4447
[2023-10-02 00:45:32] iter = 16470, loss = 1.4689
[2023-10-02 00:45:33] iter = 16480, loss = 1.4514
[2023-10-02 00:45:34] iter = 16490, loss = 1.5252
[2023-10-02 00:45:35] iter = 16500, loss = 1.5412
[2023-10-02 00:45:36] iter = 16510, loss = 1.5647
[2023-10-02 00:45:37] iter = 16520, loss = 1.4502
[2023-10-02 00:45:38] iter = 16530, loss = 1.5418
[2023-10-02 00:45:39] iter = 16540, loss = 1.4592
[2023-10-02 00:45:40] iter = 16550, loss = 1.6188
[2023-10-02 00:45:41] iter = 16560, loss = 1.5498
[2023-10-02 00:45:41] iter = 16570, loss = 1.6101
[2023-10-02 00:45:42] iter = 16580, loss = 1.5513
[2023-10-02 00:45:43] iter = 16590, loss = 1.4315
[2023-10-02 00:45:44] iter = 16600, loss = 1.5983
[2023-10-02 00:45:45] iter = 16610, loss = 1.4432
[2023-10-02 00:45:46] iter = 16620, loss = 1.4052
[2023-10-02 00:45:47] iter = 16630, loss = 1.3904
[2023-10-02 00:45:48] iter = 16640, loss = 1.4263
[2023-10-02 00:45:49] iter = 16650, loss = 1.4544
[2023-10-02 00:45:50] iter = 16660, loss = 1.7364
[2023-10-02 00:45:50] iter = 16670, loss = 1.4672
[2023-10-02 00:45:51] iter = 16680, loss = 1.4643
[2023-10-02 00:45:52] iter = 16690, loss = 1.3582
[2023-10-02 00:45:53] iter = 16700, loss = 1.4480
[2023-10-02 00:45:54] iter = 16710, loss = 1.3662
[2023-10-02 00:45:55] iter = 16720, loss = 1.5226
[2023-10-02 00:45:56] iter = 16730, loss = 1.5548
[2023-10-02 00:45:57] iter = 16740, loss = 1.5346
[2023-10-02 00:45:58] iter = 16750, loss = 1.4313
[2023-10-02 00:45:58] iter = 16760, loss = 1.4271
[2023-10-02 00:45:59] iter = 16770, loss = 1.5066
[2023-10-02 00:46:00] iter = 16780, loss = 1.5026
[2023-10-02 00:46:01] iter = 16790, loss = 1.4196
[2023-10-02 00:46:02] iter = 16800, loss = 1.4995
[2023-10-02 00:46:03] iter = 16810, loss = 1.4496
[2023-10-02 00:46:04] iter = 16820, loss = 1.5888
[2023-10-02 00:46:05] iter = 16830, loss = 1.6006
[2023-10-02 00:46:06] iter = 16840, loss = 1.4526
[2023-10-02 00:46:06] iter = 16850, loss = 1.5841
[2023-10-02 00:46:07] iter = 16860, loss = 1.5194
[2023-10-02 00:46:08] iter = 16870, loss = 1.4704
[2023-10-02 00:46:09] iter = 16880, loss = 1.6532
[2023-10-02 00:46:10] iter = 16890, loss = 1.3947
[2023-10-02 00:46:11] iter = 16900, loss = 1.4878
[2023-10-02 00:46:12] iter = 16910, loss = 1.4705
[2023-10-02 00:46:13] iter = 16920, loss = 1.4962
[2023-10-02 00:46:14] iter = 16930, loss = 1.5073
[2023-10-02 00:46:15] iter = 16940, loss = 1.5435
[2023-10-02 00:46:16] iter = 16950, loss = 1.5106
[2023-10-02 00:46:16] iter = 16960, loss = 1.5806
[2023-10-02 00:46:17] iter = 16970, loss = 1.4740
[2023-10-02 00:46:18] iter = 16980, loss = 1.4420
[2023-10-02 00:46:19] iter = 16990, loss = 1.4939
[2023-10-02 00:46:20] iter = 17000, loss = 1.4276
[2023-10-02 00:46:21] iter = 17010, loss = 1.5539
[2023-10-02 00:46:22] iter = 17020, loss = 1.6940
[2023-10-02 00:46:23] iter = 17030, loss = 1.3912
[2023-10-02 00:46:24] iter = 17040, loss = 1.5044
[2023-10-02 00:46:25] iter = 17050, loss = 1.2969
[2023-10-02 00:46:26] iter = 17060, loss = 1.4855
[2023-10-02 00:46:26] iter = 17070, loss = 1.3933
[2023-10-02 00:46:27] iter = 17080, loss = 1.4715
[2023-10-02 00:46:28] iter = 17090, loss = 1.3834
[2023-10-02 00:46:29] iter = 17100, loss = 1.5076
[2023-10-02 00:46:30] iter = 17110, loss = 1.3707
[2023-10-02 00:46:31] iter = 17120, loss = 1.5132
[2023-10-02 00:46:32] iter = 17130, loss = 1.3482
[2023-10-02 00:46:33] iter = 17140, loss = 1.6026
[2023-10-02 00:46:34] iter = 17150, loss = 1.5864
[2023-10-02 00:46:35] iter = 17160, loss = 1.4691
[2023-10-02 00:46:36] iter = 17170, loss = 1.4536
[2023-10-02 00:46:37] iter = 17180, loss = 1.5756
[2023-10-02 00:46:38] iter = 17190, loss = 1.4351
[2023-10-02 00:46:38] iter = 17200, loss = 1.4238
[2023-10-02 00:46:39] iter = 17210, loss = 1.4406
[2023-10-02 00:46:40] iter = 17220, loss = 1.4914
[2023-10-02 00:46:41] iter = 17230, loss = 1.5551
[2023-10-02 00:46:42] iter = 17240, loss = 1.5270
[2023-10-02 00:46:43] iter = 17250, loss = 1.3913
[2023-10-02 00:46:44] iter = 17260, loss = 1.4191
[2023-10-02 00:46:45] iter = 17270, loss = 1.5511
[2023-10-02 00:46:46] iter = 17280, loss = 1.5700
[2023-10-02 00:46:47] iter = 17290, loss = 1.4577
[2023-10-02 00:46:48] iter = 17300, loss = 1.4081
[2023-10-02 00:46:48] iter = 17310, loss = 1.4521
[2023-10-02 00:46:49] iter = 17320, loss = 1.6115
[2023-10-02 00:46:50] iter = 17330, loss = 1.4353
[2023-10-02 00:46:51] iter = 17340, loss = 1.7204
[2023-10-02 00:46:52] iter = 17350, loss = 1.4638
[2023-10-02 00:46:53] iter = 17360, loss = 1.5670
[2023-10-02 00:46:54] iter = 17370, loss = 1.5681
[2023-10-02 00:46:55] iter = 17380, loss = 1.5207
[2023-10-02 00:46:56] iter = 17390, loss = 1.4146
[2023-10-02 00:46:57] iter = 17400, loss = 1.5811
[2023-10-02 00:46:58] iter = 17410, loss = 1.4631
[2023-10-02 00:46:58] iter = 17420, loss = 1.5071
[2023-10-02 00:46:59] iter = 17430, loss = 1.4805
[2023-10-02 00:47:00] iter = 17440, loss = 1.4668
[2023-10-02 00:47:01] iter = 17450, loss = 1.5870
[2023-10-02 00:47:02] iter = 17460, loss = 1.6375
[2023-10-02 00:47:03] iter = 17470, loss = 1.8088
[2023-10-02 00:47:04] iter = 17480, loss = 1.5147
[2023-10-02 00:47:05] iter = 17490, loss = 1.5399
[2023-10-02 00:47:06] iter = 17500, loss = 1.5837
[2023-10-02 00:47:06] iter = 17510, loss = 1.4955
[2023-10-02 00:47:07] iter = 17520, loss = 1.5698
[2023-10-02 00:47:08] iter = 17530, loss = 1.4824
[2023-10-02 00:47:09] iter = 17540, loss = 1.5852
[2023-10-02 00:47:10] iter = 17550, loss = 1.3829
[2023-10-02 00:47:11] iter = 17560, loss = 1.5036
[2023-10-02 00:47:12] iter = 17570, loss = 1.5795
[2023-10-02 00:47:13] iter = 17580, loss = 1.4336
[2023-10-02 00:47:14] iter = 17590, loss = 1.5741
[2023-10-02 00:47:15] iter = 17600, loss = 1.7151
[2023-10-02 00:47:15] iter = 17610, loss = 1.4956
[2023-10-02 00:47:16] iter = 17620, loss = 1.5469
[2023-10-02 00:47:17] iter = 17630, loss = 1.6449
[2023-10-02 00:47:18] iter = 17640, loss = 1.5017
[2023-10-02 00:47:19] iter = 17650, loss = 1.3887
[2023-10-02 00:47:20] iter = 17660, loss = 1.4168
[2023-10-02 00:47:21] iter = 17670, loss = 1.4893
[2023-10-02 00:47:22] iter = 17680, loss = 1.5485
[2023-10-02 00:47:23] iter = 17690, loss = 1.5211
[2023-10-02 00:47:24] iter = 17700, loss = 1.3924
[2023-10-02 00:47:25] iter = 17710, loss = 1.5419
[2023-10-02 00:47:25] iter = 17720, loss = 1.3842
[2023-10-02 00:47:26] iter = 17730, loss = 1.4729
[2023-10-02 00:47:27] iter = 17740, loss = 1.4724
[2023-10-02 00:47:28] iter = 17750, loss = 1.5440
[2023-10-02 00:47:29] iter = 17760, loss = 1.4079
[2023-10-02 00:47:30] iter = 17770, loss = 1.4649
[2023-10-02 00:47:31] iter = 17780, loss = 1.3718
[2023-10-02 00:47:32] iter = 17790, loss = 1.5844
[2023-10-02 00:47:33] iter = 17800, loss = 1.3158
[2023-10-02 00:47:34] iter = 17810, loss = 1.4155
[2023-10-02 00:47:34] iter = 17820, loss = 1.5314
[2023-10-02 00:47:35] iter = 17830, loss = 1.2965
[2023-10-02 00:47:36] iter = 17840, loss = 1.5060
[2023-10-02 00:47:37] iter = 17850, loss = 1.4221
[2023-10-02 00:47:38] iter = 17860, loss = 1.5678
[2023-10-02 00:47:39] iter = 17870, loss = 1.4458
[2023-10-02 00:47:40] iter = 17880, loss = 1.4703
[2023-10-02 00:47:41] iter = 17890, loss = 1.4670
[2023-10-02 00:47:42] iter = 17900, loss = 1.4287
[2023-10-02 00:47:43] iter = 17910, loss = 1.3904
[2023-10-02 00:47:43] iter = 17920, loss = 1.4806
[2023-10-02 00:47:44] iter = 17930, loss = 1.4995
[2023-10-02 00:47:45] iter = 17940, loss = 1.4145
[2023-10-02 00:47:46] iter = 17950, loss = 1.5376
[2023-10-02 00:47:47] iter = 17960, loss = 1.5074
[2023-10-02 00:47:48] iter = 17970, loss = 1.4205
[2023-10-02 00:47:49] iter = 17980, loss = 1.3491
[2023-10-02 00:47:50] iter = 17990, loss = 1.5892
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 70959}
[2023-10-02 00:48:15] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004092 train acc = 1.0000, test acc = 0.6182
[2023-10-02 00:48:39] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.002865 train acc = 1.0000, test acc = 0.6229
[2023-10-02 00:49:04] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.031899 train acc = 0.9980, test acc = 0.6296
[2023-10-02 00:49:28] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.031556 train acc = 0.9960, test acc = 0.6274
[2023-10-02 00:49:52] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004465 train acc = 1.0000, test acc = 0.6262
[2023-10-02 00:50:16] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003888 train acc = 1.0000, test acc = 0.6273
[2023-10-02 00:50:40] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.022654 train acc = 1.0000, test acc = 0.6290
[2023-10-02 00:51:04] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003537 train acc = 1.0000, test acc = 0.6294
[2023-10-02 00:51:29] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.013834 train acc = 1.0000, test acc = 0.6319
[2023-10-02 00:51:53] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013492 train acc = 0.9980, test acc = 0.6233
[2023-10-02 00:52:17] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003725 train acc = 1.0000, test acc = 0.6254
[2023-10-02 00:52:41] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.017142 train acc = 0.9980, test acc = 0.6239
[2023-10-02 00:53:05] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004769 train acc = 1.0000, test acc = 0.6196
[2023-10-02 00:53:30] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004685 train acc = 1.0000, test acc = 0.6278
[2023-10-02 00:53:54] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002014 train acc = 1.0000, test acc = 0.6308
[2023-10-02 00:54:18] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.002259 train acc = 1.0000, test acc = 0.6318
[2023-10-02 00:54:42] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.013220 train acc = 1.0000, test acc = 0.6250
[2023-10-02 00:55:06] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004427 train acc = 1.0000, test acc = 0.6217
[2023-10-02 00:55:30] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.012334 train acc = 1.0000, test acc = 0.6248
[2023-10-02 00:55:54] Evaluate_19: epoch = 1000 train time = 21 s train loss = 0.018272 train acc = 1.0000, test acc = 0.6273
Evaluate 20 random ConvNet, mean = 0.6262 std = 0.0037
-------------------------
[2023-10-02 00:55:54] iter = 18000, loss = 1.3356
[2023-10-02 00:55:55] iter = 18010, loss = 1.4268
[2023-10-02 00:55:56] iter = 18020, loss = 1.5194
[2023-10-02 00:55:57] iter = 18030, loss = 1.5383
[2023-10-02 00:55:58] iter = 18040, loss = 1.5325
[2023-10-02 00:55:59] iter = 18050, loss = 1.5354
[2023-10-02 00:56:00] iter = 18060, loss = 1.5600
[2023-10-02 00:56:01] iter = 18070, loss = 1.4805
[2023-10-02 00:56:01] iter = 18080, loss = 1.5755
[2023-10-02 00:56:02] iter = 18090, loss = 1.4352
[2023-10-02 00:56:03] iter = 18100, loss = 1.5602
[2023-10-02 00:56:04] iter = 18110, loss = 1.5695
[2023-10-02 00:56:05] iter = 18120, loss = 1.3940
[2023-10-02 00:56:06] iter = 18130, loss = 1.3898
[2023-10-02 00:56:07] iter = 18140, loss = 1.5731
[2023-10-02 00:56:08] iter = 18150, loss = 1.4378
[2023-10-02 00:56:09] iter = 18160, loss = 1.5103
[2023-10-02 00:56:10] iter = 18170, loss = 1.4308
[2023-10-02 00:56:11] iter = 18180, loss = 1.5657
[2023-10-02 00:56:11] iter = 18190, loss = 1.3946
[2023-10-02 00:56:12] iter = 18200, loss = 1.5606
[2023-10-02 00:56:13] iter = 18210, loss = 1.4978
[2023-10-02 00:56:14] iter = 18220, loss = 1.4099
[2023-10-02 00:56:15] iter = 18230, loss = 1.5396
[2023-10-02 00:56:16] iter = 18240, loss = 1.5528
[2023-10-02 00:56:17] iter = 18250, loss = 1.5669
[2023-10-02 00:56:18] iter = 18260, loss = 1.4634
[2023-10-02 00:56:19] iter = 18270, loss = 1.4706
[2023-10-02 00:56:19] iter = 18280, loss = 1.5154
[2023-10-02 00:56:20] iter = 18290, loss = 1.4823
[2023-10-02 00:56:21] iter = 18300, loss = 1.4001
[2023-10-02 00:56:22] iter = 18310, loss = 1.4858
[2023-10-02 00:56:23] iter = 18320, loss = 1.4697
[2023-10-02 00:56:24] iter = 18330, loss = 1.5332
[2023-10-02 00:56:25] iter = 18340, loss = 1.4975
[2023-10-02 00:56:26] iter = 18350, loss = 1.5863
[2023-10-02 00:56:27] iter = 18360, loss = 1.6571
[2023-10-02 00:56:28] iter = 18370, loss = 1.5207
[2023-10-02 00:56:29] iter = 18380, loss = 1.3997
[2023-10-02 00:56:30] iter = 18390, loss = 1.5904
[2023-10-02 00:56:31] iter = 18400, loss = 1.5410
[2023-10-02 00:56:32] iter = 18410, loss = 1.3919
[2023-10-02 00:56:33] iter = 18420, loss = 1.4862
[2023-10-02 00:56:33] iter = 18430, loss = 1.4001
[2023-10-02 00:56:34] iter = 18440, loss = 1.5113
[2023-10-02 00:56:35] iter = 18450, loss = 1.5249
[2023-10-02 00:56:36] iter = 18460, loss = 1.4805
[2023-10-02 00:56:37] iter = 18470, loss = 1.4696
[2023-10-02 00:56:38] iter = 18480, loss = 1.4334
[2023-10-02 00:56:39] iter = 18490, loss = 1.5012
[2023-10-02 00:56:40] iter = 18500, loss = 1.6004
[2023-10-02 00:56:40] iter = 18510, loss = 1.4528
[2023-10-02 00:56:41] iter = 18520, loss = 1.6803
[2023-10-02 00:56:42] iter = 18530, loss = 1.4113
[2023-10-02 00:56:43] iter = 18540, loss = 1.5087
[2023-10-02 00:56:44] iter = 18550, loss = 1.5475
[2023-10-02 00:56:45] iter = 18560, loss = 1.4825
[2023-10-02 00:56:46] iter = 18570, loss = 1.5668
[2023-10-02 00:56:47] iter = 18580, loss = 1.4272
[2023-10-02 00:56:48] iter = 18590, loss = 1.4687
[2023-10-02 00:56:49] iter = 18600, loss = 1.3293
[2023-10-02 00:56:50] iter = 18610, loss = 1.5912
[2023-10-02 00:56:51] iter = 18620, loss = 1.5130
[2023-10-02 00:56:51] iter = 18630, loss = 1.4802
[2023-10-02 00:56:52] iter = 18640, loss = 1.4083
[2023-10-02 00:56:53] iter = 18650, loss = 1.4076
[2023-10-02 00:56:54] iter = 18660, loss = 1.4484
[2023-10-02 00:56:55] iter = 18670, loss = 1.4600
[2023-10-02 00:56:56] iter = 18680, loss = 1.4880
[2023-10-02 00:56:57] iter = 18690, loss = 1.5247
[2023-10-02 00:56:58] iter = 18700, loss = 1.5069
[2023-10-02 00:56:59] iter = 18710, loss = 1.6012
[2023-10-02 00:57:00] iter = 18720, loss = 1.5498
[2023-10-02 00:57:00] iter = 18730, loss = 1.4392
[2023-10-02 00:57:01] iter = 18740, loss = 1.4683
[2023-10-02 00:57:02] iter = 18750, loss = 1.5571
[2023-10-02 00:57:03] iter = 18760, loss = 1.4615
[2023-10-02 00:57:04] iter = 18770, loss = 1.5193
[2023-10-02 00:57:05] iter = 18780, loss = 1.4664
[2023-10-02 00:57:06] iter = 18790, loss = 1.3794
[2023-10-02 00:57:07] iter = 18800, loss = 1.4239
[2023-10-02 00:57:07] iter = 18810, loss = 1.4054
[2023-10-02 00:57:08] iter = 18820, loss = 1.5953
[2023-10-02 00:57:09] iter = 18830, loss = 1.3852
[2023-10-02 00:57:10] iter = 18840, loss = 1.6173
[2023-10-02 00:57:11] iter = 18850, loss = 1.3934
[2023-10-02 00:57:12] iter = 18860, loss = 1.3878
[2023-10-02 00:57:13] iter = 18870, loss = 1.4186
[2023-10-02 00:57:14] iter = 18880, loss = 1.3850
[2023-10-02 00:57:15] iter = 18890, loss = 1.4568
[2023-10-02 00:57:16] iter = 18900, loss = 1.5600
[2023-10-02 00:57:16] iter = 18910, loss = 1.4624
[2023-10-02 00:57:17] iter = 18920, loss = 1.5192
[2023-10-02 00:57:18] iter = 18930, loss = 1.4237
[2023-10-02 00:57:19] iter = 18940, loss = 1.5113
[2023-10-02 00:57:20] iter = 18950, loss = 1.3661
[2023-10-02 00:57:21] iter = 18960, loss = 1.4908
[2023-10-02 00:57:22] iter = 18970, loss = 1.4599
[2023-10-02 00:57:23] iter = 18980, loss = 1.5084
[2023-10-02 00:57:24] iter = 18990, loss = 1.4416
[2023-10-02 00:57:24] iter = 19000, loss = 1.3648
[2023-10-02 00:57:25] iter = 19010, loss = 1.5534
[2023-10-02 00:57:26] iter = 19020, loss = 1.4506
[2023-10-02 00:57:27] iter = 19030, loss = 1.7641
[2023-10-02 00:57:28] iter = 19040, loss = 1.6911
[2023-10-02 00:57:29] iter = 19050, loss = 1.4627
[2023-10-02 00:57:30] iter = 19060, loss = 1.3332
[2023-10-02 00:57:31] iter = 19070, loss = 1.4925
[2023-10-02 00:57:32] iter = 19080, loss = 1.3917
[2023-10-02 00:57:33] iter = 19090, loss = 1.5046
[2023-10-02 00:57:33] iter = 19100, loss = 1.4497
[2023-10-02 00:57:34] iter = 19110, loss = 1.4115
[2023-10-02 00:57:35] iter = 19120, loss = 1.4482
[2023-10-02 00:57:36] iter = 19130, loss = 1.4452
[2023-10-02 00:57:37] iter = 19140, loss = 1.3826
[2023-10-02 00:57:38] iter = 19150, loss = 1.5858
[2023-10-02 00:57:39] iter = 19160, loss = 1.4617
[2023-10-02 00:57:40] iter = 19170, loss = 1.4617
[2023-10-02 00:57:41] iter = 19180, loss = 1.4697
[2023-10-02 00:57:42] iter = 19190, loss = 1.3315
[2023-10-02 00:57:43] iter = 19200, loss = 1.4198
[2023-10-02 00:57:44] iter = 19210, loss = 1.4574
[2023-10-02 00:57:44] iter = 19220, loss = 1.5508
[2023-10-02 00:57:45] iter = 19230, loss = 1.5059
[2023-10-02 00:57:46] iter = 19240, loss = 1.3804
[2023-10-02 00:57:47] iter = 19250, loss = 1.4384
[2023-10-02 00:57:48] iter = 19260, loss = 1.4743
[2023-10-02 00:57:49] iter = 19270, loss = 1.4005
[2023-10-02 00:57:50] iter = 19280, loss = 1.5721
[2023-10-02 00:57:51] iter = 19290, loss = 1.4664
[2023-10-02 00:57:52] iter = 19300, loss = 1.4201
[2023-10-02 00:57:52] iter = 19310, loss = 1.5731
[2023-10-02 00:57:53] iter = 19320, loss = 1.3793
[2023-10-02 00:57:54] iter = 19330, loss = 1.6321
[2023-10-02 00:57:55] iter = 19340, loss = 1.5948
[2023-10-02 00:57:56] iter = 19350, loss = 1.4794
[2023-10-02 00:57:57] iter = 19360, loss = 1.4471
[2023-10-02 00:57:58] iter = 19370, loss = 1.4832
[2023-10-02 00:57:59] iter = 19380, loss = 1.5576
[2023-10-02 00:58:00] iter = 19390, loss = 1.5668
[2023-10-02 00:58:01] iter = 19400, loss = 1.4688
[2023-10-02 00:58:02] iter = 19410, loss = 1.5258
[2023-10-02 00:58:03] iter = 19420, loss = 1.4881
[2023-10-02 00:58:03] iter = 19430, loss = 1.5083
[2023-10-02 00:58:04] iter = 19440, loss = 1.3467
[2023-10-02 00:58:05] iter = 19450, loss = 1.5065
[2023-10-02 00:58:06] iter = 19460, loss = 1.3490
[2023-10-02 00:58:07] iter = 19470, loss = 1.4008
[2023-10-02 00:58:08] iter = 19480, loss = 1.4538
[2023-10-02 00:58:09] iter = 19490, loss = 1.4256
[2023-10-02 00:58:10] iter = 19500, loss = 1.3216
[2023-10-02 00:58:11] iter = 19510, loss = 1.4038
[2023-10-02 00:58:12] iter = 19520, loss = 1.3036
[2023-10-02 00:58:13] iter = 19530, loss = 1.5407
[2023-10-02 00:58:14] iter = 19540, loss = 1.4615
[2023-10-02 00:58:15] iter = 19550, loss = 1.4503
[2023-10-02 00:58:16] iter = 19560, loss = 1.5045
[2023-10-02 00:58:16] iter = 19570, loss = 1.5318
[2023-10-02 00:58:17] iter = 19580, loss = 1.4397
[2023-10-02 00:58:18] iter = 19590, loss = 1.5789
[2023-10-02 00:58:19] iter = 19600, loss = 1.5899
[2023-10-02 00:58:20] iter = 19610, loss = 1.6403
[2023-10-02 00:58:21] iter = 19620, loss = 1.4194
[2023-10-02 00:58:22] iter = 19630, loss = 1.5409
[2023-10-02 00:58:23] iter = 19640, loss = 1.5630
[2023-10-02 00:58:24] iter = 19650, loss = 1.5121
[2023-10-02 00:58:24] iter = 19660, loss = 1.4697
[2023-10-02 00:58:25] iter = 19670, loss = 1.5661
[2023-10-02 00:58:26] iter = 19680, loss = 1.4393
[2023-10-02 00:58:27] iter = 19690, loss = 1.4513
[2023-10-02 00:58:28] iter = 19700, loss = 1.4785
[2023-10-02 00:58:29] iter = 19710, loss = 1.4137
[2023-10-02 00:58:30] iter = 19720, loss = 1.5609
[2023-10-02 00:58:31] iter = 19730, loss = 1.5242
[2023-10-02 00:58:31] iter = 19740, loss = 1.5782
[2023-10-02 00:58:32] iter = 19750, loss = 1.6168
[2023-10-02 00:58:33] iter = 19760, loss = 1.4062
[2023-10-02 00:58:34] iter = 19770, loss = 1.3934
[2023-10-02 00:58:35] iter = 19780, loss = 1.5588
[2023-10-02 00:58:36] iter = 19790, loss = 1.5539
[2023-10-02 00:58:37] iter = 19800, loss = 1.4750
[2023-10-02 00:58:38] iter = 19810, loss = 1.4082
[2023-10-02 00:58:39] iter = 19820, loss = 1.4496
[2023-10-02 00:58:40] iter = 19830, loss = 1.6706
[2023-10-02 00:58:40] iter = 19840, loss = 1.5299
[2023-10-02 00:58:41] iter = 19850, loss = 1.4214
[2023-10-02 00:58:42] iter = 19860, loss = 1.5487
[2023-10-02 00:58:43] iter = 19870, loss = 1.6422
[2023-10-02 00:58:44] iter = 19880, loss = 1.5826
[2023-10-02 00:58:45] iter = 19890, loss = 1.4978
[2023-10-02 00:58:46] iter = 19900, loss = 1.5018
[2023-10-02 00:58:47] iter = 19910, loss = 1.4058
[2023-10-02 00:58:48] iter = 19920, loss = 1.4093
[2023-10-02 00:58:49] iter = 19930, loss = 1.5087
[2023-10-02 00:58:50] iter = 19940, loss = 1.5616
[2023-10-02 00:58:50] iter = 19950, loss = 1.5793
[2023-10-02 00:58:51] iter = 19960, loss = 1.4009
[2023-10-02 00:58:52] iter = 19970, loss = 1.4291
[2023-10-02 00:58:53] iter = 19980, loss = 1.4470
[2023-10-02 00:58:54] iter = 19990, loss = 1.4968
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 35418}
[2023-10-02 00:59:19] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.014642 train acc = 1.0000, test acc = 0.6292
[2023-10-02 00:59:43] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003370 train acc = 1.0000, test acc = 0.6264
[2023-10-02 01:00:07] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.009237 train acc = 1.0000, test acc = 0.6367
[2023-10-02 01:00:31] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.019686 train acc = 1.0000, test acc = 0.6234
[2023-10-02 01:00:56] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.025389 train acc = 0.9980, test acc = 0.6282
[2023-10-02 01:01:20] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004642 train acc = 1.0000, test acc = 0.6325
[2023-10-02 01:01:44] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002864 train acc = 1.0000, test acc = 0.6205
[2023-10-02 01:02:08] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.006283 train acc = 1.0000, test acc = 0.6235
[2023-10-02 01:02:32] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.015421 train acc = 1.0000, test acc = 0.6254
[2023-10-02 01:02:57] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.021299 train acc = 1.0000, test acc = 0.6247
[2023-10-02 01:03:21] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.014407 train acc = 1.0000, test acc = 0.6233
[2023-10-02 01:03:45] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002262 train acc = 1.0000, test acc = 0.6296
[2023-10-02 01:04:09] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004542 train acc = 1.0000, test acc = 0.6265
[2023-10-02 01:04:34] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.014753 train acc = 1.0000, test acc = 0.6239
[2023-10-02 01:04:58] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.014146 train acc = 1.0000, test acc = 0.6254
[2023-10-02 01:05:22] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.016607 train acc = 1.0000, test acc = 0.6264
[2023-10-02 01:05:46] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004296 train acc = 1.0000, test acc = 0.6274
[2023-10-02 01:06:10] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.016107 train acc = 1.0000, test acc = 0.6296
[2023-10-02 01:06:34] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003006 train acc = 1.0000, test acc = 0.6302
[2023-10-02 01:06:59] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004296 train acc = 1.0000, test acc = 0.6244
Evaluate 20 random ConvNet, mean = 0.6269 std = 0.0036
-------------------------
[2023-10-02 01:06:59] iter = 20000, loss = 1.3739

================== Exp 2 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f7bf1f7b400>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-02 01:07:16] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 19485}
[2023-10-02 01:07:40] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.001172 train acc = 1.0000, test acc = 0.4868
[2023-10-02 01:08:04] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.005660 train acc = 1.0000, test acc = 0.4848
[2023-10-02 01:08:28] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003413 train acc = 1.0000, test acc = 0.5058
[2023-10-02 01:08:53] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.008025 train acc = 1.0000, test acc = 0.4923
[2023-10-02 01:09:17] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.022051 train acc = 1.0000, test acc = 0.4990
[2023-10-02 01:09:41] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004426 train acc = 1.0000, test acc = 0.4905
[2023-10-02 01:10:05] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.014957 train acc = 1.0000, test acc = 0.4878
[2023-10-02 01:10:29] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004297 train acc = 1.0000, test acc = 0.4981
[2023-10-02 01:10:54] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.011933 train acc = 1.0000, test acc = 0.4979
[2023-10-02 01:11:18] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005342 train acc = 1.0000, test acc = 0.4983
[2023-10-02 01:11:42] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.000936 train acc = 1.0000, test acc = 0.4943
[2023-10-02 01:12:07] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.010372 train acc = 1.0000, test acc = 0.4955
[2023-10-02 01:12:31] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.010119 train acc = 1.0000, test acc = 0.5012
[2023-10-02 01:12:55] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.000989 train acc = 1.0000, test acc = 0.4945
[2023-10-02 01:13:19] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.008587 train acc = 1.0000, test acc = 0.4868
[2023-10-02 01:13:43] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003228 train acc = 1.0000, test acc = 0.4927
[2023-10-02 01:14:07] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.010442 train acc = 1.0000, test acc = 0.4859
[2023-10-02 01:14:31] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.017552 train acc = 1.0000, test acc = 0.4959
[2023-10-02 01:14:55] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.007282 train acc = 1.0000, test acc = 0.4912
[2023-10-02 01:15:19] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.006808 train acc = 1.0000, test acc = 0.4967
Evaluate 20 random ConvNet, mean = 0.4938 std = 0.0055
-------------------------
[2023-10-02 01:15:20] iter = 00000, loss = 6.6009
[2023-10-02 01:15:21] iter = 00010, loss = 5.9198
[2023-10-02 01:15:21] iter = 00020, loss = 5.5270
[2023-10-02 01:15:22] iter = 00030, loss = 4.9682
[2023-10-02 01:15:23] iter = 00040, loss = 4.6642
[2023-10-02 01:15:24] iter = 00050, loss = 4.1416
[2023-10-02 01:15:25] iter = 00060, loss = 3.9132
[2023-10-02 01:15:26] iter = 00070, loss = 3.4906
[2023-10-02 01:15:27] iter = 00080, loss = 3.9239
[2023-10-02 01:15:28] iter = 00090, loss = 3.1330
[2023-10-02 01:15:29] iter = 00100, loss = 3.4284
[2023-10-02 01:15:29] iter = 00110, loss = 3.1890
[2023-10-02 01:15:30] iter = 00120, loss = 3.0826
[2023-10-02 01:15:31] iter = 00130, loss = 2.9886
[2023-10-02 01:15:32] iter = 00140, loss = 2.9338
[2023-10-02 01:15:33] iter = 00150, loss = 3.0409
[2023-10-02 01:15:34] iter = 00160, loss = 3.0628
[2023-10-02 01:15:35] iter = 00170, loss = 2.8241
[2023-10-02 01:15:36] iter = 00180, loss = 2.8327
[2023-10-02 01:15:37] iter = 00190, loss = 2.6438
[2023-10-02 01:15:38] iter = 00200, loss = 2.8117
[2023-10-02 01:15:38] iter = 00210, loss = 2.6612
[2023-10-02 01:15:39] iter = 00220, loss = 2.7316
[2023-10-02 01:15:40] iter = 00230, loss = 2.6703
[2023-10-02 01:15:41] iter = 00240, loss = 2.7338
[2023-10-02 01:15:42] iter = 00250, loss = 2.4859
[2023-10-02 01:15:43] iter = 00260, loss = 2.7384
[2023-10-02 01:15:44] iter = 00270, loss = 2.4409
[2023-10-02 01:15:45] iter = 00280, loss = 2.4173
[2023-10-02 01:15:46] iter = 00290, loss = 2.5584
[2023-10-02 01:15:47] iter = 00300, loss = 2.5662
[2023-10-02 01:15:47] iter = 00310, loss = 2.3134
[2023-10-02 01:15:48] iter = 00320, loss = 2.3792
[2023-10-02 01:15:49] iter = 00330, loss = 2.3262
[2023-10-02 01:15:50] iter = 00340, loss = 2.3439
[2023-10-02 01:15:51] iter = 00350, loss = 2.4300
[2023-10-02 01:15:52] iter = 00360, loss = 2.3804
[2023-10-02 01:15:53] iter = 00370, loss = 2.3592
[2023-10-02 01:15:54] iter = 00380, loss = 2.3242
[2023-10-02 01:15:55] iter = 00390, loss = 2.2239
[2023-10-02 01:15:56] iter = 00400, loss = 2.3741
[2023-10-02 01:15:57] iter = 00410, loss = 2.4007
[2023-10-02 01:15:58] iter = 00420, loss = 2.2189
[2023-10-02 01:15:59] iter = 00430, loss = 2.3611
[2023-10-02 01:15:59] iter = 00440, loss = 2.3299
[2023-10-02 01:16:00] iter = 00450, loss = 2.2420
[2023-10-02 01:16:01] iter = 00460, loss = 2.2492
[2023-10-02 01:16:02] iter = 00470, loss = 2.2266
[2023-10-02 01:16:03] iter = 00480, loss = 2.2350
[2023-10-02 01:16:04] iter = 00490, loss = 2.3237
[2023-10-02 01:16:05] iter = 00500, loss = 2.0930
[2023-10-02 01:16:06] iter = 00510, loss = 2.3465
[2023-10-02 01:16:07] iter = 00520, loss = 2.1425
[2023-10-02 01:16:08] iter = 00530, loss = 2.1411
[2023-10-02 01:16:09] iter = 00540, loss = 2.1573
[2023-10-02 01:16:10] iter = 00550, loss = 2.5359
[2023-10-02 01:16:10] iter = 00560, loss = 2.2210
[2023-10-02 01:16:11] iter = 00570, loss = 2.1525
[2023-10-02 01:16:12] iter = 00580, loss = 2.1771
[2023-10-02 01:16:13] iter = 00590, loss = 2.4020
[2023-10-02 01:16:14] iter = 00600, loss = 2.0486
[2023-10-02 01:16:15] iter = 00610, loss = 2.2919
[2023-10-02 01:16:16] iter = 00620, loss = 2.2386
[2023-10-02 01:16:17] iter = 00630, loss = 2.2116
[2023-10-02 01:16:17] iter = 00640, loss = 2.1179
[2023-10-02 01:16:18] iter = 00650, loss = 2.0649
[2023-10-02 01:16:19] iter = 00660, loss = 2.2959
[2023-10-02 01:16:20] iter = 00670, loss = 2.1506
[2023-10-02 01:16:21] iter = 00680, loss = 2.1392
[2023-10-02 01:16:22] iter = 00690, loss = 2.1363
[2023-10-02 01:16:23] iter = 00700, loss = 2.0888
[2023-10-02 01:16:24] iter = 00710, loss = 2.0749
[2023-10-02 01:16:25] iter = 00720, loss = 2.1081
[2023-10-02 01:16:26] iter = 00730, loss = 2.0887
[2023-10-02 01:16:26] iter = 00740, loss = 2.1521
[2023-10-02 01:16:27] iter = 00750, loss = 2.0825
[2023-10-02 01:16:28] iter = 00760, loss = 2.0554
[2023-10-02 01:16:29] iter = 00770, loss = 2.1284
[2023-10-02 01:16:30] iter = 00780, loss = 2.1724
[2023-10-02 01:16:31] iter = 00790, loss = 2.1500
[2023-10-02 01:16:32] iter = 00800, loss = 2.0119
[2023-10-02 01:16:33] iter = 00810, loss = 2.0783
[2023-10-02 01:16:34] iter = 00820, loss = 2.0348
[2023-10-02 01:16:35] iter = 00830, loss = 2.0417
[2023-10-02 01:16:35] iter = 00840, loss = 2.1022
[2023-10-02 01:16:36] iter = 00850, loss = 1.9703
[2023-10-02 01:16:37] iter = 00860, loss = 2.1686
[2023-10-02 01:16:38] iter = 00870, loss = 2.2502
[2023-10-02 01:16:39] iter = 00880, loss = 2.1131
[2023-10-02 01:16:40] iter = 00890, loss = 1.9466
[2023-10-02 01:16:41] iter = 00900, loss = 2.1063
[2023-10-02 01:16:42] iter = 00910, loss = 2.1045
[2023-10-02 01:16:43] iter = 00920, loss = 2.1139
[2023-10-02 01:16:44] iter = 00930, loss = 2.1674
[2023-10-02 01:16:44] iter = 00940, loss = 1.9494
[2023-10-02 01:16:45] iter = 00950, loss = 2.1321
[2023-10-02 01:16:46] iter = 00960, loss = 2.1375
[2023-10-02 01:16:47] iter = 00970, loss = 2.0837
[2023-10-02 01:16:48] iter = 00980, loss = 1.9694
[2023-10-02 01:16:49] iter = 00990, loss = 2.1889
[2023-10-02 01:16:50] iter = 01000, loss = 1.9058
[2023-10-02 01:16:51] iter = 01010, loss = 2.0232
[2023-10-02 01:16:51] iter = 01020, loss = 2.0285
[2023-10-02 01:16:52] iter = 01030, loss = 1.9500
[2023-10-02 01:16:53] iter = 01040, loss = 1.9771
[2023-10-02 01:16:54] iter = 01050, loss = 1.8630
[2023-10-02 01:16:55] iter = 01060, loss = 2.0181
[2023-10-02 01:16:56] iter = 01070, loss = 2.0858
[2023-10-02 01:16:57] iter = 01080, loss = 2.1005
[2023-10-02 01:16:58] iter = 01090, loss = 2.0680
[2023-10-02 01:16:59] iter = 01100, loss = 2.0353
[2023-10-02 01:17:00] iter = 01110, loss = 2.1335
[2023-10-02 01:17:01] iter = 01120, loss = 2.1160
[2023-10-02 01:17:02] iter = 01130, loss = 1.8911
[2023-10-02 01:17:02] iter = 01140, loss = 1.7953
[2023-10-02 01:17:03] iter = 01150, loss = 1.8075
[2023-10-02 01:17:04] iter = 01160, loss = 1.9257
[2023-10-02 01:17:05] iter = 01170, loss = 1.9840
[2023-10-02 01:17:06] iter = 01180, loss = 1.8339
[2023-10-02 01:17:07] iter = 01190, loss = 2.0994
[2023-10-02 01:17:08] iter = 01200, loss = 2.0699
[2023-10-02 01:17:09] iter = 01210, loss = 2.0982
[2023-10-02 01:17:10] iter = 01220, loss = 2.0172
[2023-10-02 01:17:11] iter = 01230, loss = 2.0861
[2023-10-02 01:17:11] iter = 01240, loss = 2.1473
[2023-10-02 01:17:12] iter = 01250, loss = 1.9322
[2023-10-02 01:17:13] iter = 01260, loss = 2.0392
[2023-10-02 01:17:14] iter = 01270, loss = 1.9652
[2023-10-02 01:17:15] iter = 01280, loss = 1.8274
[2023-10-02 01:17:16] iter = 01290, loss = 2.0778
[2023-10-02 01:17:17] iter = 01300, loss = 1.8483
[2023-10-02 01:17:18] iter = 01310, loss = 1.9703
[2023-10-02 01:17:19] iter = 01320, loss = 1.9603
[2023-10-02 01:17:20] iter = 01330, loss = 1.9121
[2023-10-02 01:17:21] iter = 01340, loss = 1.9707
[2023-10-02 01:17:21] iter = 01350, loss = 1.9341
[2023-10-02 01:17:22] iter = 01360, loss = 1.9277
[2023-10-02 01:17:23] iter = 01370, loss = 1.9126
[2023-10-02 01:17:24] iter = 01380, loss = 1.8574
[2023-10-02 01:17:25] iter = 01390, loss = 1.9235
[2023-10-02 01:17:26] iter = 01400, loss = 1.9935
[2023-10-02 01:17:27] iter = 01410, loss = 1.8211
[2023-10-02 01:17:28] iter = 01420, loss = 1.9059
[2023-10-02 01:17:29] iter = 01430, loss = 1.8516
[2023-10-02 01:17:30] iter = 01440, loss = 1.9374
[2023-10-02 01:17:30] iter = 01450, loss = 1.7885
[2023-10-02 01:17:31] iter = 01460, loss = 1.9492
[2023-10-02 01:17:32] iter = 01470, loss = 1.8915
[2023-10-02 01:17:33] iter = 01480, loss = 1.9636
[2023-10-02 01:17:34] iter = 01490, loss = 1.7663
[2023-10-02 01:17:35] iter = 01500, loss = 1.9184
[2023-10-02 01:17:36] iter = 01510, loss = 1.9483
[2023-10-02 01:17:37] iter = 01520, loss = 1.8695
[2023-10-02 01:17:38] iter = 01530, loss = 1.7564
[2023-10-02 01:17:39] iter = 01540, loss = 1.9092
[2023-10-02 01:17:39] iter = 01550, loss = 1.9340
[2023-10-02 01:17:40] iter = 01560, loss = 1.7813
[2023-10-02 01:17:41] iter = 01570, loss = 1.8715
[2023-10-02 01:17:42] iter = 01580, loss = 1.7652
[2023-10-02 01:17:43] iter = 01590, loss = 1.9017
[2023-10-02 01:17:44] iter = 01600, loss = 1.8303
[2023-10-02 01:17:45] iter = 01610, loss = 1.9830
[2023-10-02 01:17:46] iter = 01620, loss = 1.8189
[2023-10-02 01:17:47] iter = 01630, loss = 2.0945
[2023-10-02 01:17:48] iter = 01640, loss = 1.7900
[2023-10-02 01:17:48] iter = 01650, loss = 1.7862
[2023-10-02 01:17:49] iter = 01660, loss = 1.7739
[2023-10-02 01:17:50] iter = 01670, loss = 1.8490
[2023-10-02 01:17:51] iter = 01680, loss = 1.8744
[2023-10-02 01:17:52] iter = 01690, loss = 2.0237
[2023-10-02 01:17:53] iter = 01700, loss = 1.9785
[2023-10-02 01:17:54] iter = 01710, loss = 1.9799
[2023-10-02 01:17:55] iter = 01720, loss = 1.9314
[2023-10-02 01:17:56] iter = 01730, loss = 1.8780
[2023-10-02 01:17:57] iter = 01740, loss = 1.8753
[2023-10-02 01:17:58] iter = 01750, loss = 1.8495
[2023-10-02 01:17:59] iter = 01760, loss = 1.8364
[2023-10-02 01:18:00] iter = 01770, loss = 1.8766
[2023-10-02 01:18:01] iter = 01780, loss = 1.8823
[2023-10-02 01:18:01] iter = 01790, loss = 1.9188
[2023-10-02 01:18:02] iter = 01800, loss = 1.9360
[2023-10-02 01:18:03] iter = 01810, loss = 1.8318
[2023-10-02 01:18:04] iter = 01820, loss = 1.8800
[2023-10-02 01:18:05] iter = 01830, loss = 1.9191
[2023-10-02 01:18:06] iter = 01840, loss = 1.8623
[2023-10-02 01:18:07] iter = 01850, loss = 1.9417
[2023-10-02 01:18:08] iter = 01860, loss = 1.8231
[2023-10-02 01:18:09] iter = 01870, loss = 1.9155
[2023-10-02 01:18:10] iter = 01880, loss = 1.8974
[2023-10-02 01:18:10] iter = 01890, loss = 1.9179
[2023-10-02 01:18:11] iter = 01900, loss = 1.8100
[2023-10-02 01:18:12] iter = 01910, loss = 1.8651
[2023-10-02 01:18:13] iter = 01920, loss = 1.8316
[2023-10-02 01:18:14] iter = 01930, loss = 2.0212
[2023-10-02 01:18:15] iter = 01940, loss = 1.8138
[2023-10-02 01:18:16] iter = 01950, loss = 1.6123
[2023-10-02 01:18:17] iter = 01960, loss = 1.7869
[2023-10-02 01:18:18] iter = 01970, loss = 1.8100
[2023-10-02 01:18:18] iter = 01980, loss = 1.8370
[2023-10-02 01:18:19] iter = 01990, loss = 1.9209
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 644}
[2023-10-02 01:18:44] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.021052 train acc = 0.9980, test acc = 0.5870
[2023-10-02 01:19:08] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001840 train acc = 1.0000, test acc = 0.5850
[2023-10-02 01:19:33] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.001061 train acc = 1.0000, test acc = 0.5820
[2023-10-02 01:19:57] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.007129 train acc = 1.0000, test acc = 0.5795
[2023-10-02 01:20:21] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.016724 train acc = 1.0000, test acc = 0.5867
[2023-10-02 01:20:45] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.025510 train acc = 1.0000, test acc = 0.5838
[2023-10-02 01:21:09] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.018899 train acc = 1.0000, test acc = 0.5809
[2023-10-02 01:21:33] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001121 train acc = 1.0000, test acc = 0.5849
[2023-10-02 01:21:58] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.019444 train acc = 1.0000, test acc = 0.5865
[2023-10-02 01:22:22] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005394 train acc = 1.0000, test acc = 0.5901
[2023-10-02 01:22:46] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.012002 train acc = 1.0000, test acc = 0.5825
[2023-10-02 01:23:10] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002369 train acc = 1.0000, test acc = 0.5945
[2023-10-02 01:23:34] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.008033 train acc = 1.0000, test acc = 0.5801
[2023-10-02 01:23:58] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002818 train acc = 1.0000, test acc = 0.5863
[2023-10-02 01:24:23] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004583 train acc = 0.9980, test acc = 0.5847
[2023-10-02 01:24:47] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.009209 train acc = 1.0000, test acc = 0.5800
[2023-10-02 01:25:11] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.001512 train acc = 1.0000, test acc = 0.5831
[2023-10-02 01:25:35] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.006542 train acc = 1.0000, test acc = 0.5866
[2023-10-02 01:25:59] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.005944 train acc = 1.0000, test acc = 0.5895
[2023-10-02 01:26:23] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.013237 train acc = 1.0000, test acc = 0.5859
Evaluate 20 random ConvNet, mean = 0.5850 std = 0.0037
-------------------------
[2023-10-02 01:26:24] iter = 02000, loss = 2.0453
[2023-10-02 01:26:25] iter = 02010, loss = 1.6728
[2023-10-02 01:26:25] iter = 02020, loss = 1.7125
[2023-10-02 01:26:26] iter = 02030, loss = 1.9213
[2023-10-02 01:26:27] iter = 02040, loss = 1.7630
[2023-10-02 01:26:28] iter = 02050, loss = 1.9306
[2023-10-02 01:26:29] iter = 02060, loss = 1.9315
[2023-10-02 01:26:30] iter = 02070, loss = 1.9526
[2023-10-02 01:26:31] iter = 02080, loss = 1.8293
[2023-10-02 01:26:32] iter = 02090, loss = 1.7817
[2023-10-02 01:26:33] iter = 02100, loss = 1.8235
[2023-10-02 01:26:34] iter = 02110, loss = 1.9830
[2023-10-02 01:26:34] iter = 02120, loss = 1.8480
[2023-10-02 01:26:35] iter = 02130, loss = 1.9609
[2023-10-02 01:26:36] iter = 02140, loss = 1.7861
[2023-10-02 01:26:37] iter = 02150, loss = 1.7992
[2023-10-02 01:26:38] iter = 02160, loss = 1.8618
[2023-10-02 01:26:39] iter = 02170, loss = 1.7575
[2023-10-02 01:26:40] iter = 02180, loss = 1.7270
[2023-10-02 01:26:41] iter = 02190, loss = 1.7112
[2023-10-02 01:26:42] iter = 02200, loss = 1.7130
[2023-10-02 01:26:43] iter = 02210, loss = 1.7714
[2023-10-02 01:26:43] iter = 02220, loss = 1.7929
[2023-10-02 01:26:44] iter = 02230, loss = 1.8135
[2023-10-02 01:26:45] iter = 02240, loss = 1.8212
[2023-10-02 01:26:46] iter = 02250, loss = 1.8811
[2023-10-02 01:26:47] iter = 02260, loss = 1.8720
[2023-10-02 01:26:48] iter = 02270, loss = 1.8764
[2023-10-02 01:26:49] iter = 02280, loss = 1.9077
[2023-10-02 01:26:50] iter = 02290, loss = 1.7478
[2023-10-02 01:26:51] iter = 02300, loss = 1.8906
[2023-10-02 01:26:51] iter = 02310, loss = 1.7614
[2023-10-02 01:26:52] iter = 02320, loss = 1.7872
[2023-10-02 01:26:53] iter = 02330, loss = 1.6608
[2023-10-02 01:26:54] iter = 02340, loss = 1.8488
[2023-10-02 01:26:55] iter = 02350, loss = 1.8138
[2023-10-02 01:26:56] iter = 02360, loss = 1.7131
[2023-10-02 01:26:57] iter = 02370, loss = 1.8577
[2023-10-02 01:26:58] iter = 02380, loss = 1.8689
[2023-10-02 01:26:59] iter = 02390, loss = 1.7393
[2023-10-02 01:26:59] iter = 02400, loss = 1.8723
[2023-10-02 01:27:00] iter = 02410, loss = 1.7905
[2023-10-02 01:27:01] iter = 02420, loss = 1.7006
[2023-10-02 01:27:02] iter = 02430, loss = 1.8887
[2023-10-02 01:27:03] iter = 02440, loss = 1.7380
[2023-10-02 01:27:04] iter = 02450, loss = 2.0177
[2023-10-02 01:27:05] iter = 02460, loss = 1.8084
[2023-10-02 01:27:06] iter = 02470, loss = 1.8586
[2023-10-02 01:27:07] iter = 02480, loss = 1.8922
[2023-10-02 01:27:08] iter = 02490, loss = 1.7176
[2023-10-02 01:27:09] iter = 02500, loss = 1.7574
[2023-10-02 01:27:09] iter = 02510, loss = 1.8798
[2023-10-02 01:27:10] iter = 02520, loss = 1.7169
[2023-10-02 01:27:11] iter = 02530, loss = 1.7949
[2023-10-02 01:27:12] iter = 02540, loss = 1.7969
[2023-10-02 01:27:13] iter = 02550, loss = 1.6978
[2023-10-02 01:27:14] iter = 02560, loss = 1.8144
[2023-10-02 01:27:15] iter = 02570, loss = 1.7858
[2023-10-02 01:27:16] iter = 02580, loss = 1.7114
[2023-10-02 01:27:17] iter = 02590, loss = 1.7495
[2023-10-02 01:27:18] iter = 02600, loss = 1.8252
[2023-10-02 01:27:19] iter = 02610, loss = 1.8980
[2023-10-02 01:27:20] iter = 02620, loss = 1.9269
[2023-10-02 01:27:21] iter = 02630, loss = 1.9305
[2023-10-02 01:27:21] iter = 02640, loss = 1.8314
[2023-10-02 01:27:22] iter = 02650, loss = 1.8426
[2023-10-02 01:27:23] iter = 02660, loss = 1.9391
[2023-10-02 01:27:24] iter = 02670, loss = 1.8304
[2023-10-02 01:27:25] iter = 02680, loss = 1.8497
[2023-10-02 01:27:26] iter = 02690, loss = 1.7769
[2023-10-02 01:27:27] iter = 02700, loss = 1.8338
[2023-10-02 01:27:28] iter = 02710, loss = 1.8475
[2023-10-02 01:27:29] iter = 02720, loss = 1.8053
[2023-10-02 01:27:30] iter = 02730, loss = 1.7522
[2023-10-02 01:27:30] iter = 02740, loss = 1.8112
[2023-10-02 01:27:31] iter = 02750, loss = 1.6625
[2023-10-02 01:27:32] iter = 02760, loss = 1.6708
[2023-10-02 01:27:33] iter = 02770, loss = 1.5865
[2023-10-02 01:27:34] iter = 02780, loss = 1.6555
[2023-10-02 01:27:35] iter = 02790, loss = 1.6832
[2023-10-02 01:27:36] iter = 02800, loss = 1.8080
[2023-10-02 01:27:37] iter = 02810, loss = 1.7046
[2023-10-02 01:27:38] iter = 02820, loss = 1.7589
[2023-10-02 01:27:39] iter = 02830, loss = 1.8696
[2023-10-02 01:27:39] iter = 02840, loss = 1.6455
[2023-10-02 01:27:40] iter = 02850, loss = 1.7170
[2023-10-02 01:27:41] iter = 02860, loss = 1.7615
[2023-10-02 01:27:42] iter = 02870, loss = 1.7242
[2023-10-02 01:27:43] iter = 02880, loss = 1.6369
[2023-10-02 01:27:44] iter = 02890, loss = 1.8354
[2023-10-02 01:27:45] iter = 02900, loss = 1.6992
[2023-10-02 01:27:46] iter = 02910, loss = 1.9049
[2023-10-02 01:27:47] iter = 02920, loss = 1.7163
[2023-10-02 01:27:48] iter = 02930, loss = 1.6609
[2023-10-02 01:27:48] iter = 02940, loss = 1.6132
[2023-10-02 01:27:49] iter = 02950, loss = 1.8350
[2023-10-02 01:27:50] iter = 02960, loss = 1.6663
[2023-10-02 01:27:51] iter = 02970, loss = 1.7926
[2023-10-02 01:27:52] iter = 02980, loss = 1.9105
[2023-10-02 01:27:53] iter = 02990, loss = 1.6435
[2023-10-02 01:27:54] iter = 03000, loss = 1.8551
[2023-10-02 01:27:55] iter = 03010, loss = 1.6625
[2023-10-02 01:27:55] iter = 03020, loss = 1.7485
[2023-10-02 01:27:56] iter = 03030, loss = 1.7486
[2023-10-02 01:27:57] iter = 03040, loss = 1.7123
[2023-10-02 01:27:58] iter = 03050, loss = 1.6428
[2023-10-02 01:27:59] iter = 03060, loss = 1.9704
[2023-10-02 01:28:00] iter = 03070, loss = 1.6507
[2023-10-02 01:28:01] iter = 03080, loss = 1.9342
[2023-10-02 01:28:02] iter = 03090, loss = 1.6508
[2023-10-02 01:28:03] iter = 03100, loss = 1.6183
[2023-10-02 01:28:04] iter = 03110, loss = 1.7039
[2023-10-02 01:28:05] iter = 03120, loss = 1.9073
[2023-10-02 01:28:06] iter = 03130, loss = 1.6959
[2023-10-02 01:28:07] iter = 03140, loss = 1.7884
[2023-10-02 01:28:07] iter = 03150, loss = 1.7533
[2023-10-02 01:28:08] iter = 03160, loss = 1.6762
[2023-10-02 01:28:09] iter = 03170, loss = 1.7013
[2023-10-02 01:28:10] iter = 03180, loss = 1.6491
[2023-10-02 01:28:11] iter = 03190, loss = 1.8425
[2023-10-02 01:28:12] iter = 03200, loss = 1.8674
[2023-10-02 01:28:13] iter = 03210, loss = 1.7510
[2023-10-02 01:28:14] iter = 03220, loss = 1.7667
[2023-10-02 01:28:15] iter = 03230, loss = 1.7688
[2023-10-02 01:28:16] iter = 03240, loss = 1.6603
[2023-10-02 01:28:17] iter = 03250, loss = 1.7268
[2023-10-02 01:28:18] iter = 03260, loss = 1.5970
[2023-10-02 01:28:18] iter = 03270, loss = 1.7374
[2023-10-02 01:28:19] iter = 03280, loss = 1.7322
[2023-10-02 01:28:20] iter = 03290, loss = 1.6488
[2023-10-02 01:28:21] iter = 03300, loss = 1.7112
[2023-10-02 01:28:22] iter = 03310, loss = 1.6037
[2023-10-02 01:28:23] iter = 03320, loss = 1.6941
[2023-10-02 01:28:24] iter = 03330, loss = 1.7961
[2023-10-02 01:28:25] iter = 03340, loss = 1.7605
[2023-10-02 01:28:26] iter = 03350, loss = 1.8065
[2023-10-02 01:28:27] iter = 03360, loss = 1.7231
[2023-10-02 01:28:27] iter = 03370, loss = 1.6381
[2023-10-02 01:28:28] iter = 03380, loss = 1.7154
[2023-10-02 01:28:29] iter = 03390, loss = 1.7025
[2023-10-02 01:28:30] iter = 03400, loss = 1.8356
[2023-10-02 01:28:31] iter = 03410, loss = 1.7325
[2023-10-02 01:28:32] iter = 03420, loss = 1.5501
[2023-10-02 01:28:33] iter = 03430, loss = 1.7510
[2023-10-02 01:28:34] iter = 03440, loss = 1.8581
[2023-10-02 01:28:35] iter = 03450, loss = 1.7012
[2023-10-02 01:28:36] iter = 03460, loss = 1.6742
[2023-10-02 01:28:37] iter = 03470, loss = 1.7589
[2023-10-02 01:28:37] iter = 03480, loss = 1.7265
[2023-10-02 01:28:38] iter = 03490, loss = 1.6464
[2023-10-02 01:28:39] iter = 03500, loss = 1.7692
[2023-10-02 01:28:40] iter = 03510, loss = 1.8343
[2023-10-02 01:28:41] iter = 03520, loss = 1.7400
[2023-10-02 01:28:42] iter = 03530, loss = 1.6717
[2023-10-02 01:28:43] iter = 03540, loss = 1.6666
[2023-10-02 01:28:44] iter = 03550, loss = 1.6891
[2023-10-02 01:28:45] iter = 03560, loss = 1.6291
[2023-10-02 01:28:45] iter = 03570, loss = 1.7347
[2023-10-02 01:28:46] iter = 03580, loss = 1.6310
[2023-10-02 01:28:47] iter = 03590, loss = 1.7666
[2023-10-02 01:28:48] iter = 03600, loss = 1.7979
[2023-10-02 01:28:49] iter = 03610, loss = 1.8117
[2023-10-02 01:28:50] iter = 03620, loss = 1.7643
[2023-10-02 01:28:51] iter = 03630, loss = 1.7467
[2023-10-02 01:28:52] iter = 03640, loss = 1.8241
[2023-10-02 01:28:52] iter = 03650, loss = 1.8172
[2023-10-02 01:28:53] iter = 03660, loss = 1.7607
[2023-10-02 01:28:54] iter = 03670, loss = 1.8532
[2023-10-02 01:28:55] iter = 03680, loss = 1.7960
[2023-10-02 01:28:56] iter = 03690, loss = 1.6223
[2023-10-02 01:28:57] iter = 03700, loss = 1.7931
[2023-10-02 01:28:58] iter = 03710, loss = 1.6921
[2023-10-02 01:28:59] iter = 03720, loss = 1.7062
[2023-10-02 01:29:00] iter = 03730, loss = 1.7142
[2023-10-02 01:29:01] iter = 03740, loss = 1.6712
[2023-10-02 01:29:01] iter = 03750, loss = 1.7736
[2023-10-02 01:29:02] iter = 03760, loss = 1.7735
[2023-10-02 01:29:03] iter = 03770, loss = 1.7188
[2023-10-02 01:29:04] iter = 03780, loss = 1.6651
[2023-10-02 01:29:05] iter = 03790, loss = 1.7473
[2023-10-02 01:29:06] iter = 03800, loss = 1.7328
[2023-10-02 01:29:07] iter = 03810, loss = 1.6595
[2023-10-02 01:29:08] iter = 03820, loss = 1.7115
[2023-10-02 01:29:09] iter = 03830, loss = 1.5626
[2023-10-02 01:29:09] iter = 03840, loss = 1.7576
[2023-10-02 01:29:10] iter = 03850, loss = 1.7391
[2023-10-02 01:29:11] iter = 03860, loss = 1.5998
[2023-10-02 01:29:12] iter = 03870, loss = 1.7257
[2023-10-02 01:29:13] iter = 03880, loss = 1.6486
[2023-10-02 01:29:14] iter = 03890, loss = 1.7444
[2023-10-02 01:29:15] iter = 03900, loss = 1.7605
[2023-10-02 01:29:16] iter = 03910, loss = 1.8371
[2023-10-02 01:29:17] iter = 03920, loss = 1.7935
[2023-10-02 01:29:17] iter = 03930, loss = 1.6008
[2023-10-02 01:29:18] iter = 03940, loss = 1.7230
[2023-10-02 01:29:19] iter = 03950, loss = 1.6172
[2023-10-02 01:29:20] iter = 03960, loss = 1.7199
[2023-10-02 01:29:21] iter = 03970, loss = 1.7169
[2023-10-02 01:29:22] iter = 03980, loss = 1.7247
[2023-10-02 01:29:23] iter = 03990, loss = 1.5846
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 64135}
[2023-10-02 01:29:48] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.008824 train acc = 1.0000, test acc = 0.5981
[2023-10-02 01:30:12] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.020746 train acc = 0.9980, test acc = 0.6030
[2023-10-02 01:30:36] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.010122 train acc = 1.0000, test acc = 0.6008
[2023-10-02 01:31:00] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.013550 train acc = 1.0000, test acc = 0.6138
[2023-10-02 01:31:25] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.007823 train acc = 1.0000, test acc = 0.5996
[2023-10-02 01:31:49] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004487 train acc = 1.0000, test acc = 0.5955
[2023-10-02 01:32:13] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012989 train acc = 1.0000, test acc = 0.6052
[2023-10-02 01:32:37] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.024181 train acc = 1.0000, test acc = 0.6062
[2023-10-02 01:33:01] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.001413 train acc = 1.0000, test acc = 0.6006
[2023-10-02 01:33:25] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.008972 train acc = 1.0000, test acc = 0.6062
[2023-10-02 01:33:49] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002944 train acc = 1.0000, test acc = 0.6000
[2023-10-02 01:34:13] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002662 train acc = 1.0000, test acc = 0.5990
[2023-10-02 01:34:38] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003001 train acc = 1.0000, test acc = 0.6052
[2023-10-02 01:35:02] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.012918 train acc = 1.0000, test acc = 0.6009
[2023-10-02 01:35:26] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003890 train acc = 1.0000, test acc = 0.5979
[2023-10-02 01:35:50] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004623 train acc = 1.0000, test acc = 0.6043
[2023-10-02 01:36:14] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002283 train acc = 1.0000, test acc = 0.5991
[2023-10-02 01:36:38] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.012611 train acc = 1.0000, test acc = 0.6008
[2023-10-02 01:37:02] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.004146 train acc = 1.0000, test acc = 0.6038
[2023-10-02 01:37:27] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.015557 train acc = 1.0000, test acc = 0.6002
Evaluate 20 random ConvNet, mean = 0.6020 std = 0.0040
-------------------------
[2023-10-02 01:37:27] iter = 04000, loss = 1.6622
[2023-10-02 01:37:28] iter = 04010, loss = 1.8581
[2023-10-02 01:37:29] iter = 04020, loss = 1.6383
[2023-10-02 01:37:30] iter = 04030, loss = 1.9010
[2023-10-02 01:37:31] iter = 04040, loss = 1.6963
[2023-10-02 01:37:32] iter = 04050, loss = 1.6545
[2023-10-02 01:37:32] iter = 04060, loss = 1.8054
[2023-10-02 01:37:33] iter = 04070, loss = 1.6159
[2023-10-02 01:37:34] iter = 04080, loss = 1.8023
[2023-10-02 01:37:35] iter = 04090, loss = 1.6639
[2023-10-02 01:37:36] iter = 04100, loss = 1.8750
[2023-10-02 01:37:37] iter = 04110, loss = 1.5514
[2023-10-02 01:37:38] iter = 04120, loss = 1.6433
[2023-10-02 01:37:39] iter = 04130, loss = 1.7314
[2023-10-02 01:37:40] iter = 04140, loss = 1.6348
[2023-10-02 01:37:40] iter = 04150, loss = 1.6609
[2023-10-02 01:37:41] iter = 04160, loss = 1.6092
[2023-10-02 01:37:42] iter = 04170, loss = 1.7809
[2023-10-02 01:37:43] iter = 04180, loss = 1.6009
[2023-10-02 01:37:44] iter = 04190, loss = 1.5768
[2023-10-02 01:37:45] iter = 04200, loss = 1.7179
[2023-10-02 01:37:46] iter = 04210, loss = 1.6370
[2023-10-02 01:37:47] iter = 04220, loss = 1.6829
[2023-10-02 01:37:48] iter = 04230, loss = 1.7284
[2023-10-02 01:37:49] iter = 04240, loss = 1.6497
[2023-10-02 01:37:49] iter = 04250, loss = 1.7369
[2023-10-02 01:37:50] iter = 04260, loss = 1.5113
[2023-10-02 01:37:51] iter = 04270, loss = 1.6713
[2023-10-02 01:37:52] iter = 04280, loss = 1.8682
[2023-10-02 01:37:53] iter = 04290, loss = 1.7533
[2023-10-02 01:37:54] iter = 04300, loss = 1.7133
[2023-10-02 01:37:55] iter = 04310, loss = 1.6339
[2023-10-02 01:37:56] iter = 04320, loss = 1.7514
[2023-10-02 01:37:56] iter = 04330, loss = 1.6460
[2023-10-02 01:37:57] iter = 04340, loss = 1.6557
[2023-10-02 01:37:58] iter = 04350, loss = 1.7328
[2023-10-02 01:37:59] iter = 04360, loss = 1.5328
[2023-10-02 01:38:00] iter = 04370, loss = 1.7329
[2023-10-02 01:38:01] iter = 04380, loss = 1.6650
[2023-10-02 01:38:02] iter = 04390, loss = 1.6943
[2023-10-02 01:38:03] iter = 04400, loss = 1.6652
[2023-10-02 01:38:04] iter = 04410, loss = 1.6434
[2023-10-02 01:38:04] iter = 04420, loss = 1.7527
[2023-10-02 01:38:05] iter = 04430, loss = 1.5805
[2023-10-02 01:38:06] iter = 04440, loss = 1.7032
[2023-10-02 01:38:07] iter = 04450, loss = 1.7626
[2023-10-02 01:38:08] iter = 04460, loss = 1.7911
[2023-10-02 01:38:09] iter = 04470, loss = 1.6927
[2023-10-02 01:38:10] iter = 04480, loss = 1.7360
[2023-10-02 01:38:11] iter = 04490, loss = 1.5273
[2023-10-02 01:38:12] iter = 04500, loss = 1.7137
[2023-10-02 01:38:12] iter = 04510, loss = 1.7143
[2023-10-02 01:38:13] iter = 04520, loss = 1.7452
[2023-10-02 01:38:14] iter = 04530, loss = 1.5227
[2023-10-02 01:38:15] iter = 04540, loss = 1.5942
[2023-10-02 01:38:16] iter = 04550, loss = 1.7047
[2023-10-02 01:38:17] iter = 04560, loss = 1.6286
[2023-10-02 01:38:18] iter = 04570, loss = 1.7064
[2023-10-02 01:38:19] iter = 04580, loss = 1.6928
[2023-10-02 01:38:20] iter = 04590, loss = 1.6798
[2023-10-02 01:38:21] iter = 04600, loss = 1.7596
[2023-10-02 01:38:21] iter = 04610, loss = 1.6350
[2023-10-02 01:38:22] iter = 04620, loss = 1.7283
[2023-10-02 01:38:23] iter = 04630, loss = 1.7828
[2023-10-02 01:38:24] iter = 04640, loss = 1.6444
[2023-10-02 01:38:25] iter = 04650, loss = 1.8339
[2023-10-02 01:38:26] iter = 04660, loss = 1.7663
[2023-10-02 01:38:27] iter = 04670, loss = 1.7989
[2023-10-02 01:38:28] iter = 04680, loss = 1.8132
[2023-10-02 01:38:29] iter = 04690, loss = 1.7620
[2023-10-02 01:38:29] iter = 04700, loss = 1.7824
[2023-10-02 01:38:30] iter = 04710, loss = 1.7110
[2023-10-02 01:38:31] iter = 04720, loss = 1.7019
[2023-10-02 01:38:32] iter = 04730, loss = 1.8120
[2023-10-02 01:38:33] iter = 04740, loss = 1.6683
[2023-10-02 01:38:34] iter = 04750, loss = 1.6700
[2023-10-02 01:38:35] iter = 04760, loss = 1.6395
[2023-10-02 01:38:36] iter = 04770, loss = 1.5843
[2023-10-02 01:38:37] iter = 04780, loss = 1.7707
[2023-10-02 01:38:38] iter = 04790, loss = 1.5877
[2023-10-02 01:38:39] iter = 04800, loss = 1.6764
[2023-10-02 01:38:40] iter = 04810, loss = 1.7059
[2023-10-02 01:38:40] iter = 04820, loss = 1.5337
[2023-10-02 01:38:41] iter = 04830, loss = 1.7160
[2023-10-02 01:38:42] iter = 04840, loss = 1.5716
[2023-10-02 01:38:43] iter = 04850, loss = 1.6439
[2023-10-02 01:38:44] iter = 04860, loss = 1.7362
[2023-10-02 01:38:45] iter = 04870, loss = 1.6477
[2023-10-02 01:38:46] iter = 04880, loss = 1.7193
[2023-10-02 01:38:47] iter = 04890, loss = 1.7043
[2023-10-02 01:38:48] iter = 04900, loss = 1.6119
[2023-10-02 01:38:48] iter = 04910, loss = 1.6588
[2023-10-02 01:38:49] iter = 04920, loss = 1.7064
[2023-10-02 01:38:50] iter = 04930, loss = 1.6872
[2023-10-02 01:38:51] iter = 04940, loss = 1.5841
[2023-10-02 01:38:52] iter = 04950, loss = 1.7068
[2023-10-02 01:38:53] iter = 04960, loss = 1.8045
[2023-10-02 01:38:54] iter = 04970, loss = 1.6591
[2023-10-02 01:38:55] iter = 04980, loss = 1.6597
[2023-10-02 01:38:56] iter = 04990, loss = 1.7210
[2023-10-02 01:38:57] iter = 05000, loss = 1.6714
[2023-10-02 01:38:58] iter = 05010, loss = 1.6736
[2023-10-02 01:38:58] iter = 05020, loss = 1.6908
[2023-10-02 01:38:59] iter = 05030, loss = 1.6192
[2023-10-02 01:39:00] iter = 05040, loss = 1.7105
[2023-10-02 01:39:01] iter = 05050, loss = 1.7113
[2023-10-02 01:39:02] iter = 05060, loss = 1.6296
[2023-10-02 01:39:03] iter = 05070, loss = 1.6396
[2023-10-02 01:39:04] iter = 05080, loss = 1.7720
[2023-10-02 01:39:05] iter = 05090, loss = 1.7109
[2023-10-02 01:39:06] iter = 05100, loss = 1.7845
[2023-10-02 01:39:07] iter = 05110, loss = 1.6440
[2023-10-02 01:39:08] iter = 05120, loss = 1.5962
[2023-10-02 01:39:08] iter = 05130, loss = 1.6700
[2023-10-02 01:39:09] iter = 05140, loss = 1.5998
[2023-10-02 01:39:10] iter = 05150, loss = 1.8317
[2023-10-02 01:39:11] iter = 05160, loss = 1.6063
[2023-10-02 01:39:12] iter = 05170, loss = 1.7275
[2023-10-02 01:39:13] iter = 05180, loss = 1.6397
[2023-10-02 01:39:14] iter = 05190, loss = 1.8010
[2023-10-02 01:39:15] iter = 05200, loss = 1.6137
[2023-10-02 01:39:16] iter = 05210, loss = 1.6101
[2023-10-02 01:39:16] iter = 05220, loss = 1.7591
[2023-10-02 01:39:17] iter = 05230, loss = 1.6305
[2023-10-02 01:39:18] iter = 05240, loss = 1.6381
[2023-10-02 01:39:19] iter = 05250, loss = 1.6906
[2023-10-02 01:39:20] iter = 05260, loss = 1.6172
[2023-10-02 01:39:21] iter = 05270, loss = 1.6471
[2023-10-02 01:39:22] iter = 05280, loss = 1.6157
[2023-10-02 01:39:22] iter = 05290, loss = 1.5716
[2023-10-02 01:39:23] iter = 05300, loss = 1.5589
[2023-10-02 01:39:24] iter = 05310, loss = 1.6627
[2023-10-02 01:39:25] iter = 05320, loss = 1.7826
[2023-10-02 01:39:26] iter = 05330, loss = 1.6963
[2023-10-02 01:39:27] iter = 05340, loss = 1.6649
[2023-10-02 01:39:28] iter = 05350, loss = 1.8038
[2023-10-02 01:39:29] iter = 05360, loss = 1.6987
[2023-10-02 01:39:30] iter = 05370, loss = 1.7033
[2023-10-02 01:39:31] iter = 05380, loss = 1.7132
[2023-10-02 01:39:31] iter = 05390, loss = 1.6343
[2023-10-02 01:39:33] iter = 05400, loss = 1.7354
[2023-10-02 01:39:33] iter = 05410, loss = 1.6654
[2023-10-02 01:39:34] iter = 05420, loss = 1.5997
[2023-10-02 01:39:35] iter = 05430, loss = 1.5875
[2023-10-02 01:39:36] iter = 05440, loss = 1.5434
[2023-10-02 01:39:37] iter = 05450, loss = 1.7158
[2023-10-02 01:39:38] iter = 05460, loss = 1.7164
[2023-10-02 01:39:39] iter = 05470, loss = 1.5729
[2023-10-02 01:39:40] iter = 05480, loss = 1.7674
[2023-10-02 01:39:41] iter = 05490, loss = 1.5489
[2023-10-02 01:39:42] iter = 05500, loss = 1.6190
[2023-10-02 01:39:42] iter = 05510, loss = 1.7063
[2023-10-02 01:39:43] iter = 05520, loss = 1.7960
[2023-10-02 01:39:44] iter = 05530, loss = 1.5961
[2023-10-02 01:39:45] iter = 05540, loss = 1.7174
[2023-10-02 01:39:46] iter = 05550, loss = 1.6460
[2023-10-02 01:39:47] iter = 05560, loss = 1.5900
[2023-10-02 01:39:48] iter = 05570, loss = 1.7034
[2023-10-02 01:39:49] iter = 05580, loss = 1.7167
[2023-10-02 01:39:50] iter = 05590, loss = 1.6318
[2023-10-02 01:39:51] iter = 05600, loss = 1.5610
[2023-10-02 01:39:52] iter = 05610, loss = 1.6120
[2023-10-02 01:39:52] iter = 05620, loss = 1.6491
[2023-10-02 01:39:53] iter = 05630, loss = 1.5821
[2023-10-02 01:39:54] iter = 05640, loss = 1.7386
[2023-10-02 01:39:55] iter = 05650, loss = 1.5213
[2023-10-02 01:39:56] iter = 05660, loss = 1.6620
[2023-10-02 01:39:57] iter = 05670, loss = 1.5362
[2023-10-02 01:39:58] iter = 05680, loss = 1.5316
[2023-10-02 01:39:59] iter = 05690, loss = 1.5242
[2023-10-02 01:40:00] iter = 05700, loss = 1.6959
[2023-10-02 01:40:01] iter = 05710, loss = 1.7064
[2023-10-02 01:40:01] iter = 05720, loss = 1.6401
[2023-10-02 01:40:02] iter = 05730, loss = 1.6022
[2023-10-02 01:40:03] iter = 05740, loss = 1.6202
[2023-10-02 01:40:04] iter = 05750, loss = 1.7090
[2023-10-02 01:40:05] iter = 05760, loss = 1.6864
[2023-10-02 01:40:06] iter = 05770, loss = 1.6415
[2023-10-02 01:40:07] iter = 05780, loss = 1.6222
[2023-10-02 01:40:08] iter = 05790, loss = 1.6092
[2023-10-02 01:40:09] iter = 05800, loss = 1.6311
[2023-10-02 01:40:10] iter = 05810, loss = 1.5772
[2023-10-02 01:40:11] iter = 05820, loss = 1.7655
[2023-10-02 01:40:12] iter = 05830, loss = 1.5177
[2023-10-02 01:40:12] iter = 05840, loss = 1.7180
[2023-10-02 01:40:13] iter = 05850, loss = 1.5550
[2023-10-02 01:40:14] iter = 05860, loss = 1.5582
[2023-10-02 01:40:15] iter = 05870, loss = 1.6570
[2023-10-02 01:40:16] iter = 05880, loss = 1.6154
[2023-10-02 01:40:17] iter = 05890, loss = 1.5302
[2023-10-02 01:40:18] iter = 05900, loss = 1.7102
[2023-10-02 01:40:19] iter = 05910, loss = 1.7056
[2023-10-02 01:40:19] iter = 05920, loss = 1.6485
[2023-10-02 01:40:20] iter = 05930, loss = 1.5173
[2023-10-02 01:40:21] iter = 05940, loss = 1.8540
[2023-10-02 01:40:22] iter = 05950, loss = 1.6210
[2023-10-02 01:40:23] iter = 05960, loss = 1.5402
[2023-10-02 01:40:24] iter = 05970, loss = 1.5755
[2023-10-02 01:40:25] iter = 05980, loss = 1.9391
[2023-10-02 01:40:26] iter = 05990, loss = 1.5673
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 27232}
[2023-10-02 01:40:51] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.023200 train acc = 0.9980, test acc = 0.6129
[2023-10-02 01:41:15] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001553 train acc = 1.0000, test acc = 0.6010
[2023-10-02 01:41:39] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.001665 train acc = 1.0000, test acc = 0.6161
[2023-10-02 01:42:04] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001429 train acc = 1.0000, test acc = 0.6073
[2023-10-02 01:42:28] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.001752 train acc = 1.0000, test acc = 0.6071
[2023-10-02 01:42:52] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.007015 train acc = 1.0000, test acc = 0.6130
[2023-10-02 01:43:16] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004218 train acc = 1.0000, test acc = 0.6058
[2023-10-02 01:43:40] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003525 train acc = 1.0000, test acc = 0.6060
[2023-10-02 01:44:05] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.010134 train acc = 1.0000, test acc = 0.6139
[2023-10-02 01:44:29] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013812 train acc = 1.0000, test acc = 0.6143
[2023-10-02 01:44:53] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.013814 train acc = 0.9980, test acc = 0.6085
[2023-10-02 01:45:17] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015271 train acc = 1.0000, test acc = 0.6109
[2023-10-02 01:45:41] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.014405 train acc = 1.0000, test acc = 0.6068
[2023-10-02 01:46:05] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.023750 train acc = 0.9980, test acc = 0.6090
[2023-10-02 01:46:29] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.015923 train acc = 0.9980, test acc = 0.6184
[2023-10-02 01:46:54] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.010779 train acc = 1.0000, test acc = 0.6139
[2023-10-02 01:47:18] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.013062 train acc = 1.0000, test acc = 0.6089
[2023-10-02 01:47:42] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.022380 train acc = 0.9980, test acc = 0.6094
[2023-10-02 01:48:06] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.020815 train acc = 1.0000, test acc = 0.6102
[2023-10-02 01:48:30] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.007687 train acc = 1.0000, test acc = 0.6113
Evaluate 20 random ConvNet, mean = 0.6102 std = 0.0040
-------------------------
[2023-10-02 01:48:31] iter = 06000, loss = 1.4148
[2023-10-02 01:48:31] iter = 06010, loss = 1.6349
[2023-10-02 01:48:32] iter = 06020, loss = 1.5176
[2023-10-02 01:48:33] iter = 06030, loss = 1.5446
[2023-10-02 01:48:34] iter = 06040, loss = 1.5345
[2023-10-02 01:48:35] iter = 06050, loss = 1.5019
[2023-10-02 01:48:36] iter = 06060, loss = 1.6148
[2023-10-02 01:48:37] iter = 06070, loss = 1.5546
[2023-10-02 01:48:38] iter = 06080, loss = 1.6032
[2023-10-02 01:48:39] iter = 06090, loss = 1.5887
[2023-10-02 01:48:40] iter = 06100, loss = 1.5026
[2023-10-02 01:48:41] iter = 06110, loss = 1.6963
[2023-10-02 01:48:42] iter = 06120, loss = 1.6467
[2023-10-02 01:48:42] iter = 06130, loss = 1.5525
[2023-10-02 01:48:43] iter = 06140, loss = 1.7377
[2023-10-02 01:48:44] iter = 06150, loss = 1.5000
[2023-10-02 01:48:45] iter = 06160, loss = 1.9569
[2023-10-02 01:48:46] iter = 06170, loss = 1.5521
[2023-10-02 01:48:47] iter = 06180, loss = 1.6352
[2023-10-02 01:48:48] iter = 06190, loss = 1.6516
[2023-10-02 01:48:49] iter = 06200, loss = 1.7170
[2023-10-02 01:48:50] iter = 06210, loss = 1.5209
[2023-10-02 01:48:51] iter = 06220, loss = 1.6643
[2023-10-02 01:48:51] iter = 06230, loss = 1.5273
[2023-10-02 01:48:52] iter = 06240, loss = 1.7942
[2023-10-02 01:48:53] iter = 06250, loss = 1.5675
[2023-10-02 01:48:54] iter = 06260, loss = 1.5390
[2023-10-02 01:48:55] iter = 06270, loss = 1.4854
[2023-10-02 01:48:56] iter = 06280, loss = 1.6440
[2023-10-02 01:48:57] iter = 06290, loss = 1.7136
[2023-10-02 01:48:58] iter = 06300, loss = 1.7210
[2023-10-02 01:48:59] iter = 06310, loss = 1.6241
[2023-10-02 01:49:00] iter = 06320, loss = 1.7722
[2023-10-02 01:49:00] iter = 06330, loss = 1.5016
[2023-10-02 01:49:01] iter = 06340, loss = 1.7366
[2023-10-02 01:49:02] iter = 06350, loss = 1.5800
[2023-10-02 01:49:03] iter = 06360, loss = 1.5623
[2023-10-02 01:49:04] iter = 06370, loss = 1.5608
[2023-10-02 01:49:05] iter = 06380, loss = 1.6021
[2023-10-02 01:49:06] iter = 06390, loss = 1.5910
[2023-10-02 01:49:07] iter = 06400, loss = 1.6442
[2023-10-02 01:49:08] iter = 06410, loss = 1.5999
[2023-10-02 01:49:09] iter = 06420, loss = 1.8399
[2023-10-02 01:49:10] iter = 06430, loss = 1.7193
[2023-10-02 01:49:10] iter = 06440, loss = 1.6015
[2023-10-02 01:49:11] iter = 06450, loss = 1.5695
[2023-10-02 01:49:12] iter = 06460, loss = 1.5567
[2023-10-02 01:49:13] iter = 06470, loss = 1.5734
[2023-10-02 01:49:14] iter = 06480, loss = 1.6490
[2023-10-02 01:49:15] iter = 06490, loss = 1.6593
[2023-10-02 01:49:16] iter = 06500, loss = 1.5457
[2023-10-02 01:49:17] iter = 06510, loss = 1.6790
[2023-10-02 01:49:18] iter = 06520, loss = 1.6108
[2023-10-02 01:49:19] iter = 06530, loss = 1.6670
[2023-10-02 01:49:19] iter = 06540, loss = 1.6915
[2023-10-02 01:49:20] iter = 06550, loss = 1.5551
[2023-10-02 01:49:21] iter = 06560, loss = 1.6100
[2023-10-02 01:49:22] iter = 06570, loss = 1.6508
[2023-10-02 01:49:23] iter = 06580, loss = 1.6976
[2023-10-02 01:49:24] iter = 06590, loss = 1.6713
[2023-10-02 01:49:25] iter = 06600, loss = 1.5637
[2023-10-02 01:49:26] iter = 06610, loss = 1.6427
[2023-10-02 01:49:27] iter = 06620, loss = 1.4615
[2023-10-02 01:49:28] iter = 06630, loss = 1.6679
[2023-10-02 01:49:29] iter = 06640, loss = 1.7077
[2023-10-02 01:49:29] iter = 06650, loss = 1.5866
[2023-10-02 01:49:30] iter = 06660, loss = 1.5085
[2023-10-02 01:49:31] iter = 06670, loss = 1.6041
[2023-10-02 01:49:32] iter = 06680, loss = 1.6016
[2023-10-02 01:49:33] iter = 06690, loss = 1.5800
[2023-10-02 01:49:34] iter = 06700, loss = 1.4838
[2023-10-02 01:49:35] iter = 06710, loss = 1.5569
[2023-10-02 01:49:36] iter = 06720, loss = 1.6844
[2023-10-02 01:49:37] iter = 06730, loss = 1.5932
[2023-10-02 01:49:38] iter = 06740, loss = 1.4993
[2023-10-02 01:49:38] iter = 06750, loss = 1.6193
[2023-10-02 01:49:39] iter = 06760, loss = 1.7162
[2023-10-02 01:49:40] iter = 06770, loss = 1.5917
[2023-10-02 01:49:41] iter = 06780, loss = 1.6397
[2023-10-02 01:49:42] iter = 06790, loss = 1.6618
[2023-10-02 01:49:43] iter = 06800, loss = 1.5857
[2023-10-02 01:49:44] iter = 06810, loss = 1.5611
[2023-10-02 01:49:45] iter = 06820, loss = 1.7046
[2023-10-02 01:49:46] iter = 06830, loss = 1.6085
[2023-10-02 01:49:47] iter = 06840, loss = 1.6139
[2023-10-02 01:49:48] iter = 06850, loss = 1.6035
[2023-10-02 01:49:48] iter = 06860, loss = 1.4843
[2023-10-02 01:49:49] iter = 06870, loss = 1.5448
[2023-10-02 01:49:50] iter = 06880, loss = 1.5827
[2023-10-02 01:49:51] iter = 06890, loss = 1.7063
[2023-10-02 01:49:52] iter = 06900, loss = 1.6531
[2023-10-02 01:49:53] iter = 06910, loss = 1.6848
[2023-10-02 01:49:54] iter = 06920, loss = 1.6207
[2023-10-02 01:49:55] iter = 06930, loss = 1.5929
[2023-10-02 01:49:56] iter = 06940, loss = 1.4835
[2023-10-02 01:49:57] iter = 06950, loss = 1.4865
[2023-10-02 01:49:57] iter = 06960, loss = 1.5333
[2023-10-02 01:49:58] iter = 06970, loss = 1.4620
[2023-10-02 01:49:59] iter = 06980, loss = 1.5206
[2023-10-02 01:50:00] iter = 06990, loss = 1.6448
[2023-10-02 01:50:01] iter = 07000, loss = 1.7561
[2023-10-02 01:50:02] iter = 07010, loss = 1.7520
[2023-10-02 01:50:03] iter = 07020, loss = 1.5796
[2023-10-02 01:50:04] iter = 07030, loss = 1.5713
[2023-10-02 01:50:04] iter = 07040, loss = 1.6618
[2023-10-02 01:50:05] iter = 07050, loss = 1.6226
[2023-10-02 01:50:06] iter = 07060, loss = 1.4881
[2023-10-02 01:50:07] iter = 07070, loss = 1.6407
[2023-10-02 01:50:08] iter = 07080, loss = 1.5321
[2023-10-02 01:50:09] iter = 07090, loss = 1.5722
[2023-10-02 01:50:10] iter = 07100, loss = 1.5174
[2023-10-02 01:50:11] iter = 07110, loss = 1.4536
[2023-10-02 01:50:12] iter = 07120, loss = 1.6961
[2023-10-02 01:50:13] iter = 07130, loss = 1.5316
[2023-10-02 01:50:14] iter = 07140, loss = 1.6080
[2023-10-02 01:50:15] iter = 07150, loss = 1.5897
[2023-10-02 01:50:15] iter = 07160, loss = 1.5130
[2023-10-02 01:50:16] iter = 07170, loss = 1.5340
[2023-10-02 01:50:17] iter = 07180, loss = 1.4708
[2023-10-02 01:50:18] iter = 07190, loss = 1.5560
[2023-10-02 01:50:19] iter = 07200, loss = 1.5953
[2023-10-02 01:50:20] iter = 07210, loss = 1.5620
[2023-10-02 01:50:21] iter = 07220, loss = 1.5270
[2023-10-02 01:50:22] iter = 07230, loss = 1.6220
[2023-10-02 01:50:23] iter = 07240, loss = 1.5787
[2023-10-02 01:50:24] iter = 07250, loss = 1.6309
[2023-10-02 01:50:25] iter = 07260, loss = 1.5875
[2023-10-02 01:50:25] iter = 07270, loss = 1.6729
[2023-10-02 01:50:26] iter = 07280, loss = 1.6414
[2023-10-02 01:50:27] iter = 07290, loss = 1.6963
[2023-10-02 01:50:28] iter = 07300, loss = 1.6227
[2023-10-02 01:50:29] iter = 07310, loss = 1.5532
[2023-10-02 01:50:30] iter = 07320, loss = 1.6006
[2023-10-02 01:50:31] iter = 07330, loss = 1.5595
[2023-10-02 01:50:32] iter = 07340, loss = 1.5872
[2023-10-02 01:50:33] iter = 07350, loss = 1.5433
[2023-10-02 01:50:34] iter = 07360, loss = 1.5638
[2023-10-02 01:50:34] iter = 07370, loss = 1.5640
[2023-10-02 01:50:35] iter = 07380, loss = 1.4542
[2023-10-02 01:50:36] iter = 07390, loss = 1.6706
[2023-10-02 01:50:37] iter = 07400, loss = 1.6281
[2023-10-02 01:50:38] iter = 07410, loss = 1.6069
[2023-10-02 01:50:39] iter = 07420, loss = 1.5251
[2023-10-02 01:50:40] iter = 07430, loss = 1.4882
[2023-10-02 01:50:41] iter = 07440, loss = 1.6050
[2023-10-02 01:50:42] iter = 07450, loss = 1.5400
[2023-10-02 01:50:43] iter = 07460, loss = 1.7013
[2023-10-02 01:50:43] iter = 07470, loss = 1.5718
[2023-10-02 01:50:44] iter = 07480, loss = 1.5923
[2023-10-02 01:50:45] iter = 07490, loss = 1.5287
[2023-10-02 01:50:46] iter = 07500, loss = 1.5406
[2023-10-02 01:50:47] iter = 07510, loss = 1.3907
[2023-10-02 01:50:48] iter = 07520, loss = 1.5545
[2023-10-02 01:50:49] iter = 07530, loss = 1.6850
[2023-10-02 01:50:50] iter = 07540, loss = 1.6281
[2023-10-02 01:50:51] iter = 07550, loss = 1.5388
[2023-10-02 01:50:52] iter = 07560, loss = 1.7082
[2023-10-02 01:50:53] iter = 07570, loss = 1.6032
[2023-10-02 01:50:54] iter = 07580, loss = 1.6620
[2023-10-02 01:50:55] iter = 07590, loss = 1.5275
[2023-10-02 01:50:55] iter = 07600, loss = 1.4991
[2023-10-02 01:50:56] iter = 07610, loss = 1.6871
[2023-10-02 01:50:57] iter = 07620, loss = 1.6485
[2023-10-02 01:50:58] iter = 07630, loss = 1.6077
[2023-10-02 01:50:59] iter = 07640, loss = 1.5363
[2023-10-02 01:51:00] iter = 07650, loss = 1.5989
[2023-10-02 01:51:01] iter = 07660, loss = 1.5765
[2023-10-02 01:51:02] iter = 07670, loss = 1.6702
[2023-10-02 01:51:03] iter = 07680, loss = 1.8049
[2023-10-02 01:51:04] iter = 07690, loss = 1.5377
[2023-10-02 01:51:05] iter = 07700, loss = 1.5263
[2023-10-02 01:51:05] iter = 07710, loss = 1.6664
[2023-10-02 01:51:06] iter = 07720, loss = 1.5682
[2023-10-02 01:51:07] iter = 07730, loss = 1.5719
[2023-10-02 01:51:08] iter = 07740, loss = 1.6125
[2023-10-02 01:51:09] iter = 07750, loss = 1.5180
[2023-10-02 01:51:10] iter = 07760, loss = 1.6197
[2023-10-02 01:51:11] iter = 07770, loss = 1.5405
[2023-10-02 01:51:12] iter = 07780, loss = 1.7479
[2023-10-02 01:51:13] iter = 07790, loss = 1.6156
[2023-10-02 01:51:13] iter = 07800, loss = 1.5395
[2023-10-02 01:51:14] iter = 07810, loss = 1.5786
[2023-10-02 01:51:15] iter = 07820, loss = 1.7144
[2023-10-02 01:51:16] iter = 07830, loss = 1.5614
[2023-10-02 01:51:17] iter = 07840, loss = 1.5821
[2023-10-02 01:51:18] iter = 07850, loss = 1.5834
[2023-10-02 01:51:19] iter = 07860, loss = 1.7451
[2023-10-02 01:51:20] iter = 07870, loss = 1.5900
[2023-10-02 01:51:21] iter = 07880, loss = 1.7383
[2023-10-02 01:51:22] iter = 07890, loss = 1.5151
[2023-10-02 01:51:22] iter = 07900, loss = 1.7413
[2023-10-02 01:51:23] iter = 07910, loss = 1.6417
[2023-10-02 01:51:24] iter = 07920, loss = 1.5257
[2023-10-02 01:51:25] iter = 07930, loss = 1.4895
[2023-10-02 01:51:26] iter = 07940, loss = 1.4758
[2023-10-02 01:51:27] iter = 07950, loss = 1.6403
[2023-10-02 01:51:28] iter = 07960, loss = 1.6420
[2023-10-02 01:51:29] iter = 07970, loss = 1.4395
[2023-10-02 01:51:30] iter = 07980, loss = 1.6548
[2023-10-02 01:51:30] iter = 07990, loss = 1.4920
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 91654}
[2023-10-02 01:51:55] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.027829 train acc = 1.0000, test acc = 0.6158
[2023-10-02 01:52:19] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010636 train acc = 1.0000, test acc = 0.6176
[2023-10-02 01:52:44] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.005889 train acc = 1.0000, test acc = 0.6157
[2023-10-02 01:53:08] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001912 train acc = 1.0000, test acc = 0.6152
[2023-10-02 01:53:32] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.013227 train acc = 1.0000, test acc = 0.6150
[2023-10-02 01:53:56] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004255 train acc = 1.0000, test acc = 0.6157
[2023-10-02 01:54:20] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012170 train acc = 1.0000, test acc = 0.6199
[2023-10-02 01:54:44] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001864 train acc = 1.0000, test acc = 0.6143
[2023-10-02 01:55:08] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.016461 train acc = 0.9980, test acc = 0.6175
[2023-10-02 01:55:33] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013878 train acc = 1.0000, test acc = 0.6090
[2023-10-02 01:55:57] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003746 train acc = 1.0000, test acc = 0.6105
[2023-10-02 01:56:21] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.013151 train acc = 1.0000, test acc = 0.6179
[2023-10-02 01:56:45] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004494 train acc = 1.0000, test acc = 0.6072
[2023-10-02 01:57:09] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.015414 train acc = 1.0000, test acc = 0.6190
[2023-10-02 01:57:33] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.014255 train acc = 1.0000, test acc = 0.6166
[2023-10-02 01:57:57] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.005665 train acc = 1.0000, test acc = 0.6119
[2023-10-02 01:58:21] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.011945 train acc = 0.9980, test acc = 0.6069
[2023-10-02 01:58:46] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004072 train acc = 1.0000, test acc = 0.6136
[2023-10-02 01:59:10] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003067 train acc = 1.0000, test acc = 0.6157
[2023-10-02 01:59:34] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.002053 train acc = 1.0000, test acc = 0.6172
Evaluate 20 random ConvNet, mean = 0.6146 std = 0.0036
-------------------------
[2023-10-02 01:59:34] iter = 08000, loss = 1.6242
[2023-10-02 01:59:35] iter = 08010, loss = 1.6515
[2023-10-02 01:59:36] iter = 08020, loss = 1.5626
[2023-10-02 01:59:37] iter = 08030, loss = 1.5058
[2023-10-02 01:59:38] iter = 08040, loss = 1.6383
[2023-10-02 01:59:39] iter = 08050, loss = 1.5619
[2023-10-02 01:59:40] iter = 08060, loss = 1.6129
[2023-10-02 01:59:41] iter = 08070, loss = 1.5895
[2023-10-02 01:59:42] iter = 08080, loss = 1.7006
[2023-10-02 01:59:42] iter = 08090, loss = 1.7699
[2023-10-02 01:59:43] iter = 08100, loss = 1.6025
[2023-10-02 01:59:44] iter = 08110, loss = 1.4313
[2023-10-02 01:59:45] iter = 08120, loss = 1.5104
[2023-10-02 01:59:46] iter = 08130, loss = 1.5859
[2023-10-02 01:59:47] iter = 08140, loss = 1.5905
[2023-10-02 01:59:48] iter = 08150, loss = 1.6557
[2023-10-02 01:59:49] iter = 08160, loss = 1.5398
[2023-10-02 01:59:50] iter = 08170, loss = 1.5293
[2023-10-02 01:59:51] iter = 08180, loss = 1.5417
[2023-10-02 01:59:51] iter = 08190, loss = 1.5223
[2023-10-02 01:59:52] iter = 08200, loss = 1.5686
[2023-10-02 01:59:53] iter = 08210, loss = 1.5875
[2023-10-02 01:59:54] iter = 08220, loss = 1.4993
[2023-10-02 01:59:55] iter = 08230, loss = 1.4539
[2023-10-02 01:59:56] iter = 08240, loss = 1.5060
[2023-10-02 01:59:57] iter = 08250, loss = 1.5335
[2023-10-02 01:59:58] iter = 08260, loss = 1.3951
[2023-10-02 01:59:59] iter = 08270, loss = 1.7356
[2023-10-02 02:00:00] iter = 08280, loss = 1.5828
[2023-10-02 02:00:01] iter = 08290, loss = 1.5734
[2023-10-02 02:00:02] iter = 08300, loss = 1.5375
[2023-10-02 02:00:03] iter = 08310, loss = 1.5526
[2023-10-02 02:00:03] iter = 08320, loss = 1.4889
[2023-10-02 02:00:04] iter = 08330, loss = 1.6562
[2023-10-02 02:00:05] iter = 08340, loss = 1.5382
[2023-10-02 02:00:06] iter = 08350, loss = 1.5273
[2023-10-02 02:00:07] iter = 08360, loss = 1.7311
[2023-10-02 02:00:08] iter = 08370, loss = 1.4927
[2023-10-02 02:00:09] iter = 08380, loss = 1.6073
[2023-10-02 02:00:10] iter = 08390, loss = 1.4862
[2023-10-02 02:00:11] iter = 08400, loss = 1.4734
[2023-10-02 02:00:12] iter = 08410, loss = 1.5631
[2023-10-02 02:00:13] iter = 08420, loss = 1.5080
[2023-10-02 02:00:13] iter = 08430, loss = 1.8283
[2023-10-02 02:00:14] iter = 08440, loss = 1.5218
[2023-10-02 02:00:15] iter = 08450, loss = 1.6425
[2023-10-02 02:00:16] iter = 08460, loss = 1.5452
[2023-10-02 02:00:17] iter = 08470, loss = 1.4648
[2023-10-02 02:00:18] iter = 08480, loss = 1.5414
[2023-10-02 02:00:19] iter = 08490, loss = 1.5509
[2023-10-02 02:00:20] iter = 08500, loss = 1.4807
[2023-10-02 02:00:21] iter = 08510, loss = 1.4767
[2023-10-02 02:00:22] iter = 08520, loss = 1.4517
[2023-10-02 02:00:23] iter = 08530, loss = 1.5912
[2023-10-02 02:00:23] iter = 08540, loss = 1.5343
[2023-10-02 02:00:24] iter = 08550, loss = 1.6530
[2023-10-02 02:00:25] iter = 08560, loss = 1.5979
[2023-10-02 02:00:26] iter = 08570, loss = 1.6050
[2023-10-02 02:00:27] iter = 08580, loss = 1.6329
[2023-10-02 02:00:28] iter = 08590, loss = 1.6365
[2023-10-02 02:00:29] iter = 08600, loss = 1.5054
[2023-10-02 02:00:30] iter = 08610, loss = 1.5379
[2023-10-02 02:00:31] iter = 08620, loss = 1.4818
[2023-10-02 02:00:32] iter = 08630, loss = 1.6069
[2023-10-02 02:00:33] iter = 08640, loss = 1.4017
[2023-10-02 02:00:34] iter = 08650, loss = 1.5601
[2023-10-02 02:00:34] iter = 08660, loss = 1.6837
[2023-10-02 02:00:35] iter = 08670, loss = 1.4928
[2023-10-02 02:00:36] iter = 08680, loss = 1.7810
[2023-10-02 02:00:37] iter = 08690, loss = 1.6314
[2023-10-02 02:00:38] iter = 08700, loss = 1.6189
[2023-10-02 02:00:39] iter = 08710, loss = 1.5191
[2023-10-02 02:00:40] iter = 08720, loss = 1.7364
[2023-10-02 02:00:41] iter = 08730, loss = 1.6117
[2023-10-02 02:00:42] iter = 08740, loss = 1.4962
[2023-10-02 02:00:42] iter = 08750, loss = 1.5938
[2023-10-02 02:00:43] iter = 08760, loss = 1.5356
[2023-10-02 02:00:44] iter = 08770, loss = 1.4222
[2023-10-02 02:00:45] iter = 08780, loss = 1.6537
[2023-10-02 02:00:46] iter = 08790, loss = 1.6497
[2023-10-02 02:00:47] iter = 08800, loss = 1.5176
[2023-10-02 02:00:48] iter = 08810, loss = 1.6440
[2023-10-02 02:00:49] iter = 08820, loss = 1.5120
[2023-10-02 02:00:50] iter = 08830, loss = 1.6914
[2023-10-02 02:00:51] iter = 08840, loss = 1.4870
[2023-10-02 02:00:51] iter = 08850, loss = 1.7033
[2023-10-02 02:00:52] iter = 08860, loss = 1.4359
[2023-10-02 02:00:53] iter = 08870, loss = 1.4774
[2023-10-02 02:00:54] iter = 08880, loss = 1.6216
[2023-10-02 02:00:55] iter = 08890, loss = 1.6125
[2023-10-02 02:00:56] iter = 08900, loss = 1.6727
[2023-10-02 02:00:57] iter = 08910, loss = 1.5616
[2023-10-02 02:00:58] iter = 08920, loss = 1.6790
[2023-10-02 02:00:59] iter = 08930, loss = 1.6495
[2023-10-02 02:01:00] iter = 08940, loss = 1.5503
[2023-10-02 02:01:01] iter = 08950, loss = 1.7562
[2023-10-02 02:01:01] iter = 08960, loss = 1.6335
[2023-10-02 02:01:02] iter = 08970, loss = 1.4622
[2023-10-02 02:01:03] iter = 08980, loss = 1.6938
[2023-10-02 02:01:04] iter = 08990, loss = 1.5407
[2023-10-02 02:01:05] iter = 09000, loss = 1.6512
[2023-10-02 02:01:06] iter = 09010, loss = 1.5504
[2023-10-02 02:01:07] iter = 09020, loss = 1.5520
[2023-10-02 02:01:08] iter = 09030, loss = 1.5574
[2023-10-02 02:01:08] iter = 09040, loss = 1.6079
[2023-10-02 02:01:09] iter = 09050, loss = 1.4755
[2023-10-02 02:01:10] iter = 09060, loss = 1.5273
[2023-10-02 02:01:11] iter = 09070, loss = 1.5331
[2023-10-02 02:01:12] iter = 09080, loss = 1.4971
[2023-10-02 02:01:13] iter = 09090, loss = 1.5294
[2023-10-02 02:01:14] iter = 09100, loss = 1.4383
[2023-10-02 02:01:15] iter = 09110, loss = 1.5868
[2023-10-02 02:01:16] iter = 09120, loss = 1.6239
[2023-10-02 02:01:17] iter = 09130, loss = 1.4815
[2023-10-02 02:01:18] iter = 09140, loss = 1.4626
[2023-10-02 02:01:18] iter = 09150, loss = 1.5213
[2023-10-02 02:01:19] iter = 09160, loss = 1.5636
[2023-10-02 02:01:20] iter = 09170, loss = 1.6299
[2023-10-02 02:01:21] iter = 09180, loss = 1.8017
[2023-10-02 02:01:22] iter = 09190, loss = 1.5049
[2023-10-02 02:01:23] iter = 09200, loss = 1.6011
[2023-10-02 02:01:24] iter = 09210, loss = 1.6438
[2023-10-02 02:01:25] iter = 09220, loss = 1.5214
[2023-10-02 02:01:26] iter = 09230, loss = 1.6506
[2023-10-02 02:01:27] iter = 09240, loss = 1.5518
[2023-10-02 02:01:28] iter = 09250, loss = 1.5473
[2023-10-02 02:01:29] iter = 09260, loss = 1.5642
[2023-10-02 02:01:30] iter = 09270, loss = 1.6606
[2023-10-02 02:01:30] iter = 09280, loss = 1.6942
[2023-10-02 02:01:31] iter = 09290, loss = 1.5429
[2023-10-02 02:01:32] iter = 09300, loss = 1.6743
[2023-10-02 02:01:33] iter = 09310, loss = 1.5277
[2023-10-02 02:01:34] iter = 09320, loss = 1.5508
[2023-10-02 02:01:35] iter = 09330, loss = 1.4723
[2023-10-02 02:01:36] iter = 09340, loss = 1.5535
[2023-10-02 02:01:37] iter = 09350, loss = 1.5477
[2023-10-02 02:01:38] iter = 09360, loss = 1.5329
[2023-10-02 02:01:39] iter = 09370, loss = 1.6624
[2023-10-02 02:01:40] iter = 09380, loss = 1.4053
[2023-10-02 02:01:41] iter = 09390, loss = 1.6119
[2023-10-02 02:01:41] iter = 09400, loss = 1.7413
[2023-10-02 02:01:42] iter = 09410, loss = 1.5912
[2023-10-02 02:01:43] iter = 09420, loss = 1.4462
[2023-10-02 02:01:44] iter = 09430, loss = 1.6353
[2023-10-02 02:01:45] iter = 09440, loss = 1.5403
[2023-10-02 02:01:46] iter = 09450, loss = 1.6374
[2023-10-02 02:01:47] iter = 09460, loss = 1.4445
[2023-10-02 02:01:48] iter = 09470, loss = 1.5593
[2023-10-02 02:01:48] iter = 09480, loss = 1.6885
[2023-10-02 02:01:49] iter = 09490, loss = 1.4607
[2023-10-02 02:01:50] iter = 09500, loss = 1.5823
[2023-10-02 02:01:51] iter = 09510, loss = 1.4608
[2023-10-02 02:01:52] iter = 09520, loss = 1.5652
[2023-10-02 02:01:53] iter = 09530, loss = 1.4384
[2023-10-02 02:01:54] iter = 09540, loss = 1.4227
[2023-10-02 02:01:55] iter = 09550, loss = 1.4675
[2023-10-02 02:01:56] iter = 09560, loss = 1.6084
[2023-10-02 02:01:57] iter = 09570, loss = 1.5118
[2023-10-02 02:01:57] iter = 09580, loss = 1.4387
[2023-10-02 02:01:58] iter = 09590, loss = 1.6256
[2023-10-02 02:01:59] iter = 09600, loss = 1.7021
[2023-10-02 02:02:00] iter = 09610, loss = 1.5667
[2023-10-02 02:02:01] iter = 09620, loss = 1.4957
[2023-10-02 02:02:02] iter = 09630, loss = 1.5293
[2023-10-02 02:02:03] iter = 09640, loss = 1.7545
[2023-10-02 02:02:04] iter = 09650, loss = 1.5352
[2023-10-02 02:02:04] iter = 09660, loss = 1.5110
[2023-10-02 02:02:05] iter = 09670, loss = 1.6842
[2023-10-02 02:02:06] iter = 09680, loss = 1.5471
[2023-10-02 02:02:07] iter = 09690, loss = 1.5889
[2023-10-02 02:02:08] iter = 09700, loss = 1.5548
[2023-10-02 02:02:09] iter = 09710, loss = 1.4641
[2023-10-02 02:02:10] iter = 09720, loss = 1.5187
[2023-10-02 02:02:11] iter = 09730, loss = 1.5891
[2023-10-02 02:02:12] iter = 09740, loss = 1.6437
[2023-10-02 02:02:13] iter = 09750, loss = 1.6032
[2023-10-02 02:02:14] iter = 09760, loss = 1.5063
[2023-10-02 02:02:15] iter = 09770, loss = 1.6539
[2023-10-02 02:02:16] iter = 09780, loss = 1.7209
[2023-10-02 02:02:16] iter = 09790, loss = 1.5351
[2023-10-02 02:02:17] iter = 09800, loss = 1.5798
[2023-10-02 02:02:18] iter = 09810, loss = 1.6166
[2023-10-02 02:02:19] iter = 09820, loss = 1.6044
[2023-10-02 02:02:20] iter = 09830, loss = 1.4921
[2023-10-02 02:02:21] iter = 09840, loss = 1.5100
[2023-10-02 02:02:22] iter = 09850, loss = 1.5096
[2023-10-02 02:02:23] iter = 09860, loss = 1.7090
[2023-10-02 02:02:24] iter = 09870, loss = 1.6118
[2023-10-02 02:02:25] iter = 09880, loss = 1.6925
[2023-10-02 02:02:26] iter = 09890, loss = 1.4610
[2023-10-02 02:02:26] iter = 09900, loss = 1.5767
[2023-10-02 02:02:27] iter = 09910, loss = 1.5320
[2023-10-02 02:02:28] iter = 09920, loss = 1.5372
[2023-10-02 02:02:29] iter = 09930, loss = 1.4700
[2023-10-02 02:02:30] iter = 09940, loss = 1.5575
[2023-10-02 02:02:31] iter = 09950, loss = 1.5063
[2023-10-02 02:02:32] iter = 09960, loss = 1.5427
[2023-10-02 02:02:33] iter = 09970, loss = 1.5345
[2023-10-02 02:02:34] iter = 09980, loss = 1.5162
[2023-10-02 02:02:35] iter = 09990, loss = 1.6183
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 56159}
[2023-10-02 02:03:00] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.003718 train acc = 1.0000, test acc = 0.6299
[2023-10-02 02:03:24] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.028676 train acc = 0.9980, test acc = 0.6183
[2023-10-02 02:03:48] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.018053 train acc = 0.9980, test acc = 0.6126
[2023-10-02 02:04:12] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.011357 train acc = 1.0000, test acc = 0.6232
[2023-10-02 02:04:37] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011155 train acc = 1.0000, test acc = 0.6194
[2023-10-02 02:05:01] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013454 train acc = 1.0000, test acc = 0.6104
[2023-10-02 02:05:25] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012344 train acc = 1.0000, test acc = 0.6169
[2023-10-02 02:05:49] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003063 train acc = 1.0000, test acc = 0.6207
[2023-10-02 02:06:13] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.015431 train acc = 1.0000, test acc = 0.6200
[2023-10-02 02:06:38] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.017444 train acc = 1.0000, test acc = 0.6228
[2023-10-02 02:07:02] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.005243 train acc = 1.0000, test acc = 0.6202
[2023-10-02 02:07:26] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.012094 train acc = 0.9980, test acc = 0.6234
[2023-10-02 02:07:50] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.007818 train acc = 0.9960, test acc = 0.6184
[2023-10-02 02:08:14] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.016322 train acc = 1.0000, test acc = 0.6275
[2023-10-02 02:08:38] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003616 train acc = 1.0000, test acc = 0.6261
[2023-10-02 02:09:02] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.011028 train acc = 1.0000, test acc = 0.6233
[2023-10-02 02:09:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.017806 train acc = 1.0000, test acc = 0.6132
[2023-10-02 02:09:51] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.012763 train acc = 1.0000, test acc = 0.6223
[2023-10-02 02:10:15] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001737 train acc = 1.0000, test acc = 0.6156
[2023-10-02 02:10:39] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.023491 train acc = 0.9980, test acc = 0.6196
Evaluate 20 random ConvNet, mean = 0.6202 std = 0.0048
-------------------------
[2023-10-02 02:10:39] iter = 10000, loss = 1.6133
[2023-10-02 02:10:40] iter = 10010, loss = 1.5589
[2023-10-02 02:10:41] iter = 10020, loss = 1.5220
[2023-10-02 02:10:42] iter = 10030, loss = 1.4969
[2023-10-02 02:10:43] iter = 10040, loss = 1.5856
[2023-10-02 02:10:44] iter = 10050, loss = 1.7650
[2023-10-02 02:10:45] iter = 10060, loss = 1.5926
[2023-10-02 02:10:46] iter = 10070, loss = 1.5575
[2023-10-02 02:10:47] iter = 10080, loss = 1.5449
[2023-10-02 02:10:48] iter = 10090, loss = 1.5544
[2023-10-02 02:10:48] iter = 10100, loss = 1.4684
[2023-10-02 02:10:49] iter = 10110, loss = 1.5539
[2023-10-02 02:10:50] iter = 10120, loss = 1.4204
[2023-10-02 02:10:51] iter = 10130, loss = 1.3957
[2023-10-02 02:10:52] iter = 10140, loss = 1.6564
[2023-10-02 02:10:53] iter = 10150, loss = 1.5581
[2023-10-02 02:10:54] iter = 10160, loss = 1.5547
[2023-10-02 02:10:55] iter = 10170, loss = 1.6199
[2023-10-02 02:10:56] iter = 10180, loss = 1.6415
[2023-10-02 02:10:57] iter = 10190, loss = 1.5400
[2023-10-02 02:10:58] iter = 10200, loss = 1.5740
[2023-10-02 02:10:58] iter = 10210, loss = 1.5056
[2023-10-02 02:10:59] iter = 10220, loss = 1.5268
[2023-10-02 02:11:00] iter = 10230, loss = 1.6328
[2023-10-02 02:11:01] iter = 10240, loss = 1.5262
[2023-10-02 02:11:02] iter = 10250, loss = 1.5938
[2023-10-02 02:11:03] iter = 10260, loss = 1.5050
[2023-10-02 02:11:04] iter = 10270, loss = 1.5573
[2023-10-02 02:11:05] iter = 10280, loss = 1.6873
[2023-10-02 02:11:06] iter = 10290, loss = 1.4810
[2023-10-02 02:11:06] iter = 10300, loss = 1.4626
[2023-10-02 02:11:07] iter = 10310, loss = 1.4916
[2023-10-02 02:11:08] iter = 10320, loss = 1.4469
[2023-10-02 02:11:09] iter = 10330, loss = 1.6298
[2023-10-02 02:11:10] iter = 10340, loss = 1.5078
[2023-10-02 02:11:11] iter = 10350, loss = 1.5995
[2023-10-02 02:11:12] iter = 10360, loss = 1.7413
[2023-10-02 02:11:13] iter = 10370, loss = 1.5312
[2023-10-02 02:11:14] iter = 10380, loss = 1.4894
[2023-10-02 02:11:15] iter = 10390, loss = 1.5853
[2023-10-02 02:11:15] iter = 10400, loss = 1.5233
[2023-10-02 02:11:16] iter = 10410, loss = 1.6285
[2023-10-02 02:11:17] iter = 10420, loss = 1.6149
[2023-10-02 02:11:18] iter = 10430, loss = 1.4077
[2023-10-02 02:11:19] iter = 10440, loss = 1.4789
[2023-10-02 02:11:20] iter = 10450, loss = 1.6064
[2023-10-02 02:11:21] iter = 10460, loss = 1.5801
[2023-10-02 02:11:22] iter = 10470, loss = 1.6072
[2023-10-02 02:11:23] iter = 10480, loss = 1.5676
[2023-10-02 02:11:24] iter = 10490, loss = 1.5546
[2023-10-02 02:11:25] iter = 10500, loss = 1.5154
[2023-10-02 02:11:26] iter = 10510, loss = 1.4710
[2023-10-02 02:11:27] iter = 10520, loss = 1.6669
[2023-10-02 02:11:27] iter = 10530, loss = 1.5414
[2023-10-02 02:11:28] iter = 10540, loss = 1.5565
[2023-10-02 02:11:29] iter = 10550, loss = 1.5783
[2023-10-02 02:11:30] iter = 10560, loss = 1.5085
[2023-10-02 02:11:31] iter = 10570, loss = 1.6995
[2023-10-02 02:11:32] iter = 10580, loss = 1.6909
[2023-10-02 02:11:33] iter = 10590, loss = 1.5439
[2023-10-02 02:11:34] iter = 10600, loss = 1.5414
[2023-10-02 02:11:35] iter = 10610, loss = 1.6587
[2023-10-02 02:11:35] iter = 10620, loss = 1.5687
[2023-10-02 02:11:36] iter = 10630, loss = 1.5410
[2023-10-02 02:11:37] iter = 10640, loss = 1.6853
[2023-10-02 02:11:38] iter = 10650, loss = 1.6723
[2023-10-02 02:11:39] iter = 10660, loss = 1.5378
[2023-10-02 02:11:40] iter = 10670, loss = 1.4107
[2023-10-02 02:11:41] iter = 10680, loss = 1.4948
[2023-10-02 02:11:42] iter = 10690, loss = 1.4662
[2023-10-02 02:11:42] iter = 10700, loss = 1.6426
[2023-10-02 02:11:43] iter = 10710, loss = 1.5563
[2023-10-02 02:11:44] iter = 10720, loss = 1.5016
[2023-10-02 02:11:45] iter = 10730, loss = 1.5026
[2023-10-02 02:11:46] iter = 10740, loss = 1.5669
[2023-10-02 02:11:47] iter = 10750, loss = 1.5340
[2023-10-02 02:11:48] iter = 10760, loss = 1.5605
[2023-10-02 02:11:49] iter = 10770, loss = 1.4167
[2023-10-02 02:11:50] iter = 10780, loss = 1.5531
[2023-10-02 02:11:51] iter = 10790, loss = 1.4294
[2023-10-02 02:11:52] iter = 10800, loss = 1.5221
[2023-10-02 02:11:53] iter = 10810, loss = 1.5559
[2023-10-02 02:11:53] iter = 10820, loss = 1.4381
[2023-10-02 02:11:54] iter = 10830, loss = 1.5696
[2023-10-02 02:11:55] iter = 10840, loss = 1.5532
[2023-10-02 02:11:56] iter = 10850, loss = 1.6054
[2023-10-02 02:11:57] iter = 10860, loss = 1.5811
[2023-10-02 02:11:58] iter = 10870, loss = 1.4840
[2023-10-02 02:11:59] iter = 10880, loss = 1.4132
[2023-10-02 02:12:00] iter = 10890, loss = 1.5777
[2023-10-02 02:12:01] iter = 10900, loss = 1.6786
[2023-10-02 02:12:01] iter = 10910, loss = 1.5788
[2023-10-02 02:12:02] iter = 10920, loss = 1.4732
[2023-10-02 02:12:03] iter = 10930, loss = 1.5954
[2023-10-02 02:12:04] iter = 10940, loss = 1.5701
[2023-10-02 02:12:05] iter = 10950, loss = 1.5574
[2023-10-02 02:12:06] iter = 10960, loss = 1.6133
[2023-10-02 02:12:07] iter = 10970, loss = 1.5103
[2023-10-02 02:12:08] iter = 10980, loss = 1.5284
[2023-10-02 02:12:09] iter = 10990, loss = 1.4219
[2023-10-02 02:12:10] iter = 11000, loss = 1.6014
[2023-10-02 02:12:10] iter = 11010, loss = 1.6587
[2023-10-02 02:12:11] iter = 11020, loss = 1.6075
[2023-10-02 02:12:12] iter = 11030, loss = 1.6024
[2023-10-02 02:12:13] iter = 11040, loss = 1.5120
[2023-10-02 02:12:14] iter = 11050, loss = 1.4788
[2023-10-02 02:12:15] iter = 11060, loss = 1.5511
[2023-10-02 02:12:16] iter = 11070, loss = 1.6088
[2023-10-02 02:12:17] iter = 11080, loss = 1.5014
[2023-10-02 02:12:18] iter = 11090, loss = 1.5051
[2023-10-02 02:12:19] iter = 11100, loss = 1.4362
[2023-10-02 02:12:20] iter = 11110, loss = 1.4907
[2023-10-02 02:12:20] iter = 11120, loss = 1.4786
[2023-10-02 02:12:21] iter = 11130, loss = 1.4767
[2023-10-02 02:12:22] iter = 11140, loss = 1.6106
[2023-10-02 02:12:23] iter = 11150, loss = 1.8665
[2023-10-02 02:12:24] iter = 11160, loss = 1.6166
[2023-10-02 02:12:25] iter = 11170, loss = 1.4490
[2023-10-02 02:12:26] iter = 11180, loss = 1.7057
[2023-10-02 02:12:27] iter = 11190, loss = 1.4784
[2023-10-02 02:12:28] iter = 11200, loss = 1.6047
[2023-10-02 02:12:28] iter = 11210, loss = 1.4728
[2023-10-02 02:12:29] iter = 11220, loss = 1.5983
[2023-10-02 02:12:30] iter = 11230, loss = 1.6140
[2023-10-02 02:12:31] iter = 11240, loss = 1.6105
[2023-10-02 02:12:32] iter = 11250, loss = 1.7314
[2023-10-02 02:12:33] iter = 11260, loss = 1.4533
[2023-10-02 02:12:34] iter = 11270, loss = 1.5645
[2023-10-02 02:12:35] iter = 11280, loss = 1.4463
[2023-10-02 02:12:36] iter = 11290, loss = 1.5494
[2023-10-02 02:12:37] iter = 11300, loss = 1.4956
[2023-10-02 02:12:38] iter = 11310, loss = 1.5062
[2023-10-02 02:12:38] iter = 11320, loss = 1.3847
[2023-10-02 02:12:39] iter = 11330, loss = 1.5008
[2023-10-02 02:12:40] iter = 11340, loss = 1.5903
[2023-10-02 02:12:41] iter = 11350, loss = 1.6015
[2023-10-02 02:12:42] iter = 11360, loss = 1.5055
[2023-10-02 02:12:43] iter = 11370, loss = 1.6711
[2023-10-02 02:12:44] iter = 11380, loss = 1.5289
[2023-10-02 02:12:45] iter = 11390, loss = 1.5397
[2023-10-02 02:12:46] iter = 11400, loss = 1.4979
[2023-10-02 02:12:47] iter = 11410, loss = 1.6043
[2023-10-02 02:12:48] iter = 11420, loss = 1.5198
[2023-10-02 02:12:48] iter = 11430, loss = 1.6254
[2023-10-02 02:12:49] iter = 11440, loss = 1.5604
[2023-10-02 02:12:50] iter = 11450, loss = 1.5904
[2023-10-02 02:12:51] iter = 11460, loss = 1.4157
[2023-10-02 02:12:52] iter = 11470, loss = 1.5095
[2023-10-02 02:12:53] iter = 11480, loss = 1.6146
[2023-10-02 02:12:54] iter = 11490, loss = 1.4506
[2023-10-02 02:12:55] iter = 11500, loss = 1.4744
[2023-10-02 02:12:56] iter = 11510, loss = 1.4614
[2023-10-02 02:12:57] iter = 11520, loss = 1.5495
[2023-10-02 02:12:57] iter = 11530, loss = 1.5546
[2023-10-02 02:12:58] iter = 11540, loss = 1.6305
[2023-10-02 02:12:59] iter = 11550, loss = 1.4999
[2023-10-02 02:13:00] iter = 11560, loss = 1.5313
[2023-10-02 02:13:01] iter = 11570, loss = 1.5683
[2023-10-02 02:13:02] iter = 11580, loss = 1.5623
[2023-10-02 02:13:03] iter = 11590, loss = 1.4941
[2023-10-02 02:13:04] iter = 11600, loss = 1.6889
[2023-10-02 02:13:05] iter = 11610, loss = 1.7039
[2023-10-02 02:13:06] iter = 11620, loss = 1.4130
[2023-10-02 02:13:07] iter = 11630, loss = 1.4788
[2023-10-02 02:13:07] iter = 11640, loss = 1.4909
[2023-10-02 02:13:08] iter = 11650, loss = 1.4870
[2023-10-02 02:13:09] iter = 11660, loss = 1.5957
[2023-10-02 02:13:10] iter = 11670, loss = 1.5163
[2023-10-02 02:13:11] iter = 11680, loss = 1.5063
[2023-10-02 02:13:12] iter = 11690, loss = 1.5201
[2023-10-02 02:13:13] iter = 11700, loss = 1.5291
[2023-10-02 02:13:14] iter = 11710, loss = 1.5177
[2023-10-02 02:13:15] iter = 11720, loss = 1.5070
[2023-10-02 02:13:16] iter = 11730, loss = 1.5827
[2023-10-02 02:13:17] iter = 11740, loss = 1.4559
[2023-10-02 02:13:17] iter = 11750, loss = 1.5236
[2023-10-02 02:13:18] iter = 11760, loss = 1.5978
[2023-10-02 02:13:19] iter = 11770, loss = 1.5644
[2023-10-02 02:13:20] iter = 11780, loss = 1.5497
[2023-10-02 02:13:21] iter = 11790, loss = 1.6220
[2023-10-02 02:13:22] iter = 11800, loss = 1.5196
[2023-10-02 02:13:23] iter = 11810, loss = 1.4626
[2023-10-02 02:13:24] iter = 11820, loss = 1.5791
[2023-10-02 02:13:25] iter = 11830, loss = 1.5596
[2023-10-02 02:13:25] iter = 11840, loss = 1.4538
[2023-10-02 02:13:26] iter = 11850, loss = 1.4682
[2023-10-02 02:13:27] iter = 11860, loss = 1.5532
[2023-10-02 02:13:28] iter = 11870, loss = 1.5812
[2023-10-02 02:13:29] iter = 11880, loss = 1.6520
[2023-10-02 02:13:30] iter = 11890, loss = 1.5481
[2023-10-02 02:13:31] iter = 11900, loss = 1.4964
[2023-10-02 02:13:32] iter = 11910, loss = 1.6217
[2023-10-02 02:13:33] iter = 11920, loss = 1.3918
[2023-10-02 02:13:33] iter = 11930, loss = 1.6186
[2023-10-02 02:13:34] iter = 11940, loss = 1.6137
[2023-10-02 02:13:35] iter = 11950, loss = 1.5567
[2023-10-02 02:13:36] iter = 11960, loss = 1.4644
[2023-10-02 02:13:37] iter = 11970, loss = 1.5306
[2023-10-02 02:13:38] iter = 11980, loss = 1.4783
[2023-10-02 02:13:39] iter = 11990, loss = 1.5972
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 20044}
[2023-10-02 02:14:04] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004233 train acc = 1.0000, test acc = 0.6253
[2023-10-02 02:14:28] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004271 train acc = 0.9980, test acc = 0.6235
[2023-10-02 02:14:52] Evaluate_02: epoch = 1000 train time = 21 s train loss = 0.012155 train acc = 1.0000, test acc = 0.6260
[2023-10-02 02:15:15] Evaluate_03: epoch = 1000 train time = 21 s train loss = 0.004091 train acc = 1.0000, test acc = 0.6313
[2023-10-02 02:15:40] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.001815 train acc = 1.0000, test acc = 0.6187
[2023-10-02 02:16:04] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002554 train acc = 1.0000, test acc = 0.6228
[2023-10-02 02:16:28] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.011593 train acc = 1.0000, test acc = 0.6206
[2023-10-02 02:16:52] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.028243 train acc = 0.9960, test acc = 0.6167
[2023-10-02 02:17:16] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.011521 train acc = 1.0000, test acc = 0.6240
[2023-10-02 02:17:40] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.028264 train acc = 0.9980, test acc = 0.6226
[2023-10-02 02:18:04] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004347 train acc = 1.0000, test acc = 0.6226
[2023-10-02 02:18:28] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.025650 train acc = 0.9980, test acc = 0.6247
[2023-10-02 02:18:53] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003788 train acc = 1.0000, test acc = 0.6254
[2023-10-02 02:19:17] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002417 train acc = 1.0000, test acc = 0.6216
[2023-10-02 02:19:41] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004544 train acc = 1.0000, test acc = 0.6174
[2023-10-02 02:20:05] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.008458 train acc = 1.0000, test acc = 0.6176
[2023-10-02 02:20:29] Evaluate_16: epoch = 1000 train time = 21 s train loss = 0.008263 train acc = 1.0000, test acc = 0.6218
[2023-10-02 02:20:53] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.017475 train acc = 1.0000, test acc = 0.6247
[2023-10-02 02:21:17] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002032 train acc = 1.0000, test acc = 0.6198
[2023-10-02 02:21:41] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004105 train acc = 1.0000, test acc = 0.6235
Evaluate 20 random ConvNet, mean = 0.6225 std = 0.0034
-------------------------
[2023-10-02 02:21:41] iter = 12000, loss = 1.4170
[2023-10-02 02:21:42] iter = 12010, loss = 1.6525
[2023-10-02 02:21:43] iter = 12020, loss = 1.5393
[2023-10-02 02:21:44] iter = 12030, loss = 1.6564
[2023-10-02 02:21:45] iter = 12040, loss = 1.4540
[2023-10-02 02:21:46] iter = 12050, loss = 1.4680
[2023-10-02 02:21:47] iter = 12060, loss = 1.5864
[2023-10-02 02:21:48] iter = 12070, loss = 1.7750
[2023-10-02 02:21:49] iter = 12080, loss = 1.6233
[2023-10-02 02:21:49] iter = 12090, loss = 1.4591
[2023-10-02 02:21:50] iter = 12100, loss = 1.4784
[2023-10-02 02:21:51] iter = 12110, loss = 1.5819
[2023-10-02 02:21:52] iter = 12120, loss = 1.5133
[2023-10-02 02:21:53] iter = 12130, loss = 1.5795
[2023-10-02 02:21:54] iter = 12140, loss = 1.4060
[2023-10-02 02:21:55] iter = 12150, loss = 1.5927
[2023-10-02 02:21:56] iter = 12160, loss = 1.4571
[2023-10-02 02:21:57] iter = 12170, loss = 1.4199
[2023-10-02 02:21:58] iter = 12180, loss = 1.5691
[2023-10-02 02:21:58] iter = 12190, loss = 1.5094
[2023-10-02 02:21:59] iter = 12200, loss = 1.4799
[2023-10-02 02:22:00] iter = 12210, loss = 1.6116
[2023-10-02 02:22:01] iter = 12220, loss = 1.5785
[2023-10-02 02:22:02] iter = 12230, loss = 1.3858
[2023-10-02 02:22:03] iter = 12240, loss = 1.5826
[2023-10-02 02:22:04] iter = 12250, loss = 1.4244
[2023-10-02 02:22:05] iter = 12260, loss = 1.6312
[2023-10-02 02:22:05] iter = 12270, loss = 1.4917
[2023-10-02 02:22:06] iter = 12280, loss = 1.4284
[2023-10-02 02:22:07] iter = 12290, loss = 1.5521
[2023-10-02 02:22:08] iter = 12300, loss = 1.5083
[2023-10-02 02:22:09] iter = 12310, loss = 1.3902
[2023-10-02 02:22:10] iter = 12320, loss = 1.6173
[2023-10-02 02:22:11] iter = 12330, loss = 1.4207
[2023-10-02 02:22:12] iter = 12340, loss = 1.4345
[2023-10-02 02:22:13] iter = 12350, loss = 1.5907
[2023-10-02 02:22:14] iter = 12360, loss = 1.5847
[2023-10-02 02:22:15] iter = 12370, loss = 1.4838
[2023-10-02 02:22:15] iter = 12380, loss = 1.5589
[2023-10-02 02:22:16] iter = 12390, loss = 1.4687
[2023-10-02 02:22:17] iter = 12400, loss = 1.6072
[2023-10-02 02:22:18] iter = 12410, loss = 1.4092
[2023-10-02 02:22:19] iter = 12420, loss = 1.5349
[2023-10-02 02:22:20] iter = 12430, loss = 1.7629
[2023-10-02 02:22:21] iter = 12440, loss = 1.3906
[2023-10-02 02:22:22] iter = 12450, loss = 1.5508
[2023-10-02 02:22:23] iter = 12460, loss = 1.5049
[2023-10-02 02:22:24] iter = 12470, loss = 1.4837
[2023-10-02 02:22:25] iter = 12480, loss = 1.5869
[2023-10-02 02:22:26] iter = 12490, loss = 1.4461
[2023-10-02 02:22:26] iter = 12500, loss = 1.4618
[2023-10-02 02:22:27] iter = 12510, loss = 1.5195
[2023-10-02 02:22:28] iter = 12520, loss = 1.4861
[2023-10-02 02:22:29] iter = 12530, loss = 1.4556
[2023-10-02 02:22:30] iter = 12540, loss = 1.5188
[2023-10-02 02:22:31] iter = 12550, loss = 1.7852
[2023-10-02 02:22:32] iter = 12560, loss = 1.5719
[2023-10-02 02:22:33] iter = 12570, loss = 1.5106
[2023-10-02 02:22:34] iter = 12580, loss = 1.5493
[2023-10-02 02:22:34] iter = 12590, loss = 1.6820
[2023-10-02 02:22:35] iter = 12600, loss = 1.4792
[2023-10-02 02:22:36] iter = 12610, loss = 1.5289
[2023-10-02 02:22:37] iter = 12620, loss = 1.6010
[2023-10-02 02:22:38] iter = 12630, loss = 1.5172
[2023-10-02 02:22:39] iter = 12640, loss = 1.3730
[2023-10-02 02:22:40] iter = 12650, loss = 1.4678
[2023-10-02 02:22:41] iter = 12660, loss = 1.5034
[2023-10-02 02:22:42] iter = 12670, loss = 1.4907
[2023-10-02 02:22:43] iter = 12680, loss = 1.4144
[2023-10-02 02:22:44] iter = 12690, loss = 1.5937
[2023-10-02 02:22:44] iter = 12700, loss = 1.5239
[2023-10-02 02:22:45] iter = 12710, loss = 1.6793
[2023-10-02 02:22:46] iter = 12720, loss = 1.4106
[2023-10-02 02:22:47] iter = 12730, loss = 1.4814
[2023-10-02 02:22:48] iter = 12740, loss = 1.4607
[2023-10-02 02:22:49] iter = 12750, loss = 1.4685
[2023-10-02 02:22:50] iter = 12760, loss = 1.3661
[2023-10-02 02:22:51] iter = 12770, loss = 1.6197
[2023-10-02 02:22:52] iter = 12780, loss = 1.7611
[2023-10-02 02:22:53] iter = 12790, loss = 1.5003
[2023-10-02 02:22:54] iter = 12800, loss = 1.5520
[2023-10-02 02:22:55] iter = 12810, loss = 1.5034
[2023-10-02 02:22:56] iter = 12820, loss = 1.4579
[2023-10-02 02:22:56] iter = 12830, loss = 1.4664
[2023-10-02 02:22:57] iter = 12840, loss = 1.5182
[2023-10-02 02:22:58] iter = 12850, loss = 1.5129
[2023-10-02 02:22:59] iter = 12860, loss = 1.6126
[2023-10-02 02:23:00] iter = 12870, loss = 1.3312
[2023-10-02 02:23:01] iter = 12880, loss = 1.4540
[2023-10-02 02:23:02] iter = 12890, loss = 1.5676
[2023-10-02 02:23:03] iter = 12900, loss = 1.4818
[2023-10-02 02:23:04] iter = 12910, loss = 1.5576
[2023-10-02 02:23:05] iter = 12920, loss = 1.6051
[2023-10-02 02:23:06] iter = 12930, loss = 1.5525
[2023-10-02 02:23:06] iter = 12940, loss = 1.5175
[2023-10-02 02:23:07] iter = 12950, loss = 1.2817
[2023-10-02 02:23:08] iter = 12960, loss = 1.5073
[2023-10-02 02:23:09] iter = 12970, loss = 1.4193
[2023-10-02 02:23:10] iter = 12980, loss = 1.5814
[2023-10-02 02:23:11] iter = 12990, loss = 1.4338
[2023-10-02 02:23:12] iter = 13000, loss = 1.5486
[2023-10-02 02:23:13] iter = 13010, loss = 1.4958
[2023-10-02 02:23:14] iter = 13020, loss = 1.6061
[2023-10-02 02:23:14] iter = 13030, loss = 1.3866
[2023-10-02 02:23:15] iter = 13040, loss = 1.7224
[2023-10-02 02:23:16] iter = 13050, loss = 1.5241
[2023-10-02 02:23:17] iter = 13060, loss = 1.5823
[2023-10-02 02:23:18] iter = 13070, loss = 1.6514
[2023-10-02 02:23:19] iter = 13080, loss = 1.5384
[2023-10-02 02:23:20] iter = 13090, loss = 1.5642
[2023-10-02 02:23:21] iter = 13100, loss = 1.4485
[2023-10-02 02:23:22] iter = 13110, loss = 1.5274
[2023-10-02 02:23:22] iter = 13120, loss = 1.5624
[2023-10-02 02:23:23] iter = 13130, loss = 1.4728
[2023-10-02 02:23:24] iter = 13140, loss = 1.4840
[2023-10-02 02:23:25] iter = 13150, loss = 1.5639
[2023-10-02 02:23:26] iter = 13160, loss = 1.5033
[2023-10-02 02:23:27] iter = 13170, loss = 1.4815
[2023-10-02 02:23:28] iter = 13180, loss = 1.5736
[2023-10-02 02:23:29] iter = 13190, loss = 1.4208
[2023-10-02 02:23:30] iter = 13200, loss = 1.5484
[2023-10-02 02:23:31] iter = 13210, loss = 1.5259
[2023-10-02 02:23:32] iter = 13220, loss = 1.5996
[2023-10-02 02:23:33] iter = 13230, loss = 1.4414
[2023-10-02 02:23:34] iter = 13240, loss = 1.5386
[2023-10-02 02:23:34] iter = 13250, loss = 1.4201
[2023-10-02 02:23:35] iter = 13260, loss = 1.4467
[2023-10-02 02:23:36] iter = 13270, loss = 1.5914
[2023-10-02 02:23:37] iter = 13280, loss = 1.5410
[2023-10-02 02:23:38] iter = 13290, loss = 1.5764
[2023-10-02 02:23:39] iter = 13300, loss = 1.5710
[2023-10-02 02:23:40] iter = 13310, loss = 1.5000
[2023-10-02 02:23:41] iter = 13320, loss = 1.5442
[2023-10-02 02:23:42] iter = 13330, loss = 1.3772
[2023-10-02 02:23:43] iter = 13340, loss = 1.6081
[2023-10-02 02:23:44] iter = 13350, loss = 1.4700
[2023-10-02 02:23:44] iter = 13360, loss = 1.5234
[2023-10-02 02:23:45] iter = 13370, loss = 1.4019
[2023-10-02 02:23:46] iter = 13380, loss = 1.3996
[2023-10-02 02:23:47] iter = 13390, loss = 1.4853
[2023-10-02 02:23:48] iter = 13400, loss = 1.4627
[2023-10-02 02:23:49] iter = 13410, loss = 1.6782
[2023-10-02 02:23:50] iter = 13420, loss = 1.5611
[2023-10-02 02:23:51] iter = 13430, loss = 1.3991
[2023-10-02 02:23:52] iter = 13440, loss = 1.5269
[2023-10-02 02:23:53] iter = 13450, loss = 1.3595
[2023-10-02 02:23:54] iter = 13460, loss = 1.4856
[2023-10-02 02:23:55] iter = 13470, loss = 1.5613
[2023-10-02 02:23:55] iter = 13480, loss = 1.4538
[2023-10-02 02:23:56] iter = 13490, loss = 1.4292
[2023-10-02 02:23:57] iter = 13500, loss = 1.5846
[2023-10-02 02:23:58] iter = 13510, loss = 1.4421
[2023-10-02 02:23:59] iter = 13520, loss = 1.4688
[2023-10-02 02:24:00] iter = 13530, loss = 1.5963
[2023-10-02 02:24:01] iter = 13540, loss = 1.6871
[2023-10-02 02:24:02] iter = 13550, loss = 1.4314
[2023-10-02 02:24:03] iter = 13560, loss = 1.6137
[2023-10-02 02:24:04] iter = 13570, loss = 1.6077
[2023-10-02 02:24:05] iter = 13580, loss = 1.5024
[2023-10-02 02:24:06] iter = 13590, loss = 1.5437
[2023-10-02 02:24:07] iter = 13600, loss = 1.5057
[2023-10-02 02:24:07] iter = 13610, loss = 1.7234
[2023-10-02 02:24:08] iter = 13620, loss = 1.5813
[2023-10-02 02:24:09] iter = 13630, loss = 1.5060
[2023-10-02 02:24:10] iter = 13640, loss = 1.4352
[2023-10-02 02:24:11] iter = 13650, loss = 1.4861
[2023-10-02 02:24:12] iter = 13660, loss = 1.5768
[2023-10-02 02:24:13] iter = 13670, loss = 1.4482
[2023-10-02 02:24:14] iter = 13680, loss = 1.6158
[2023-10-02 02:24:15] iter = 13690, loss = 1.5868
[2023-10-02 02:24:15] iter = 13700, loss = 1.4622
[2023-10-02 02:24:16] iter = 13710, loss = 1.5061
[2023-10-02 02:24:17] iter = 13720, loss = 1.4906
[2023-10-02 02:24:18] iter = 13730, loss = 1.4320
[2023-10-02 02:24:19] iter = 13740, loss = 1.5770
[2023-10-02 02:24:20] iter = 13750, loss = 1.6345
[2023-10-02 02:24:21] iter = 13760, loss = 1.3990
[2023-10-02 02:24:22] iter = 13770, loss = 1.5653
[2023-10-02 02:24:23] iter = 13780, loss = 1.3985
[2023-10-02 02:24:24] iter = 13790, loss = 1.4639
[2023-10-02 02:24:24] iter = 13800, loss = 1.4694
[2023-10-02 02:24:25] iter = 13810, loss = 1.6770
[2023-10-02 02:24:26] iter = 13820, loss = 1.5718
[2023-10-02 02:24:27] iter = 13830, loss = 1.5953
[2023-10-02 02:24:28] iter = 13840, loss = 1.3773
[2023-10-02 02:24:29] iter = 13850, loss = 1.4739
[2023-10-02 02:24:30] iter = 13860, loss = 1.5479
[2023-10-02 02:24:31] iter = 13870, loss = 1.5460
[2023-10-02 02:24:32] iter = 13880, loss = 1.6762
[2023-10-02 02:24:32] iter = 13890, loss = 1.5320
[2023-10-02 02:24:33] iter = 13900, loss = 1.4761
[2023-10-02 02:24:34] iter = 13910, loss = 1.5336
[2023-10-02 02:24:35] iter = 13920, loss = 1.6446
[2023-10-02 02:24:36] iter = 13930, loss = 1.5193
[2023-10-02 02:24:37] iter = 13940, loss = 1.4793
[2023-10-02 02:24:38] iter = 13950, loss = 1.4355
[2023-10-02 02:24:39] iter = 13960, loss = 1.4842
[2023-10-02 02:24:40] iter = 13970, loss = 1.5132
[2023-10-02 02:24:41] iter = 13980, loss = 1.4788
[2023-10-02 02:24:42] iter = 13990, loss = 1.5482
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 82929}
[2023-10-02 02:25:07] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.018529 train acc = 1.0000, test acc = 0.6322
[2023-10-02 02:25:31] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.009316 train acc = 0.9980, test acc = 0.6265
[2023-10-02 02:25:55] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.029748 train acc = 1.0000, test acc = 0.6236
[2023-10-02 02:26:19] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.015572 train acc = 0.9980, test acc = 0.6244
[2023-10-02 02:26:44] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.020817 train acc = 0.9980, test acc = 0.6151
[2023-10-02 02:27:08] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.023479 train acc = 0.9980, test acc = 0.6184
[2023-10-02 02:27:32] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.013980 train acc = 0.9980, test acc = 0.6269
[2023-10-02 02:27:56] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.009669 train acc = 1.0000, test acc = 0.6269
[2023-10-02 02:28:20] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.026579 train acc = 0.9980, test acc = 0.6280
[2023-10-02 02:28:44] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.010577 train acc = 1.0000, test acc = 0.6199
[2023-10-02 02:29:08] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002296 train acc = 1.0000, test acc = 0.6203
[2023-10-02 02:29:32] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.018973 train acc = 0.9980, test acc = 0.6246
[2023-10-02 02:29:57] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.001988 train acc = 1.0000, test acc = 0.6248
[2023-10-02 02:30:21] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004900 train acc = 1.0000, test acc = 0.6191
[2023-10-02 02:30:45] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.016952 train acc = 0.9980, test acc = 0.6249
[2023-10-02 02:31:09] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.017825 train acc = 1.0000, test acc = 0.6235
[2023-10-02 02:31:33] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.024435 train acc = 1.0000, test acc = 0.6205
[2023-10-02 02:31:57] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004441 train acc = 1.0000, test acc = 0.6248
[2023-10-02 02:32:21] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.019418 train acc = 1.0000, test acc = 0.6259
[2023-10-02 02:32:45] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.021074 train acc = 0.9960, test acc = 0.6266
Evaluate 20 random ConvNet, mean = 0.6238 std = 0.0039
-------------------------
[2023-10-02 02:32:45] iter = 14000, loss = 1.6029
[2023-10-02 02:32:46] iter = 14010, loss = 1.4744
[2023-10-02 02:32:47] iter = 14020, loss = 1.4537
[2023-10-02 02:32:48] iter = 14030, loss = 1.4310
[2023-10-02 02:32:49] iter = 14040, loss = 1.5485
[2023-10-02 02:32:50] iter = 14050, loss = 1.5704
[2023-10-02 02:32:51] iter = 14060, loss = 1.4913
[2023-10-02 02:32:52] iter = 14070, loss = 1.7098
[2023-10-02 02:32:52] iter = 14080, loss = 1.6275
[2023-10-02 02:32:53] iter = 14090, loss = 1.4961
[2023-10-02 02:32:54] iter = 14100, loss = 1.5416
[2023-10-02 02:32:55] iter = 14110, loss = 1.4211
[2023-10-02 02:32:56] iter = 14120, loss = 1.4411
[2023-10-02 02:32:57] iter = 14130, loss = 1.4963
[2023-10-02 02:32:58] iter = 14140, loss = 1.4320
[2023-10-02 02:32:59] iter = 14150, loss = 1.4674
[2023-10-02 02:33:00] iter = 14160, loss = 1.4744
[2023-10-02 02:33:01] iter = 14170, loss = 1.5324
[2023-10-02 02:33:02] iter = 14180, loss = 1.4041
[2023-10-02 02:33:02] iter = 14190, loss = 1.5537
[2023-10-02 02:33:03] iter = 14200, loss = 1.5143
[2023-10-02 02:33:04] iter = 14210, loss = 1.4543
[2023-10-02 02:33:05] iter = 14220, loss = 1.5862
[2023-10-02 02:33:06] iter = 14230, loss = 1.5028
[2023-10-02 02:33:07] iter = 14240, loss = 1.4976
[2023-10-02 02:33:08] iter = 14250, loss = 1.6162
[2023-10-02 02:33:09] iter = 14260, loss = 1.6725
[2023-10-02 02:33:10] iter = 14270, loss = 1.6621
[2023-10-02 02:33:11] iter = 14280, loss = 1.5537
[2023-10-02 02:33:12] iter = 14290, loss = 1.6197
[2023-10-02 02:33:12] iter = 14300, loss = 1.6856
[2023-10-02 02:33:13] iter = 14310, loss = 1.5827
[2023-10-02 02:33:14] iter = 14320, loss = 1.4282
[2023-10-02 02:33:15] iter = 14330, loss = 1.4893
[2023-10-02 02:33:16] iter = 14340, loss = 1.6026
[2023-10-02 02:33:17] iter = 14350, loss = 1.4525
[2023-10-02 02:33:18] iter = 14360, loss = 1.5935
[2023-10-02 02:33:19] iter = 14370, loss = 1.5177
[2023-10-02 02:33:20] iter = 14380, loss = 1.4595
[2023-10-02 02:33:21] iter = 14390, loss = 1.3651
[2023-10-02 02:33:21] iter = 14400, loss = 1.6291
[2023-10-02 02:33:22] iter = 14410, loss = 1.5978
[2023-10-02 02:33:23] iter = 14420, loss = 1.5262
[2023-10-02 02:33:24] iter = 14430, loss = 1.5509
[2023-10-02 02:33:25] iter = 14440, loss = 1.5201
[2023-10-02 02:33:26] iter = 14450, loss = 1.5781
[2023-10-02 02:33:27] iter = 14460, loss = 1.7547
[2023-10-02 02:33:28] iter = 14470, loss = 1.4445
[2023-10-02 02:33:29] iter = 14480, loss = 1.4437
[2023-10-02 02:33:30] iter = 14490, loss = 1.5896
[2023-10-02 02:33:31] iter = 14500, loss = 1.4695
[2023-10-02 02:33:32] iter = 14510, loss = 1.4373
[2023-10-02 02:33:32] iter = 14520, loss = 1.6094
[2023-10-02 02:33:33] iter = 14530, loss = 1.4931
[2023-10-02 02:33:34] iter = 14540, loss = 1.4608
[2023-10-02 02:33:35] iter = 14550, loss = 1.3772
[2023-10-02 02:33:36] iter = 14560, loss = 1.4975
[2023-10-02 02:33:37] iter = 14570, loss = 1.4606
[2023-10-02 02:33:38] iter = 14580, loss = 1.5025
[2023-10-02 02:33:39] iter = 14590, loss = 1.4965
[2023-10-02 02:33:40] iter = 14600, loss = 1.3835
[2023-10-02 02:33:40] iter = 14610, loss = 1.4712
[2023-10-02 02:33:41] iter = 14620, loss = 1.4243
[2023-10-02 02:33:42] iter = 14630, loss = 1.4269
[2023-10-02 02:33:43] iter = 14640, loss = 1.5469
[2023-10-02 02:33:44] iter = 14650, loss = 1.4583
[2023-10-02 02:33:45] iter = 14660, loss = 1.5245
[2023-10-02 02:33:46] iter = 14670, loss = 1.4811
[2023-10-02 02:33:47] iter = 14680, loss = 1.6245
[2023-10-02 02:33:48] iter = 14690, loss = 1.4852
[2023-10-02 02:33:48] iter = 14700, loss = 1.4774
[2023-10-02 02:33:49] iter = 14710, loss = 1.4312
[2023-10-02 02:33:50] iter = 14720, loss = 1.4699
[2023-10-02 02:33:51] iter = 14730, loss = 1.4475
[2023-10-02 02:33:52] iter = 14740, loss = 1.5713
[2023-10-02 02:33:53] iter = 14750, loss = 1.5358
[2023-10-02 02:33:54] iter = 14760, loss = 1.5222
[2023-10-02 02:33:55] iter = 14770, loss = 1.5860
[2023-10-02 02:33:56] iter = 14780, loss = 1.4403
[2023-10-02 02:33:57] iter = 14790, loss = 1.5298
[2023-10-02 02:33:58] iter = 14800, loss = 1.4580
[2023-10-02 02:33:59] iter = 14810, loss = 1.4262
[2023-10-02 02:33:59] iter = 14820, loss = 1.4166
[2023-10-02 02:34:00] iter = 14830, loss = 1.5582
[2023-10-02 02:34:01] iter = 14840, loss = 1.4931
[2023-10-02 02:34:02] iter = 14850, loss = 1.4272
[2023-10-02 02:34:03] iter = 14860, loss = 1.3865
[2023-10-02 02:34:04] iter = 14870, loss = 1.5699
[2023-10-02 02:34:05] iter = 14880, loss = 1.4170
[2023-10-02 02:34:06] iter = 14890, loss = 1.5004
[2023-10-02 02:34:06] iter = 14900, loss = 1.4261
[2023-10-02 02:34:07] iter = 14910, loss = 1.4993
[2023-10-02 02:34:08] iter = 14920, loss = 1.4465
[2023-10-02 02:34:09] iter = 14930, loss = 1.4881
[2023-10-02 02:34:10] iter = 14940, loss = 1.5296
[2023-10-02 02:34:11] iter = 14950, loss = 1.7461
[2023-10-02 02:34:12] iter = 14960, loss = 1.3753
[2023-10-02 02:34:13] iter = 14970, loss = 1.4253
[2023-10-02 02:34:14] iter = 14980, loss = 1.4892
[2023-10-02 02:34:15] iter = 14990, loss = 1.6508
[2023-10-02 02:34:15] iter = 15000, loss = 1.4384
[2023-10-02 02:34:16] iter = 15010, loss = 1.4437
[2023-10-02 02:34:17] iter = 15020, loss = 1.4432
[2023-10-02 02:34:18] iter = 15030, loss = 1.4893
[2023-10-02 02:34:19] iter = 15040, loss = 1.4893
[2023-10-02 02:34:20] iter = 15050, loss = 1.5966
[2023-10-02 02:34:21] iter = 15060, loss = 1.4606
[2023-10-02 02:34:22] iter = 15070, loss = 1.4441
[2023-10-02 02:34:23] iter = 15080, loss = 1.4687
[2023-10-02 02:34:24] iter = 15090, loss = 1.6104
[2023-10-02 02:34:25] iter = 15100, loss = 1.7487
[2023-10-02 02:34:25] iter = 15110, loss = 1.5165
[2023-10-02 02:34:26] iter = 15120, loss = 1.4845
[2023-10-02 02:34:27] iter = 15130, loss = 1.3756
[2023-10-02 02:34:28] iter = 15140, loss = 1.4354
[2023-10-02 02:34:29] iter = 15150, loss = 1.3906
[2023-10-02 02:34:30] iter = 15160, loss = 1.5392
[2023-10-02 02:34:31] iter = 15170, loss = 1.5781
[2023-10-02 02:34:32] iter = 15180, loss = 1.5502
[2023-10-02 02:34:33] iter = 15190, loss = 1.5393
[2023-10-02 02:34:34] iter = 15200, loss = 1.4983
[2023-10-02 02:34:34] iter = 15210, loss = 1.6300
[2023-10-02 02:34:35] iter = 15220, loss = 1.5056
[2023-10-02 02:34:36] iter = 15230, loss = 1.5465
[2023-10-02 02:34:37] iter = 15240, loss = 1.6705
[2023-10-02 02:34:38] iter = 15250, loss = 1.3407
[2023-10-02 02:34:39] iter = 15260, loss = 1.5054
[2023-10-02 02:34:40] iter = 15270, loss = 1.4416
[2023-10-02 02:34:41] iter = 15280, loss = 1.6104
[2023-10-02 02:34:42] iter = 15290, loss = 1.5151
[2023-10-02 02:34:42] iter = 15300, loss = 1.5779
[2023-10-02 02:34:43] iter = 15310, loss = 1.3884
[2023-10-02 02:34:44] iter = 15320, loss = 1.6744
[2023-10-02 02:34:45] iter = 15330, loss = 1.4432
[2023-10-02 02:34:46] iter = 15340, loss = 1.4861
[2023-10-02 02:34:47] iter = 15350, loss = 1.5436
[2023-10-02 02:34:48] iter = 15360, loss = 1.4621
[2023-10-02 02:34:49] iter = 15370, loss = 1.3482
[2023-10-02 02:34:50] iter = 15380, loss = 1.5691
[2023-10-02 02:34:51] iter = 15390, loss = 1.5389
[2023-10-02 02:34:52] iter = 15400, loss = 1.4939
[2023-10-02 02:34:52] iter = 15410, loss = 1.4389
[2023-10-02 02:34:53] iter = 15420, loss = 1.5830
[2023-10-02 02:34:54] iter = 15430, loss = 1.6257
[2023-10-02 02:34:55] iter = 15440, loss = 1.4795
[2023-10-02 02:34:56] iter = 15450, loss = 1.4054
[2023-10-02 02:34:57] iter = 15460, loss = 1.4669
[2023-10-02 02:34:58] iter = 15470, loss = 1.5786
[2023-10-02 02:34:59] iter = 15480, loss = 1.5001
[2023-10-02 02:35:00] iter = 15490, loss = 1.3699
[2023-10-02 02:35:00] iter = 15500, loss = 1.5595
[2023-10-02 02:35:01] iter = 15510, loss = 1.4936
[2023-10-02 02:35:02] iter = 15520, loss = 1.6100
[2023-10-02 02:35:03] iter = 15530, loss = 1.5722
[2023-10-02 02:35:04] iter = 15540, loss = 1.4764
[2023-10-02 02:35:05] iter = 15550, loss = 1.5492
[2023-10-02 02:35:06] iter = 15560, loss = 1.5173
[2023-10-02 02:35:07] iter = 15570, loss = 1.4866
[2023-10-02 02:35:08] iter = 15580, loss = 1.4303
[2023-10-02 02:35:09] iter = 15590, loss = 1.5898
[2023-10-02 02:35:09] iter = 15600, loss = 1.6425
[2023-10-02 02:35:10] iter = 15610, loss = 1.5387
[2023-10-02 02:35:11] iter = 15620, loss = 1.6820
[2023-10-02 02:35:12] iter = 15630, loss = 1.5666
[2023-10-02 02:35:13] iter = 15640, loss = 1.7259
[2023-10-02 02:35:14] iter = 15650, loss = 1.5416
[2023-10-02 02:35:15] iter = 15660, loss = 1.5359
[2023-10-02 02:35:16] iter = 15670, loss = 1.7283
[2023-10-02 02:35:17] iter = 15680, loss = 1.5473
[2023-10-02 02:35:18] iter = 15690, loss = 1.4840
[2023-10-02 02:35:19] iter = 15700, loss = 1.6426
[2023-10-02 02:35:20] iter = 15710, loss = 1.6435
[2023-10-02 02:35:21] iter = 15720, loss = 1.6670
[2023-10-02 02:35:21] iter = 15730, loss = 1.4554
[2023-10-02 02:35:22] iter = 15740, loss = 1.4989
[2023-10-02 02:35:23] iter = 15750, loss = 1.4198
[2023-10-02 02:35:24] iter = 15760, loss = 1.5294
[2023-10-02 02:35:25] iter = 15770, loss = 1.7471
[2023-10-02 02:35:26] iter = 15780, loss = 1.5426
[2023-10-02 02:35:27] iter = 15790, loss = 1.5513
[2023-10-02 02:35:28] iter = 15800, loss = 1.4995
[2023-10-02 02:35:29] iter = 15810, loss = 1.5166
[2023-10-02 02:35:30] iter = 15820, loss = 1.3332
[2023-10-02 02:35:30] iter = 15830, loss = 1.4205
[2023-10-02 02:35:31] iter = 15840, loss = 1.6122
[2023-10-02 02:35:32] iter = 15850, loss = 1.4611
[2023-10-02 02:35:33] iter = 15860, loss = 1.4862
[2023-10-02 02:35:34] iter = 15870, loss = 1.4473
[2023-10-02 02:35:35] iter = 15880, loss = 1.4567
[2023-10-02 02:35:36] iter = 15890, loss = 1.4254
[2023-10-02 02:35:37] iter = 15900, loss = 1.6343
[2023-10-02 02:35:38] iter = 15910, loss = 1.4690
[2023-10-02 02:35:39] iter = 15920, loss = 1.5416
[2023-10-02 02:35:40] iter = 15930, loss = 1.4739
[2023-10-02 02:35:41] iter = 15940, loss = 1.4207
[2023-10-02 02:35:42] iter = 15950, loss = 1.4604
[2023-10-02 02:35:42] iter = 15960, loss = 1.6141
[2023-10-02 02:35:43] iter = 15970, loss = 1.5156
[2023-10-02 02:35:44] iter = 15980, loss = 1.4292
[2023-10-02 02:35:45] iter = 15990, loss = 1.5563
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 46435}
[2023-10-02 02:36:10] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005691 train acc = 1.0000, test acc = 0.6319
[2023-10-02 02:36:34] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.016783 train acc = 1.0000, test acc = 0.6190
[2023-10-02 02:36:58] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002085 train acc = 1.0000, test acc = 0.6186
[2023-10-02 02:37:22] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.016410 train acc = 1.0000, test acc = 0.6203
[2023-10-02 02:37:46] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002537 train acc = 1.0000, test acc = 0.6251
[2023-10-02 02:38:11] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002920 train acc = 1.0000, test acc = 0.6238
[2023-10-02 02:38:35] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.022945 train acc = 0.9980, test acc = 0.6253
[2023-10-02 02:38:59] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.014803 train acc = 1.0000, test acc = 0.6259
[2023-10-02 02:39:23] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.013723 train acc = 1.0000, test acc = 0.6257
[2023-10-02 02:39:47] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.016919 train acc = 0.9980, test acc = 0.6217
[2023-10-02 02:40:11] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.022230 train acc = 1.0000, test acc = 0.6343
[2023-10-02 02:40:35] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015993 train acc = 0.9980, test acc = 0.6325
[2023-10-02 02:40:59] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.006115 train acc = 1.0000, test acc = 0.6247
[2023-10-02 02:41:23] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.004802 train acc = 1.0000, test acc = 0.6231
[2023-10-02 02:41:47] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004200 train acc = 1.0000, test acc = 0.6210
[2023-10-02 02:42:11] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.008793 train acc = 1.0000, test acc = 0.6232
[2023-10-02 02:42:35] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.014992 train acc = 0.9980, test acc = 0.6185
[2023-10-02 02:43:00] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004015 train acc = 1.0000, test acc = 0.6299
[2023-10-02 02:43:24] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.014784 train acc = 1.0000, test acc = 0.6288
[2023-10-02 02:43:48] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.036383 train acc = 0.9980, test acc = 0.6187
Evaluate 20 random ConvNet, mean = 0.6246 std = 0.0047
-------------------------
[2023-10-02 02:43:48] iter = 16000, loss = 1.5472
[2023-10-02 02:43:49] iter = 16010, loss = 1.4427
[2023-10-02 02:43:50] iter = 16020, loss = 1.6267
[2023-10-02 02:43:51] iter = 16030, loss = 1.5160
[2023-10-02 02:43:52] iter = 16040, loss = 1.5254
[2023-10-02 02:43:53] iter = 16050, loss = 1.5327
[2023-10-02 02:43:54] iter = 16060, loss = 1.4771
[2023-10-02 02:43:55] iter = 16070, loss = 1.5161
[2023-10-02 02:43:56] iter = 16080, loss = 1.4831
[2023-10-02 02:43:57] iter = 16090, loss = 1.5182
[2023-10-02 02:43:57] iter = 16100, loss = 1.5093
[2023-10-02 02:43:58] iter = 16110, loss = 1.4386
[2023-10-02 02:43:59] iter = 16120, loss = 1.5105
[2023-10-02 02:44:00] iter = 16130, loss = 1.5552
[2023-10-02 02:44:01] iter = 16140, loss = 1.3514
[2023-10-02 02:44:02] iter = 16150, loss = 1.3984
[2023-10-02 02:44:03] iter = 16160, loss = 1.4300
[2023-10-02 02:44:04] iter = 16170, loss = 1.4464
[2023-10-02 02:44:05] iter = 16180, loss = 1.5355
[2023-10-02 02:44:06] iter = 16190, loss = 1.5765
[2023-10-02 02:44:07] iter = 16200, loss = 1.4923
[2023-10-02 02:44:07] iter = 16210, loss = 1.4978
[2023-10-02 02:44:08] iter = 16220, loss = 1.3539
[2023-10-02 02:44:09] iter = 16230, loss = 1.4381
[2023-10-02 02:44:10] iter = 16240, loss = 1.4811
[2023-10-02 02:44:11] iter = 16250, loss = 1.4775
[2023-10-02 02:44:12] iter = 16260, loss = 1.3608
[2023-10-02 02:44:13] iter = 16270, loss = 1.5540
[2023-10-02 02:44:14] iter = 16280, loss = 1.5836
[2023-10-02 02:44:15] iter = 16290, loss = 1.3901
[2023-10-02 02:44:16] iter = 16300, loss = 1.5251
[2023-10-02 02:44:17] iter = 16310, loss = 1.4903
[2023-10-02 02:44:17] iter = 16320, loss = 1.5675
[2023-10-02 02:44:18] iter = 16330, loss = 1.5479
[2023-10-02 02:44:19] iter = 16340, loss = 1.5029
[2023-10-02 02:44:20] iter = 16350, loss = 1.3756
[2023-10-02 02:44:21] iter = 16360, loss = 1.5440
[2023-10-02 02:44:22] iter = 16370, loss = 1.4557
[2023-10-02 02:44:23] iter = 16380, loss = 1.4348
[2023-10-02 02:44:24] iter = 16390, loss = 1.4933
[2023-10-02 02:44:25] iter = 16400, loss = 1.4636
[2023-10-02 02:44:25] iter = 16410, loss = 1.4848
[2023-10-02 02:44:26] iter = 16420, loss = 1.5172
[2023-10-02 02:44:27] iter = 16430, loss = 1.4868
[2023-10-02 02:44:28] iter = 16440, loss = 1.5754
[2023-10-02 02:44:29] iter = 16450, loss = 1.4481
[2023-10-02 02:44:30] iter = 16460, loss = 1.5176
[2023-10-02 02:44:31] iter = 16470, loss = 1.4925
[2023-10-02 02:44:32] iter = 16480, loss = 1.4089
[2023-10-02 02:44:33] iter = 16490, loss = 1.6463
[2023-10-02 02:44:34] iter = 16500, loss = 1.5261
[2023-10-02 02:44:35] iter = 16510, loss = 1.5051
[2023-10-02 02:44:36] iter = 16520, loss = 1.4809
[2023-10-02 02:44:37] iter = 16530, loss = 1.4909
[2023-10-02 02:44:37] iter = 16540, loss = 1.3828
[2023-10-02 02:44:38] iter = 16550, loss = 1.4651
[2023-10-02 02:44:39] iter = 16560, loss = 1.4185
[2023-10-02 02:44:40] iter = 16570, loss = 1.4791
[2023-10-02 02:44:41] iter = 16580, loss = 1.4250
[2023-10-02 02:44:42] iter = 16590, loss = 1.5652
[2023-10-02 02:44:43] iter = 16600, loss = 1.5287
[2023-10-02 02:44:44] iter = 16610, loss = 1.5013
[2023-10-02 02:44:44] iter = 16620, loss = 1.4491
[2023-10-02 02:44:45] iter = 16630, loss = 1.3499
[2023-10-02 02:44:46] iter = 16640, loss = 1.4866
[2023-10-02 02:44:47] iter = 16650, loss = 1.5057
[2023-10-02 02:44:48] iter = 16660, loss = 1.6154
[2023-10-02 02:44:49] iter = 16670, loss = 1.4781
[2023-10-02 02:44:50] iter = 16680, loss = 1.4710
[2023-10-02 02:44:51] iter = 16690, loss = 1.4136
[2023-10-02 02:44:52] iter = 16700, loss = 1.4338
[2023-10-02 02:44:53] iter = 16710, loss = 1.3549
[2023-10-02 02:44:54] iter = 16720, loss = 1.4556
[2023-10-02 02:44:55] iter = 16730, loss = 1.5359
[2023-10-02 02:44:55] iter = 16740, loss = 1.3941
[2023-10-02 02:44:56] iter = 16750, loss = 1.5394
[2023-10-02 02:44:57] iter = 16760, loss = 1.4889
[2023-10-02 02:44:58] iter = 16770, loss = 1.5347
[2023-10-02 02:44:59] iter = 16780, loss = 1.3571
[2023-10-02 02:45:00] iter = 16790, loss = 1.3558
[2023-10-02 02:45:01] iter = 16800, loss = 1.3810
[2023-10-02 02:45:02] iter = 16810, loss = 1.5755
[2023-10-02 02:45:03] iter = 16820, loss = 1.4694
[2023-10-02 02:45:03] iter = 16830, loss = 1.4257
[2023-10-02 02:45:04] iter = 16840, loss = 1.4954
[2023-10-02 02:45:05] iter = 16850, loss = 1.4606
[2023-10-02 02:45:06] iter = 16860, loss = 1.4944
[2023-10-02 02:45:07] iter = 16870, loss = 1.5364
[2023-10-02 02:45:08] iter = 16880, loss = 1.7033
[2023-10-02 02:45:09] iter = 16890, loss = 1.4724
[2023-10-02 02:45:10] iter = 16900, loss = 1.5658
[2023-10-02 02:45:11] iter = 16910, loss = 1.3409
[2023-10-02 02:45:12] iter = 16920, loss = 1.5476
[2023-10-02 02:45:12] iter = 16930, loss = 1.4094
[2023-10-02 02:45:13] iter = 16940, loss = 1.5345
[2023-10-02 02:45:14] iter = 16950, loss = 1.4193
[2023-10-02 02:45:15] iter = 16960, loss = 1.5063
[2023-10-02 02:45:16] iter = 16970, loss = 1.4689
[2023-10-02 02:45:17] iter = 16980, loss = 1.4693
[2023-10-02 02:45:18] iter = 16990, loss = 1.4669
[2023-10-02 02:45:19] iter = 17000, loss = 1.4298
[2023-10-02 02:45:20] iter = 17010, loss = 1.4628
[2023-10-02 02:45:21] iter = 17020, loss = 1.5221
[2023-10-02 02:45:21] iter = 17030, loss = 1.6185
[2023-10-02 02:45:22] iter = 17040, loss = 1.4842
[2023-10-02 02:45:23] iter = 17050, loss = 1.6118
[2023-10-02 02:45:24] iter = 17060, loss = 1.4576
[2023-10-02 02:45:25] iter = 17070, loss = 1.5519
[2023-10-02 02:45:26] iter = 17080, loss = 1.4251
[2023-10-02 02:45:27] iter = 17090, loss = 1.5300
[2023-10-02 02:45:28] iter = 17100, loss = 1.4846
[2023-10-02 02:45:29] iter = 17110, loss = 1.5433
[2023-10-02 02:45:30] iter = 17120, loss = 1.4382
[2023-10-02 02:45:31] iter = 17130, loss = 1.5516
[2023-10-02 02:45:31] iter = 17140, loss = 1.2870
[2023-10-02 02:45:32] iter = 17150, loss = 1.4652
[2023-10-02 02:45:33] iter = 17160, loss = 1.4708
[2023-10-02 02:45:34] iter = 17170, loss = 1.4878
[2023-10-02 02:45:35] iter = 17180, loss = 1.5031
[2023-10-02 02:45:36] iter = 17190, loss = 1.5033
[2023-10-02 02:45:37] iter = 17200, loss = 1.4733
[2023-10-02 02:45:38] iter = 17210, loss = 1.5281
[2023-10-02 02:45:39] iter = 17220, loss = 1.5182
[2023-10-02 02:45:40] iter = 17230, loss = 1.5808
[2023-10-02 02:45:40] iter = 17240, loss = 1.5843
[2023-10-02 02:45:41] iter = 17250, loss = 1.4658
[2023-10-02 02:45:42] iter = 17260, loss = 1.5055
[2023-10-02 02:45:43] iter = 17270, loss = 1.5124
[2023-10-02 02:45:44] iter = 17280, loss = 1.4268
[2023-10-02 02:45:45] iter = 17290, loss = 1.5285
[2023-10-02 02:45:46] iter = 17300, loss = 1.4579
[2023-10-02 02:45:47] iter = 17310, loss = 1.6364
[2023-10-02 02:45:48] iter = 17320, loss = 1.5380
[2023-10-02 02:45:49] iter = 17330, loss = 1.4679
[2023-10-02 02:45:50] iter = 17340, loss = 1.3975
[2023-10-02 02:45:51] iter = 17350, loss = 1.3681
[2023-10-02 02:45:51] iter = 17360, loss = 1.6859
[2023-10-02 02:45:52] iter = 17370, loss = 1.4712
[2023-10-02 02:45:53] iter = 17380, loss = 1.4964
[2023-10-02 02:45:54] iter = 17390, loss = 1.4594
[2023-10-02 02:45:55] iter = 17400, loss = 1.4640
[2023-10-02 02:45:56] iter = 17410, loss = 1.4671
[2023-10-02 02:45:57] iter = 17420, loss = 1.5093
[2023-10-02 02:45:58] iter = 17430, loss = 1.5386
[2023-10-02 02:45:59] iter = 17440, loss = 1.5287
[2023-10-02 02:45:59] iter = 17450, loss = 1.5488
[2023-10-02 02:46:00] iter = 17460, loss = 1.4917
[2023-10-02 02:46:01] iter = 17470, loss = 1.4133
[2023-10-02 02:46:02] iter = 17480, loss = 1.4253
[2023-10-02 02:46:03] iter = 17490, loss = 1.3762
[2023-10-02 02:46:04] iter = 17500, loss = 1.4757
[2023-10-02 02:46:05] iter = 17510, loss = 1.6242
[2023-10-02 02:46:06] iter = 17520, loss = 1.6194
[2023-10-02 02:46:07] iter = 17530, loss = 1.4661
[2023-10-02 02:46:08] iter = 17540, loss = 1.5353
[2023-10-02 02:46:09] iter = 17550, loss = 1.5641
[2023-10-02 02:46:09] iter = 17560, loss = 1.4467
[2023-10-02 02:46:10] iter = 17570, loss = 1.6036
[2023-10-02 02:46:11] iter = 17580, loss = 1.4684
[2023-10-02 02:46:12] iter = 17590, loss = 1.5787
[2023-10-02 02:46:13] iter = 17600, loss = 1.4805
[2023-10-02 02:46:14] iter = 17610, loss = 1.3716
[2023-10-02 02:46:15] iter = 17620, loss = 1.6001
[2023-10-02 02:46:16] iter = 17630, loss = 1.5694
[2023-10-02 02:46:17] iter = 17640, loss = 1.4685
[2023-10-02 02:46:18] iter = 17650, loss = 1.4495
[2023-10-02 02:46:19] iter = 17660, loss = 1.4376
[2023-10-02 02:46:19] iter = 17670, loss = 1.4198
[2023-10-02 02:46:20] iter = 17680, loss = 1.3286
[2023-10-02 02:46:21] iter = 17690, loss = 1.3831
[2023-10-02 02:46:22] iter = 17700, loss = 1.5129
[2023-10-02 02:46:23] iter = 17710, loss = 1.4217
[2023-10-02 02:46:24] iter = 17720, loss = 1.5616
[2023-10-02 02:46:25] iter = 17730, loss = 1.4788
[2023-10-02 02:46:26] iter = 17740, loss = 1.5446
[2023-10-02 02:46:27] iter = 17750, loss = 1.5094
[2023-10-02 02:46:28] iter = 17760, loss = 1.4297
[2023-10-02 02:46:29] iter = 17770, loss = 1.6014
[2023-10-02 02:46:30] iter = 17780, loss = 1.4474
[2023-10-02 02:46:30] iter = 17790, loss = 1.4066
[2023-10-02 02:46:31] iter = 17800, loss = 1.5912
[2023-10-02 02:46:32] iter = 17810, loss = 1.4504
[2023-10-02 02:46:33] iter = 17820, loss = 1.4471
[2023-10-02 02:46:34] iter = 17830, loss = 1.4598
[2023-10-02 02:46:35] iter = 17840, loss = 1.4903
[2023-10-02 02:46:36] iter = 17850, loss = 1.4751
[2023-10-02 02:46:37] iter = 17860, loss = 1.5633
[2023-10-02 02:46:37] iter = 17870, loss = 1.3820
[2023-10-02 02:46:38] iter = 17880, loss = 1.4669
[2023-10-02 02:46:39] iter = 17890, loss = 1.4604
[2023-10-02 02:46:40] iter = 17900, loss = 1.5603
[2023-10-02 02:46:41] iter = 17910, loss = 1.6723
[2023-10-02 02:46:42] iter = 17920, loss = 1.3767
[2023-10-02 02:46:43] iter = 17930, loss = 1.5267
[2023-10-02 02:46:44] iter = 17940, loss = 1.4291
[2023-10-02 02:46:45] iter = 17950, loss = 1.4907
[2023-10-02 02:46:46] iter = 17960, loss = 1.6654
[2023-10-02 02:46:47] iter = 17970, loss = 1.4674
[2023-10-02 02:46:47] iter = 17980, loss = 1.4448
[2023-10-02 02:46:48] iter = 17990, loss = 1.3483
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 9701}
[2023-10-02 02:47:13] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.003841 train acc = 1.0000, test acc = 0.6212
[2023-10-02 02:47:37] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006046 train acc = 1.0000, test acc = 0.6262
[2023-10-02 02:48:01] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.005622 train acc = 1.0000, test acc = 0.6263
[2023-10-02 02:48:26] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002211 train acc = 1.0000, test acc = 0.6238
[2023-10-02 02:48:50] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004456 train acc = 1.0000, test acc = 0.6211
[2023-10-02 02:49:14] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001928 train acc = 1.0000, test acc = 0.6272
[2023-10-02 02:49:38] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.021871 train acc = 1.0000, test acc = 0.6326
[2023-10-02 02:50:03] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.010871 train acc = 1.0000, test acc = 0.6268
[2023-10-02 02:50:27] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.033778 train acc = 0.9980, test acc = 0.6250
[2023-10-02 02:50:51] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.010145 train acc = 1.0000, test acc = 0.6294
[2023-10-02 02:51:15] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.007316 train acc = 0.9980, test acc = 0.6264
[2023-10-02 02:51:39] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.007429 train acc = 0.9980, test acc = 0.6244
[2023-10-02 02:52:03] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.002521 train acc = 1.0000, test acc = 0.6269
[2023-10-02 02:52:27] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002437 train acc = 1.0000, test acc = 0.6237
[2023-10-02 02:52:51] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.007481 train acc = 1.0000, test acc = 0.6222
[2023-10-02 02:53:15] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015462 train acc = 1.0000, test acc = 0.6254
[2023-10-02 02:53:39] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002106 train acc = 1.0000, test acc = 0.6271
[2023-10-02 02:54:04] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004678 train acc = 1.0000, test acc = 0.6274
[2023-10-02 02:54:28] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.011982 train acc = 1.0000, test acc = 0.6257
[2023-10-02 02:54:52] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.013623 train acc = 1.0000, test acc = 0.6280
Evaluate 20 random ConvNet, mean = 0.6258 std = 0.0027
-------------------------
[2023-10-02 02:54:52] iter = 18000, loss = 1.5513
[2023-10-02 02:54:53] iter = 18010, loss = 1.3531
[2023-10-02 02:54:54] iter = 18020, loss = 1.5321
[2023-10-02 02:54:55] iter = 18030, loss = 1.4337
[2023-10-02 02:54:56] iter = 18040, loss = 1.5723
[2023-10-02 02:54:57] iter = 18050, loss = 1.4870
[2023-10-02 02:54:58] iter = 18060, loss = 1.5980
[2023-10-02 02:54:59] iter = 18070, loss = 1.5178
[2023-10-02 02:55:00] iter = 18080, loss = 1.5941
[2023-10-02 02:55:00] iter = 18090, loss = 1.3722
[2023-10-02 02:55:01] iter = 18100, loss = 1.4760
[2023-10-02 02:55:02] iter = 18110, loss = 1.6031
[2023-10-02 02:55:03] iter = 18120, loss = 1.4787
[2023-10-02 02:55:04] iter = 18130, loss = 1.5026
[2023-10-02 02:55:05] iter = 18140, loss = 1.5068
[2023-10-02 02:55:06] iter = 18150, loss = 1.4230
[2023-10-02 02:55:07] iter = 18160, loss = 1.4714
[2023-10-02 02:55:08] iter = 18170, loss = 1.3653
[2023-10-02 02:55:09] iter = 18180, loss = 1.6062
[2023-10-02 02:55:09] iter = 18190, loss = 1.5944
[2023-10-02 02:55:10] iter = 18200, loss = 1.5840
[2023-10-02 02:55:11] iter = 18210, loss = 1.5819
[2023-10-02 02:55:12] iter = 18220, loss = 1.3828
[2023-10-02 02:55:13] iter = 18230, loss = 1.5808
[2023-10-02 02:55:14] iter = 18240, loss = 1.5313
[2023-10-02 02:55:15] iter = 18250, loss = 1.4349
[2023-10-02 02:55:16] iter = 18260, loss = 1.4632
[2023-10-02 02:55:17] iter = 18270, loss = 1.5465
[2023-10-02 02:55:18] iter = 18280, loss = 1.5492
[2023-10-02 02:55:18] iter = 18290, loss = 1.4316
[2023-10-02 02:55:19] iter = 18300, loss = 1.4156
[2023-10-02 02:55:20] iter = 18310, loss = 1.5345
[2023-10-02 02:55:21] iter = 18320, loss = 1.5402
[2023-10-02 02:55:22] iter = 18330, loss = 1.5593
[2023-10-02 02:55:23] iter = 18340, loss = 1.3684
[2023-10-02 02:55:24] iter = 18350, loss = 1.5763
[2023-10-02 02:55:25] iter = 18360, loss = 1.4985
[2023-10-02 02:55:25] iter = 18370, loss = 1.4181
[2023-10-02 02:55:26] iter = 18380, loss = 1.4945
[2023-10-02 02:55:27] iter = 18390, loss = 1.5463
[2023-10-02 02:55:28] iter = 18400, loss = 1.5257
[2023-10-02 02:55:29] iter = 18410, loss = 1.5210
[2023-10-02 02:55:30] iter = 18420, loss = 1.3982
[2023-10-02 02:55:31] iter = 18430, loss = 1.3959
[2023-10-02 02:55:32] iter = 18440, loss = 1.5070
[2023-10-02 02:55:33] iter = 18450, loss = 1.4035
[2023-10-02 02:55:34] iter = 18460, loss = 1.4885
[2023-10-02 02:55:35] iter = 18470, loss = 1.5002
[2023-10-02 02:55:35] iter = 18480, loss = 1.5855
[2023-10-02 02:55:36] iter = 18490, loss = 1.6482
[2023-10-02 02:55:37] iter = 18500, loss = 1.6229
[2023-10-02 02:55:38] iter = 18510, loss = 1.5669
[2023-10-02 02:55:39] iter = 18520, loss = 1.5086
[2023-10-02 02:55:40] iter = 18530, loss = 1.4692
[2023-10-02 02:55:41] iter = 18540, loss = 1.3078
[2023-10-02 02:55:42] iter = 18550, loss = 1.5288
[2023-10-02 02:55:43] iter = 18560, loss = 1.4385
[2023-10-02 02:55:44] iter = 18570, loss = 1.5283
[2023-10-02 02:55:44] iter = 18580, loss = 1.3776
[2023-10-02 02:55:45] iter = 18590, loss = 1.4446
[2023-10-02 02:55:46] iter = 18600, loss = 1.5537
[2023-10-02 02:55:47] iter = 18610, loss = 1.5407
[2023-10-02 02:55:48] iter = 18620, loss = 1.5410
[2023-10-02 02:55:49] iter = 18630, loss = 1.5650
[2023-10-02 02:55:50] iter = 18640, loss = 1.4837
[2023-10-02 02:55:51] iter = 18650, loss = 1.4712
[2023-10-02 02:55:52] iter = 18660, loss = 1.4826
[2023-10-02 02:55:53] iter = 18670, loss = 1.6368
[2023-10-02 02:55:54] iter = 18680, loss = 1.5167
[2023-10-02 02:55:54] iter = 18690, loss = 1.5951
[2023-10-02 02:55:55] iter = 18700, loss = 1.5622
[2023-10-02 02:55:56] iter = 18710, loss = 1.6051
[2023-10-02 02:55:57] iter = 18720, loss = 1.5589
[2023-10-02 02:55:58] iter = 18730, loss = 1.3497
[2023-10-02 02:55:59] iter = 18740, loss = 1.4544
[2023-10-02 02:56:00] iter = 18750, loss = 1.4690
[2023-10-02 02:56:01] iter = 18760, loss = 1.3925
[2023-10-02 02:56:02] iter = 18770, loss = 1.3783
[2023-10-02 02:56:03] iter = 18780, loss = 1.4990
[2023-10-02 02:56:03] iter = 18790, loss = 1.4973
[2023-10-02 02:56:04] iter = 18800, loss = 1.4745
[2023-10-02 02:56:05] iter = 18810, loss = 1.4530
[2023-10-02 02:56:06] iter = 18820, loss = 1.4632
[2023-10-02 02:56:07] iter = 18830, loss = 1.3823
[2023-10-02 02:56:08] iter = 18840, loss = 1.6134
[2023-10-02 02:56:09] iter = 18850, loss = 1.4947
[2023-10-02 02:56:10] iter = 18860, loss = 1.3679
[2023-10-02 02:56:10] iter = 18870, loss = 1.3676
[2023-10-02 02:56:11] iter = 18880, loss = 1.3579
[2023-10-02 02:56:12] iter = 18890, loss = 1.5814
[2023-10-02 02:56:13] iter = 18900, loss = 1.6679
[2023-10-02 02:56:14] iter = 18910, loss = 1.4846
[2023-10-02 02:56:15] iter = 18920, loss = 1.4041
[2023-10-02 02:56:16] iter = 18930, loss = 1.3567
[2023-10-02 02:56:17] iter = 18940, loss = 1.3875
[2023-10-02 02:56:18] iter = 18950, loss = 1.3902
[2023-10-02 02:56:19] iter = 18960, loss = 1.3766
[2023-10-02 02:56:20] iter = 18970, loss = 1.5444
[2023-10-02 02:56:21] iter = 18980, loss = 1.4276
[2023-10-02 02:56:21] iter = 18990, loss = 1.5225
[2023-10-02 02:56:22] iter = 19000, loss = 1.4605
[2023-10-02 02:56:23] iter = 19010, loss = 1.5145
[2023-10-02 02:56:24] iter = 19020, loss = 1.6510
[2023-10-02 02:56:25] iter = 19030, loss = 1.4648
[2023-10-02 02:56:26] iter = 19040, loss = 1.4348
[2023-10-02 02:56:27] iter = 19050, loss = 1.4151
[2023-10-02 02:56:28] iter = 19060, loss = 1.3896
[2023-10-02 02:56:29] iter = 19070, loss = 1.4140
[2023-10-02 02:56:30] iter = 19080, loss = 1.3554
[2023-10-02 02:56:31] iter = 19090, loss = 1.3644
[2023-10-02 02:56:31] iter = 19100, loss = 1.2494
[2023-10-02 02:56:32] iter = 19110, loss = 1.4716
[2023-10-02 02:56:33] iter = 19120, loss = 1.6321
[2023-10-02 02:56:34] iter = 19130, loss = 1.5407
[2023-10-02 02:56:35] iter = 19140, loss = 1.4823
[2023-10-02 02:56:36] iter = 19150, loss = 1.4541
[2023-10-02 02:56:37] iter = 19160, loss = 1.3902
[2023-10-02 02:56:38] iter = 19170, loss = 1.4078
[2023-10-02 02:56:39] iter = 19180, loss = 1.5285
[2023-10-02 02:56:40] iter = 19190, loss = 1.4800
[2023-10-02 02:56:40] iter = 19200, loss = 1.4397
[2023-10-02 02:56:41] iter = 19210, loss = 1.4496
[2023-10-02 02:56:42] iter = 19220, loss = 1.5226
[2023-10-02 02:56:43] iter = 19230, loss = 1.4380
[2023-10-02 02:56:44] iter = 19240, loss = 1.4897
[2023-10-02 02:56:45] iter = 19250, loss = 1.4609
[2023-10-02 02:56:46] iter = 19260, loss = 1.3231
[2023-10-02 02:56:47] iter = 19270, loss = 1.5216
[2023-10-02 02:56:48] iter = 19280, loss = 1.4295
[2023-10-02 02:56:49] iter = 19290, loss = 1.4402
[2023-10-02 02:56:50] iter = 19300, loss = 1.5336
[2023-10-02 02:56:50] iter = 19310, loss = 1.5229
[2023-10-02 02:56:51] iter = 19320, loss = 1.6543
[2023-10-02 02:56:52] iter = 19330, loss = 1.3866
[2023-10-02 02:56:53] iter = 19340, loss = 1.3947
[2023-10-02 02:56:54] iter = 19350, loss = 1.4375
[2023-10-02 02:56:55] iter = 19360, loss = 1.5084
[2023-10-02 02:56:56] iter = 19370, loss = 1.5903
[2023-10-02 02:56:57] iter = 19380, loss = 1.5367
[2023-10-02 02:56:57] iter = 19390, loss = 1.5319
[2023-10-02 02:56:58] iter = 19400, loss = 1.4941
[2023-10-02 02:56:59] iter = 19410, loss = 1.4508
[2023-10-02 02:57:00] iter = 19420, loss = 1.5396
[2023-10-02 02:57:01] iter = 19430, loss = 1.4441
[2023-10-02 02:57:02] iter = 19440, loss = 1.3398
[2023-10-02 02:57:03] iter = 19450, loss = 1.4589
[2023-10-02 02:57:04] iter = 19460, loss = 1.5071
[2023-10-02 02:57:05] iter = 19470, loss = 1.4149
[2023-10-02 02:57:06] iter = 19480, loss = 1.5876
[2023-10-02 02:57:06] iter = 19490, loss = 1.6249
[2023-10-02 02:57:08] iter = 19500, loss = 1.5113
[2023-10-02 02:57:09] iter = 19510, loss = 1.4976
[2023-10-02 02:57:09] iter = 19520, loss = 1.4881
[2023-10-02 02:57:10] iter = 19530, loss = 1.4440
[2023-10-02 02:57:11] iter = 19540, loss = 1.3498
[2023-10-02 02:57:12] iter = 19550, loss = 1.4781
[2023-10-02 02:57:13] iter = 19560, loss = 1.4595
[2023-10-02 02:57:14] iter = 19570, loss = 1.4192
[2023-10-02 02:57:15] iter = 19580, loss = 1.3852
[2023-10-02 02:57:16] iter = 19590, loss = 1.3711
[2023-10-02 02:57:17] iter = 19600, loss = 1.5798
[2023-10-02 02:57:18] iter = 19610, loss = 1.5017
[2023-10-02 02:57:19] iter = 19620, loss = 1.3502
[2023-10-02 02:57:19] iter = 19630, loss = 1.3438
[2023-10-02 02:57:20] iter = 19640, loss = 1.6299
[2023-10-02 02:57:21] iter = 19650, loss = 1.4765
[2023-10-02 02:57:22] iter = 19660, loss = 1.3914
[2023-10-02 02:57:23] iter = 19670, loss = 1.4032
[2023-10-02 02:57:24] iter = 19680, loss = 1.4091
[2023-10-02 02:57:25] iter = 19690, loss = 1.6874
[2023-10-02 02:57:26] iter = 19700, loss = 1.4927
[2023-10-02 02:57:27] iter = 19710, loss = 1.5495
[2023-10-02 02:57:28] iter = 19720, loss = 1.4271
[2023-10-02 02:57:28] iter = 19730, loss = 1.3845
[2023-10-02 02:57:29] iter = 19740, loss = 1.3795
[2023-10-02 02:57:30] iter = 19750, loss = 1.5064
[2023-10-02 02:57:31] iter = 19760, loss = 1.4160
[2023-10-02 02:57:32] iter = 19770, loss = 1.4470
[2023-10-02 02:57:33] iter = 19780, loss = 1.4955
[2023-10-02 02:57:34] iter = 19790, loss = 1.5011
[2023-10-02 02:57:35] iter = 19800, loss = 1.6035
[2023-10-02 02:57:35] iter = 19810, loss = 1.4862
[2023-10-02 02:57:36] iter = 19820, loss = 1.4993
[2023-10-02 02:57:37] iter = 19830, loss = 1.4800
[2023-10-02 02:57:38] iter = 19840, loss = 1.5602
[2023-10-02 02:57:39] iter = 19850, loss = 1.4745
[2023-10-02 02:57:40] iter = 19860, loss = 1.5018
[2023-10-02 02:57:41] iter = 19870, loss = 1.4250
[2023-10-02 02:57:42] iter = 19880, loss = 1.5664
[2023-10-02 02:57:43] iter = 19890, loss = 1.3725
[2023-10-02 02:57:44] iter = 19900, loss = 1.3923
[2023-10-02 02:57:45] iter = 19910, loss = 1.4391
[2023-10-02 02:57:45] iter = 19920, loss = 1.5473
[2023-10-02 02:57:46] iter = 19930, loss = 1.6471
[2023-10-02 02:57:47] iter = 19940, loss = 1.6046
[2023-10-02 02:57:48] iter = 19950, loss = 1.4183
[2023-10-02 02:57:49] iter = 19960, loss = 1.6154
[2023-10-02 02:57:50] iter = 19970, loss = 1.4662
[2023-10-02 02:57:51] iter = 19980, loss = 1.5793
[2023-10-02 02:57:52] iter = 19990, loss = 1.6189
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 73164}
[2023-10-02 02:58:17] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002165 train acc = 1.0000, test acc = 0.6290
[2023-10-02 02:58:41] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.003679 train acc = 1.0000, test acc = 0.6272
[2023-10-02 02:59:05] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.017361 train acc = 1.0000, test acc = 0.6238
[2023-10-02 02:59:29] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002365 train acc = 1.0000, test acc = 0.6211
[2023-10-02 02:59:53] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.004326 train acc = 1.0000, test acc = 0.6252
[2023-10-02 03:00:17] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003324 train acc = 1.0000, test acc = 0.6252
[2023-10-02 03:00:41] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.014438 train acc = 0.9980, test acc = 0.6245
[2023-10-02 03:01:06] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.021640 train acc = 0.9980, test acc = 0.6314
[2023-10-02 03:01:30] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002438 train acc = 1.0000, test acc = 0.6306
[2023-10-02 03:01:54] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.015449 train acc = 1.0000, test acc = 0.6269
[2023-10-02 03:02:18] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.018064 train acc = 0.9980, test acc = 0.6311
[2023-10-02 03:02:42] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.014540 train acc = 1.0000, test acc = 0.6247
[2023-10-02 03:03:06] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.014078 train acc = 1.0000, test acc = 0.6309
[2023-10-02 03:03:30] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002450 train acc = 1.0000, test acc = 0.6250
[2023-10-02 03:03:55] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.015476 train acc = 1.0000, test acc = 0.6217
[2023-10-02 03:04:19] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015879 train acc = 1.0000, test acc = 0.6259
[2023-10-02 03:04:43] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003872 train acc = 1.0000, test acc = 0.6255
[2023-10-02 03:05:07] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.002632 train acc = 1.0000, test acc = 0.6281
[2023-10-02 03:05:31] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001981 train acc = 1.0000, test acc = 0.6217
[2023-10-02 03:05:55] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.022896 train acc = 1.0000, test acc = 0.6285
Evaluate 20 random ConvNet, mean = 0.6264 std = 0.0031
-------------------------
[2023-10-02 03:05:56] iter = 20000, loss = 1.5104

================== Exp 3 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f7bf1f7b400>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-02 03:06:13] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 56032}
[2023-10-02 03:06:37] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.007311 train acc = 1.0000, test acc = 0.4874
[2023-10-02 03:07:01] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004843 train acc = 1.0000, test acc = 0.4844
[2023-10-02 03:07:25] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.000994 train acc = 1.0000, test acc = 0.4785
[2023-10-02 03:07:49] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.009809 train acc = 0.9980, test acc = 0.4803
[2023-10-02 03:08:14] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002225 train acc = 1.0000, test acc = 0.4816
[2023-10-02 03:08:38] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.003972 train acc = 1.0000, test acc = 0.4858
[2023-10-02 03:09:02] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.000933 train acc = 1.0000, test acc = 0.4888
[2023-10-02 03:09:26] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.014416 train acc = 1.0000, test acc = 0.4763
[2023-10-02 03:09:50] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.008195 train acc = 1.0000, test acc = 0.4835
[2023-10-02 03:10:14] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005534 train acc = 1.0000, test acc = 0.4890
[2023-10-02 03:10:39] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.011366 train acc = 1.0000, test acc = 0.4772
[2023-10-02 03:11:03] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.005890 train acc = 1.0000, test acc = 0.4882
[2023-10-02 03:11:27] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003721 train acc = 0.9980, test acc = 0.4821
[2023-10-02 03:11:51] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.023093 train acc = 1.0000, test acc = 0.4846
[2023-10-02 03:12:15] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.007309 train acc = 1.0000, test acc = 0.4752
[2023-10-02 03:12:40] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004602 train acc = 1.0000, test acc = 0.4899
[2023-10-02 03:13:04] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.015306 train acc = 0.9980, test acc = 0.4849
[2023-10-02 03:13:28] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.006329 train acc = 1.0000, test acc = 0.4841
[2023-10-02 03:13:52] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.005146 train acc = 0.9980, test acc = 0.4886
[2023-10-02 03:14:17] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.009943 train acc = 1.0000, test acc = 0.4836
Evaluate 20 random ConvNet, mean = 0.4837 std = 0.0043
-------------------------
[2023-10-02 03:14:17] iter = 00000, loss = 7.1303
[2023-10-02 03:14:18] iter = 00010, loss = 6.3443
[2023-10-02 03:14:19] iter = 00020, loss = 5.2745
[2023-10-02 03:14:19] iter = 00030, loss = 5.4208
[2023-10-02 03:14:20] iter = 00040, loss = 4.5851
[2023-10-02 03:14:21] iter = 00050, loss = 4.5981
[2023-10-02 03:14:22] iter = 00060, loss = 4.3766
[2023-10-02 03:14:23] iter = 00070, loss = 3.8408
[2023-10-02 03:14:24] iter = 00080, loss = 3.9072
[2023-10-02 03:14:25] iter = 00090, loss = 3.4998
[2023-10-02 03:14:26] iter = 00100, loss = 3.3749
[2023-10-02 03:14:27] iter = 00110, loss = 3.5082
[2023-10-02 03:14:27] iter = 00120, loss = 3.9259
[2023-10-02 03:14:28] iter = 00130, loss = 2.8747
[2023-10-02 03:14:29] iter = 00140, loss = 2.8605
[2023-10-02 03:14:30] iter = 00150, loss = 3.2509
[2023-10-02 03:14:31] iter = 00160, loss = 3.1051
[2023-10-02 03:14:32] iter = 00170, loss = 2.9855
[2023-10-02 03:14:33] iter = 00180, loss = 2.9901
[2023-10-02 03:14:34] iter = 00190, loss = 2.7706
[2023-10-02 03:14:35] iter = 00200, loss = 2.9253
[2023-10-02 03:14:36] iter = 00210, loss = 2.7033
[2023-10-02 03:14:37] iter = 00220, loss = 2.8240
[2023-10-02 03:14:37] iter = 00230, loss = 3.0607
[2023-10-02 03:14:38] iter = 00240, loss = 2.6396
[2023-10-02 03:14:39] iter = 00250, loss = 2.6071
[2023-10-02 03:14:40] iter = 00260, loss = 2.9327
[2023-10-02 03:14:41] iter = 00270, loss = 2.8113
[2023-10-02 03:14:42] iter = 00280, loss = 2.8231
[2023-10-02 03:14:43] iter = 00290, loss = 2.6506
[2023-10-02 03:14:44] iter = 00300, loss = 2.5644
[2023-10-02 03:14:45] iter = 00310, loss = 2.5826
[2023-10-02 03:14:46] iter = 00320, loss = 2.4689
[2023-10-02 03:14:47] iter = 00330, loss = 2.4994
[2023-10-02 03:14:47] iter = 00340, loss = 2.4129
[2023-10-02 03:14:48] iter = 00350, loss = 2.6201
[2023-10-02 03:14:49] iter = 00360, loss = 2.4364
[2023-10-02 03:14:50] iter = 00370, loss = 2.5098
[2023-10-02 03:14:51] iter = 00380, loss = 2.1655
[2023-10-02 03:14:52] iter = 00390, loss = 2.8504
[2023-10-02 03:14:53] iter = 00400, loss = 2.2949
[2023-10-02 03:14:54] iter = 00410, loss = 2.2857
[2023-10-02 03:14:55] iter = 00420, loss = 2.2790
[2023-10-02 03:14:56] iter = 00430, loss = 2.3777
[2023-10-02 03:14:56] iter = 00440, loss = 2.4225
[2023-10-02 03:14:57] iter = 00450, loss = 2.3490
[2023-10-02 03:14:58] iter = 00460, loss = 2.2098
[2023-10-02 03:14:59] iter = 00470, loss = 2.2996
[2023-10-02 03:15:00] iter = 00480, loss = 2.1246
[2023-10-02 03:15:01] iter = 00490, loss = 2.5172
[2023-10-02 03:15:02] iter = 00500, loss = 2.3719
[2023-10-02 03:15:03] iter = 00510, loss = 2.3836
[2023-10-02 03:15:04] iter = 00520, loss = 2.3280
[2023-10-02 03:15:04] iter = 00530, loss = 2.3708
[2023-10-02 03:15:05] iter = 00540, loss = 2.1343
[2023-10-02 03:15:06] iter = 00550, loss = 2.1981
[2023-10-02 03:15:07] iter = 00560, loss = 2.4573
[2023-10-02 03:15:08] iter = 00570, loss = 2.2591
[2023-10-02 03:15:09] iter = 00580, loss = 2.0898
[2023-10-02 03:15:10] iter = 00590, loss = 2.1622
[2023-10-02 03:15:11] iter = 00600, loss = 2.2847
[2023-10-02 03:15:12] iter = 00610, loss = 2.1823
[2023-10-02 03:15:13] iter = 00620, loss = 2.1834
[2023-10-02 03:15:14] iter = 00630, loss = 2.2173
[2023-10-02 03:15:14] iter = 00640, loss = 2.2197
[2023-10-02 03:15:15] iter = 00650, loss = 2.1809
[2023-10-02 03:15:16] iter = 00660, loss = 2.1874
[2023-10-02 03:15:17] iter = 00670, loss = 2.3071
[2023-10-02 03:15:18] iter = 00680, loss = 2.0442
[2023-10-02 03:15:19] iter = 00690, loss = 2.1075
[2023-10-02 03:15:20] iter = 00700, loss = 2.0811
[2023-10-02 03:15:21] iter = 00710, loss = 2.1187
[2023-10-02 03:15:22] iter = 00720, loss = 2.0934
[2023-10-02 03:15:23] iter = 00730, loss = 2.1790
[2023-10-02 03:15:24] iter = 00740, loss = 2.0581
[2023-10-02 03:15:24] iter = 00750, loss = 1.8378
[2023-10-02 03:15:25] iter = 00760, loss = 2.3752
[2023-10-02 03:15:26] iter = 00770, loss = 2.0306
[2023-10-02 03:15:27] iter = 00780, loss = 2.0978
[2023-10-02 03:15:28] iter = 00790, loss = 2.0168
[2023-10-02 03:15:29] iter = 00800, loss = 2.0014
[2023-10-02 03:15:30] iter = 00810, loss = 1.9925
[2023-10-02 03:15:31] iter = 00820, loss = 2.0764
[2023-10-02 03:15:31] iter = 00830, loss = 2.1045
[2023-10-02 03:15:32] iter = 00840, loss = 2.0741
[2023-10-02 03:15:33] iter = 00850, loss = 2.0711
[2023-10-02 03:15:34] iter = 00860, loss = 2.0060
[2023-10-02 03:15:35] iter = 00870, loss = 2.0328
[2023-10-02 03:15:36] iter = 00880, loss = 2.1334
[2023-10-02 03:15:37] iter = 00890, loss = 2.1611
[2023-10-02 03:15:38] iter = 00900, loss = 2.1306
[2023-10-02 03:15:39] iter = 00910, loss = 2.0489
[2023-10-02 03:15:40] iter = 00920, loss = 2.1217
[2023-10-02 03:15:41] iter = 00930, loss = 2.0188
[2023-10-02 03:15:41] iter = 00940, loss = 1.9404
[2023-10-02 03:15:42] iter = 00950, loss = 2.0758
[2023-10-02 03:15:43] iter = 00960, loss = 2.1523
[2023-10-02 03:15:44] iter = 00970, loss = 2.0734
[2023-10-02 03:15:45] iter = 00980, loss = 1.9025
[2023-10-02 03:15:46] iter = 00990, loss = 2.2094
[2023-10-02 03:15:47] iter = 01000, loss = 1.8854
[2023-10-02 03:15:48] iter = 01010, loss = 2.1083
[2023-10-02 03:15:49] iter = 01020, loss = 1.9239
[2023-10-02 03:15:50] iter = 01030, loss = 1.8886
[2023-10-02 03:15:51] iter = 01040, loss = 2.1727
[2023-10-02 03:15:51] iter = 01050, loss = 2.0930
[2023-10-02 03:15:52] iter = 01060, loss = 2.0816
[2023-10-02 03:15:53] iter = 01070, loss = 1.9364
[2023-10-02 03:15:54] iter = 01080, loss = 1.9601
[2023-10-02 03:15:55] iter = 01090, loss = 2.0425
[2023-10-02 03:15:56] iter = 01100, loss = 2.0163
[2023-10-02 03:15:57] iter = 01110, loss = 2.0177
[2023-10-02 03:15:58] iter = 01120, loss = 1.9458
[2023-10-02 03:15:59] iter = 01130, loss = 2.0366
[2023-10-02 03:16:00] iter = 01140, loss = 2.0662
[2023-10-02 03:16:01] iter = 01150, loss = 2.0583
[2023-10-02 03:16:01] iter = 01160, loss = 2.1797
[2023-10-02 03:16:02] iter = 01170, loss = 1.9188
[2023-10-02 03:16:03] iter = 01180, loss = 2.0616
[2023-10-02 03:16:04] iter = 01190, loss = 1.9683
[2023-10-02 03:16:05] iter = 01200, loss = 1.8723
[2023-10-02 03:16:06] iter = 01210, loss = 1.9156
[2023-10-02 03:16:07] iter = 01220, loss = 1.8934
[2023-10-02 03:16:08] iter = 01230, loss = 2.0174
[2023-10-02 03:16:09] iter = 01240, loss = 1.9011
[2023-10-02 03:16:09] iter = 01250, loss = 1.9558
[2023-10-02 03:16:10] iter = 01260, loss = 1.9678
[2023-10-02 03:16:11] iter = 01270, loss = 1.8973
[2023-10-02 03:16:12] iter = 01280, loss = 1.8797
[2023-10-02 03:16:13] iter = 01290, loss = 1.9349
[2023-10-02 03:16:14] iter = 01300, loss = 1.9758
[2023-10-02 03:16:15] iter = 01310, loss = 1.9228
[2023-10-02 03:16:16] iter = 01320, loss = 1.8723
[2023-10-02 03:16:17] iter = 01330, loss = 1.9178
[2023-10-02 03:16:17] iter = 01340, loss = 2.1682
[2023-10-02 03:16:18] iter = 01350, loss = 2.0158
[2023-10-02 03:16:19] iter = 01360, loss = 1.9154
[2023-10-02 03:16:20] iter = 01370, loss = 1.8252
[2023-10-02 03:16:21] iter = 01380, loss = 2.0201
[2023-10-02 03:16:22] iter = 01390, loss = 1.7758
[2023-10-02 03:16:23] iter = 01400, loss = 2.0022
[2023-10-02 03:16:24] iter = 01410, loss = 1.9140
[2023-10-02 03:16:25] iter = 01420, loss = 2.0929
[2023-10-02 03:16:26] iter = 01430, loss = 2.0156
[2023-10-02 03:16:27] iter = 01440, loss = 2.0020
[2023-10-02 03:16:28] iter = 01450, loss = 1.9664
[2023-10-02 03:16:28] iter = 01460, loss = 1.8572
[2023-10-02 03:16:29] iter = 01470, loss = 1.7184
[2023-10-02 03:16:30] iter = 01480, loss = 1.9310
[2023-10-02 03:16:31] iter = 01490, loss = 1.7123
[2023-10-02 03:16:32] iter = 01500, loss = 2.0148
[2023-10-02 03:16:33] iter = 01510, loss = 1.8590
[2023-10-02 03:16:34] iter = 01520, loss = 2.1716
[2023-10-02 03:16:35] iter = 01530, loss = 1.8376
[2023-10-02 03:16:36] iter = 01540, loss = 1.7268
[2023-10-02 03:16:36] iter = 01550, loss = 1.9124
[2023-10-02 03:16:37] iter = 01560, loss = 1.9668
[2023-10-02 03:16:38] iter = 01570, loss = 1.9271
[2023-10-02 03:16:39] iter = 01580, loss = 1.9789
[2023-10-02 03:16:40] iter = 01590, loss = 1.7699
[2023-10-02 03:16:41] iter = 01600, loss = 1.9294
[2023-10-02 03:16:42] iter = 01610, loss = 1.7793
[2023-10-02 03:16:43] iter = 01620, loss = 1.7588
[2023-10-02 03:16:44] iter = 01630, loss = 1.8363
[2023-10-02 03:16:44] iter = 01640, loss = 2.1880
[2023-10-02 03:16:45] iter = 01650, loss = 1.9244
[2023-10-02 03:16:46] iter = 01660, loss = 1.9353
[2023-10-02 03:16:47] iter = 01670, loss = 1.8729
[2023-10-02 03:16:48] iter = 01680, loss = 1.8562
[2023-10-02 03:16:49] iter = 01690, loss = 1.8344
[2023-10-02 03:16:50] iter = 01700, loss = 1.8152
[2023-10-02 03:16:51] iter = 01710, loss = 1.8670
[2023-10-02 03:16:52] iter = 01720, loss = 1.9122
[2023-10-02 03:16:53] iter = 01730, loss = 1.8678
[2023-10-02 03:16:54] iter = 01740, loss = 1.7243
[2023-10-02 03:16:54] iter = 01750, loss = 1.9381
[2023-10-02 03:16:55] iter = 01760, loss = 1.8214
[2023-10-02 03:16:56] iter = 01770, loss = 1.9547
[2023-10-02 03:16:57] iter = 01780, loss = 1.9321
[2023-10-02 03:16:58] iter = 01790, loss = 1.9648
[2023-10-02 03:16:59] iter = 01800, loss = 1.8301
[2023-10-02 03:17:00] iter = 01810, loss = 1.9948
[2023-10-02 03:17:01] iter = 01820, loss = 1.9171
[2023-10-02 03:17:02] iter = 01830, loss = 1.7869
[2023-10-02 03:17:03] iter = 01840, loss = 1.8640
[2023-10-02 03:17:03] iter = 01850, loss = 1.8231
[2023-10-02 03:17:04] iter = 01860, loss = 1.9513
[2023-10-02 03:17:05] iter = 01870, loss = 1.9522
[2023-10-02 03:17:06] iter = 01880, loss = 1.8956
[2023-10-02 03:17:07] iter = 01890, loss = 1.8995
[2023-10-02 03:17:08] iter = 01900, loss = 1.7987
[2023-10-02 03:17:09] iter = 01910, loss = 1.8449
[2023-10-02 03:17:10] iter = 01920, loss = 1.8803
[2023-10-02 03:17:11] iter = 01930, loss = 2.0557
[2023-10-02 03:17:12] iter = 01940, loss = 2.0465
[2023-10-02 03:17:13] iter = 01950, loss = 1.7976
[2023-10-02 03:17:14] iter = 01960, loss = 1.8139
[2023-10-02 03:17:14] iter = 01970, loss = 1.7836
[2023-10-02 03:17:15] iter = 01980, loss = 1.9889
[2023-10-02 03:17:16] iter = 01990, loss = 1.8712
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 37412}
[2023-10-02 03:17:41] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.008825 train acc = 1.0000, test acc = 0.5855
[2023-10-02 03:18:05] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.009019 train acc = 1.0000, test acc = 0.5854
[2023-10-02 03:18:29] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.009801 train acc = 1.0000, test acc = 0.5801
[2023-10-02 03:18:53] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002677 train acc = 1.0000, test acc = 0.5825
[2023-10-02 03:19:17] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.007970 train acc = 1.0000, test acc = 0.5780
[2023-10-02 03:19:42] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004383 train acc = 1.0000, test acc = 0.5787
[2023-10-02 03:20:06] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016151 train acc = 1.0000, test acc = 0.5955
[2023-10-02 03:20:30] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.007490 train acc = 1.0000, test acc = 0.5865
[2023-10-02 03:20:54] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.009784 train acc = 1.0000, test acc = 0.5824
[2023-10-02 03:21:18] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002255 train acc = 1.0000, test acc = 0.5810
[2023-10-02 03:21:42] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.012322 train acc = 1.0000, test acc = 0.5875
[2023-10-02 03:22:06] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.012172 train acc = 1.0000, test acc = 0.5861
[2023-10-02 03:22:31] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.008770 train acc = 1.0000, test acc = 0.5838
[2023-10-02 03:22:55] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003553 train acc = 1.0000, test acc = 0.5871
[2023-10-02 03:23:19] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003290 train acc = 1.0000, test acc = 0.5767
[2023-10-02 03:23:43] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.007798 train acc = 1.0000, test acc = 0.5781
[2023-10-02 03:24:07] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002730 train acc = 1.0000, test acc = 0.5870
[2023-10-02 03:24:31] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.002210 train acc = 1.0000, test acc = 0.5880
[2023-10-02 03:24:55] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.012519 train acc = 1.0000, test acc = 0.5850
[2023-10-02 03:25:20] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012982 train acc = 1.0000, test acc = 0.5825
Evaluate 20 random ConvNet, mean = 0.5839 std = 0.0043
-------------------------
[2023-10-02 03:25:20] iter = 02000, loss = 1.6709
[2023-10-02 03:25:21] iter = 02010, loss = 1.7109
[2023-10-02 03:25:22] iter = 02020, loss = 1.7358
[2023-10-02 03:25:23] iter = 02030, loss = 1.6874
[2023-10-02 03:25:24] iter = 02040, loss = 1.8502
[2023-10-02 03:25:24] iter = 02050, loss = 1.9264
[2023-10-02 03:25:25] iter = 02060, loss = 1.9132
[2023-10-02 03:25:26] iter = 02070, loss = 1.6807
[2023-10-02 03:25:27] iter = 02080, loss = 1.8503
[2023-10-02 03:25:28] iter = 02090, loss = 1.7340
[2023-10-02 03:25:29] iter = 02100, loss = 1.9181
[2023-10-02 03:25:30] iter = 02110, loss = 1.7918
[2023-10-02 03:25:31] iter = 02120, loss = 1.8040
[2023-10-02 03:25:32] iter = 02130, loss = 1.9009
[2023-10-02 03:25:33] iter = 02140, loss = 1.7901
[2023-10-02 03:25:34] iter = 02150, loss = 1.7285
[2023-10-02 03:25:35] iter = 02160, loss = 1.6740
[2023-10-02 03:25:35] iter = 02170, loss = 1.8199
[2023-10-02 03:25:36] iter = 02180, loss = 1.8003
[2023-10-02 03:25:37] iter = 02190, loss = 1.8585
[2023-10-02 03:25:38] iter = 02200, loss = 1.7662
[2023-10-02 03:25:39] iter = 02210, loss = 1.9331
[2023-10-02 03:25:40] iter = 02220, loss = 1.7919
[2023-10-02 03:25:41] iter = 02230, loss = 1.9998
[2023-10-02 03:25:42] iter = 02240, loss = 1.7584
[2023-10-02 03:25:43] iter = 02250, loss = 1.9167
[2023-10-02 03:25:44] iter = 02260, loss = 1.7854
[2023-10-02 03:25:45] iter = 02270, loss = 1.8627
[2023-10-02 03:25:45] iter = 02280, loss = 1.7568
[2023-10-02 03:25:46] iter = 02290, loss = 1.8478
[2023-10-02 03:25:47] iter = 02300, loss = 1.9392
[2023-10-02 03:25:48] iter = 02310, loss = 1.7802
[2023-10-02 03:25:49] iter = 02320, loss = 1.8516
[2023-10-02 03:25:50] iter = 02330, loss = 1.9584
[2023-10-02 03:25:51] iter = 02340, loss = 1.9550
[2023-10-02 03:25:52] iter = 02350, loss = 1.7916
[2023-10-02 03:25:53] iter = 02360, loss = 1.9860
[2023-10-02 03:25:54] iter = 02370, loss = 1.7774
[2023-10-02 03:25:54] iter = 02380, loss = 1.8614
[2023-10-02 03:25:55] iter = 02390, loss = 1.9789
[2023-10-02 03:25:56] iter = 02400, loss = 1.8199
[2023-10-02 03:25:57] iter = 02410, loss = 1.9451
[2023-10-02 03:25:58] iter = 02420, loss = 1.8870
[2023-10-02 03:25:59] iter = 02430, loss = 1.8008
[2023-10-02 03:26:00] iter = 02440, loss = 1.8010
[2023-10-02 03:26:01] iter = 02450, loss = 1.7694
[2023-10-02 03:26:02] iter = 02460, loss = 1.8695
[2023-10-02 03:26:02] iter = 02470, loss = 1.7935
[2023-10-02 03:26:03] iter = 02480, loss = 1.8628
[2023-10-02 03:26:04] iter = 02490, loss = 1.8398
[2023-10-02 03:26:05] iter = 02500, loss = 1.9827
[2023-10-02 03:26:06] iter = 02510, loss = 1.8251
[2023-10-02 03:26:07] iter = 02520, loss = 1.6837
[2023-10-02 03:26:08] iter = 02530, loss = 1.7584
[2023-10-02 03:26:09] iter = 02540, loss = 1.6958
[2023-10-02 03:26:10] iter = 02550, loss = 1.7883
[2023-10-02 03:26:11] iter = 02560, loss = 1.9121
[2023-10-02 03:26:11] iter = 02570, loss = 1.8997
[2023-10-02 03:26:12] iter = 02580, loss = 1.9227
[2023-10-02 03:26:13] iter = 02590, loss = 1.7325
[2023-10-02 03:26:14] iter = 02600, loss = 1.6406
[2023-10-02 03:26:15] iter = 02610, loss = 1.6510
[2023-10-02 03:26:16] iter = 02620, loss = 1.6809
[2023-10-02 03:26:17] iter = 02630, loss = 1.6837
[2023-10-02 03:26:18] iter = 02640, loss = 1.8330
[2023-10-02 03:26:19] iter = 02650, loss = 1.9504
[2023-10-02 03:26:20] iter = 02660, loss = 1.7358
[2023-10-02 03:26:21] iter = 02670, loss = 1.9202
[2023-10-02 03:26:22] iter = 02680, loss = 1.7576
[2023-10-02 03:26:23] iter = 02690, loss = 1.6972
[2023-10-02 03:26:23] iter = 02700, loss = 1.7393
[2023-10-02 03:26:24] iter = 02710, loss = 1.8137
[2023-10-02 03:26:25] iter = 02720, loss = 1.8926
[2023-10-02 03:26:26] iter = 02730, loss = 1.8495
[2023-10-02 03:26:27] iter = 02740, loss = 1.8139
[2023-10-02 03:26:28] iter = 02750, loss = 1.7131
[2023-10-02 03:26:29] iter = 02760, loss = 1.8379
[2023-10-02 03:26:30] iter = 02770, loss = 1.8750
[2023-10-02 03:26:31] iter = 02780, loss = 1.8545
[2023-10-02 03:26:31] iter = 02790, loss = 1.7578
[2023-10-02 03:26:32] iter = 02800, loss = 1.7215
[2023-10-02 03:26:33] iter = 02810, loss = 1.7553
[2023-10-02 03:26:34] iter = 02820, loss = 1.8789
[2023-10-02 03:26:35] iter = 02830, loss = 1.7559
[2023-10-02 03:26:36] iter = 02840, loss = 1.5812
[2023-10-02 03:26:37] iter = 02850, loss = 1.8188
[2023-10-02 03:26:38] iter = 02860, loss = 1.8470
[2023-10-02 03:26:39] iter = 02870, loss = 1.9040
[2023-10-02 03:26:39] iter = 02880, loss = 1.7171
[2023-10-02 03:26:40] iter = 02890, loss = 1.7006
[2023-10-02 03:26:41] iter = 02900, loss = 1.8048
[2023-10-02 03:26:42] iter = 02910, loss = 1.6658
[2023-10-02 03:26:43] iter = 02920, loss = 1.8906
[2023-10-02 03:26:44] iter = 02930, loss = 1.6024
[2023-10-02 03:26:45] iter = 02940, loss = 1.7799
[2023-10-02 03:26:46] iter = 02950, loss = 1.7583
[2023-10-02 03:26:47] iter = 02960, loss = 1.9174
[2023-10-02 03:26:48] iter = 02970, loss = 1.8591
[2023-10-02 03:26:48] iter = 02980, loss = 1.7104
[2023-10-02 03:26:49] iter = 02990, loss = 1.9508
[2023-10-02 03:26:50] iter = 03000, loss = 1.8065
[2023-10-02 03:26:51] iter = 03010, loss = 1.6110
[2023-10-02 03:26:52] iter = 03020, loss = 1.5696
[2023-10-02 03:26:53] iter = 03030, loss = 1.8171
[2023-10-02 03:26:54] iter = 03040, loss = 1.6911
[2023-10-02 03:26:55] iter = 03050, loss = 1.7208
[2023-10-02 03:26:56] iter = 03060, loss = 1.6904
[2023-10-02 03:26:57] iter = 03070, loss = 1.7616
[2023-10-02 03:26:58] iter = 03080, loss = 1.7811
[2023-10-02 03:26:59] iter = 03090, loss = 1.6855
[2023-10-02 03:27:00] iter = 03100, loss = 1.6660
[2023-10-02 03:27:00] iter = 03110, loss = 1.7374
[2023-10-02 03:27:01] iter = 03120, loss = 1.6970
[2023-10-02 03:27:02] iter = 03130, loss = 1.6487
[2023-10-02 03:27:03] iter = 03140, loss = 1.6812
[2023-10-02 03:27:04] iter = 03150, loss = 1.7083
[2023-10-02 03:27:05] iter = 03160, loss = 1.7325
[2023-10-02 03:27:06] iter = 03170, loss = 1.6040
[2023-10-02 03:27:07] iter = 03180, loss = 1.8148
[2023-10-02 03:27:08] iter = 03190, loss = 1.6402
[2023-10-02 03:27:09] iter = 03200, loss = 1.6980
[2023-10-02 03:27:10] iter = 03210, loss = 1.6092
[2023-10-02 03:27:10] iter = 03220, loss = 1.8018
[2023-10-02 03:27:11] iter = 03230, loss = 1.6833
[2023-10-02 03:27:12] iter = 03240, loss = 1.8126
[2023-10-02 03:27:13] iter = 03250, loss = 1.7252
[2023-10-02 03:27:14] iter = 03260, loss = 1.7612
[2023-10-02 03:27:15] iter = 03270, loss = 1.7263
[2023-10-02 03:27:16] iter = 03280, loss = 1.7601
[2023-10-02 03:27:17] iter = 03290, loss = 1.7131
[2023-10-02 03:27:18] iter = 03300, loss = 1.8283
[2023-10-02 03:27:19] iter = 03310, loss = 1.6910
[2023-10-02 03:27:19] iter = 03320, loss = 1.7606
[2023-10-02 03:27:20] iter = 03330, loss = 1.6969
[2023-10-02 03:27:21] iter = 03340, loss = 1.7517
[2023-10-02 03:27:22] iter = 03350, loss = 1.7638
[2023-10-02 03:27:23] iter = 03360, loss = 1.7220
[2023-10-02 03:27:24] iter = 03370, loss = 1.9551
[2023-10-02 03:27:25] iter = 03380, loss = 1.7171
[2023-10-02 03:27:26] iter = 03390, loss = 1.7448
[2023-10-02 03:27:27] iter = 03400, loss = 1.5852
[2023-10-02 03:27:28] iter = 03410, loss = 1.7299
[2023-10-02 03:27:29] iter = 03420, loss = 1.8255
[2023-10-02 03:27:30] iter = 03430, loss = 1.6263
[2023-10-02 03:27:30] iter = 03440, loss = 1.6420
[2023-10-02 03:27:31] iter = 03450, loss = 1.7484
[2023-10-02 03:27:32] iter = 03460, loss = 1.7561
[2023-10-02 03:27:33] iter = 03470, loss = 1.7245
[2023-10-02 03:27:34] iter = 03480, loss = 1.7872
[2023-10-02 03:27:35] iter = 03490, loss = 1.5707
[2023-10-02 03:27:36] iter = 03500, loss = 1.6712
[2023-10-02 03:27:37] iter = 03510, loss = 1.6119
[2023-10-02 03:27:38] iter = 03520, loss = 1.7145
[2023-10-02 03:27:39] iter = 03530, loss = 1.8524
[2023-10-02 03:27:39] iter = 03540, loss = 1.7224
[2023-10-02 03:27:40] iter = 03550, loss = 1.5969
[2023-10-02 03:27:41] iter = 03560, loss = 1.6415
[2023-10-02 03:27:42] iter = 03570, loss = 1.6730
[2023-10-02 03:27:43] iter = 03580, loss = 1.7273
[2023-10-02 03:27:44] iter = 03590, loss = 1.7712
[2023-10-02 03:27:45] iter = 03600, loss = 1.7952
[2023-10-02 03:27:46] iter = 03610, loss = 1.6421
[2023-10-02 03:27:47] iter = 03620, loss = 1.7595
[2023-10-02 03:27:47] iter = 03630, loss = 1.6934
[2023-10-02 03:27:48] iter = 03640, loss = 1.6899
[2023-10-02 03:27:49] iter = 03650, loss = 1.7929
[2023-10-02 03:27:50] iter = 03660, loss = 1.7869
[2023-10-02 03:27:51] iter = 03670, loss = 1.7613
[2023-10-02 03:27:52] iter = 03680, loss = 1.8890
[2023-10-02 03:27:53] iter = 03690, loss = 1.6207
[2023-10-02 03:27:54] iter = 03700, loss = 1.6924
[2023-10-02 03:27:55] iter = 03710, loss = 1.8150
[2023-10-02 03:27:56] iter = 03720, loss = 1.5713
[2023-10-02 03:27:56] iter = 03730, loss = 1.9585
[2023-10-02 03:27:57] iter = 03740, loss = 1.6550
[2023-10-02 03:27:58] iter = 03750, loss = 1.6665
[2023-10-02 03:27:59] iter = 03760, loss = 1.6523
[2023-10-02 03:28:00] iter = 03770, loss = 1.8619
[2023-10-02 03:28:01] iter = 03780, loss = 1.7201
[2023-10-02 03:28:02] iter = 03790, loss = 1.6801
[2023-10-02 03:28:03] iter = 03800, loss = 1.7023
[2023-10-02 03:28:04] iter = 03810, loss = 1.6035
[2023-10-02 03:28:05] iter = 03820, loss = 1.5855
[2023-10-02 03:28:06] iter = 03830, loss = 1.5994
[2023-10-02 03:28:07] iter = 03840, loss = 1.8809
[2023-10-02 03:28:08] iter = 03850, loss = 1.6916
[2023-10-02 03:28:08] iter = 03860, loss = 1.6276
[2023-10-02 03:28:09] iter = 03870, loss = 1.7150
[2023-10-02 03:28:10] iter = 03880, loss = 1.6545
[2023-10-02 03:28:11] iter = 03890, loss = 1.8824
[2023-10-02 03:28:12] iter = 03900, loss = 1.6571
[2023-10-02 03:28:13] iter = 03910, loss = 1.9461
[2023-10-02 03:28:14] iter = 03920, loss = 1.6770
[2023-10-02 03:28:15] iter = 03930, loss = 1.5555
[2023-10-02 03:28:16] iter = 03940, loss = 1.6406
[2023-10-02 03:28:17] iter = 03950, loss = 1.7130
[2023-10-02 03:28:17] iter = 03960, loss = 1.8631
[2023-10-02 03:28:18] iter = 03970, loss = 1.7902
[2023-10-02 03:28:19] iter = 03980, loss = 1.8095
[2023-10-02 03:28:20] iter = 03990, loss = 1.7939
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 1451}
[2023-10-02 03:28:45] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002512 train acc = 1.0000, test acc = 0.6043
[2023-10-02 03:29:09] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004984 train acc = 1.0000, test acc = 0.5969
[2023-10-02 03:29:34] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.017389 train acc = 0.9980, test acc = 0.6070
[2023-10-02 03:29:58] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.003431 train acc = 1.0000, test acc = 0.6062
[2023-10-02 03:30:22] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.014943 train acc = 1.0000, test acc = 0.6021
[2023-10-02 03:30:46] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.009589 train acc = 1.0000, test acc = 0.6044
[2023-10-02 03:31:10] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.005786 train acc = 1.0000, test acc = 0.6047
[2023-10-02 03:31:34] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001698 train acc = 1.0000, test acc = 0.6030
[2023-10-02 03:31:58] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014087 train acc = 1.0000, test acc = 0.6014
[2023-10-02 03:32:22] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003243 train acc = 1.0000, test acc = 0.6029
[2023-10-02 03:32:47] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003523 train acc = 1.0000, test acc = 0.6023
[2023-10-02 03:33:11] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.009534 train acc = 0.9980, test acc = 0.6030
[2023-10-02 03:33:35] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004429 train acc = 1.0000, test acc = 0.5977
[2023-10-02 03:33:59] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003399 train acc = 1.0000, test acc = 0.6014
[2023-10-02 03:34:23] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003470 train acc = 1.0000, test acc = 0.6028
[2023-10-02 03:34:47] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.012971 train acc = 0.9980, test acc = 0.6014
[2023-10-02 03:35:11] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003607 train acc = 1.0000, test acc = 0.6127
[2023-10-02 03:35:35] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.013044 train acc = 1.0000, test acc = 0.5993
[2023-10-02 03:35:59] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002992 train acc = 1.0000, test acc = 0.6083
[2023-10-02 03:36:23] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003236 train acc = 1.0000, test acc = 0.5933
Evaluate 20 random ConvNet, mean = 0.6028 std = 0.0041
-------------------------
[2023-10-02 03:36:23] iter = 04000, loss = 1.6540
[2023-10-02 03:36:24] iter = 04010, loss = 1.8560
[2023-10-02 03:36:25] iter = 04020, loss = 1.7046
[2023-10-02 03:36:26] iter = 04030, loss = 1.6806
[2023-10-02 03:36:27] iter = 04040, loss = 1.6384
[2023-10-02 03:36:28] iter = 04050, loss = 1.6348
[2023-10-02 03:36:29] iter = 04060, loss = 1.7579
[2023-10-02 03:36:30] iter = 04070, loss = 1.6437
[2023-10-02 03:36:31] iter = 04080, loss = 1.6715
[2023-10-02 03:36:32] iter = 04090, loss = 1.8751
[2023-10-02 03:36:32] iter = 04100, loss = 1.7157
[2023-10-02 03:36:33] iter = 04110, loss = 1.7445
[2023-10-02 03:36:34] iter = 04120, loss = 1.6946
[2023-10-02 03:36:35] iter = 04130, loss = 1.6736
[2023-10-02 03:36:36] iter = 04140, loss = 1.5769
[2023-10-02 03:36:37] iter = 04150, loss = 1.6517
[2023-10-02 03:36:38] iter = 04160, loss = 1.5748
[2023-10-02 03:36:39] iter = 04170, loss = 1.6180
[2023-10-02 03:36:40] iter = 04180, loss = 1.6346
[2023-10-02 03:36:41] iter = 04190, loss = 1.6023
[2023-10-02 03:36:42] iter = 04200, loss = 1.6218
[2023-10-02 03:36:43] iter = 04210, loss = 1.7605
[2023-10-02 03:36:44] iter = 04220, loss = 1.6665
[2023-10-02 03:36:44] iter = 04230, loss = 1.6126
[2023-10-02 03:36:45] iter = 04240, loss = 1.7140
[2023-10-02 03:36:46] iter = 04250, loss = 1.7662
[2023-10-02 03:36:47] iter = 04260, loss = 1.6203
[2023-10-02 03:36:48] iter = 04270, loss = 1.5187
[2023-10-02 03:36:49] iter = 04280, loss = 1.6958
[2023-10-02 03:36:50] iter = 04290, loss = 1.5969
[2023-10-02 03:36:51] iter = 04300, loss = 1.7080
[2023-10-02 03:36:52] iter = 04310, loss = 1.8287
[2023-10-02 03:36:52] iter = 04320, loss = 1.8179
[2023-10-02 03:36:53] iter = 04330, loss = 1.8844
[2023-10-02 03:36:54] iter = 04340, loss = 1.9180
[2023-10-02 03:36:55] iter = 04350, loss = 1.6868
[2023-10-02 03:36:56] iter = 04360, loss = 1.6723
[2023-10-02 03:36:57] iter = 04370, loss = 1.7914
[2023-10-02 03:36:58] iter = 04380, loss = 1.6120
[2023-10-02 03:36:59] iter = 04390, loss = 1.7651
[2023-10-02 03:37:00] iter = 04400, loss = 1.8017
[2023-10-02 03:37:01] iter = 04410, loss = 1.7707
[2023-10-02 03:37:02] iter = 04420, loss = 1.5530
[2023-10-02 03:37:02] iter = 04430, loss = 1.7641
[2023-10-02 03:37:03] iter = 04440, loss = 1.7255
[2023-10-02 03:37:04] iter = 04450, loss = 1.7260
[2023-10-02 03:37:05] iter = 04460, loss = 1.5654
[2023-10-02 03:37:06] iter = 04470, loss = 1.6755
[2023-10-02 03:37:07] iter = 04480, loss = 1.6533
[2023-10-02 03:37:08] iter = 04490, loss = 1.6848
[2023-10-02 03:37:09] iter = 04500, loss = 1.7817
[2023-10-02 03:37:10] iter = 04510, loss = 1.6325
[2023-10-02 03:37:11] iter = 04520, loss = 1.7806
[2023-10-02 03:37:12] iter = 04530, loss = 1.7191
[2023-10-02 03:37:12] iter = 04540, loss = 1.7579
[2023-10-02 03:37:13] iter = 04550, loss = 1.9118
[2023-10-02 03:37:14] iter = 04560, loss = 1.6312
[2023-10-02 03:37:15] iter = 04570, loss = 1.6290
[2023-10-02 03:37:16] iter = 04580, loss = 1.8406
[2023-10-02 03:37:17] iter = 04590, loss = 1.6252
[2023-10-02 03:37:18] iter = 04600, loss = 1.8659
[2023-10-02 03:37:19] iter = 04610, loss = 1.5665
[2023-10-02 03:37:20] iter = 04620, loss = 1.5757
[2023-10-02 03:37:21] iter = 04630, loss = 1.6948
[2023-10-02 03:37:21] iter = 04640, loss = 1.5882
[2023-10-02 03:37:22] iter = 04650, loss = 1.6756
[2023-10-02 03:37:23] iter = 04660, loss = 1.8358
[2023-10-02 03:37:24] iter = 04670, loss = 1.6292
[2023-10-02 03:37:25] iter = 04680, loss = 1.6701
[2023-10-02 03:37:26] iter = 04690, loss = 1.7688
[2023-10-02 03:37:27] iter = 04700, loss = 1.6785
[2023-10-02 03:37:28] iter = 04710, loss = 1.7188
[2023-10-02 03:37:29] iter = 04720, loss = 1.6833
[2023-10-02 03:37:29] iter = 04730, loss = 1.6495
[2023-10-02 03:37:30] iter = 04740, loss = 1.6662
[2023-10-02 03:37:31] iter = 04750, loss = 1.6785
[2023-10-02 03:37:32] iter = 04760, loss = 1.6173
[2023-10-02 03:37:33] iter = 04770, loss = 1.5289
[2023-10-02 03:37:34] iter = 04780, loss = 1.9383
[2023-10-02 03:37:35] iter = 04790, loss = 1.8143
[2023-10-02 03:37:36] iter = 04800, loss = 1.5900
[2023-10-02 03:37:36] iter = 04810, loss = 1.7754
[2023-10-02 03:37:37] iter = 04820, loss = 1.6251
[2023-10-02 03:37:38] iter = 04830, loss = 1.6453
[2023-10-02 03:37:39] iter = 04840, loss = 1.5161
[2023-10-02 03:37:40] iter = 04850, loss = 1.6278
[2023-10-02 03:37:41] iter = 04860, loss = 1.7285
[2023-10-02 03:37:42] iter = 04870, loss = 1.6814
[2023-10-02 03:37:43] iter = 04880, loss = 1.7651
[2023-10-02 03:37:44] iter = 04890, loss = 1.4844
[2023-10-02 03:37:45] iter = 04900, loss = 1.6007
[2023-10-02 03:37:46] iter = 04910, loss = 1.6427
[2023-10-02 03:37:46] iter = 04920, loss = 1.5854
[2023-10-02 03:37:48] iter = 04930, loss = 1.7876
[2023-10-02 03:37:48] iter = 04940, loss = 1.6919
[2023-10-02 03:37:49] iter = 04950, loss = 1.6186
[2023-10-02 03:37:50] iter = 04960, loss = 1.5926
[2023-10-02 03:37:51] iter = 04970, loss = 1.8265
[2023-10-02 03:37:52] iter = 04980, loss = 1.5118
[2023-10-02 03:37:53] iter = 04990, loss = 1.6482
[2023-10-02 03:37:54] iter = 05000, loss = 1.6554
[2023-10-02 03:37:55] iter = 05010, loss = 1.5908
[2023-10-02 03:37:56] iter = 05020, loss = 1.6948
[2023-10-02 03:37:57] iter = 05030, loss = 1.6319
[2023-10-02 03:37:57] iter = 05040, loss = 1.7477
[2023-10-02 03:37:58] iter = 05050, loss = 1.7666
[2023-10-02 03:37:59] iter = 05060, loss = 1.6029
[2023-10-02 03:38:00] iter = 05070, loss = 1.7030
[2023-10-02 03:38:01] iter = 05080, loss = 1.6962
[2023-10-02 03:38:02] iter = 05090, loss = 1.6504
[2023-10-02 03:38:03] iter = 05100, loss = 1.5505
[2023-10-02 03:38:04] iter = 05110, loss = 1.7839
[2023-10-02 03:38:05] iter = 05120, loss = 1.6483
[2023-10-02 03:38:06] iter = 05130, loss = 1.7492
[2023-10-02 03:38:07] iter = 05140, loss = 1.7036
[2023-10-02 03:38:07] iter = 05150, loss = 1.5690
[2023-10-02 03:38:08] iter = 05160, loss = 1.6753
[2023-10-02 03:38:09] iter = 05170, loss = 1.7320
[2023-10-02 03:38:10] iter = 05180, loss = 1.6864
[2023-10-02 03:38:11] iter = 05190, loss = 1.6364
[2023-10-02 03:38:12] iter = 05200, loss = 1.6914
[2023-10-02 03:38:13] iter = 05210, loss = 1.6997
[2023-10-02 03:38:14] iter = 05220, loss = 1.7387
[2023-10-02 03:38:15] iter = 05230, loss = 1.6473
[2023-10-02 03:38:15] iter = 05240, loss = 1.6894
[2023-10-02 03:38:16] iter = 05250, loss = 1.6404
[2023-10-02 03:38:17] iter = 05260, loss = 1.7787
[2023-10-02 03:38:18] iter = 05270, loss = 1.6766
[2023-10-02 03:38:19] iter = 05280, loss = 1.7342
[2023-10-02 03:38:20] iter = 05290, loss = 1.7166
[2023-10-02 03:38:21] iter = 05300, loss = 1.5418
[2023-10-02 03:38:22] iter = 05310, loss = 1.8101
[2023-10-02 03:38:23] iter = 05320, loss = 1.5642
[2023-10-02 03:38:23] iter = 05330, loss = 1.6662
[2023-10-02 03:38:24] iter = 05340, loss = 1.7771
[2023-10-02 03:38:25] iter = 05350, loss = 1.7507
[2023-10-02 03:38:26] iter = 05360, loss = 1.7758
[2023-10-02 03:38:27] iter = 05370, loss = 1.5551
[2023-10-02 03:38:28] iter = 05380, loss = 1.6618
[2023-10-02 03:38:29] iter = 05390, loss = 1.6870
[2023-10-02 03:38:30] iter = 05400, loss = 1.7152
[2023-10-02 03:38:31] iter = 05410, loss = 1.6306
[2023-10-02 03:38:32] iter = 05420, loss = 1.8158
[2023-10-02 03:38:33] iter = 05430, loss = 1.6019
[2023-10-02 03:38:34] iter = 05440, loss = 1.5895
[2023-10-02 03:38:34] iter = 05450, loss = 1.6927
[2023-10-02 03:38:35] iter = 05460, loss = 1.7224
[2023-10-02 03:38:36] iter = 05470, loss = 1.6000
[2023-10-02 03:38:37] iter = 05480, loss = 1.6297
[2023-10-02 03:38:38] iter = 05490, loss = 1.6941
[2023-10-02 03:38:39] iter = 05500, loss = 1.5495
[2023-10-02 03:38:40] iter = 05510, loss = 1.5861
[2023-10-02 03:38:41] iter = 05520, loss = 1.6951
[2023-10-02 03:38:42] iter = 05530, loss = 1.7892
[2023-10-02 03:38:43] iter = 05540, loss = 1.5515
[2023-10-02 03:38:44] iter = 05550, loss = 1.6426
[2023-10-02 03:38:44] iter = 05560, loss = 1.7348
[2023-10-02 03:38:45] iter = 05570, loss = 1.6821
[2023-10-02 03:38:46] iter = 05580, loss = 1.5464
[2023-10-02 03:38:47] iter = 05590, loss = 1.8901
[2023-10-02 03:38:48] iter = 05600, loss = 1.6654
[2023-10-02 03:38:49] iter = 05610, loss = 1.7774
[2023-10-02 03:38:50] iter = 05620, loss = 1.7331
[2023-10-02 03:38:51] iter = 05630, loss = 1.6456
[2023-10-02 03:38:52] iter = 05640, loss = 1.6855
[2023-10-02 03:38:52] iter = 05650, loss = 1.3983
[2023-10-02 03:38:53] iter = 05660, loss = 1.5729
[2023-10-02 03:38:54] iter = 05670, loss = 1.6972
[2023-10-02 03:38:55] iter = 05680, loss = 1.5526
[2023-10-02 03:38:56] iter = 05690, loss = 1.6093
[2023-10-02 03:38:57] iter = 05700, loss = 1.5281
[2023-10-02 03:38:58] iter = 05710, loss = 1.7694
[2023-10-02 03:38:59] iter = 05720, loss = 1.6182
[2023-10-02 03:39:00] iter = 05730, loss = 1.7435
[2023-10-02 03:39:01] iter = 05740, loss = 1.7922
[2023-10-02 03:39:02] iter = 05750, loss = 1.5923
[2023-10-02 03:39:02] iter = 05760, loss = 1.5384
[2023-10-02 03:39:03] iter = 05770, loss = 1.8268
[2023-10-02 03:39:04] iter = 05780, loss = 1.5184
[2023-10-02 03:39:05] iter = 05790, loss = 1.6596
[2023-10-02 03:39:06] iter = 05800, loss = 1.5244
[2023-10-02 03:39:07] iter = 05810, loss = 1.5641
[2023-10-02 03:39:08] iter = 05820, loss = 1.8721
[2023-10-02 03:39:09] iter = 05830, loss = 1.6868
[2023-10-02 03:39:09] iter = 05840, loss = 1.6628
[2023-10-02 03:39:10] iter = 05850, loss = 1.6015
[2023-10-02 03:39:11] iter = 05860, loss = 1.6645
[2023-10-02 03:39:12] iter = 05870, loss = 1.7243
[2023-10-02 03:39:13] iter = 05880, loss = 1.5647
[2023-10-02 03:39:14] iter = 05890, loss = 1.7238
[2023-10-02 03:39:15] iter = 05900, loss = 1.5256
[2023-10-02 03:39:16] iter = 05910, loss = 1.5845
[2023-10-02 03:39:17] iter = 05920, loss = 1.5456
[2023-10-02 03:39:18] iter = 05930, loss = 1.7153
[2023-10-02 03:39:19] iter = 05940, loss = 1.4786
[2023-10-02 03:39:19] iter = 05950, loss = 1.7692
[2023-10-02 03:39:20] iter = 05960, loss = 1.6392
[2023-10-02 03:39:21] iter = 05970, loss = 1.5937
[2023-10-02 03:39:22] iter = 05980, loss = 1.5529
[2023-10-02 03:39:23] iter = 05990, loss = 1.5755
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 64293}
[2023-10-02 03:39:48] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004124 train acc = 1.0000, test acc = 0.6082
[2023-10-02 03:40:12] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.010148 train acc = 1.0000, test acc = 0.6147
[2023-10-02 03:40:36] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.014952 train acc = 1.0000, test acc = 0.6073
[2023-10-02 03:41:00] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.008161 train acc = 1.0000, test acc = 0.6076
[2023-10-02 03:41:25] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011105 train acc = 1.0000, test acc = 0.6109
[2023-10-02 03:41:49] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.012066 train acc = 1.0000, test acc = 0.6083
[2023-10-02 03:42:13] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.001514 train acc = 1.0000, test acc = 0.6117
[2023-10-02 03:42:37] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001508 train acc = 1.0000, test acc = 0.6077
[2023-10-02 03:43:01] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.001568 train acc = 1.0000, test acc = 0.6124
[2023-10-02 03:43:25] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005282 train acc = 1.0000, test acc = 0.6078
[2023-10-02 03:43:49] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.005691 train acc = 1.0000, test acc = 0.6094
[2023-10-02 03:44:13] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.014918 train acc = 1.0000, test acc = 0.6022
[2023-10-02 03:44:37] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.023029 train acc = 1.0000, test acc = 0.6133
[2023-10-02 03:45:01] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.014913 train acc = 0.9980, test acc = 0.6095
[2023-10-02 03:45:25] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013594 train acc = 1.0000, test acc = 0.6124
[2023-10-02 03:45:50] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.001421 train acc = 1.0000, test acc = 0.6181
[2023-10-02 03:46:14] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.012298 train acc = 1.0000, test acc = 0.6087
[2023-10-02 03:46:38] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.022609 train acc = 1.0000, test acc = 0.6026
[2023-10-02 03:47:02] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.009517 train acc = 1.0000, test acc = 0.6184
[2023-10-02 03:47:26] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003816 train acc = 1.0000, test acc = 0.6210
Evaluate 20 random ConvNet, mean = 0.6106 std = 0.0047
-------------------------
[2023-10-02 03:47:27] iter = 06000, loss = 1.5839
[2023-10-02 03:47:28] iter = 06010, loss = 1.6736
[2023-10-02 03:47:28] iter = 06020, loss = 1.4891
[2023-10-02 03:47:29] iter = 06030, loss = 1.8197
[2023-10-02 03:47:30] iter = 06040, loss = 1.5976
[2023-10-02 03:47:31] iter = 06050, loss = 1.8602
[2023-10-02 03:47:32] iter = 06060, loss = 1.6701
[2023-10-02 03:47:33] iter = 06070, loss = 1.5853
[2023-10-02 03:47:34] iter = 06080, loss = 1.9954
[2023-10-02 03:47:35] iter = 06090, loss = 1.5596
[2023-10-02 03:47:36] iter = 06100, loss = 1.6235
[2023-10-02 03:47:37] iter = 06110, loss = 1.7014
[2023-10-02 03:47:37] iter = 06120, loss = 1.6093
[2023-10-02 03:47:38] iter = 06130, loss = 1.6063
[2023-10-02 03:47:39] iter = 06140, loss = 1.6882
[2023-10-02 03:47:40] iter = 06150, loss = 1.5445
[2023-10-02 03:47:41] iter = 06160, loss = 1.5561
[2023-10-02 03:47:42] iter = 06170, loss = 1.5320
[2023-10-02 03:47:43] iter = 06180, loss = 1.6007
[2023-10-02 03:47:44] iter = 06190, loss = 1.5689
[2023-10-02 03:47:44] iter = 06200, loss = 1.6648
[2023-10-02 03:47:45] iter = 06210, loss = 1.7253
[2023-10-02 03:47:46] iter = 06220, loss = 1.5636
[2023-10-02 03:47:47] iter = 06230, loss = 1.5319
[2023-10-02 03:47:48] iter = 06240, loss = 1.6876
[2023-10-02 03:47:49] iter = 06250, loss = 1.8016
[2023-10-02 03:47:50] iter = 06260, loss = 1.6337
[2023-10-02 03:47:51] iter = 06270, loss = 1.7204
[2023-10-02 03:47:52] iter = 06280, loss = 1.6571
[2023-10-02 03:47:53] iter = 06290, loss = 1.7261
[2023-10-02 03:47:54] iter = 06300, loss = 1.6335
[2023-10-02 03:47:54] iter = 06310, loss = 1.6452
[2023-10-02 03:47:55] iter = 06320, loss = 1.6608
[2023-10-02 03:47:56] iter = 06330, loss = 1.6923
[2023-10-02 03:47:57] iter = 06340, loss = 1.5431
[2023-10-02 03:47:58] iter = 06350, loss = 1.6037
[2023-10-02 03:47:59] iter = 06360, loss = 1.4966
[2023-10-02 03:48:00] iter = 06370, loss = 1.6033
[2023-10-02 03:48:01] iter = 06380, loss = 1.7227
[2023-10-02 03:48:02] iter = 06390, loss = 1.5992
[2023-10-02 03:48:03] iter = 06400, loss = 1.5594
[2023-10-02 03:48:04] iter = 06410, loss = 1.6105
[2023-10-02 03:48:05] iter = 06420, loss = 1.5870
[2023-10-02 03:48:06] iter = 06430, loss = 1.5443
[2023-10-02 03:48:07] iter = 06440, loss = 1.7602
[2023-10-02 03:48:07] iter = 06450, loss = 1.5806
[2023-10-02 03:48:08] iter = 06460, loss = 1.7249
[2023-10-02 03:48:09] iter = 06470, loss = 1.5456
[2023-10-02 03:48:10] iter = 06480, loss = 1.6094
[2023-10-02 03:48:11] iter = 06490, loss = 1.6368
[2023-10-02 03:48:12] iter = 06500, loss = 1.6055
[2023-10-02 03:48:13] iter = 06510, loss = 1.6390
[2023-10-02 03:48:14] iter = 06520, loss = 1.4412
[2023-10-02 03:48:15] iter = 06530, loss = 1.6772
[2023-10-02 03:48:15] iter = 06540, loss = 1.7845
[2023-10-02 03:48:16] iter = 06550, loss = 1.6336
[2023-10-02 03:48:17] iter = 06560, loss = 1.5439
[2023-10-02 03:48:18] iter = 06570, loss = 1.6264
[2023-10-02 03:48:19] iter = 06580, loss = 1.6266
[2023-10-02 03:48:20] iter = 06590, loss = 1.4820
[2023-10-02 03:48:21] iter = 06600, loss = 1.5266
[2023-10-02 03:48:22] iter = 06610, loss = 1.4973
[2023-10-02 03:48:22] iter = 06620, loss = 1.5258
[2023-10-02 03:48:23] iter = 06630, loss = 1.5927
[2023-10-02 03:48:24] iter = 06640, loss = 1.5427
[2023-10-02 03:48:25] iter = 06650, loss = 1.6657
[2023-10-02 03:48:26] iter = 06660, loss = 1.7487
[2023-10-02 03:48:27] iter = 06670, loss = 1.5223
[2023-10-02 03:48:28] iter = 06680, loss = 1.6917
[2023-10-02 03:48:29] iter = 06690, loss = 1.5262
[2023-10-02 03:48:30] iter = 06700, loss = 1.5794
[2023-10-02 03:48:31] iter = 06710, loss = 1.5728
[2023-10-02 03:48:32] iter = 06720, loss = 1.6750
[2023-10-02 03:48:33] iter = 06730, loss = 1.5935
[2023-10-02 03:48:34] iter = 06740, loss = 1.5747
[2023-10-02 03:48:35] iter = 06750, loss = 1.5495
[2023-10-02 03:48:35] iter = 06760, loss = 1.5318
[2023-10-02 03:48:36] iter = 06770, loss = 1.5821
[2023-10-02 03:48:37] iter = 06780, loss = 1.5632
[2023-10-02 03:48:38] iter = 06790, loss = 1.5788
[2023-10-02 03:48:39] iter = 06800, loss = 1.7189
[2023-10-02 03:48:40] iter = 06810, loss = 1.6583
[2023-10-02 03:48:41] iter = 06820, loss = 1.5090
[2023-10-02 03:48:42] iter = 06830, loss = 1.6720
[2023-10-02 03:48:43] iter = 06840, loss = 1.7955
[2023-10-02 03:48:44] iter = 06850, loss = 1.6635
[2023-10-02 03:48:45] iter = 06860, loss = 1.6112
[2023-10-02 03:48:45] iter = 06870, loss = 1.6252
[2023-10-02 03:48:46] iter = 06880, loss = 1.5932
[2023-10-02 03:48:47] iter = 06890, loss = 1.6125
[2023-10-02 03:48:48] iter = 06900, loss = 1.5225
[2023-10-02 03:48:49] iter = 06910, loss = 1.5739
[2023-10-02 03:48:50] iter = 06920, loss = 1.5695
[2023-10-02 03:48:51] iter = 06930, loss = 1.6274
[2023-10-02 03:48:52] iter = 06940, loss = 1.5300
[2023-10-02 03:48:53] iter = 06950, loss = 1.5936
[2023-10-02 03:48:54] iter = 06960, loss = 1.6029
[2023-10-02 03:48:55] iter = 06970, loss = 1.4729
[2023-10-02 03:48:55] iter = 06980, loss = 1.6380
[2023-10-02 03:48:56] iter = 06990, loss = 1.7033
[2023-10-02 03:48:57] iter = 07000, loss = 1.8377
[2023-10-02 03:48:58] iter = 07010, loss = 1.6376
[2023-10-02 03:48:59] iter = 07020, loss = 1.5325
[2023-10-02 03:49:00] iter = 07030, loss = 1.5438
[2023-10-02 03:49:01] iter = 07040, loss = 1.7037
[2023-10-02 03:49:02] iter = 07050, loss = 1.7172
[2023-10-02 03:49:03] iter = 07060, loss = 1.6098
[2023-10-02 03:49:04] iter = 07070, loss = 1.6708
[2023-10-02 03:49:04] iter = 07080, loss = 1.5428
[2023-10-02 03:49:05] iter = 07090, loss = 1.6300
[2023-10-02 03:49:06] iter = 07100, loss = 1.8452
[2023-10-02 03:49:07] iter = 07110, loss = 1.6023
[2023-10-02 03:49:08] iter = 07120, loss = 1.6003
[2023-10-02 03:49:09] iter = 07130, loss = 1.5886
[2023-10-02 03:49:10] iter = 07140, loss = 1.6432
[2023-10-02 03:49:11] iter = 07150, loss = 1.5849
[2023-10-02 03:49:12] iter = 07160, loss = 1.6119
[2023-10-02 03:49:13] iter = 07170, loss = 1.6336
[2023-10-02 03:49:14] iter = 07180, loss = 1.5603
[2023-10-02 03:49:15] iter = 07190, loss = 1.6854
[2023-10-02 03:49:15] iter = 07200, loss = 1.6292
[2023-10-02 03:49:16] iter = 07210, loss = 1.6682
[2023-10-02 03:49:17] iter = 07220, loss = 1.5954
[2023-10-02 03:49:18] iter = 07230, loss = 1.5867
[2023-10-02 03:49:19] iter = 07240, loss = 1.5749
[2023-10-02 03:49:20] iter = 07250, loss = 1.5578
[2023-10-02 03:49:21] iter = 07260, loss = 1.5377
[2023-10-02 03:49:22] iter = 07270, loss = 1.6522
[2023-10-02 03:49:23] iter = 07280, loss = 1.6567
[2023-10-02 03:49:23] iter = 07290, loss = 1.7006
[2023-10-02 03:49:24] iter = 07300, loss = 1.6041
[2023-10-02 03:49:25] iter = 07310, loss = 1.6487
[2023-10-02 03:49:26] iter = 07320, loss = 1.6560
[2023-10-02 03:49:27] iter = 07330, loss = 1.7887
[2023-10-02 03:49:28] iter = 07340, loss = 1.6841
[2023-10-02 03:49:29] iter = 07350, loss = 1.5777
[2023-10-02 03:49:30] iter = 07360, loss = 1.5936
[2023-10-02 03:49:30] iter = 07370, loss = 1.6637
[2023-10-02 03:49:31] iter = 07380, loss = 1.5826
[2023-10-02 03:49:32] iter = 07390, loss = 1.6266
[2023-10-02 03:49:33] iter = 07400, loss = 1.5538
[2023-10-02 03:49:34] iter = 07410, loss = 1.4635
[2023-10-02 03:49:35] iter = 07420, loss = 1.5704
[2023-10-02 03:49:36] iter = 07430, loss = 1.7078
[2023-10-02 03:49:37] iter = 07440, loss = 1.6765
[2023-10-02 03:49:38] iter = 07450, loss = 1.5756
[2023-10-02 03:49:39] iter = 07460, loss = 1.4303
[2023-10-02 03:49:39] iter = 07470, loss = 1.4839
[2023-10-02 03:49:40] iter = 07480, loss = 1.5601
[2023-10-02 03:49:41] iter = 07490, loss = 1.5980
[2023-10-02 03:49:42] iter = 07500, loss = 1.7283
[2023-10-02 03:49:43] iter = 07510, loss = 1.6690
[2023-10-02 03:49:44] iter = 07520, loss = 1.7106
[2023-10-02 03:49:45] iter = 07530, loss = 1.4860
[2023-10-02 03:49:46] iter = 07540, loss = 1.6759
[2023-10-02 03:49:47] iter = 07550, loss = 1.5188
[2023-10-02 03:49:48] iter = 07560, loss = 1.5221
[2023-10-02 03:49:49] iter = 07570, loss = 1.5282
[2023-10-02 03:49:50] iter = 07580, loss = 1.7803
[2023-10-02 03:49:51] iter = 07590, loss = 1.5340
[2023-10-02 03:49:51] iter = 07600, loss = 1.7092
[2023-10-02 03:49:52] iter = 07610, loss = 1.5709
[2023-10-02 03:49:53] iter = 07620, loss = 1.6223
[2023-10-02 03:49:54] iter = 07630, loss = 1.6313
[2023-10-02 03:49:55] iter = 07640, loss = 1.6632
[2023-10-02 03:49:56] iter = 07650, loss = 1.6502
[2023-10-02 03:49:57] iter = 07660, loss = 1.5474
[2023-10-02 03:49:58] iter = 07670, loss = 1.6232
[2023-10-02 03:49:59] iter = 07680, loss = 1.6118
[2023-10-02 03:50:00] iter = 07690, loss = 1.5567
[2023-10-02 03:50:00] iter = 07700, loss = 1.5730
[2023-10-02 03:50:01] iter = 07710, loss = 1.6061
[2023-10-02 03:50:02] iter = 07720, loss = 1.5511
[2023-10-02 03:50:03] iter = 07730, loss = 1.6193
[2023-10-02 03:50:04] iter = 07740, loss = 1.6503
[2023-10-02 03:50:05] iter = 07750, loss = 1.6082
[2023-10-02 03:50:06] iter = 07760, loss = 1.5583
[2023-10-02 03:50:07] iter = 07770, loss = 1.5669
[2023-10-02 03:50:08] iter = 07780, loss = 1.6177
[2023-10-02 03:50:09] iter = 07790, loss = 1.6550
[2023-10-02 03:50:10] iter = 07800, loss = 1.5348
[2023-10-02 03:50:10] iter = 07810, loss = 1.7083
[2023-10-02 03:50:11] iter = 07820, loss = 1.5749
[2023-10-02 03:50:12] iter = 07830, loss = 1.4935
[2023-10-02 03:50:13] iter = 07840, loss = 1.5751
[2023-10-02 03:50:14] iter = 07850, loss = 1.6177
[2023-10-02 03:50:15] iter = 07860, loss = 1.6353
[2023-10-02 03:50:16] iter = 07870, loss = 1.5481
[2023-10-02 03:50:17] iter = 07880, loss = 1.6714
[2023-10-02 03:50:18] iter = 07890, loss = 1.5607
[2023-10-02 03:50:18] iter = 07900, loss = 1.7180
[2023-10-02 03:50:19] iter = 07910, loss = 1.5803
[2023-10-02 03:50:20] iter = 07920, loss = 1.5852
[2023-10-02 03:50:21] iter = 07930, loss = 1.5968
[2023-10-02 03:50:22] iter = 07940, loss = 1.7265
[2023-10-02 03:50:23] iter = 07950, loss = 1.5387
[2023-10-02 03:50:24] iter = 07960, loss = 1.6465
[2023-10-02 03:50:25] iter = 07970, loss = 1.6088
[2023-10-02 03:50:26] iter = 07980, loss = 1.6611
[2023-10-02 03:50:26] iter = 07990, loss = 1.5658
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 27708}
[2023-10-02 03:50:51] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.020726 train acc = 0.9980, test acc = 0.6222
[2023-10-02 03:51:15] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.013449 train acc = 1.0000, test acc = 0.6150
[2023-10-02 03:51:40] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002398 train acc = 1.0000, test acc = 0.6208
[2023-10-02 03:52:04] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001657 train acc = 1.0000, test acc = 0.6232
[2023-10-02 03:52:28] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.009374 train acc = 1.0000, test acc = 0.6229
[2023-10-02 03:52:52] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001909 train acc = 1.0000, test acc = 0.6159
[2023-10-02 03:53:17] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.011555 train acc = 1.0000, test acc = 0.6217
[2023-10-02 03:53:41] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.024916 train acc = 1.0000, test acc = 0.6202
[2023-10-02 03:54:05] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.011876 train acc = 1.0000, test acc = 0.6155
[2023-10-02 03:54:29] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.023286 train acc = 0.9980, test acc = 0.6146
[2023-10-02 03:54:53] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.029066 train acc = 0.9960, test acc = 0.6184
[2023-10-02 03:55:17] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.018900 train acc = 1.0000, test acc = 0.6204
[2023-10-02 03:55:42] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.003890 train acc = 1.0000, test acc = 0.6126
[2023-10-02 03:56:06] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.018884 train acc = 0.9980, test acc = 0.6141
[2023-10-02 03:56:30] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004272 train acc = 1.0000, test acc = 0.6198
[2023-10-02 03:56:54] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015867 train acc = 0.9980, test acc = 0.6207
[2023-10-02 03:57:18] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.001638 train acc = 1.0000, test acc = 0.6229
[2023-10-02 03:57:42] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003840 train acc = 1.0000, test acc = 0.6174
[2023-10-02 03:58:06] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003642 train acc = 1.0000, test acc = 0.6191
[2023-10-02 03:58:30] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004805 train acc = 1.0000, test acc = 0.6198
Evaluate 20 random ConvNet, mean = 0.6189 std = 0.0032
-------------------------
[2023-10-02 03:58:31] iter = 08000, loss = 1.5408
[2023-10-02 03:58:31] iter = 08010, loss = 1.4654
[2023-10-02 03:58:32] iter = 08020, loss = 1.5339
[2023-10-02 03:58:33] iter = 08030, loss = 1.8619
[2023-10-02 03:58:34] iter = 08040, loss = 1.6351
[2023-10-02 03:58:35] iter = 08050, loss = 1.5022
[2023-10-02 03:58:36] iter = 08060, loss = 1.5460
[2023-10-02 03:58:37] iter = 08070, loss = 1.5667
[2023-10-02 03:58:38] iter = 08080, loss = 1.5858
[2023-10-02 03:58:39] iter = 08090, loss = 1.4642
[2023-10-02 03:58:39] iter = 08100, loss = 1.5896
[2023-10-02 03:58:40] iter = 08110, loss = 1.5330
[2023-10-02 03:58:41] iter = 08120, loss = 1.5316
[2023-10-02 03:58:42] iter = 08130, loss = 1.5418
[2023-10-02 03:58:43] iter = 08140, loss = 1.5773
[2023-10-02 03:58:44] iter = 08150, loss = 1.4533
[2023-10-02 03:58:45] iter = 08160, loss = 1.5151
[2023-10-02 03:58:46] iter = 08170, loss = 1.5304
[2023-10-02 03:58:47] iter = 08180, loss = 1.6015
[2023-10-02 03:58:48] iter = 08190, loss = 1.5350
[2023-10-02 03:58:49] iter = 08200, loss = 1.5735
[2023-10-02 03:58:49] iter = 08210, loss = 1.6421
[2023-10-02 03:58:50] iter = 08220, loss = 1.4909
[2023-10-02 03:58:51] iter = 08230, loss = 1.6224
[2023-10-02 03:58:52] iter = 08240, loss = 1.6201
[2023-10-02 03:58:53] iter = 08250, loss = 1.4496
[2023-10-02 03:58:54] iter = 08260, loss = 1.7598
[2023-10-02 03:58:55] iter = 08270, loss = 1.6392
[2023-10-02 03:58:56] iter = 08280, loss = 1.5448
[2023-10-02 03:58:57] iter = 08290, loss = 1.6288
[2023-10-02 03:58:58] iter = 08300, loss = 1.7054
[2023-10-02 03:58:58] iter = 08310, loss = 1.6773
[2023-10-02 03:58:59] iter = 08320, loss = 1.5882
[2023-10-02 03:59:00] iter = 08330, loss = 1.6326
[2023-10-02 03:59:01] iter = 08340, loss = 1.5495
[2023-10-02 03:59:02] iter = 08350, loss = 1.5547
[2023-10-02 03:59:03] iter = 08360, loss = 1.5661
[2023-10-02 03:59:04] iter = 08370, loss = 1.5896
[2023-10-02 03:59:05] iter = 08380, loss = 1.7957
[2023-10-02 03:59:06] iter = 08390, loss = 1.5179
[2023-10-02 03:59:07] iter = 08400, loss = 1.5644
[2023-10-02 03:59:07] iter = 08410, loss = 1.4434
[2023-10-02 03:59:08] iter = 08420, loss = 1.5948
[2023-10-02 03:59:09] iter = 08430, loss = 1.6449
[2023-10-02 03:59:10] iter = 08440, loss = 1.4840
[2023-10-02 03:59:11] iter = 08450, loss = 1.5551
[2023-10-02 03:59:12] iter = 08460, loss = 1.5442
[2023-10-02 03:59:13] iter = 08470, loss = 1.5814
[2023-10-02 03:59:14] iter = 08480, loss = 1.6677
[2023-10-02 03:59:15] iter = 08490, loss = 1.5217
[2023-10-02 03:59:16] iter = 08500, loss = 1.5390
[2023-10-02 03:59:16] iter = 08510, loss = 1.5635
[2023-10-02 03:59:17] iter = 08520, loss = 1.6735
[2023-10-02 03:59:18] iter = 08530, loss = 1.7720
[2023-10-02 03:59:19] iter = 08540, loss = 1.5973
[2023-10-02 03:59:20] iter = 08550, loss = 1.7081
[2023-10-02 03:59:21] iter = 08560, loss = 1.5261
[2023-10-02 03:59:22] iter = 08570, loss = 1.4943
[2023-10-02 03:59:23] iter = 08580, loss = 1.6623
[2023-10-02 03:59:24] iter = 08590, loss = 1.5624
[2023-10-02 03:59:25] iter = 08600, loss = 1.5950
[2023-10-02 03:59:26] iter = 08610, loss = 1.5193
[2023-10-02 03:59:27] iter = 08620, loss = 1.5419
[2023-10-02 03:59:27] iter = 08630, loss = 1.6193
[2023-10-02 03:59:28] iter = 08640, loss = 1.5826
[2023-10-02 03:59:29] iter = 08650, loss = 1.3866
[2023-10-02 03:59:30] iter = 08660, loss = 1.5052
[2023-10-02 03:59:31] iter = 08670, loss = 1.5648
[2023-10-02 03:59:32] iter = 08680, loss = 1.7023
[2023-10-02 03:59:33] iter = 08690, loss = 1.6171
[2023-10-02 03:59:34] iter = 08700, loss = 1.6029
[2023-10-02 03:59:35] iter = 08710, loss = 1.4443
[2023-10-02 03:59:36] iter = 08720, loss = 1.3887
[2023-10-02 03:59:36] iter = 08730, loss = 1.6269
[2023-10-02 03:59:37] iter = 08740, loss = 1.5638
[2023-10-02 03:59:38] iter = 08750, loss = 1.5868
[2023-10-02 03:59:39] iter = 08760, loss = 1.6097
[2023-10-02 03:59:40] iter = 08770, loss = 1.6762
[2023-10-02 03:59:41] iter = 08780, loss = 1.6796
[2023-10-02 03:59:42] iter = 08790, loss = 1.5737
[2023-10-02 03:59:43] iter = 08800, loss = 1.4806
[2023-10-02 03:59:44] iter = 08810, loss = 1.5663
[2023-10-02 03:59:45] iter = 08820, loss = 1.5959
[2023-10-02 03:59:46] iter = 08830, loss = 1.6182
[2023-10-02 03:59:46] iter = 08840, loss = 1.4392
[2023-10-02 03:59:47] iter = 08850, loss = 1.6317
[2023-10-02 03:59:48] iter = 08860, loss = 1.5238
[2023-10-02 03:59:49] iter = 08870, loss = 1.5747
[2023-10-02 03:59:50] iter = 08880, loss = 1.7131
[2023-10-02 03:59:51] iter = 08890, loss = 1.4430
[2023-10-02 03:59:52] iter = 08900, loss = 1.5755
[2023-10-02 03:59:53] iter = 08910, loss = 1.5445
[2023-10-02 03:59:54] iter = 08920, loss = 1.4583
[2023-10-02 03:59:55] iter = 08930, loss = 1.5831
[2023-10-02 03:59:56] iter = 08940, loss = 1.5914
[2023-10-02 03:59:56] iter = 08950, loss = 1.4996
[2023-10-02 03:59:57] iter = 08960, loss = 1.4577
[2023-10-02 03:59:58] iter = 08970, loss = 1.5642
[2023-10-02 03:59:59] iter = 08980, loss = 1.5473
[2023-10-02 04:00:00] iter = 08990, loss = 1.5144
[2023-10-02 04:00:01] iter = 09000, loss = 1.5105
[2023-10-02 04:00:02] iter = 09010, loss = 1.4976
[2023-10-02 04:00:03] iter = 09020, loss = 1.6371
[2023-10-02 04:00:04] iter = 09030, loss = 1.5342
[2023-10-02 04:00:05] iter = 09040, loss = 1.4945
[2023-10-02 04:00:05] iter = 09050, loss = 1.5299
[2023-10-02 04:00:06] iter = 09060, loss = 1.5114
[2023-10-02 04:00:07] iter = 09070, loss = 1.5844
[2023-10-02 04:00:08] iter = 09080, loss = 1.5706
[2023-10-02 04:00:09] iter = 09090, loss = 1.5419
[2023-10-02 04:00:10] iter = 09100, loss = 1.5739
[2023-10-02 04:00:11] iter = 09110, loss = 1.6285
[2023-10-02 04:00:12] iter = 09120, loss = 1.6042
[2023-10-02 04:00:13] iter = 09130, loss = 1.4760
[2023-10-02 04:00:14] iter = 09140, loss = 1.4544
[2023-10-02 04:00:14] iter = 09150, loss = 1.4217
[2023-10-02 04:00:15] iter = 09160, loss = 1.5506
[2023-10-02 04:00:16] iter = 09170, loss = 1.4994
[2023-10-02 04:00:17] iter = 09180, loss = 1.6819
[2023-10-02 04:00:18] iter = 09190, loss = 1.5688
[2023-10-02 04:00:19] iter = 09200, loss = 1.5310
[2023-10-02 04:00:20] iter = 09210, loss = 1.6159
[2023-10-02 04:00:21] iter = 09220, loss = 1.4248
[2023-10-02 04:00:21] iter = 09230, loss = 1.5799
[2023-10-02 04:00:22] iter = 09240, loss = 1.5337
[2023-10-02 04:00:23] iter = 09250, loss = 1.5408
[2023-10-02 04:00:24] iter = 09260, loss = 1.5541
[2023-10-02 04:00:25] iter = 09270, loss = 1.7062
[2023-10-02 04:00:26] iter = 09280, loss = 1.6278
[2023-10-02 04:00:27] iter = 09290, loss = 1.4997
[2023-10-02 04:00:28] iter = 09300, loss = 1.6093
[2023-10-02 04:00:29] iter = 09310, loss = 1.4496
[2023-10-02 04:00:30] iter = 09320, loss = 1.5668
[2023-10-02 04:00:30] iter = 09330, loss = 1.5273
[2023-10-02 04:00:31] iter = 09340, loss = 1.5143
[2023-10-02 04:00:32] iter = 09350, loss = 1.5084
[2023-10-02 04:00:33] iter = 09360, loss = 1.5370
[2023-10-02 04:00:34] iter = 09370, loss = 1.6660
[2023-10-02 04:00:35] iter = 09380, loss = 1.5115
[2023-10-02 04:00:36] iter = 09390, loss = 1.5278
[2023-10-02 04:00:37] iter = 09400, loss = 1.6936
[2023-10-02 04:00:38] iter = 09410, loss = 1.5574
[2023-10-02 04:00:39] iter = 09420, loss = 1.4577
[2023-10-02 04:00:40] iter = 09430, loss = 1.5754
[2023-10-02 04:00:41] iter = 09440, loss = 1.4770
[2023-10-02 04:00:42] iter = 09450, loss = 1.5188
[2023-10-02 04:00:42] iter = 09460, loss = 1.6677
[2023-10-02 04:00:43] iter = 09470, loss = 1.6222
[2023-10-02 04:00:44] iter = 09480, loss = 1.5169
[2023-10-02 04:00:45] iter = 09490, loss = 1.7047
[2023-10-02 04:00:46] iter = 09500, loss = 1.5835
[2023-10-02 04:00:47] iter = 09510, loss = 1.5472
[2023-10-02 04:00:48] iter = 09520, loss = 1.8424
[2023-10-02 04:00:49] iter = 09530, loss = 1.7155
[2023-10-02 04:00:50] iter = 09540, loss = 1.3940
[2023-10-02 04:00:50] iter = 09550, loss = 1.5201
[2023-10-02 04:00:51] iter = 09560, loss = 1.5881
[2023-10-02 04:00:52] iter = 09570, loss = 1.4529
[2023-10-02 04:00:53] iter = 09580, loss = 1.5676
[2023-10-02 04:00:54] iter = 09590, loss = 1.5355
[2023-10-02 04:00:55] iter = 09600, loss = 1.4353
[2023-10-02 04:00:56] iter = 09610, loss = 1.5299
[2023-10-02 04:00:57] iter = 09620, loss = 1.5644
[2023-10-02 04:00:58] iter = 09630, loss = 1.5551
[2023-10-02 04:00:59] iter = 09640, loss = 1.5344
[2023-10-02 04:00:59] iter = 09650, loss = 1.5255
[2023-10-02 04:01:00] iter = 09660, loss = 1.5939
[2023-10-02 04:01:01] iter = 09670, loss = 1.6874
[2023-10-02 04:01:02] iter = 09680, loss = 1.6041
[2023-10-02 04:01:03] iter = 09690, loss = 1.4901
[2023-10-02 04:01:04] iter = 09700, loss = 1.4613
[2023-10-02 04:01:05] iter = 09710, loss = 1.5713
[2023-10-02 04:01:06] iter = 09720, loss = 1.7759
[2023-10-02 04:01:06] iter = 09730, loss = 1.5207
[2023-10-02 04:01:07] iter = 09740, loss = 1.6025
[2023-10-02 04:01:08] iter = 09750, loss = 1.5758
[2023-10-02 04:01:09] iter = 09760, loss = 1.5638
[2023-10-02 04:01:10] iter = 09770, loss = 1.5239
[2023-10-02 04:01:11] iter = 09780, loss = 1.7042
[2023-10-02 04:01:12] iter = 09790, loss = 1.6796
[2023-10-02 04:01:13] iter = 09800, loss = 1.6420
[2023-10-02 04:01:14] iter = 09810, loss = 1.5608
[2023-10-02 04:01:15] iter = 09820, loss = 1.4444
[2023-10-02 04:01:16] iter = 09830, loss = 1.6266
[2023-10-02 04:01:16] iter = 09840, loss = 1.6919
[2023-10-02 04:01:17] iter = 09850, loss = 1.4013
[2023-10-02 04:01:18] iter = 09860, loss = 1.5922
[2023-10-02 04:01:19] iter = 09870, loss = 1.5996
[2023-10-02 04:01:20] iter = 09880, loss = 1.6456
[2023-10-02 04:01:21] iter = 09890, loss = 1.6375
[2023-10-02 04:01:22] iter = 09900, loss = 1.6395
[2023-10-02 04:01:23] iter = 09910, loss = 1.3793
[2023-10-02 04:01:24] iter = 09920, loss = 1.6930
[2023-10-02 04:01:25] iter = 09930, loss = 1.6641
[2023-10-02 04:01:26] iter = 09940, loss = 1.4745
[2023-10-02 04:01:27] iter = 09950, loss = 1.6100
[2023-10-02 04:01:27] iter = 09960, loss = 1.5758
[2023-10-02 04:01:28] iter = 09970, loss = 1.5456
[2023-10-02 04:01:29] iter = 09980, loss = 1.5904
[2023-10-02 04:01:30] iter = 09990, loss = 1.6041
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 91401}
[2023-10-02 04:01:55] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.006318 train acc = 0.9980, test acc = 0.6239
[2023-10-02 04:02:19] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.005725 train acc = 1.0000, test acc = 0.6182
[2023-10-02 04:02:44] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.025185 train acc = 1.0000, test acc = 0.6234
[2023-10-02 04:03:08] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.026788 train acc = 0.9980, test acc = 0.6222
[2023-10-02 04:03:32] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.007257 train acc = 1.0000, test acc = 0.6148
[2023-10-02 04:03:56] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002140 train acc = 1.0000, test acc = 0.6235
[2023-10-02 04:04:20] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.022932 train acc = 1.0000, test acc = 0.6281
[2023-10-02 04:04:44] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.011589 train acc = 1.0000, test acc = 0.6144
[2023-10-02 04:05:09] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014462 train acc = 1.0000, test acc = 0.6222
[2023-10-02 04:05:32] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.011865 train acc = 1.0000, test acc = 0.6207
[2023-10-02 04:05:57] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.011032 train acc = 1.0000, test acc = 0.6226
[2023-10-02 04:06:21] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.009808 train acc = 1.0000, test acc = 0.6241
[2023-10-02 04:06:45] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.015991 train acc = 1.0000, test acc = 0.6189
[2023-10-02 04:07:09] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.015246 train acc = 0.9980, test acc = 0.6190
[2023-10-02 04:07:34] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.005699 train acc = 1.0000, test acc = 0.6217
[2023-10-02 04:07:58] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.012444 train acc = 1.0000, test acc = 0.6211
[2023-10-02 04:08:22] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.003945 train acc = 1.0000, test acc = 0.6199
[2023-10-02 04:08:46] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.017886 train acc = 1.0000, test acc = 0.6246
[2023-10-02 04:09:10] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.009385 train acc = 1.0000, test acc = 0.6179
[2023-10-02 04:09:34] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003350 train acc = 1.0000, test acc = 0.6204
Evaluate 20 random ConvNet, mean = 0.6211 std = 0.0032
-------------------------
[2023-10-02 04:09:35] iter = 10000, loss = 1.5863
[2023-10-02 04:09:35] iter = 10010, loss = 1.5356
[2023-10-02 04:09:36] iter = 10020, loss = 1.6410
[2023-10-02 04:09:37] iter = 10030, loss = 1.3647
[2023-10-02 04:09:38] iter = 10040, loss = 1.5434
[2023-10-02 04:09:39] iter = 10050, loss = 1.5716
[2023-10-02 04:09:40] iter = 10060, loss = 1.6031
[2023-10-02 04:09:41] iter = 10070, loss = 1.5501
[2023-10-02 04:09:42] iter = 10080, loss = 1.4994
[2023-10-02 04:09:43] iter = 10090, loss = 1.7023
[2023-10-02 04:09:43] iter = 10100, loss = 1.6248
[2023-10-02 04:09:44] iter = 10110, loss = 1.6764
[2023-10-02 04:09:45] iter = 10120, loss = 1.6396
[2023-10-02 04:09:46] iter = 10130, loss = 1.7753
[2023-10-02 04:09:47] iter = 10140, loss = 1.5775
[2023-10-02 04:09:48] iter = 10150, loss = 1.6681
[2023-10-02 04:09:49] iter = 10160, loss = 1.6174
[2023-10-02 04:09:50] iter = 10170, loss = 1.6610
[2023-10-02 04:09:51] iter = 10180, loss = 1.5355
[2023-10-02 04:09:52] iter = 10190, loss = 1.5057
[2023-10-02 04:09:53] iter = 10200, loss = 1.4383
[2023-10-02 04:09:53] iter = 10210, loss = 1.4501
[2023-10-02 04:09:54] iter = 10220, loss = 1.4307
[2023-10-02 04:09:55] iter = 10230, loss = 1.5478
[2023-10-02 04:09:56] iter = 10240, loss = 1.5994
[2023-10-02 04:09:57] iter = 10250, loss = 1.4806
[2023-10-02 04:09:58] iter = 10260, loss = 1.6353
[2023-10-02 04:09:59] iter = 10270, loss = 1.6068
[2023-10-02 04:10:00] iter = 10280, loss = 1.6561
[2023-10-02 04:10:01] iter = 10290, loss = 1.3662
[2023-10-02 04:10:02] iter = 10300, loss = 1.4372
[2023-10-02 04:10:03] iter = 10310, loss = 1.6125
[2023-10-02 04:10:03] iter = 10320, loss = 1.6768
[2023-10-02 04:10:04] iter = 10330, loss = 1.5855
[2023-10-02 04:10:05] iter = 10340, loss = 1.5755
[2023-10-02 04:10:06] iter = 10350, loss = 1.6442
[2023-10-02 04:10:07] iter = 10360, loss = 1.3995
[2023-10-02 04:10:08] iter = 10370, loss = 1.4758
[2023-10-02 04:10:09] iter = 10380, loss = 1.6354
[2023-10-02 04:10:10] iter = 10390, loss = 1.5607
[2023-10-02 04:10:11] iter = 10400, loss = 1.6465
[2023-10-02 04:10:12] iter = 10410, loss = 1.3663
[2023-10-02 04:10:13] iter = 10420, loss = 1.6411
[2023-10-02 04:10:13] iter = 10430, loss = 1.5920
[2023-10-02 04:10:14] iter = 10440, loss = 1.5948
[2023-10-02 04:10:15] iter = 10450, loss = 1.4949
[2023-10-02 04:10:16] iter = 10460, loss = 1.5858
[2023-10-02 04:10:17] iter = 10470, loss = 1.4548
[2023-10-02 04:10:18] iter = 10480, loss = 1.6740
[2023-10-02 04:10:19] iter = 10490, loss = 1.7194
[2023-10-02 04:10:20] iter = 10500, loss = 1.4581
[2023-10-02 04:10:21] iter = 10510, loss = 1.4903
[2023-10-02 04:10:22] iter = 10520, loss = 1.4826
[2023-10-02 04:10:22] iter = 10530, loss = 1.6213
[2023-10-02 04:10:23] iter = 10540, loss = 1.3628
[2023-10-02 04:10:24] iter = 10550, loss = 1.6227
[2023-10-02 04:10:25] iter = 10560, loss = 1.5532
[2023-10-02 04:10:26] iter = 10570, loss = 1.5941
[2023-10-02 04:10:27] iter = 10580, loss = 1.5917
[2023-10-02 04:10:28] iter = 10590, loss = 1.3738
[2023-10-02 04:10:29] iter = 10600, loss = 1.6788
[2023-10-02 04:10:30] iter = 10610, loss = 1.6974
[2023-10-02 04:10:31] iter = 10620, loss = 1.6963
[2023-10-02 04:10:32] iter = 10630, loss = 1.5190
[2023-10-02 04:10:32] iter = 10640, loss = 1.5741
[2023-10-02 04:10:33] iter = 10650, loss = 1.3738
[2023-10-02 04:10:34] iter = 10660, loss = 1.5322
[2023-10-02 04:10:35] iter = 10670, loss = 1.7018
[2023-10-02 04:10:36] iter = 10680, loss = 1.6803
[2023-10-02 04:10:37] iter = 10690, loss = 1.7446
[2023-10-02 04:10:38] iter = 10700, loss = 1.6559
[2023-10-02 04:10:39] iter = 10710, loss = 1.4120
[2023-10-02 04:10:40] iter = 10720, loss = 1.5131
[2023-10-02 04:10:41] iter = 10730, loss = 1.4861
[2023-10-02 04:10:42] iter = 10740, loss = 1.5329
[2023-10-02 04:10:42] iter = 10750, loss = 1.5823
[2023-10-02 04:10:43] iter = 10760, loss = 1.5248
[2023-10-02 04:10:44] iter = 10770, loss = 1.4524
[2023-10-02 04:10:45] iter = 10780, loss = 1.6609
[2023-10-02 04:10:46] iter = 10790, loss = 1.4947
[2023-10-02 04:10:47] iter = 10800, loss = 1.5186
[2023-10-02 04:10:48] iter = 10810, loss = 1.6660
[2023-10-02 04:10:49] iter = 10820, loss = 1.6119
[2023-10-02 04:10:50] iter = 10830, loss = 1.5394
[2023-10-02 04:10:51] iter = 10840, loss = 1.5630
[2023-10-02 04:10:51] iter = 10850, loss = 1.5463
[2023-10-02 04:10:52] iter = 10860, loss = 1.4675
[2023-10-02 04:10:53] iter = 10870, loss = 1.5445
[2023-10-02 04:10:54] iter = 10880, loss = 1.5772
[2023-10-02 04:10:55] iter = 10890, loss = 1.5195
[2023-10-02 04:10:56] iter = 10900, loss = 1.6381
[2023-10-02 04:10:57] iter = 10910, loss = 1.4631
[2023-10-02 04:10:58] iter = 10920, loss = 1.5584
[2023-10-02 04:10:59] iter = 10930, loss = 1.4423
[2023-10-02 04:11:00] iter = 10940, loss = 1.5400
[2023-10-02 04:11:00] iter = 10950, loss = 1.6100
[2023-10-02 04:11:01] iter = 10960, loss = 1.4449
[2023-10-02 04:11:02] iter = 10970, loss = 1.5331
[2023-10-02 04:11:03] iter = 10980, loss = 1.5274
[2023-10-02 04:11:04] iter = 10990, loss = 1.6550
[2023-10-02 04:11:05] iter = 11000, loss = 1.4979
[2023-10-02 04:11:06] iter = 11010, loss = 1.5260
[2023-10-02 04:11:07] iter = 11020, loss = 1.4141
[2023-10-02 04:11:08] iter = 11030, loss = 1.3520
[2023-10-02 04:11:09] iter = 11040, loss = 1.3819
[2023-10-02 04:11:10] iter = 11050, loss = 1.5626
[2023-10-02 04:11:10] iter = 11060, loss = 1.5695
[2023-10-02 04:11:11] iter = 11070, loss = 1.4880
[2023-10-02 04:11:12] iter = 11080, loss = 1.4474
[2023-10-02 04:11:13] iter = 11090, loss = 1.5227
[2023-10-02 04:11:14] iter = 11100, loss = 1.6092
[2023-10-02 04:11:15] iter = 11110, loss = 1.6287
[2023-10-02 04:11:16] iter = 11120, loss = 1.5655
[2023-10-02 04:11:17] iter = 11130, loss = 1.4778
[2023-10-02 04:11:18] iter = 11140, loss = 1.5979
[2023-10-02 04:11:19] iter = 11150, loss = 1.5269
[2023-10-02 04:11:20] iter = 11160, loss = 1.6334
[2023-10-02 04:11:20] iter = 11170, loss = 1.4943
[2023-10-02 04:11:21] iter = 11180, loss = 1.4408
[2023-10-02 04:11:22] iter = 11190, loss = 1.5896
[2023-10-02 04:11:23] iter = 11200, loss = 1.6898
[2023-10-02 04:11:24] iter = 11210, loss = 1.7111
[2023-10-02 04:11:25] iter = 11220, loss = 1.6674
[2023-10-02 04:11:26] iter = 11230, loss = 1.6067
[2023-10-02 04:11:27] iter = 11240, loss = 1.4757
[2023-10-02 04:11:28] iter = 11250, loss = 1.4852
[2023-10-02 04:11:29] iter = 11260, loss = 1.5826
[2023-10-02 04:11:30] iter = 11270, loss = 1.5689
[2023-10-02 04:11:31] iter = 11280, loss = 1.5364
[2023-10-02 04:11:31] iter = 11290, loss = 1.4846
[2023-10-02 04:11:32] iter = 11300, loss = 1.5105
[2023-10-02 04:11:33] iter = 11310, loss = 1.4872
[2023-10-02 04:11:34] iter = 11320, loss = 1.5736
[2023-10-02 04:11:35] iter = 11330, loss = 1.4909
[2023-10-02 04:11:36] iter = 11340, loss = 1.4458
[2023-10-02 04:11:37] iter = 11350, loss = 1.5578
[2023-10-02 04:11:38] iter = 11360, loss = 1.5048
[2023-10-02 04:11:39] iter = 11370, loss = 1.4563
[2023-10-02 04:11:40] iter = 11380, loss = 1.5599
[2023-10-02 04:11:40] iter = 11390, loss = 1.4469
[2023-10-02 04:11:41] iter = 11400, loss = 1.5574
[2023-10-02 04:11:42] iter = 11410, loss = 1.5792
[2023-10-02 04:11:43] iter = 11420, loss = 1.5636
[2023-10-02 04:11:44] iter = 11430, loss = 1.3843
[2023-10-02 04:11:45] iter = 11440, loss = 1.5993
[2023-10-02 04:11:46] iter = 11450, loss = 1.6424
[2023-10-02 04:11:47] iter = 11460, loss = 1.4942
[2023-10-02 04:11:48] iter = 11470, loss = 1.5052
[2023-10-02 04:11:49] iter = 11480, loss = 1.6597
[2023-10-02 04:11:50] iter = 11490, loss = 1.3757
[2023-10-02 04:11:51] iter = 11500, loss = 1.6775
[2023-10-02 04:11:51] iter = 11510, loss = 1.5775
[2023-10-02 04:11:52] iter = 11520, loss = 1.4792
[2023-10-02 04:11:53] iter = 11530, loss = 1.6256
[2023-10-02 04:11:54] iter = 11540, loss = 1.4513
[2023-10-02 04:11:55] iter = 11550, loss = 1.4782
[2023-10-02 04:11:56] iter = 11560, loss = 1.5488
[2023-10-02 04:11:57] iter = 11570, loss = 1.4785
[2023-10-02 04:11:58] iter = 11580, loss = 1.3833
[2023-10-02 04:11:59] iter = 11590, loss = 1.5287
[2023-10-02 04:12:00] iter = 11600, loss = 1.4089
[2023-10-02 04:12:00] iter = 11610, loss = 1.5063
[2023-10-02 04:12:01] iter = 11620, loss = 1.6051
[2023-10-02 04:12:02] iter = 11630, loss = 1.5533
[2023-10-02 04:12:03] iter = 11640, loss = 1.6761
[2023-10-02 04:12:04] iter = 11650, loss = 1.4745
[2023-10-02 04:12:05] iter = 11660, loss = 1.7304
[2023-10-02 04:12:06] iter = 11670, loss = 1.6435
[2023-10-02 04:12:07] iter = 11680, loss = 1.4437
[2023-10-02 04:12:08] iter = 11690, loss = 1.5613
[2023-10-02 04:12:09] iter = 11700, loss = 1.4615
[2023-10-02 04:12:09] iter = 11710, loss = 1.4550
[2023-10-02 04:12:10] iter = 11720, loss = 1.4923
[2023-10-02 04:12:11] iter = 11730, loss = 1.4802
[2023-10-02 04:12:12] iter = 11740, loss = 1.4301
[2023-10-02 04:12:13] iter = 11750, loss = 1.4756
[2023-10-02 04:12:14] iter = 11760, loss = 1.5349
[2023-10-02 04:12:15] iter = 11770, loss = 1.6578
[2023-10-02 04:12:16] iter = 11780, loss = 1.5557
[2023-10-02 04:12:17] iter = 11790, loss = 1.3907
[2023-10-02 04:12:18] iter = 11800, loss = 1.5453
[2023-10-02 04:12:19] iter = 11810, loss = 1.5702
[2023-10-02 04:12:20] iter = 11820, loss = 1.5782
[2023-10-02 04:12:21] iter = 11830, loss = 1.4264
[2023-10-02 04:12:21] iter = 11840, loss = 1.5180
[2023-10-02 04:12:22] iter = 11850, loss = 1.5099
[2023-10-02 04:12:23] iter = 11860, loss = 1.4451
[2023-10-02 04:12:24] iter = 11870, loss = 1.5994
[2023-10-02 04:12:25] iter = 11880, loss = 1.5603
[2023-10-02 04:12:26] iter = 11890, loss = 1.4366
[2023-10-02 04:12:27] iter = 11900, loss = 1.5736
[2023-10-02 04:12:28] iter = 11910, loss = 1.6543
[2023-10-02 04:12:29] iter = 11920, loss = 1.4593
[2023-10-02 04:12:30] iter = 11930, loss = 1.5081
[2023-10-02 04:12:31] iter = 11940, loss = 1.4802
[2023-10-02 04:12:31] iter = 11950, loss = 1.4300
[2023-10-02 04:12:32] iter = 11960, loss = 1.5030
[2023-10-02 04:12:33] iter = 11970, loss = 1.6258
[2023-10-02 04:12:34] iter = 11980, loss = 1.6207
[2023-10-02 04:12:35] iter = 11990, loss = 1.4729
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 56341}
[2023-10-02 04:13:00] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002682 train acc = 1.0000, test acc = 0.6143
[2023-10-02 04:13:24] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.009053 train acc = 1.0000, test acc = 0.6261
[2023-10-02 04:13:48] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003812 train acc = 1.0000, test acc = 0.6317
[2023-10-02 04:14:13] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.007814 train acc = 1.0000, test acc = 0.6289
[2023-10-02 04:14:37] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.006309 train acc = 1.0000, test acc = 0.6222
[2023-10-02 04:15:01] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.010115 train acc = 1.0000, test acc = 0.6252
[2023-10-02 04:15:25] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.006671 train acc = 1.0000, test acc = 0.6134
[2023-10-02 04:15:49] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.013541 train acc = 1.0000, test acc = 0.6130
[2023-10-02 04:16:13] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.019665 train acc = 1.0000, test acc = 0.6251
[2023-10-02 04:16:37] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.012409 train acc = 1.0000, test acc = 0.6231
[2023-10-02 04:17:01] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002097 train acc = 1.0000, test acc = 0.6243
[2023-10-02 04:17:25] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001699 train acc = 1.0000, test acc = 0.6180
[2023-10-02 04:17:49] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004392 train acc = 1.0000, test acc = 0.6191
[2023-10-02 04:18:13] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.010218 train acc = 1.0000, test acc = 0.6222
[2023-10-02 04:18:37] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002413 train acc = 1.0000, test acc = 0.6281
[2023-10-02 04:19:02] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003437 train acc = 1.0000, test acc = 0.6168
[2023-10-02 04:19:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.013210 train acc = 1.0000, test acc = 0.6232
[2023-10-02 04:19:50] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.011897 train acc = 1.0000, test acc = 0.6219
[2023-10-02 04:20:14] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.008493 train acc = 1.0000, test acc = 0.6276
[2023-10-02 04:20:39] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012887 train acc = 1.0000, test acc = 0.6201
Evaluate 20 random ConvNet, mean = 0.6222 std = 0.0051
-------------------------
[2023-10-02 04:20:39] iter = 12000, loss = 1.4899
[2023-10-02 04:20:40] iter = 12010, loss = 1.5703
[2023-10-02 04:20:41] iter = 12020, loss = 1.5158
[2023-10-02 04:20:42] iter = 12030, loss = 1.5459
[2023-10-02 04:20:43] iter = 12040, loss = 1.4815
[2023-10-02 04:20:43] iter = 12050, loss = 1.6277
[2023-10-02 04:20:44] iter = 12060, loss = 1.3510
[2023-10-02 04:20:45] iter = 12070, loss = 1.5050
[2023-10-02 04:20:46] iter = 12080, loss = 1.6041
[2023-10-02 04:20:47] iter = 12090, loss = 1.5967
[2023-10-02 04:20:48] iter = 12100, loss = 1.4773
[2023-10-02 04:20:49] iter = 12110, loss = 1.6078
[2023-10-02 04:20:50] iter = 12120, loss = 1.4033
[2023-10-02 04:20:51] iter = 12130, loss = 1.4721
[2023-10-02 04:20:52] iter = 12140, loss = 1.5147
[2023-10-02 04:20:52] iter = 12150, loss = 1.5749
[2023-10-02 04:20:53] iter = 12160, loss = 1.5141
[2023-10-02 04:20:54] iter = 12170, loss = 1.5479
[2023-10-02 04:20:55] iter = 12180, loss = 1.6240
[2023-10-02 04:20:56] iter = 12190, loss = 1.4948
[2023-10-02 04:20:57] iter = 12200, loss = 1.6110
[2023-10-02 04:20:58] iter = 12210, loss = 1.4982
[2023-10-02 04:20:59] iter = 12220, loss = 1.5186
[2023-10-02 04:20:59] iter = 12230, loss = 1.4590
[2023-10-02 04:21:00] iter = 12240, loss = 1.5236
[2023-10-02 04:21:01] iter = 12250, loss = 1.5595
[2023-10-02 04:21:02] iter = 12260, loss = 1.4723
[2023-10-02 04:21:03] iter = 12270, loss = 1.5419
[2023-10-02 04:21:04] iter = 12280, loss = 1.5743
[2023-10-02 04:21:05] iter = 12290, loss = 1.6038
[2023-10-02 04:21:06] iter = 12300, loss = 1.4880
[2023-10-02 04:21:07] iter = 12310, loss = 1.5259
[2023-10-02 04:21:07] iter = 12320, loss = 1.6013
[2023-10-02 04:21:08] iter = 12330, loss = 1.6747
[2023-10-02 04:21:09] iter = 12340, loss = 1.4230
[2023-10-02 04:21:10] iter = 12350, loss = 1.5832
[2023-10-02 04:21:11] iter = 12360, loss = 1.6292
[2023-10-02 04:21:12] iter = 12370, loss = 1.5841
[2023-10-02 04:21:13] iter = 12380, loss = 1.4602
[2023-10-02 04:21:14] iter = 12390, loss = 1.6143
[2023-10-02 04:21:15] iter = 12400, loss = 1.5773
[2023-10-02 04:21:16] iter = 12410, loss = 1.4927
[2023-10-02 04:21:17] iter = 12420, loss = 1.5409
[2023-10-02 04:21:18] iter = 12430, loss = 1.5620
[2023-10-02 04:21:19] iter = 12440, loss = 1.5216
[2023-10-02 04:21:19] iter = 12450, loss = 1.6207
[2023-10-02 04:21:20] iter = 12460, loss = 1.6043
[2023-10-02 04:21:21] iter = 12470, loss = 1.5104
[2023-10-02 04:21:22] iter = 12480, loss = 1.5140
[2023-10-02 04:21:23] iter = 12490, loss = 1.5340
[2023-10-02 04:21:24] iter = 12500, loss = 1.5140
[2023-10-02 04:21:25] iter = 12510, loss = 1.5963
[2023-10-02 04:21:26] iter = 12520, loss = 1.5247
[2023-10-02 04:21:27] iter = 12530, loss = 1.5649
[2023-10-02 04:21:28] iter = 12540, loss = 1.5782
[2023-10-02 04:21:29] iter = 12550, loss = 1.4577
[2023-10-02 04:21:30] iter = 12560, loss = 1.4034
[2023-10-02 04:21:31] iter = 12570, loss = 1.4404
[2023-10-02 04:21:31] iter = 12580, loss = 1.5654
[2023-10-02 04:21:32] iter = 12590, loss = 1.4489
[2023-10-02 04:21:33] iter = 12600, loss = 1.4027
[2023-10-02 04:21:34] iter = 12610, loss = 1.5006
[2023-10-02 04:21:35] iter = 12620, loss = 1.3859
[2023-10-02 04:21:36] iter = 12630, loss = 1.5172
[2023-10-02 04:21:37] iter = 12640, loss = 1.6826
[2023-10-02 04:21:38] iter = 12650, loss = 1.5799
[2023-10-02 04:21:39] iter = 12660, loss = 1.5083
[2023-10-02 04:21:40] iter = 12670, loss = 1.5714
[2023-10-02 04:21:41] iter = 12680, loss = 1.4908
[2023-10-02 04:21:41] iter = 12690, loss = 1.4621
[2023-10-02 04:21:42] iter = 12700, loss = 1.6074
[2023-10-02 04:21:43] iter = 12710, loss = 1.4817
[2023-10-02 04:21:44] iter = 12720, loss = 1.5542
[2023-10-02 04:21:45] iter = 12730, loss = 1.4948
[2023-10-02 04:21:46] iter = 12740, loss = 1.4668
[2023-10-02 04:21:47] iter = 12750, loss = 1.5330
[2023-10-02 04:21:48] iter = 12760, loss = 1.4902
[2023-10-02 04:21:49] iter = 12770, loss = 1.3948
[2023-10-02 04:21:50] iter = 12780, loss = 1.5556
[2023-10-02 04:21:50] iter = 12790, loss = 1.4999
[2023-10-02 04:21:51] iter = 12800, loss = 1.4843
[2023-10-02 04:21:52] iter = 12810, loss = 1.3695
[2023-10-02 04:21:53] iter = 12820, loss = 1.4535
[2023-10-02 04:21:54] iter = 12830, loss = 1.5259
[2023-10-02 04:21:55] iter = 12840, loss = 1.5300
[2023-10-02 04:21:56] iter = 12850, loss = 1.5373
[2023-10-02 04:21:57] iter = 12860, loss = 1.4083
[2023-10-02 04:21:58] iter = 12870, loss = 1.5747
[2023-10-02 04:21:59] iter = 12880, loss = 1.5554
[2023-10-02 04:22:00] iter = 12890, loss = 1.4750
[2023-10-02 04:22:00] iter = 12900, loss = 1.4783
[2023-10-02 04:22:01] iter = 12910, loss = 1.4262
[2023-10-02 04:22:02] iter = 12920, loss = 1.4805
[2023-10-02 04:22:03] iter = 12930, loss = 1.5100
[2023-10-02 04:22:04] iter = 12940, loss = 1.5237
[2023-10-02 04:22:05] iter = 12950, loss = 1.6080
[2023-10-02 04:22:06] iter = 12960, loss = 1.4615
[2023-10-02 04:22:07] iter = 12970, loss = 1.4529
[2023-10-02 04:22:08] iter = 12980, loss = 1.5129
[2023-10-02 04:22:09] iter = 12990, loss = 1.6194
[2023-10-02 04:22:10] iter = 13000, loss = 1.6044
[2023-10-02 04:22:11] iter = 13010, loss = 1.5362
[2023-10-02 04:22:11] iter = 13020, loss = 1.5876
[2023-10-02 04:22:12] iter = 13030, loss = 1.6173
[2023-10-02 04:22:13] iter = 13040, loss = 1.4223
[2023-10-02 04:22:14] iter = 13050, loss = 1.5073
[2023-10-02 04:22:15] iter = 13060, loss = 1.4782
[2023-10-02 04:22:16] iter = 13070, loss = 1.5005
[2023-10-02 04:22:17] iter = 13080, loss = 1.7158
[2023-10-02 04:22:18] iter = 13090, loss = 1.6296
[2023-10-02 04:22:19] iter = 13100, loss = 1.5462
[2023-10-02 04:22:20] iter = 13110, loss = 1.5878
[2023-10-02 04:22:21] iter = 13120, loss = 1.5673
[2023-10-02 04:22:21] iter = 13130, loss = 1.5216
[2023-10-02 04:22:22] iter = 13140, loss = 1.5797
[2023-10-02 04:22:23] iter = 13150, loss = 1.6741
[2023-10-02 04:22:24] iter = 13160, loss = 1.5610
[2023-10-02 04:22:25] iter = 13170, loss = 1.5647
[2023-10-02 04:22:26] iter = 13180, loss = 1.6632
[2023-10-02 04:22:27] iter = 13190, loss = 1.6107
[2023-10-02 04:22:28] iter = 13200, loss = 1.5427
[2023-10-02 04:22:29] iter = 13210, loss = 1.5907
[2023-10-02 04:22:30] iter = 13220, loss = 1.5800
[2023-10-02 04:22:31] iter = 13230, loss = 1.5969
[2023-10-02 04:22:31] iter = 13240, loss = 1.6784
[2023-10-02 04:22:32] iter = 13250, loss = 1.5017
[2023-10-02 04:22:33] iter = 13260, loss = 1.6141
[2023-10-02 04:22:34] iter = 13270, loss = 1.4883
[2023-10-02 04:22:35] iter = 13280, loss = 1.4953
[2023-10-02 04:22:36] iter = 13290, loss = 1.5948
[2023-10-02 04:22:37] iter = 13300, loss = 1.5063
[2023-10-02 04:22:38] iter = 13310, loss = 1.5847
[2023-10-02 04:22:38] iter = 13320, loss = 1.4692
[2023-10-02 04:22:39] iter = 13330, loss = 1.5156
[2023-10-02 04:22:40] iter = 13340, loss = 1.4035
[2023-10-02 04:22:41] iter = 13350, loss = 1.5320
[2023-10-02 04:22:42] iter = 13360, loss = 1.5082
[2023-10-02 04:22:43] iter = 13370, loss = 1.5223
[2023-10-02 04:22:44] iter = 13380, loss = 1.4211
[2023-10-02 04:22:45] iter = 13390, loss = 1.5398
[2023-10-02 04:22:46] iter = 13400, loss = 1.6153
[2023-10-02 04:22:47] iter = 13410, loss = 1.4997
[2023-10-02 04:22:48] iter = 13420, loss = 1.7748
[2023-10-02 04:22:48] iter = 13430, loss = 1.5087
[2023-10-02 04:22:49] iter = 13440, loss = 1.4734
[2023-10-02 04:22:50] iter = 13450, loss = 1.5449
[2023-10-02 04:22:51] iter = 13460, loss = 1.5000
[2023-10-02 04:22:52] iter = 13470, loss = 1.5492
[2023-10-02 04:22:53] iter = 13480, loss = 1.4592
[2023-10-02 04:22:54] iter = 13490, loss = 1.7095
[2023-10-02 04:22:55] iter = 13500, loss = 1.4548
[2023-10-02 04:22:56] iter = 13510, loss = 1.7318
[2023-10-02 04:22:57] iter = 13520, loss = 1.4863
[2023-10-02 04:22:58] iter = 13530, loss = 1.5231
[2023-10-02 04:22:58] iter = 13540, loss = 1.5070
[2023-10-02 04:22:59] iter = 13550, loss = 1.4074
[2023-10-02 04:23:00] iter = 13560, loss = 1.6227
[2023-10-02 04:23:01] iter = 13570, loss = 1.4809
[2023-10-02 04:23:02] iter = 13580, loss = 1.4990
[2023-10-02 04:23:03] iter = 13590, loss = 1.5989
[2023-10-02 04:23:04] iter = 13600, loss = 1.5213
[2023-10-02 04:23:05] iter = 13610, loss = 1.6602
[2023-10-02 04:23:06] iter = 13620, loss = 1.6277
[2023-10-02 04:23:07] iter = 13630, loss = 1.5308
[2023-10-02 04:23:08] iter = 13640, loss = 1.4169
[2023-10-02 04:23:08] iter = 13650, loss = 1.5054
[2023-10-02 04:23:09] iter = 13660, loss = 1.5043
[2023-10-02 04:23:10] iter = 13670, loss = 1.4949
[2023-10-02 04:23:11] iter = 13680, loss = 1.5601
[2023-10-02 04:23:12] iter = 13690, loss = 1.5354
[2023-10-02 04:23:13] iter = 13700, loss = 1.4840
[2023-10-02 04:23:14] iter = 13710, loss = 1.6397
[2023-10-02 04:23:15] iter = 13720, loss = 1.4674
[2023-10-02 04:23:16] iter = 13730, loss = 1.4893
[2023-10-02 04:23:17] iter = 13740, loss = 1.5048
[2023-10-02 04:23:18] iter = 13750, loss = 1.7408
[2023-10-02 04:23:18] iter = 13760, loss = 1.4573
[2023-10-02 04:23:19] iter = 13770, loss = 1.4906
[2023-10-02 04:23:20] iter = 13780, loss = 1.5230
[2023-10-02 04:23:21] iter = 13790, loss = 1.6128
[2023-10-02 04:23:22] iter = 13800, loss = 1.5084
[2023-10-02 04:23:23] iter = 13810, loss = 1.4782
[2023-10-02 04:23:24] iter = 13820, loss = 1.4280
[2023-10-02 04:23:25] iter = 13830, loss = 1.5598
[2023-10-02 04:23:26] iter = 13840, loss = 1.5315
[2023-10-02 04:23:27] iter = 13850, loss = 1.4347
[2023-10-02 04:23:28] iter = 13860, loss = 1.4895
[2023-10-02 04:23:28] iter = 13870, loss = 1.5023
[2023-10-02 04:23:29] iter = 13880, loss = 1.5311
[2023-10-02 04:23:30] iter = 13890, loss = 1.5754
[2023-10-02 04:23:31] iter = 13900, loss = 1.5190
[2023-10-02 04:23:32] iter = 13910, loss = 1.4856
[2023-10-02 04:23:33] iter = 13920, loss = 1.7257
[2023-10-02 04:23:34] iter = 13930, loss = 1.5776
[2023-10-02 04:23:35] iter = 13940, loss = 1.7067
[2023-10-02 04:23:36] iter = 13950, loss = 1.5034
[2023-10-02 04:23:37] iter = 13960, loss = 1.6344
[2023-10-02 04:23:37] iter = 13970, loss = 1.4669
[2023-10-02 04:23:38] iter = 13980, loss = 1.5957
[2023-10-02 04:23:39] iter = 13990, loss = 1.5516
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 20357}
[2023-10-02 04:24:04] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.004599 train acc = 1.0000, test acc = 0.6239
[2023-10-02 04:24:28] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.029555 train acc = 0.9980, test acc = 0.6259
[2023-10-02 04:24:52] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.015202 train acc = 0.9980, test acc = 0.6225
[2023-10-02 04:25:16] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002906 train acc = 1.0000, test acc = 0.6295
[2023-10-02 04:25:40] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003850 train acc = 1.0000, test acc = 0.6217
[2023-10-02 04:26:05] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.015934 train acc = 0.9980, test acc = 0.6247
[2023-10-02 04:26:29] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.007046 train acc = 1.0000, test acc = 0.6246
[2023-10-02 04:26:53] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.015951 train acc = 0.9980, test acc = 0.6291
[2023-10-02 04:27:17] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014104 train acc = 1.0000, test acc = 0.6309
[2023-10-02 04:27:42] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013131 train acc = 1.0000, test acc = 0.6165
[2023-10-02 04:28:06] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.023265 train acc = 0.9980, test acc = 0.6241
[2023-10-02 04:28:30] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.005668 train acc = 1.0000, test acc = 0.6252
[2023-10-02 04:28:54] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.021501 train acc = 0.9980, test acc = 0.6333
[2023-10-02 04:29:18] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002064 train acc = 1.0000, test acc = 0.6224
[2023-10-02 04:29:42] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003844 train acc = 1.0000, test acc = 0.6144
[2023-10-02 04:30:06] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.006553 train acc = 1.0000, test acc = 0.6257
[2023-10-02 04:30:31] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.020678 train acc = 0.9980, test acc = 0.6212
[2023-10-02 04:30:55] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.012146 train acc = 1.0000, test acc = 0.6248
[2023-10-02 04:31:19] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013036 train acc = 0.9980, test acc = 0.6272
[2023-10-02 04:31:43] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.010617 train acc = 1.0000, test acc = 0.6290
Evaluate 20 random ConvNet, mean = 0.6248 std = 0.0044
-------------------------
[2023-10-02 04:31:43] iter = 14000, loss = 1.3879
[2023-10-02 04:31:44] iter = 14010, loss = 1.5268
[2023-10-02 04:31:45] iter = 14020, loss = 1.4880
[2023-10-02 04:31:46] iter = 14030, loss = 1.5274
[2023-10-02 04:31:47] iter = 14040, loss = 1.4059
[2023-10-02 04:31:47] iter = 14050, loss = 1.5611
[2023-10-02 04:31:48] iter = 14060, loss = 1.6113
[2023-10-02 04:31:49] iter = 14070, loss = 1.4084
[2023-10-02 04:31:50] iter = 14080, loss = 1.5153
[2023-10-02 04:31:51] iter = 14090, loss = 1.5267
[2023-10-02 04:31:52] iter = 14100, loss = 1.2846
[2023-10-02 04:31:53] iter = 14110, loss = 1.5324
[2023-10-02 04:31:54] iter = 14120, loss = 1.5491
[2023-10-02 04:31:55] iter = 14130, loss = 1.5620
[2023-10-02 04:31:56] iter = 14140, loss = 1.2804
[2023-10-02 04:31:57] iter = 14150, loss = 1.5690
[2023-10-02 04:31:57] iter = 14160, loss = 1.5039
[2023-10-02 04:31:58] iter = 14170, loss = 1.5288
[2023-10-02 04:31:59] iter = 14180, loss = 1.4926
[2023-10-02 04:32:00] iter = 14190, loss = 1.4453
[2023-10-02 04:32:01] iter = 14200, loss = 1.6715
[2023-10-02 04:32:02] iter = 14210, loss = 1.5102
[2023-10-02 04:32:03] iter = 14220, loss = 1.3978
[2023-10-02 04:32:04] iter = 14230, loss = 1.4777
[2023-10-02 04:32:04] iter = 14240, loss = 1.7568
[2023-10-02 04:32:05] iter = 14250, loss = 1.6954
[2023-10-02 04:32:06] iter = 14260, loss = 1.5094
[2023-10-02 04:32:07] iter = 14270, loss = 1.5763
[2023-10-02 04:32:08] iter = 14280, loss = 1.4432
[2023-10-02 04:32:09] iter = 14290, loss = 1.5500
[2023-10-02 04:32:10] iter = 14300, loss = 1.5419
[2023-10-02 04:32:11] iter = 14310, loss = 1.4084
[2023-10-02 04:32:12] iter = 14320, loss = 1.5300
[2023-10-02 04:32:13] iter = 14330, loss = 1.5271
[2023-10-02 04:32:14] iter = 14340, loss = 1.5109
[2023-10-02 04:32:14] iter = 14350, loss = 1.4994
[2023-10-02 04:32:15] iter = 14360, loss = 1.6366
[2023-10-02 04:32:16] iter = 14370, loss = 1.5137
[2023-10-02 04:32:17] iter = 14380, loss = 1.6589
[2023-10-02 04:32:18] iter = 14390, loss = 1.5658
[2023-10-02 04:32:19] iter = 14400, loss = 1.4766
[2023-10-02 04:32:20] iter = 14410, loss = 1.5149
[2023-10-02 04:32:21] iter = 14420, loss = 1.6174
[2023-10-02 04:32:22] iter = 14430, loss = 1.5705
[2023-10-02 04:32:23] iter = 14440, loss = 1.5107
[2023-10-02 04:32:24] iter = 14450, loss = 1.4372
[2023-10-02 04:32:24] iter = 14460, loss = 1.4506
[2023-10-02 04:32:25] iter = 14470, loss = 1.5668
[2023-10-02 04:32:26] iter = 14480, loss = 1.5661
[2023-10-02 04:32:27] iter = 14490, loss = 1.5314
[2023-10-02 04:32:28] iter = 14500, loss = 1.6759
[2023-10-02 04:32:29] iter = 14510, loss = 1.4812
[2023-10-02 04:32:30] iter = 14520, loss = 1.5459
[2023-10-02 04:32:31] iter = 14530, loss = 1.5070
[2023-10-02 04:32:32] iter = 14540, loss = 1.4437
[2023-10-02 04:32:33] iter = 14550, loss = 1.5178
[2023-10-02 04:32:33] iter = 14560, loss = 1.4982
[2023-10-02 04:32:34] iter = 14570, loss = 1.5685
[2023-10-02 04:32:35] iter = 14580, loss = 1.4098
[2023-10-02 04:32:36] iter = 14590, loss = 1.4351
[2023-10-02 04:32:37] iter = 14600, loss = 1.4106
[2023-10-02 04:32:38] iter = 14610, loss = 1.4792
[2023-10-02 04:32:39] iter = 14620, loss = 1.3507
[2023-10-02 04:32:40] iter = 14630, loss = 1.4679
[2023-10-02 04:32:41] iter = 14640, loss = 1.3812
[2023-10-02 04:32:41] iter = 14650, loss = 1.4688
[2023-10-02 04:32:42] iter = 14660, loss = 1.6192
[2023-10-02 04:32:43] iter = 14670, loss = 1.5862
[2023-10-02 04:32:44] iter = 14680, loss = 1.7590
[2023-10-02 04:32:45] iter = 14690, loss = 1.5976
[2023-10-02 04:32:46] iter = 14700, loss = 1.5808
[2023-10-02 04:32:47] iter = 14710, loss = 1.5242
[2023-10-02 04:32:48] iter = 14720, loss = 1.4006
[2023-10-02 04:32:48] iter = 14730, loss = 1.3929
[2023-10-02 04:32:49] iter = 14740, loss = 1.5147
[2023-10-02 04:32:50] iter = 14750, loss = 1.4602
[2023-10-02 04:32:51] iter = 14760, loss = 1.4907
[2023-10-02 04:32:52] iter = 14770, loss = 1.5191
[2023-10-02 04:32:53] iter = 14780, loss = 1.6256
[2023-10-02 04:32:54] iter = 14790, loss = 1.5670
[2023-10-02 04:32:55] iter = 14800, loss = 1.5788
[2023-10-02 04:32:56] iter = 14810, loss = 1.5405
[2023-10-02 04:32:57] iter = 14820, loss = 1.4600
[2023-10-02 04:32:58] iter = 14830, loss = 1.4217
[2023-10-02 04:32:59] iter = 14840, loss = 1.6401
[2023-10-02 04:33:00] iter = 14850, loss = 1.4280
[2023-10-02 04:33:01] iter = 14860, loss = 1.5237
[2023-10-02 04:33:01] iter = 14870, loss = 1.3596
[2023-10-02 04:33:02] iter = 14880, loss = 1.3972
[2023-10-02 04:33:03] iter = 14890, loss = 1.5280
[2023-10-02 04:33:04] iter = 14900, loss = 1.3903
[2023-10-02 04:33:05] iter = 14910, loss = 1.6157
[2023-10-02 04:33:06] iter = 14920, loss = 1.4985
[2023-10-02 04:33:07] iter = 14930, loss = 1.4274
[2023-10-02 04:33:08] iter = 14940, loss = 1.4260
[2023-10-02 04:33:09] iter = 14950, loss = 1.8454
[2023-10-02 04:33:10] iter = 14960, loss = 1.4377
[2023-10-02 04:33:11] iter = 14970, loss = 1.4783
[2023-10-02 04:33:11] iter = 14980, loss = 1.5737
[2023-10-02 04:33:12] iter = 14990, loss = 1.5192
[2023-10-02 04:33:13] iter = 15000, loss = 1.4903
[2023-10-02 04:33:14] iter = 15010, loss = 1.5100
[2023-10-02 04:33:15] iter = 15020, loss = 1.4506
[2023-10-02 04:33:16] iter = 15030, loss = 1.7623
[2023-10-02 04:33:17] iter = 15040, loss = 1.4436
[2023-10-02 04:33:18] iter = 15050, loss = 1.6094
[2023-10-02 04:33:19] iter = 15060, loss = 1.5138
[2023-10-02 04:33:20] iter = 15070, loss = 1.5522
[2023-10-02 04:33:20] iter = 15080, loss = 1.6102
[2023-10-02 04:33:21] iter = 15090, loss = 1.4887
[2023-10-02 04:33:22] iter = 15100, loss = 1.6753
[2023-10-02 04:33:23] iter = 15110, loss = 1.5071
[2023-10-02 04:33:24] iter = 15120, loss = 1.4005
[2023-10-02 04:33:25] iter = 15130, loss = 1.5142
[2023-10-02 04:33:26] iter = 15140, loss = 1.5336
[2023-10-02 04:33:27] iter = 15150, loss = 1.5310
[2023-10-02 04:33:28] iter = 15160, loss = 1.5679
[2023-10-02 04:33:29] iter = 15170, loss = 1.4914
[2023-10-02 04:33:30] iter = 15180, loss = 1.4319
[2023-10-02 04:33:30] iter = 15190, loss = 1.5308
[2023-10-02 04:33:31] iter = 15200, loss = 1.4966
[2023-10-02 04:33:32] iter = 15210, loss = 1.5190
[2023-10-02 04:33:33] iter = 15220, loss = 1.6248
[2023-10-02 04:33:34] iter = 15230, loss = 1.4828
[2023-10-02 04:33:35] iter = 15240, loss = 1.4733
[2023-10-02 04:33:36] iter = 15250, loss = 1.4708
[2023-10-02 04:33:37] iter = 15260, loss = 1.5827
[2023-10-02 04:33:38] iter = 15270, loss = 1.5227
[2023-10-02 04:33:39] iter = 15280, loss = 1.5580
[2023-10-02 04:33:40] iter = 15290, loss = 1.5631
[2023-10-02 04:33:41] iter = 15300, loss = 1.4868
[2023-10-02 04:33:41] iter = 15310, loss = 1.5164
[2023-10-02 04:33:42] iter = 15320, loss = 1.4699
[2023-10-02 04:33:43] iter = 15330, loss = 1.5451
[2023-10-02 04:33:44] iter = 15340, loss = 1.5671
[2023-10-02 04:33:45] iter = 15350, loss = 1.5325
[2023-10-02 04:33:46] iter = 15360, loss = 1.5199
[2023-10-02 04:33:47] iter = 15370, loss = 1.4149
[2023-10-02 04:33:48] iter = 15380, loss = 1.4934
[2023-10-02 04:33:49] iter = 15390, loss = 1.5454
[2023-10-02 04:33:50] iter = 15400, loss = 1.5515
[2023-10-02 04:33:51] iter = 15410, loss = 1.3871
[2023-10-02 04:33:52] iter = 15420, loss = 1.5094
[2023-10-02 04:33:52] iter = 15430, loss = 1.5204
[2023-10-02 04:33:53] iter = 15440, loss = 1.4837
[2023-10-02 04:33:54] iter = 15450, loss = 1.6086
[2023-10-02 04:33:55] iter = 15460, loss = 1.4146
[2023-10-02 04:33:56] iter = 15470, loss = 1.5633
[2023-10-02 04:33:57] iter = 15480, loss = 1.4438
[2023-10-02 04:33:58] iter = 15490, loss = 1.5159
[2023-10-02 04:33:59] iter = 15500, loss = 1.5485
[2023-10-02 04:34:00] iter = 15510, loss = 1.5017
[2023-10-02 04:34:01] iter = 15520, loss = 1.4580
[2023-10-02 04:34:01] iter = 15530, loss = 1.3627
[2023-10-02 04:34:02] iter = 15540, loss = 1.5316
[2023-10-02 04:34:03] iter = 15550, loss = 1.5031
[2023-10-02 04:34:04] iter = 15560, loss = 1.5757
[2023-10-02 04:34:05] iter = 15570, loss = 1.4461
[2023-10-02 04:34:06] iter = 15580, loss = 1.4853
[2023-10-02 04:34:07] iter = 15590, loss = 1.4279
[2023-10-02 04:34:08] iter = 15600, loss = 1.4309
[2023-10-02 04:34:09] iter = 15610, loss = 1.4990
[2023-10-02 04:34:10] iter = 15620, loss = 1.4306
[2023-10-02 04:34:11] iter = 15630, loss = 1.4525
[2023-10-02 04:34:11] iter = 15640, loss = 1.5069
[2023-10-02 04:34:12] iter = 15650, loss = 1.4684
[2023-10-02 04:34:13] iter = 15660, loss = 1.4808
[2023-10-02 04:34:14] iter = 15670, loss = 1.3955
[2023-10-02 04:34:15] iter = 15680, loss = 1.6675
[2023-10-02 04:34:16] iter = 15690, loss = 1.4746
[2023-10-02 04:34:17] iter = 15700, loss = 1.5854
[2023-10-02 04:34:18] iter = 15710, loss = 1.3713
[2023-10-02 04:34:19] iter = 15720, loss = 1.4262
[2023-10-02 04:34:20] iter = 15730, loss = 1.5283
[2023-10-02 04:34:21] iter = 15740, loss = 1.4519
[2023-10-02 04:34:21] iter = 15750, loss = 1.5275
[2023-10-02 04:34:22] iter = 15760, loss = 1.4418
[2023-10-02 04:34:23] iter = 15770, loss = 1.6245
[2023-10-02 04:34:24] iter = 15780, loss = 1.6153
[2023-10-02 04:34:25] iter = 15790, loss = 1.5029
[2023-10-02 04:34:26] iter = 15800, loss = 1.6275
[2023-10-02 04:34:27] iter = 15810, loss = 1.5801
[2023-10-02 04:34:28] iter = 15820, loss = 1.4315
[2023-10-02 04:34:29] iter = 15830, loss = 1.5620
[2023-10-02 04:34:29] iter = 15840, loss = 1.4794
[2023-10-02 04:34:30] iter = 15850, loss = 1.4964
[2023-10-02 04:34:31] iter = 15860, loss = 1.6356
[2023-10-02 04:34:32] iter = 15870, loss = 1.6031
[2023-10-02 04:34:33] iter = 15880, loss = 1.4993
[2023-10-02 04:34:34] iter = 15890, loss = 1.6860
[2023-10-02 04:34:35] iter = 15900, loss = 1.5652
[2023-10-02 04:34:36] iter = 15910, loss = 1.6048
[2023-10-02 04:34:37] iter = 15920, loss = 1.6594
[2023-10-02 04:34:38] iter = 15930, loss = 1.2871
[2023-10-02 04:34:39] iter = 15940, loss = 1.5082
[2023-10-02 04:34:39] iter = 15950, loss = 1.5807
[2023-10-02 04:34:40] iter = 15960, loss = 1.4480
[2023-10-02 04:34:41] iter = 15970, loss = 1.4656
[2023-10-02 04:34:42] iter = 15980, loss = 1.3988
[2023-10-02 04:34:43] iter = 15990, loss = 1.7907
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 84417}
[2023-10-02 04:35:08] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005716 train acc = 1.0000, test acc = 0.6230
[2023-10-02 04:35:32] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.022999 train acc = 1.0000, test acc = 0.6270
[2023-10-02 04:35:56] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.017935 train acc = 0.9960, test acc = 0.6251
[2023-10-02 04:36:20] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.007454 train acc = 1.0000, test acc = 0.6245
[2023-10-02 04:36:45] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003554 train acc = 1.0000, test acc = 0.6280
[2023-10-02 04:37:09] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001914 train acc = 1.0000, test acc = 0.6240
[2023-10-02 04:37:33] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.013247 train acc = 1.0000, test acc = 0.6226
[2023-10-02 04:37:57] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.021798 train acc = 1.0000, test acc = 0.6347
[2023-10-02 04:38:21] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.024853 train acc = 0.9980, test acc = 0.6136
[2023-10-02 04:38:45] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.019125 train acc = 1.0000, test acc = 0.6243
[2023-10-02 04:39:10] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.024352 train acc = 1.0000, test acc = 0.6259
[2023-10-02 04:39:34] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.018223 train acc = 1.0000, test acc = 0.6218
[2023-10-02 04:39:58] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.002752 train acc = 1.0000, test acc = 0.6293
[2023-10-02 04:40:22] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002513 train acc = 1.0000, test acc = 0.6293
[2023-10-02 04:40:46] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003947 train acc = 1.0000, test acc = 0.6250
[2023-10-02 04:41:10] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.027483 train acc = 0.9960, test acc = 0.6241
[2023-10-02 04:41:35] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.008652 train acc = 1.0000, test acc = 0.6260
[2023-10-02 04:41:58] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003736 train acc = 1.0000, test acc = 0.6145
[2023-10-02 04:42:23] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.011356 train acc = 1.0000, test acc = 0.6282
[2023-10-02 04:42:47] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003641 train acc = 1.0000, test acc = 0.6215
Evaluate 20 random ConvNet, mean = 0.6246 std = 0.0046
-------------------------
[2023-10-02 04:42:47] iter = 16000, loss = 1.4540
[2023-10-02 04:42:48] iter = 16010, loss = 1.5868
[2023-10-02 04:42:49] iter = 16020, loss = 1.7621
[2023-10-02 04:42:50] iter = 16030, loss = 1.5112
[2023-10-02 04:42:51] iter = 16040, loss = 1.4855
[2023-10-02 04:42:52] iter = 16050, loss = 1.4735
[2023-10-02 04:42:53] iter = 16060, loss = 1.6686
[2023-10-02 04:42:54] iter = 16070, loss = 1.4444
[2023-10-02 04:42:55] iter = 16080, loss = 1.5355
[2023-10-02 04:42:56] iter = 16090, loss = 1.5136
[2023-10-02 04:42:56] iter = 16100, loss = 1.5943
[2023-10-02 04:42:57] iter = 16110, loss = 1.4715
[2023-10-02 04:42:58] iter = 16120, loss = 1.3892
[2023-10-02 04:42:59] iter = 16130, loss = 1.4993
[2023-10-02 04:43:00] iter = 16140, loss = 1.5132
[2023-10-02 04:43:01] iter = 16150, loss = 1.3665
[2023-10-02 04:43:02] iter = 16160, loss = 1.6055
[2023-10-02 04:43:03] iter = 16170, loss = 1.4856
[2023-10-02 04:43:04] iter = 16180, loss = 1.4915
[2023-10-02 04:43:05] iter = 16190, loss = 1.6092
[2023-10-02 04:43:05] iter = 16200, loss = 1.3865
[2023-10-02 04:43:07] iter = 16210, loss = 1.4956
[2023-10-02 04:43:07] iter = 16220, loss = 1.4884
[2023-10-02 04:43:08] iter = 16230, loss = 1.5896
[2023-10-02 04:43:09] iter = 16240, loss = 1.5172
[2023-10-02 04:43:10] iter = 16250, loss = 1.4920
[2023-10-02 04:43:11] iter = 16260, loss = 1.4277
[2023-10-02 04:43:12] iter = 16270, loss = 1.5062
[2023-10-02 04:43:13] iter = 16280, loss = 1.4529
[2023-10-02 04:43:14] iter = 16290, loss = 1.4480
[2023-10-02 04:43:15] iter = 16300, loss = 1.5208
[2023-10-02 04:43:16] iter = 16310, loss = 1.4184
[2023-10-02 04:43:17] iter = 16320, loss = 1.4986
[2023-10-02 04:43:17] iter = 16330, loss = 1.5095
[2023-10-02 04:43:18] iter = 16340, loss = 1.4497
[2023-10-02 04:43:19] iter = 16350, loss = 1.6177
[2023-10-02 04:43:20] iter = 16360, loss = 1.6649
[2023-10-02 04:43:21] iter = 16370, loss = 1.5395
[2023-10-02 04:43:22] iter = 16380, loss = 1.4707
[2023-10-02 04:43:23] iter = 16390, loss = 1.4155
[2023-10-02 04:43:24] iter = 16400, loss = 1.5978
[2023-10-02 04:43:25] iter = 16410, loss = 1.4494
[2023-10-02 04:43:26] iter = 16420, loss = 1.5370
[2023-10-02 04:43:27] iter = 16430, loss = 1.5106
[2023-10-02 04:43:28] iter = 16440, loss = 1.4228
[2023-10-02 04:43:28] iter = 16450, loss = 1.5409
[2023-10-02 04:43:29] iter = 16460, loss = 1.6414
[2023-10-02 04:43:30] iter = 16470, loss = 1.5608
[2023-10-02 04:43:31] iter = 16480, loss = 1.3615
[2023-10-02 04:43:32] iter = 16490, loss = 1.4733
[2023-10-02 04:43:33] iter = 16500, loss = 1.4894
[2023-10-02 04:43:34] iter = 16510, loss = 1.5924
[2023-10-02 04:43:35] iter = 16520, loss = 1.5542
[2023-10-02 04:43:36] iter = 16530, loss = 1.3889
[2023-10-02 04:43:37] iter = 16540, loss = 1.5044
[2023-10-02 04:43:38] iter = 16550, loss = 1.4137
[2023-10-02 04:43:38] iter = 16560, loss = 1.6405
[2023-10-02 04:43:39] iter = 16570, loss = 1.4055
[2023-10-02 04:43:40] iter = 16580, loss = 1.5342
[2023-10-02 04:43:41] iter = 16590, loss = 1.3588
[2023-10-02 04:43:42] iter = 16600, loss = 1.4786
[2023-10-02 04:43:43] iter = 16610, loss = 1.5035
[2023-10-02 04:43:44] iter = 16620, loss = 1.5532
[2023-10-02 04:43:45] iter = 16630, loss = 1.5865
[2023-10-02 04:43:46] iter = 16640, loss = 1.4607
[2023-10-02 04:43:46] iter = 16650, loss = 1.5831
[2023-10-02 04:43:47] iter = 16660, loss = 1.5197
[2023-10-02 04:43:48] iter = 16670, loss = 1.6684
[2023-10-02 04:43:49] iter = 16680, loss = 1.6903
[2023-10-02 04:43:50] iter = 16690, loss = 1.5449
[2023-10-02 04:43:51] iter = 16700, loss = 1.5163
[2023-10-02 04:43:52] iter = 16710, loss = 1.4399
[2023-10-02 04:43:53] iter = 16720, loss = 1.4381
[2023-10-02 04:43:54] iter = 16730, loss = 1.4803
[2023-10-02 04:43:55] iter = 16740, loss = 1.4059
[2023-10-02 04:43:55] iter = 16750, loss = 1.4708
[2023-10-02 04:43:56] iter = 16760, loss = 1.5513
[2023-10-02 04:43:57] iter = 16770, loss = 1.4707
[2023-10-02 04:43:58] iter = 16780, loss = 1.5658
[2023-10-02 04:43:59] iter = 16790, loss = 1.5973
[2023-10-02 04:44:00] iter = 16800, loss = 1.3573
[2023-10-02 04:44:01] iter = 16810, loss = 1.4350
[2023-10-02 04:44:02] iter = 16820, loss = 1.4485
[2023-10-02 04:44:03] iter = 16830, loss = 1.4163
[2023-10-02 04:44:04] iter = 16840, loss = 1.4089
[2023-10-02 04:44:05] iter = 16850, loss = 1.4464
[2023-10-02 04:44:05] iter = 16860, loss = 1.5290
[2023-10-02 04:44:06] iter = 16870, loss = 1.6048
[2023-10-02 04:44:07] iter = 16880, loss = 1.4309
[2023-10-02 04:44:08] iter = 16890, loss = 1.5017
[2023-10-02 04:44:09] iter = 16900, loss = 1.4862
[2023-10-02 04:44:10] iter = 16910, loss = 1.4115
[2023-10-02 04:44:11] iter = 16920, loss = 1.5258
[2023-10-02 04:44:12] iter = 16930, loss = 1.5244
[2023-10-02 04:44:13] iter = 16940, loss = 1.6450
[2023-10-02 04:44:14] iter = 16950, loss = 1.3795
[2023-10-02 04:44:14] iter = 16960, loss = 1.4838
[2023-10-02 04:44:15] iter = 16970, loss = 1.6512
[2023-10-02 04:44:16] iter = 16980, loss = 1.5540
[2023-10-02 04:44:17] iter = 16990, loss = 1.4966
[2023-10-02 04:44:18] iter = 17000, loss = 1.5211
[2023-10-02 04:44:19] iter = 17010, loss = 1.4759
[2023-10-02 04:44:20] iter = 17020, loss = 1.6139
[2023-10-02 04:44:21] iter = 17030, loss = 1.4081
[2023-10-02 04:44:22] iter = 17040, loss = 1.5447
[2023-10-02 04:44:23] iter = 17050, loss = 1.4137
[2023-10-02 04:44:23] iter = 17060, loss = 1.3659
[2023-10-02 04:44:24] iter = 17070, loss = 1.6080
[2023-10-02 04:44:25] iter = 17080, loss = 1.3963
[2023-10-02 04:44:26] iter = 17090, loss = 1.5037
[2023-10-02 04:44:27] iter = 17100, loss = 1.4088
[2023-10-02 04:44:28] iter = 17110, loss = 1.5417
[2023-10-02 04:44:29] iter = 17120, loss = 1.3981
[2023-10-02 04:44:30] iter = 17130, loss = 1.6139
[2023-10-02 04:44:30] iter = 17140, loss = 1.4420
[2023-10-02 04:44:31] iter = 17150, loss = 1.4926
[2023-10-02 04:44:32] iter = 17160, loss = 1.4919
[2023-10-02 04:44:33] iter = 17170, loss = 1.3970
[2023-10-02 04:44:34] iter = 17180, loss = 1.5059
[2023-10-02 04:44:35] iter = 17190, loss = 1.4239
[2023-10-02 04:44:36] iter = 17200, loss = 1.5486
[2023-10-02 04:44:37] iter = 17210, loss = 1.5442
[2023-10-02 04:44:38] iter = 17220, loss = 1.7277
[2023-10-02 04:44:39] iter = 17230, loss = 1.3621
[2023-10-02 04:44:39] iter = 17240, loss = 1.4821
[2023-10-02 04:44:40] iter = 17250, loss = 1.4559
[2023-10-02 04:44:41] iter = 17260, loss = 1.4878
[2023-10-02 04:44:42] iter = 17270, loss = 1.4363
[2023-10-02 04:44:43] iter = 17280, loss = 1.4546
[2023-10-02 04:44:44] iter = 17290, loss = 1.5050
[2023-10-02 04:44:45] iter = 17300, loss = 1.3609
[2023-10-02 04:44:46] iter = 17310, loss = 1.4735
[2023-10-02 04:44:47] iter = 17320, loss = 1.6105
[2023-10-02 04:44:48] iter = 17330, loss = 1.4448
[2023-10-02 04:44:49] iter = 17340, loss = 1.4986
[2023-10-02 04:44:50] iter = 17350, loss = 1.5521
[2023-10-02 04:44:50] iter = 17360, loss = 1.4285
[2023-10-02 04:44:51] iter = 17370, loss = 1.6948
[2023-10-02 04:44:52] iter = 17380, loss = 1.6589
[2023-10-02 04:44:53] iter = 17390, loss = 1.4266
[2023-10-02 04:44:54] iter = 17400, loss = 1.4112
[2023-10-02 04:44:55] iter = 17410, loss = 1.5695
[2023-10-02 04:44:56] iter = 17420, loss = 1.6328
[2023-10-02 04:44:57] iter = 17430, loss = 1.7480
[2023-10-02 04:44:58] iter = 17440, loss = 1.4291
[2023-10-02 04:44:59] iter = 17450, loss = 1.4683
[2023-10-02 04:45:00] iter = 17460, loss = 1.3723
[2023-10-02 04:45:00] iter = 17470, loss = 1.7607
[2023-10-02 04:45:01] iter = 17480, loss = 1.3787
[2023-10-02 04:45:02] iter = 17490, loss = 1.6762
[2023-10-02 04:45:03] iter = 17500, loss = 1.4591
[2023-10-02 04:45:04] iter = 17510, loss = 1.4810
[2023-10-02 04:45:05] iter = 17520, loss = 1.4141
[2023-10-02 04:45:06] iter = 17530, loss = 1.4375
[2023-10-02 04:45:07] iter = 17540, loss = 1.4802
[2023-10-02 04:45:07] iter = 17550, loss = 1.5445
[2023-10-02 04:45:08] iter = 17560, loss = 1.6414
[2023-10-02 04:45:09] iter = 17570, loss = 1.6088
[2023-10-02 04:45:10] iter = 17580, loss = 1.7980
[2023-10-02 04:45:11] iter = 17590, loss = 1.6782
[2023-10-02 04:45:12] iter = 17600, loss = 1.4310
[2023-10-02 04:45:13] iter = 17610, loss = 1.5473
[2023-10-02 04:45:14] iter = 17620, loss = 1.5985
[2023-10-02 04:45:15] iter = 17630, loss = 1.4540
[2023-10-02 04:45:16] iter = 17640, loss = 1.5774
[2023-10-02 04:45:17] iter = 17650, loss = 1.5265
[2023-10-02 04:45:18] iter = 17660, loss = 1.4054
[2023-10-02 04:45:19] iter = 17670, loss = 1.5154
[2023-10-02 04:45:19] iter = 17680, loss = 1.4514
[2023-10-02 04:45:20] iter = 17690, loss = 1.5288
[2023-10-02 04:45:21] iter = 17700, loss = 1.4200
[2023-10-02 04:45:22] iter = 17710, loss = 1.5738
[2023-10-02 04:45:23] iter = 17720, loss = 1.5115
[2023-10-02 04:45:24] iter = 17730, loss = 1.4289
[2023-10-02 04:45:25] iter = 17740, loss = 1.4978
[2023-10-02 04:45:26] iter = 17750, loss = 1.3802
[2023-10-02 04:45:27] iter = 17760, loss = 1.4635
[2023-10-02 04:45:27] iter = 17770, loss = 1.6040
[2023-10-02 04:45:28] iter = 17780, loss = 1.3227
[2023-10-02 04:45:29] iter = 17790, loss = 1.4522
[2023-10-02 04:45:30] iter = 17800, loss = 1.5095
[2023-10-02 04:45:31] iter = 17810, loss = 1.4669
[2023-10-02 04:45:32] iter = 17820, loss = 1.5254
[2023-10-02 04:45:33] iter = 17830, loss = 1.5521
[2023-10-02 04:45:34] iter = 17840, loss = 1.4072
[2023-10-02 04:45:35] iter = 17850, loss = 1.5892
[2023-10-02 04:45:36] iter = 17860, loss = 1.4185
[2023-10-02 04:45:37] iter = 17870, loss = 1.4812
[2023-10-02 04:45:37] iter = 17880, loss = 1.5873
[2023-10-02 04:45:38] iter = 17890, loss = 1.4608
[2023-10-02 04:45:39] iter = 17900, loss = 1.4671
[2023-10-02 04:45:40] iter = 17910, loss = 1.3500
[2023-10-02 04:45:41] iter = 17920, loss = 1.4546
[2023-10-02 04:45:42] iter = 17930, loss = 1.5304
[2023-10-02 04:45:43] iter = 17940, loss = 1.5543
[2023-10-02 04:45:44] iter = 17950, loss = 1.4396
[2023-10-02 04:45:45] iter = 17960, loss = 1.4164
[2023-10-02 04:45:46] iter = 17970, loss = 1.4955
[2023-10-02 04:45:47] iter = 17980, loss = 1.5159
[2023-10-02 04:45:47] iter = 17990, loss = 1.6246
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 48737}
[2023-10-02 04:46:13] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.016476 train acc = 0.9980, test acc = 0.6274
[2023-10-02 04:46:37] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.012065 train acc = 1.0000, test acc = 0.6228
[2023-10-02 04:47:01] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003737 train acc = 1.0000, test acc = 0.6245
[2023-10-02 04:47:25] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004269 train acc = 1.0000, test acc = 0.6205
[2023-10-02 04:47:50] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.014885 train acc = 1.0000, test acc = 0.6286
[2023-10-02 04:48:14] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011148 train acc = 1.0000, test acc = 0.6277
[2023-10-02 04:48:38] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003298 train acc = 1.0000, test acc = 0.6206
[2023-10-02 04:49:02] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004921 train acc = 1.0000, test acc = 0.6265
[2023-10-02 04:49:26] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.014482 train acc = 0.9980, test acc = 0.6274
[2023-10-02 04:49:50] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.014584 train acc = 0.9980, test acc = 0.6197
[2023-10-02 04:50:15] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.017647 train acc = 1.0000, test acc = 0.6280
[2023-10-02 04:50:39] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.017705 train acc = 0.9960, test acc = 0.6229
[2023-10-02 04:51:03] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.011823 train acc = 1.0000, test acc = 0.6332
[2023-10-02 04:51:27] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.013575 train acc = 1.0000, test acc = 0.6279
[2023-10-02 04:51:51] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002335 train acc = 1.0000, test acc = 0.6213
[2023-10-02 04:52:16] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.003667 train acc = 1.0000, test acc = 0.6309
[2023-10-02 04:52:39] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.005047 train acc = 1.0000, test acc = 0.6268
[2023-10-02 04:53:04] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.019406 train acc = 0.9980, test acc = 0.6327
[2023-10-02 04:53:28] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.012174 train acc = 1.0000, test acc = 0.6290
[2023-10-02 04:53:52] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.003593 train acc = 1.0000, test acc = 0.6292
Evaluate 20 random ConvNet, mean = 0.6264 std = 0.0039
-------------------------
[2023-10-02 04:53:52] iter = 18000, loss = 1.4298
[2023-10-02 04:53:53] iter = 18010, loss = 1.5628
[2023-10-02 04:53:54] iter = 18020, loss = 1.5240
[2023-10-02 04:53:55] iter = 18030, loss = 1.4069
[2023-10-02 04:53:56] iter = 18040, loss = 1.4979
[2023-10-02 04:53:57] iter = 18050, loss = 1.5388
[2023-10-02 04:53:58] iter = 18060, loss = 1.4543
[2023-10-02 04:53:59] iter = 18070, loss = 1.5230
[2023-10-02 04:54:00] iter = 18080, loss = 1.5212
[2023-10-02 04:54:01] iter = 18090, loss = 1.6642
[2023-10-02 04:54:01] iter = 18100, loss = 1.4250
[2023-10-02 04:54:02] iter = 18110, loss = 1.4912
[2023-10-02 04:54:03] iter = 18120, loss = 1.5040
[2023-10-02 04:54:04] iter = 18130, loss = 1.4011
[2023-10-02 04:54:05] iter = 18140, loss = 1.5080
[2023-10-02 04:54:06] iter = 18150, loss = 1.4887
[2023-10-02 04:54:07] iter = 18160, loss = 1.5513
[2023-10-02 04:54:08] iter = 18170, loss = 1.4501
[2023-10-02 04:54:09] iter = 18180, loss = 1.6325
[2023-10-02 04:54:09] iter = 18190, loss = 1.6081
[2023-10-02 04:54:10] iter = 18200, loss = 1.4761
[2023-10-02 04:54:11] iter = 18210, loss = 1.4485
[2023-10-02 04:54:12] iter = 18220, loss = 1.5340
[2023-10-02 04:54:13] iter = 18230, loss = 1.6451
[2023-10-02 04:54:14] iter = 18240, loss = 1.5716
[2023-10-02 04:54:15] iter = 18250, loss = 1.5508
[2023-10-02 04:54:16] iter = 18260, loss = 1.5197
[2023-10-02 04:54:17] iter = 18270, loss = 1.4753
[2023-10-02 04:54:18] iter = 18280, loss = 1.4790
[2023-10-02 04:54:19] iter = 18290, loss = 1.4731
[2023-10-02 04:54:19] iter = 18300, loss = 1.7446
[2023-10-02 04:54:20] iter = 18310, loss = 1.6003
[2023-10-02 04:54:21] iter = 18320, loss = 1.5373
[2023-10-02 04:54:22] iter = 18330, loss = 1.3749
[2023-10-02 04:54:23] iter = 18340, loss = 1.3519
[2023-10-02 04:54:24] iter = 18350, loss = 1.5020
[2023-10-02 04:54:25] iter = 18360, loss = 1.3740
[2023-10-02 04:54:26] iter = 18370, loss = 1.5528
[2023-10-02 04:54:27] iter = 18380, loss = 1.4498
[2023-10-02 04:54:28] iter = 18390, loss = 1.5959
[2023-10-02 04:54:28] iter = 18400, loss = 1.5860
[2023-10-02 04:54:29] iter = 18410, loss = 1.4781
[2023-10-02 04:54:30] iter = 18420, loss = 1.4314
[2023-10-02 04:54:31] iter = 18430, loss = 1.3478
[2023-10-02 04:54:32] iter = 18440, loss = 1.4804
[2023-10-02 04:54:33] iter = 18450, loss = 1.4776
[2023-10-02 04:54:34] iter = 18460, loss = 1.4426
[2023-10-02 04:54:35] iter = 18470, loss = 1.5237
[2023-10-02 04:54:36] iter = 18480, loss = 1.5127
[2023-10-02 04:54:37] iter = 18490, loss = 1.4619
[2023-10-02 04:54:37] iter = 18500, loss = 1.4448
[2023-10-02 04:54:38] iter = 18510, loss = 1.3713
[2023-10-02 04:54:39] iter = 18520, loss = 1.5586
[2023-10-02 04:54:40] iter = 18530, loss = 1.4352
[2023-10-02 04:54:41] iter = 18540, loss = 1.6066
[2023-10-02 04:54:42] iter = 18550, loss = 1.3914
[2023-10-02 04:54:43] iter = 18560, loss = 1.4398
[2023-10-02 04:54:44] iter = 18570, loss = 1.4857
[2023-10-02 04:54:45] iter = 18580, loss = 1.5079
[2023-10-02 04:54:46] iter = 18590, loss = 1.4040
[2023-10-02 04:54:47] iter = 18600, loss = 1.5906
[2023-10-02 04:54:47] iter = 18610, loss = 1.5080
[2023-10-02 04:54:48] iter = 18620, loss = 1.4741
[2023-10-02 04:54:49] iter = 18630, loss = 1.5618
[2023-10-02 04:54:50] iter = 18640, loss = 1.4503
[2023-10-02 04:54:51] iter = 18650, loss = 1.4698
[2023-10-02 04:54:52] iter = 18660, loss = 1.4436
[2023-10-02 04:54:53] iter = 18670, loss = 1.4584
[2023-10-02 04:54:54] iter = 18680, loss = 1.4484
[2023-10-02 04:54:55] iter = 18690, loss = 1.4418
[2023-10-02 04:54:56] iter = 18700, loss = 1.5551
[2023-10-02 04:54:56] iter = 18710, loss = 1.4584
[2023-10-02 04:54:57] iter = 18720, loss = 1.6355
[2023-10-02 04:54:58] iter = 18730, loss = 1.4942
[2023-10-02 04:54:59] iter = 18740, loss = 1.4795
[2023-10-02 04:55:00] iter = 18750, loss = 1.5094
[2023-10-02 04:55:01] iter = 18760, loss = 1.5464
[2023-10-02 04:55:02] iter = 18770, loss = 1.5197
[2023-10-02 04:55:03] iter = 18780, loss = 1.4941
[2023-10-02 04:55:03] iter = 18790, loss = 1.5455
[2023-10-02 04:55:04] iter = 18800, loss = 1.6042
[2023-10-02 04:55:05] iter = 18810, loss = 1.4405
[2023-10-02 04:55:06] iter = 18820, loss = 1.2991
[2023-10-02 04:55:07] iter = 18830, loss = 1.4402
[2023-10-02 04:55:08] iter = 18840, loss = 1.5199
[2023-10-02 04:55:09] iter = 18850, loss = 1.3082
[2023-10-02 04:55:10] iter = 18860, loss = 1.4048
[2023-10-02 04:55:11] iter = 18870, loss = 1.5197
[2023-10-02 04:55:12] iter = 18880, loss = 1.3787
[2023-10-02 04:55:13] iter = 18890, loss = 1.4699
[2023-10-02 04:55:13] iter = 18900, loss = 1.4506
[2023-10-02 04:55:14] iter = 18910, loss = 1.5460
[2023-10-02 04:55:15] iter = 18920, loss = 1.4886
[2023-10-02 04:55:16] iter = 18930, loss = 1.5493
[2023-10-02 04:55:17] iter = 18940, loss = 1.4187
[2023-10-02 04:55:18] iter = 18950, loss = 1.4545
[2023-10-02 04:55:19] iter = 18960, loss = 1.5213
[2023-10-02 04:55:20] iter = 18970, loss = 1.5156
[2023-10-02 04:55:21] iter = 18980, loss = 1.4417
[2023-10-02 04:55:21] iter = 18990, loss = 1.3446
[2023-10-02 04:55:22] iter = 19000, loss = 1.7015
[2023-10-02 04:55:23] iter = 19010, loss = 1.4148
[2023-10-02 04:55:24] iter = 19020, loss = 1.5041
[2023-10-02 04:55:25] iter = 19030, loss = 1.5146
[2023-10-02 04:55:26] iter = 19040, loss = 1.4411
[2023-10-02 04:55:27] iter = 19050, loss = 1.5225
[2023-10-02 04:55:28] iter = 19060, loss = 1.4328
[2023-10-02 04:55:29] iter = 19070, loss = 1.6223
[2023-10-02 04:55:30] iter = 19080, loss = 1.3385
[2023-10-02 04:55:30] iter = 19090, loss = 1.4518
[2023-10-02 04:55:31] iter = 19100, loss = 1.4095
[2023-10-02 04:55:32] iter = 19110, loss = 1.6453
[2023-10-02 04:55:33] iter = 19120, loss = 1.4027
[2023-10-02 04:55:34] iter = 19130, loss = 1.4843
[2023-10-02 04:55:35] iter = 19140, loss = 1.6058
[2023-10-02 04:55:36] iter = 19150, loss = 1.4755
[2023-10-02 04:55:37] iter = 19160, loss = 1.4457
[2023-10-02 04:55:38] iter = 19170, loss = 1.4509
[2023-10-02 04:55:39] iter = 19180, loss = 1.4630
[2023-10-02 04:55:40] iter = 19190, loss = 1.4784
[2023-10-02 04:55:40] iter = 19200, loss = 1.3791
[2023-10-02 04:55:41] iter = 19210, loss = 1.4578
[2023-10-02 04:55:42] iter = 19220, loss = 1.4018
[2023-10-02 04:55:43] iter = 19230, loss = 1.6023
[2023-10-02 04:55:44] iter = 19240, loss = 1.5670
[2023-10-02 04:55:45] iter = 19250, loss = 1.4057
[2023-10-02 04:55:46] iter = 19260, loss = 1.5356
[2023-10-02 04:55:47] iter = 19270, loss = 1.5475
[2023-10-02 04:55:48] iter = 19280, loss = 1.4208
[2023-10-02 04:55:48] iter = 19290, loss = 1.4291
[2023-10-02 04:55:49] iter = 19300, loss = 1.5581
[2023-10-02 04:55:50] iter = 19310, loss = 1.4135
[2023-10-02 04:55:51] iter = 19320, loss = 1.4874
[2023-10-02 04:55:52] iter = 19330, loss = 1.4214
[2023-10-02 04:55:53] iter = 19340, loss = 1.3483
[2023-10-02 04:55:54] iter = 19350, loss = 1.7352
[2023-10-02 04:55:55] iter = 19360, loss = 1.6431
[2023-10-02 04:55:56] iter = 19370, loss = 1.4794
[2023-10-02 04:55:57] iter = 19380, loss = 1.4158
[2023-10-02 04:55:57] iter = 19390, loss = 1.4773
[2023-10-02 04:55:58] iter = 19400, loss = 1.3998
[2023-10-02 04:55:59] iter = 19410, loss = 1.4689
[2023-10-02 04:56:00] iter = 19420, loss = 1.5303
[2023-10-02 04:56:01] iter = 19430, loss = 1.5584
[2023-10-02 04:56:02] iter = 19440, loss = 1.5756
[2023-10-02 04:56:03] iter = 19450, loss = 1.5064
[2023-10-02 04:56:04] iter = 19460, loss = 1.4221
[2023-10-02 04:56:05] iter = 19470, loss = 1.4740
[2023-10-02 04:56:05] iter = 19480, loss = 1.4491
[2023-10-02 04:56:06] iter = 19490, loss = 1.3978
[2023-10-02 04:56:07] iter = 19500, loss = 1.4765
[2023-10-02 04:56:08] iter = 19510, loss = 1.5836
[2023-10-02 04:56:09] iter = 19520, loss = 1.4530
[2023-10-02 04:56:10] iter = 19530, loss = 1.5253
[2023-10-02 04:56:11] iter = 19540, loss = 1.5470
[2023-10-02 04:56:12] iter = 19550, loss = 1.5550
[2023-10-02 04:56:12] iter = 19560, loss = 1.7800
[2023-10-02 04:56:13] iter = 19570, loss = 1.4811
[2023-10-02 04:56:14] iter = 19580, loss = 1.4401
[2023-10-02 04:56:15] iter = 19590, loss = 1.4622
[2023-10-02 04:56:16] iter = 19600, loss = 1.5378
[2023-10-02 04:56:17] iter = 19610, loss = 1.4601
[2023-10-02 04:56:18] iter = 19620, loss = 1.3483
[2023-10-02 04:56:19] iter = 19630, loss = 1.5296
[2023-10-02 04:56:20] iter = 19640, loss = 1.4756
[2023-10-02 04:56:21] iter = 19650, loss = 1.4858
[2023-10-02 04:56:22] iter = 19660, loss = 1.5556
[2023-10-02 04:56:23] iter = 19670, loss = 1.4058
[2023-10-02 04:56:23] iter = 19680, loss = 1.5284
[2023-10-02 04:56:24] iter = 19690, loss = 1.4238
[2023-10-02 04:56:25] iter = 19700, loss = 1.6081
[2023-10-02 04:56:26] iter = 19710, loss = 1.4849
[2023-10-02 04:56:27] iter = 19720, loss = 1.4318
[2023-10-02 04:56:28] iter = 19730, loss = 1.5428
[2023-10-02 04:56:29] iter = 19740, loss = 1.6853
[2023-10-02 04:56:30] iter = 19750, loss = 1.5511
[2023-10-02 04:56:31] iter = 19760, loss = 1.4954
[2023-10-02 04:56:32] iter = 19770, loss = 1.4825
[2023-10-02 04:56:33] iter = 19780, loss = 1.4224
[2023-10-02 04:56:34] iter = 19790, loss = 1.4248
[2023-10-02 04:56:34] iter = 19800, loss = 1.4837
[2023-10-02 04:56:35] iter = 19810, loss = 1.4507
[2023-10-02 04:56:36] iter = 19820, loss = 1.5346
[2023-10-02 04:56:37] iter = 19830, loss = 1.4081
[2023-10-02 04:56:38] iter = 19840, loss = 1.4428
[2023-10-02 04:56:39] iter = 19850, loss = 1.4661
[2023-10-02 04:56:40] iter = 19860, loss = 1.3081
[2023-10-02 04:56:41] iter = 19870, loss = 1.5195
[2023-10-02 04:56:42] iter = 19880, loss = 1.4470
[2023-10-02 04:56:43] iter = 19890, loss = 1.4803
[2023-10-02 04:56:44] iter = 19900, loss = 1.4559
[2023-10-02 04:56:45] iter = 19910, loss = 1.5271
[2023-10-02 04:56:45] iter = 19920, loss = 1.4991
[2023-10-02 04:56:46] iter = 19930, loss = 1.4713
[2023-10-02 04:56:47] iter = 19940, loss = 1.6779
[2023-10-02 04:56:48] iter = 19950, loss = 1.4936
[2023-10-02 04:56:49] iter = 19960, loss = 1.4251
[2023-10-02 04:56:50] iter = 19970, loss = 1.4454
[2023-10-02 04:56:51] iter = 19980, loss = 1.4014
[2023-10-02 04:56:52] iter = 19990, loss = 1.5325
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 13070}
[2023-10-02 04:57:17] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.026123 train acc = 1.0000, test acc = 0.6263
[2023-10-02 04:57:41] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.013850 train acc = 1.0000, test acc = 0.6287
[2023-10-02 04:58:05] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.013530 train acc = 1.0000, test acc = 0.6244
[2023-10-02 04:58:29] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.007130 train acc = 0.9980, test acc = 0.6327
[2023-10-02 04:58:53] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.011298 train acc = 1.0000, test acc = 0.6277
[2023-10-02 04:59:18] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.010745 train acc = 1.0000, test acc = 0.6348
[2023-10-02 04:59:42] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002676 train acc = 1.0000, test acc = 0.6249
[2023-10-02 05:00:06] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.005898 train acc = 1.0000, test acc = 0.6296
[2023-10-02 05:00:30] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.018676 train acc = 1.0000, test acc = 0.6274
[2023-10-02 05:00:54] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002580 train acc = 1.0000, test acc = 0.6263
[2023-10-02 05:01:18] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.020410 train acc = 1.0000, test acc = 0.6257
[2023-10-02 05:01:43] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.001946 train acc = 1.0000, test acc = 0.6276
[2023-10-02 05:02:07] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.028908 train acc = 1.0000, test acc = 0.6272
[2023-10-02 05:02:31] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.014164 train acc = 1.0000, test acc = 0.6329
[2023-10-02 05:02:55] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.006875 train acc = 1.0000, test acc = 0.6315
[2023-10-02 05:03:19] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.006799 train acc = 1.0000, test acc = 0.6261
[2023-10-02 05:03:44] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.002314 train acc = 1.0000, test acc = 0.6207
[2023-10-02 05:04:08] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.007942 train acc = 1.0000, test acc = 0.6184
[2023-10-02 05:04:32] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.003437 train acc = 1.0000, test acc = 0.6256
[2023-10-02 05:04:56] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.001934 train acc = 1.0000, test acc = 0.6238
Evaluate 20 random ConvNet, mean = 0.6271 std = 0.0039
-------------------------
[2023-10-02 05:04:56] iter = 20000, loss = 1.5834

================== Exp 4 ==================
 
Hyper-parameters: 
 {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f7bf1f7b400>, 'dsa': True}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random real images
[2023-10-02 05:05:13] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 96779}
[2023-10-02 05:05:38] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.001623 train acc = 1.0000, test acc = 0.5018
[2023-10-02 05:06:02] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.008522 train acc = 1.0000, test acc = 0.5007
[2023-10-02 05:06:26] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.017932 train acc = 1.0000, test acc = 0.4993
[2023-10-02 05:06:51] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.007414 train acc = 1.0000, test acc = 0.5070
[2023-10-02 05:07:15] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.002450 train acc = 1.0000, test acc = 0.4987
[2023-10-02 05:07:39] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002317 train acc = 1.0000, test acc = 0.5015
[2023-10-02 05:08:03] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.003027 train acc = 1.0000, test acc = 0.5099
[2023-10-02 05:08:28] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003200 train acc = 1.0000, test acc = 0.5104
[2023-10-02 05:08:52] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.000945 train acc = 1.0000, test acc = 0.4965
[2023-10-02 05:09:16] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.004735 train acc = 1.0000, test acc = 0.5012
[2023-10-02 05:09:40] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.007746 train acc = 1.0000, test acc = 0.5012
[2023-10-02 05:10:04] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.000837 train acc = 1.0000, test acc = 0.5088
[2023-10-02 05:10:28] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.010481 train acc = 1.0000, test acc = 0.5011
[2023-10-02 05:10:52] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.008957 train acc = 1.0000, test acc = 0.4993
[2023-10-02 05:11:17] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003217 train acc = 1.0000, test acc = 0.5041
[2023-10-02 05:11:41] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.016185 train acc = 0.9980, test acc = 0.4975
[2023-10-02 05:12:05] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.001027 train acc = 1.0000, test acc = 0.5051
[2023-10-02 05:12:29] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.001133 train acc = 1.0000, test acc = 0.5055
[2023-10-02 05:12:53] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002543 train acc = 1.0000, test acc = 0.5013
[2023-10-02 05:13:18] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.004145 train acc = 1.0000, test acc = 0.5096
Evaluate 20 random ConvNet, mean = 0.5030 std = 0.0042
-------------------------
[2023-10-02 05:13:18] iter = 00000, loss = 6.2941
[2023-10-02 05:13:19] iter = 00010, loss = 5.6932
[2023-10-02 05:13:20] iter = 00020, loss = 4.9384
[2023-10-02 05:13:21] iter = 00030, loss = 5.1042
[2023-10-02 05:13:22] iter = 00040, loss = 3.9127
[2023-10-02 05:13:22] iter = 00050, loss = 3.9645
[2023-10-02 05:13:23] iter = 00060, loss = 3.6951
[2023-10-02 05:13:24] iter = 00070, loss = 3.4142
[2023-10-02 05:13:25] iter = 00080, loss = 3.7522
[2023-10-02 05:13:26] iter = 00090, loss = 3.3923
[2023-10-02 05:13:27] iter = 00100, loss = 3.1087
[2023-10-02 05:13:28] iter = 00110, loss = 3.1755
[2023-10-02 05:13:29] iter = 00120, loss = 3.0263
[2023-10-02 05:13:30] iter = 00130, loss = 2.7915
[2023-10-02 05:13:31] iter = 00140, loss = 3.1820
[2023-10-02 05:13:31] iter = 00150, loss = 2.9691
[2023-10-02 05:13:32] iter = 00160, loss = 3.0513
[2023-10-02 05:13:33] iter = 00170, loss = 2.8978
[2023-10-02 05:13:34] iter = 00180, loss = 3.1202
[2023-10-02 05:13:35] iter = 00190, loss = 2.8689
[2023-10-02 05:13:36] iter = 00200, loss = 3.0176
[2023-10-02 05:13:37] iter = 00210, loss = 2.9030
[2023-10-02 05:13:38] iter = 00220, loss = 2.6913
[2023-10-02 05:13:39] iter = 00230, loss = 2.5802
[2023-10-02 05:13:40] iter = 00240, loss = 2.6924
[2023-10-02 05:13:41] iter = 00250, loss = 2.5521
[2023-10-02 05:13:42] iter = 00260, loss = 2.4186
[2023-10-02 05:13:43] iter = 00270, loss = 2.8739
[2023-10-02 05:13:43] iter = 00280, loss = 2.5264
[2023-10-02 05:13:44] iter = 00290, loss = 2.5606
[2023-10-02 05:13:45] iter = 00300, loss = 2.4908
[2023-10-02 05:13:46] iter = 00310, loss = 2.5696
[2023-10-02 05:13:47] iter = 00320, loss = 2.5345
[2023-10-02 05:13:48] iter = 00330, loss = 2.5316
[2023-10-02 05:13:49] iter = 00340, loss = 2.5619
[2023-10-02 05:13:50] iter = 00350, loss = 2.4629
[2023-10-02 05:13:51] iter = 00360, loss = 2.5395
[2023-10-02 05:13:52] iter = 00370, loss = 2.4320
[2023-10-02 05:13:52] iter = 00380, loss = 2.1947
[2023-10-02 05:13:53] iter = 00390, loss = 2.2659
[2023-10-02 05:13:54] iter = 00400, loss = 2.2795
[2023-10-02 05:13:55] iter = 00410, loss = 2.3032
[2023-10-02 05:13:56] iter = 00420, loss = 2.4382
[2023-10-02 05:13:57] iter = 00430, loss = 2.3556
[2023-10-02 05:13:58] iter = 00440, loss = 2.4178
[2023-10-02 05:13:59] iter = 00450, loss = 2.4214
[2023-10-02 05:14:00] iter = 00460, loss = 2.2053
[2023-10-02 05:14:01] iter = 00470, loss = 2.3116
[2023-10-02 05:14:02] iter = 00480, loss = 2.2764
[2023-10-02 05:14:03] iter = 00490, loss = 2.1848
[2023-10-02 05:14:03] iter = 00500, loss = 2.4990
[2023-10-02 05:14:04] iter = 00510, loss = 2.1930
[2023-10-02 05:14:05] iter = 00520, loss = 2.2076
[2023-10-02 05:14:06] iter = 00530, loss = 2.2739
[2023-10-02 05:14:07] iter = 00540, loss = 2.0978
[2023-10-02 05:14:08] iter = 00550, loss = 2.4136
[2023-10-02 05:14:09] iter = 00560, loss = 2.1974
[2023-10-02 05:14:10] iter = 00570, loss = 2.3867
[2023-10-02 05:14:11] iter = 00580, loss = 2.2396
[2023-10-02 05:14:12] iter = 00590, loss = 2.1130
[2023-10-02 05:14:12] iter = 00600, loss = 2.1086
[2023-10-02 05:14:13] iter = 00610, loss = 2.2114
[2023-10-02 05:14:14] iter = 00620, loss = 2.0090
[2023-10-02 05:14:15] iter = 00630, loss = 1.9824
[2023-10-02 05:14:16] iter = 00640, loss = 2.2484
[2023-10-02 05:14:17] iter = 00650, loss = 2.2009
[2023-10-02 05:14:18] iter = 00660, loss = 2.2278
[2023-10-02 05:14:19] iter = 00670, loss = 2.0140
[2023-10-02 05:14:20] iter = 00680, loss = 2.3694
[2023-10-02 05:14:21] iter = 00690, loss = 2.2654
[2023-10-02 05:14:21] iter = 00700, loss = 2.2078
[2023-10-02 05:14:22] iter = 00710, loss = 2.2366
[2023-10-02 05:14:23] iter = 00720, loss = 2.3789
[2023-10-02 05:14:24] iter = 00730, loss = 2.3371
[2023-10-02 05:14:25] iter = 00740, loss = 2.0707
[2023-10-02 05:14:26] iter = 00750, loss = 2.0935
[2023-10-02 05:14:27] iter = 00760, loss = 2.0674
[2023-10-02 05:14:27] iter = 00770, loss = 2.2108
[2023-10-02 05:14:28] iter = 00780, loss = 2.1775
[2023-10-02 05:14:29] iter = 00790, loss = 2.1106
[2023-10-02 05:14:30] iter = 00800, loss = 2.0400
[2023-10-02 05:14:31] iter = 00810, loss = 2.0543
[2023-10-02 05:14:32] iter = 00820, loss = 1.9682
[2023-10-02 05:14:33] iter = 00830, loss = 2.1923
[2023-10-02 05:14:34] iter = 00840, loss = 2.0524
[2023-10-02 05:14:35] iter = 00850, loss = 2.0323
[2023-10-02 05:14:35] iter = 00860, loss = 2.1435
[2023-10-02 05:14:36] iter = 00870, loss = 2.0743
[2023-10-02 05:14:37] iter = 00880, loss = 2.1445
[2023-10-02 05:14:38] iter = 00890, loss = 2.1393
[2023-10-02 05:14:39] iter = 00900, loss = 2.0565
[2023-10-02 05:14:40] iter = 00910, loss = 2.1651
[2023-10-02 05:14:41] iter = 00920, loss = 2.1562
[2023-10-02 05:14:42] iter = 00930, loss = 2.1740
[2023-10-02 05:14:43] iter = 00940, loss = 2.1710
[2023-10-02 05:14:44] iter = 00950, loss = 2.0777
[2023-10-02 05:14:45] iter = 00960, loss = 2.0667
[2023-10-02 05:14:46] iter = 00970, loss = 2.0087
[2023-10-02 05:14:47] iter = 00980, loss = 2.1526
[2023-10-02 05:14:48] iter = 00990, loss = 2.0116
[2023-10-02 05:14:48] iter = 01000, loss = 2.0651
[2023-10-02 05:14:49] iter = 01010, loss = 2.0743
[2023-10-02 05:14:50] iter = 01020, loss = 2.1809
[2023-10-02 05:14:51] iter = 01030, loss = 2.3663
[2023-10-02 05:14:52] iter = 01040, loss = 1.9415
[2023-10-02 05:14:53] iter = 01050, loss = 1.9588
[2023-10-02 05:14:54] iter = 01060, loss = 2.2582
[2023-10-02 05:14:55] iter = 01070, loss = 2.0224
[2023-10-02 05:14:56] iter = 01080, loss = 2.0106
[2023-10-02 05:14:57] iter = 01090, loss = 2.0713
[2023-10-02 05:14:57] iter = 01100, loss = 2.1783
[2023-10-02 05:14:58] iter = 01110, loss = 1.9220
[2023-10-02 05:14:59] iter = 01120, loss = 2.1081
[2023-10-02 05:15:00] iter = 01130, loss = 1.9032
[2023-10-02 05:15:01] iter = 01140, loss = 2.0183
[2023-10-02 05:15:02] iter = 01150, loss = 2.1839
[2023-10-02 05:15:03] iter = 01160, loss = 2.1351
[2023-10-02 05:15:04] iter = 01170, loss = 1.9735
[2023-10-02 05:15:05] iter = 01180, loss = 1.9605
[2023-10-02 05:15:05] iter = 01190, loss = 2.0719
[2023-10-02 05:15:06] iter = 01200, loss = 2.0183
[2023-10-02 05:15:07] iter = 01210, loss = 2.0794
[2023-10-02 05:15:08] iter = 01220, loss = 2.0851
[2023-10-02 05:15:09] iter = 01230, loss = 2.0049
[2023-10-02 05:15:10] iter = 01240, loss = 2.1040
[2023-10-02 05:15:11] iter = 01250, loss = 1.9647
[2023-10-02 05:15:12] iter = 01260, loss = 1.8823
[2023-10-02 05:15:13] iter = 01270, loss = 1.9774
[2023-10-02 05:15:14] iter = 01280, loss = 2.0267
[2023-10-02 05:15:15] iter = 01290, loss = 1.9340
[2023-10-02 05:15:15] iter = 01300, loss = 1.9794
[2023-10-02 05:15:16] iter = 01310, loss = 1.8611
[2023-10-02 05:15:17] iter = 01320, loss = 1.9607
[2023-10-02 05:15:18] iter = 01330, loss = 1.8631
[2023-10-02 05:15:19] iter = 01340, loss = 1.8947
[2023-10-02 05:15:20] iter = 01350, loss = 2.0320
[2023-10-02 05:15:21] iter = 01360, loss = 1.9633
[2023-10-02 05:15:22] iter = 01370, loss = 1.9155
[2023-10-02 05:15:23] iter = 01380, loss = 1.9282
[2023-10-02 05:15:24] iter = 01390, loss = 1.9132
[2023-10-02 05:15:24] iter = 01400, loss = 1.9667
[2023-10-02 05:15:25] iter = 01410, loss = 1.7623
[2023-10-02 05:15:26] iter = 01420, loss = 1.8893
[2023-10-02 05:15:27] iter = 01430, loss = 1.9834
[2023-10-02 05:15:28] iter = 01440, loss = 1.9775
[2023-10-02 05:15:29] iter = 01450, loss = 1.8593
[2023-10-02 05:15:30] iter = 01460, loss = 1.9610
[2023-10-02 05:15:31] iter = 01470, loss = 2.0030
[2023-10-02 05:15:32] iter = 01480, loss = 1.9387
[2023-10-02 05:15:32] iter = 01490, loss = 1.9999
[2023-10-02 05:15:33] iter = 01500, loss = 1.9334
[2023-10-02 05:15:34] iter = 01510, loss = 2.0300
[2023-10-02 05:15:35] iter = 01520, loss = 1.9518
[2023-10-02 05:15:36] iter = 01530, loss = 2.1282
[2023-10-02 05:15:37] iter = 01540, loss = 1.7800
[2023-10-02 05:15:38] iter = 01550, loss = 1.9246
[2023-10-02 05:15:39] iter = 01560, loss = 1.8993
[2023-10-02 05:15:40] iter = 01570, loss = 1.8673
[2023-10-02 05:15:41] iter = 01580, loss = 1.9194
[2023-10-02 05:15:42] iter = 01590, loss = 1.9332
[2023-10-02 05:15:42] iter = 01600, loss = 1.7029
[2023-10-02 05:15:43] iter = 01610, loss = 1.8541
[2023-10-02 05:15:44] iter = 01620, loss = 1.9263
[2023-10-02 05:15:45] iter = 01630, loss = 1.9349
[2023-10-02 05:15:46] iter = 01640, loss = 2.0182
[2023-10-02 05:15:47] iter = 01650, loss = 1.9645
[2023-10-02 05:15:48] iter = 01660, loss = 1.8606
[2023-10-02 05:15:49] iter = 01670, loss = 2.0446
[2023-10-02 05:15:50] iter = 01680, loss = 1.9141
[2023-10-02 05:15:51] iter = 01690, loss = 1.8853
[2023-10-02 05:15:52] iter = 01700, loss = 1.8415
[2023-10-02 05:15:53] iter = 01710, loss = 1.8967
[2023-10-02 05:15:53] iter = 01720, loss = 1.8734
[2023-10-02 05:15:54] iter = 01730, loss = 1.7537
[2023-10-02 05:15:55] iter = 01740, loss = 1.8195
[2023-10-02 05:15:56] iter = 01750, loss = 1.9879
[2023-10-02 05:15:57] iter = 01760, loss = 1.8773
[2023-10-02 05:15:58] iter = 01770, loss = 1.9389
[2023-10-02 05:15:59] iter = 01780, loss = 2.0018
[2023-10-02 05:16:00] iter = 01790, loss = 1.8653
[2023-10-02 05:16:01] iter = 01800, loss = 1.9140
[2023-10-02 05:16:01] iter = 01810, loss = 1.8697
[2023-10-02 05:16:02] iter = 01820, loss = 2.0967
[2023-10-02 05:16:03] iter = 01830, loss = 1.7118
[2023-10-02 05:16:04] iter = 01840, loss = 1.9933
[2023-10-02 05:16:05] iter = 01850, loss = 1.8363
[2023-10-02 05:16:06] iter = 01860, loss = 1.9270
[2023-10-02 05:16:07] iter = 01870, loss = 1.8589
[2023-10-02 05:16:08] iter = 01880, loss = 1.7791
[2023-10-02 05:16:09] iter = 01890, loss = 1.8270
[2023-10-02 05:16:10] iter = 01900, loss = 1.7271
[2023-10-02 05:16:10] iter = 01910, loss = 1.7810
[2023-10-02 05:16:11] iter = 01920, loss = 2.0293
[2023-10-02 05:16:12] iter = 01930, loss = 1.8222
[2023-10-02 05:16:13] iter = 01940, loss = 1.8530
[2023-10-02 05:16:14] iter = 01950, loss = 1.8032
[2023-10-02 05:16:15] iter = 01960, loss = 1.7784
[2023-10-02 05:16:16] iter = 01970, loss = 1.8812
[2023-10-02 05:16:17] iter = 01980, loss = 1.7755
[2023-10-02 05:16:17] iter = 01990, loss = 1.8720
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 78668}
[2023-10-02 05:16:42] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.002543 train acc = 1.0000, test acc = 0.5862
[2023-10-02 05:17:06] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.002025 train acc = 1.0000, test acc = 0.5917
[2023-10-02 05:17:31] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.003232 train acc = 1.0000, test acc = 0.5942
[2023-10-02 05:17:55] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002703 train acc = 1.0000, test acc = 0.5915
[2023-10-02 05:18:19] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003352 train acc = 1.0000, test acc = 0.5880
[2023-10-02 05:18:43] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002915 train acc = 1.0000, test acc = 0.5895
[2023-10-02 05:19:07] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.015236 train acc = 0.9980, test acc = 0.5905
[2023-10-02 05:19:31] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001144 train acc = 1.0000, test acc = 0.5946
[2023-10-02 05:19:55] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.009151 train acc = 1.0000, test acc = 0.5916
[2023-10-02 05:20:20] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.006494 train acc = 1.0000, test acc = 0.5945
[2023-10-02 05:20:44] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.009200 train acc = 1.0000, test acc = 0.5919
[2023-10-02 05:21:08] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002713 train acc = 1.0000, test acc = 0.5866
[2023-10-02 05:21:32] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004920 train acc = 1.0000, test acc = 0.5858
[2023-10-02 05:21:56] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.005918 train acc = 1.0000, test acc = 0.5920
[2023-10-02 05:22:20] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.003819 train acc = 1.0000, test acc = 0.5986
[2023-10-02 05:22:44] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.017859 train acc = 0.9980, test acc = 0.5926
[2023-10-02 05:23:08] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.001235 train acc = 1.0000, test acc = 0.5932
[2023-10-02 05:23:33] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.010099 train acc = 1.0000, test acc = 0.5845
[2023-10-02 05:23:57] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.007797 train acc = 1.0000, test acc = 0.5917
[2023-10-02 05:24:21] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.000924 train acc = 1.0000, test acc = 0.5911
Evaluate 20 random ConvNet, mean = 0.5910 std = 0.0034
-------------------------
[2023-10-02 05:24:21] iter = 02000, loss = 1.8060
[2023-10-02 05:24:22] iter = 02010, loss = 1.8305
[2023-10-02 05:24:23] iter = 02020, loss = 1.9144
[2023-10-02 05:24:24] iter = 02030, loss = 1.8605
[2023-10-02 05:24:25] iter = 02040, loss = 2.0278
[2023-10-02 05:24:26] iter = 02050, loss = 1.8294
[2023-10-02 05:24:27] iter = 02060, loss = 1.7111
[2023-10-02 05:24:28] iter = 02070, loss = 1.8596
[2023-10-02 05:24:28] iter = 02080, loss = 1.8765
[2023-10-02 05:24:29] iter = 02090, loss = 1.8079
[2023-10-02 05:24:30] iter = 02100, loss = 1.7803
[2023-10-02 05:24:31] iter = 02110, loss = 1.7011
[2023-10-02 05:24:32] iter = 02120, loss = 1.8705
[2023-10-02 05:24:33] iter = 02130, loss = 2.0057
[2023-10-02 05:24:34] iter = 02140, loss = 1.8898
[2023-10-02 05:24:35] iter = 02150, loss = 1.6984
[2023-10-02 05:24:36] iter = 02160, loss = 1.8550
[2023-10-02 05:24:37] iter = 02170, loss = 1.8199
[2023-10-02 05:24:38] iter = 02180, loss = 1.8498
[2023-10-02 05:24:39] iter = 02190, loss = 1.9972
[2023-10-02 05:24:39] iter = 02200, loss = 1.8010
[2023-10-02 05:24:40] iter = 02210, loss = 1.7326
[2023-10-02 05:24:41] iter = 02220, loss = 1.8398
[2023-10-02 05:24:42] iter = 02230, loss = 1.8052
[2023-10-02 05:24:43] iter = 02240, loss = 1.9031
[2023-10-02 05:24:44] iter = 02250, loss = 1.8893
[2023-10-02 05:24:45] iter = 02260, loss = 1.7974
[2023-10-02 05:24:46] iter = 02270, loss = 1.7414
[2023-10-02 05:24:47] iter = 02280, loss = 1.7114
[2023-10-02 05:24:48] iter = 02290, loss = 1.8904
[2023-10-02 05:24:49] iter = 02300, loss = 1.8468
[2023-10-02 05:24:49] iter = 02310, loss = 1.8716
[2023-10-02 05:24:50] iter = 02320, loss = 1.7511
[2023-10-02 05:24:51] iter = 02330, loss = 1.7340
[2023-10-02 05:24:52] iter = 02340, loss = 1.8837
[2023-10-02 05:24:53] iter = 02350, loss = 1.9654
[2023-10-02 05:24:54] iter = 02360, loss = 1.9066
[2023-10-02 05:24:55] iter = 02370, loss = 1.7910
[2023-10-02 05:24:56] iter = 02380, loss = 1.9383
[2023-10-02 05:24:57] iter = 02390, loss = 1.8160
[2023-10-02 05:24:57] iter = 02400, loss = 1.6690
[2023-10-02 05:24:58] iter = 02410, loss = 1.7697
[2023-10-02 05:24:59] iter = 02420, loss = 1.6856
[2023-10-02 05:25:00] iter = 02430, loss = 1.9678
[2023-10-02 05:25:01] iter = 02440, loss = 1.7059
[2023-10-02 05:25:02] iter = 02450, loss = 1.8567
[2023-10-02 05:25:03] iter = 02460, loss = 1.8852
[2023-10-02 05:25:04] iter = 02470, loss = 1.8535
[2023-10-02 05:25:05] iter = 02480, loss = 1.9997
[2023-10-02 05:25:05] iter = 02490, loss = 1.9502
[2023-10-02 05:25:06] iter = 02500, loss = 1.8782
[2023-10-02 05:25:07] iter = 02510, loss = 1.7380
[2023-10-02 05:25:08] iter = 02520, loss = 1.7381
[2023-10-02 05:25:09] iter = 02530, loss = 1.8057
[2023-10-02 05:25:10] iter = 02540, loss = 1.9290
[2023-10-02 05:25:11] iter = 02550, loss = 1.7325
[2023-10-02 05:25:12] iter = 02560, loss = 1.7940
[2023-10-02 05:25:13] iter = 02570, loss = 1.8401
[2023-10-02 05:25:14] iter = 02580, loss = 1.7747
[2023-10-02 05:25:15] iter = 02590, loss = 1.7183
[2023-10-02 05:25:15] iter = 02600, loss = 1.8988
[2023-10-02 05:25:16] iter = 02610, loss = 1.7731
[2023-10-02 05:25:17] iter = 02620, loss = 1.7390
[2023-10-02 05:25:18] iter = 02630, loss = 1.9410
[2023-10-02 05:25:19] iter = 02640, loss = 1.8124
[2023-10-02 05:25:20] iter = 02650, loss = 1.6894
[2023-10-02 05:25:21] iter = 02660, loss = 1.7377
[2023-10-02 05:25:22] iter = 02670, loss = 1.7944
[2023-10-02 05:25:22] iter = 02680, loss = 1.8649
[2023-10-02 05:25:23] iter = 02690, loss = 1.8777
[2023-10-02 05:25:24] iter = 02700, loss = 1.7709
[2023-10-02 05:25:25] iter = 02710, loss = 1.7968
[2023-10-02 05:25:26] iter = 02720, loss = 1.7367
[2023-10-02 05:25:27] iter = 02730, loss = 1.7731
[2023-10-02 05:25:28] iter = 02740, loss = 1.6764
[2023-10-02 05:25:29] iter = 02750, loss = 1.7709
[2023-10-02 05:25:29] iter = 02760, loss = 1.7274
[2023-10-02 05:25:30] iter = 02770, loss = 1.8700
[2023-10-02 05:25:31] iter = 02780, loss = 1.7320
[2023-10-02 05:25:32] iter = 02790, loss = 1.6865
[2023-10-02 05:25:33] iter = 02800, loss = 1.8804
[2023-10-02 05:25:34] iter = 02810, loss = 1.7721
[2023-10-02 05:25:35] iter = 02820, loss = 1.6638
[2023-10-02 05:25:36] iter = 02830, loss = 1.9321
[2023-10-02 05:25:37] iter = 02840, loss = 1.8141
[2023-10-02 05:25:37] iter = 02850, loss = 1.7935
[2023-10-02 05:25:38] iter = 02860, loss = 1.7395
[2023-10-02 05:25:39] iter = 02870, loss = 1.8313
[2023-10-02 05:25:40] iter = 02880, loss = 1.7737
[2023-10-02 05:25:41] iter = 02890, loss = 1.8319
[2023-10-02 05:25:42] iter = 02900, loss = 1.7987
[2023-10-02 05:25:43] iter = 02910, loss = 1.7935
[2023-10-02 05:25:44] iter = 02920, loss = 1.7859
[2023-10-02 05:25:45] iter = 02930, loss = 1.7128
[2023-10-02 05:25:46] iter = 02940, loss = 1.6758
[2023-10-02 05:25:47] iter = 02950, loss = 1.6297
[2023-10-02 05:25:47] iter = 02960, loss = 1.7971
[2023-10-02 05:25:48] iter = 02970, loss = 1.6581
[2023-10-02 05:25:49] iter = 02980, loss = 1.6262
[2023-10-02 05:25:50] iter = 02990, loss = 2.0029
[2023-10-02 05:25:51] iter = 03000, loss = 1.8878
[2023-10-02 05:25:52] iter = 03010, loss = 1.9314
[2023-10-02 05:25:53] iter = 03020, loss = 1.7365
[2023-10-02 05:25:54] iter = 03030, loss = 1.7018
[2023-10-02 05:25:55] iter = 03040, loss = 1.6976
[2023-10-02 05:25:55] iter = 03050, loss = 1.7817
[2023-10-02 05:25:56] iter = 03060, loss = 1.8303
[2023-10-02 05:25:57] iter = 03070, loss = 1.7346
[2023-10-02 05:25:58] iter = 03080, loss = 1.6933
[2023-10-02 05:25:59] iter = 03090, loss = 1.8807
[2023-10-02 05:26:00] iter = 03100, loss = 1.8297
[2023-10-02 05:26:01] iter = 03110, loss = 1.7085
[2023-10-02 05:26:02] iter = 03120, loss = 1.7120
[2023-10-02 05:26:03] iter = 03130, loss = 1.8523
[2023-10-02 05:26:04] iter = 03140, loss = 1.7565
[2023-10-02 05:26:05] iter = 03150, loss = 1.7999
[2023-10-02 05:26:05] iter = 03160, loss = 1.6687
[2023-10-02 05:26:06] iter = 03170, loss = 1.6965
[2023-10-02 05:26:07] iter = 03180, loss = 1.8406
[2023-10-02 05:26:08] iter = 03190, loss = 1.8894
[2023-10-02 05:26:09] iter = 03200, loss = 1.7921
[2023-10-02 05:26:10] iter = 03210, loss = 1.6777
[2023-10-02 05:26:11] iter = 03220, loss = 1.8633
[2023-10-02 05:26:12] iter = 03230, loss = 1.6968
[2023-10-02 05:26:13] iter = 03240, loss = 1.7479
[2023-10-02 05:26:14] iter = 03250, loss = 1.5884
[2023-10-02 05:26:14] iter = 03260, loss = 1.7405
[2023-10-02 05:26:15] iter = 03270, loss = 1.6752
[2023-10-02 05:26:16] iter = 03280, loss = 1.7018
[2023-10-02 05:26:17] iter = 03290, loss = 1.6635
[2023-10-02 05:26:18] iter = 03300, loss = 1.7017
[2023-10-02 05:26:19] iter = 03310, loss = 1.7462
[2023-10-02 05:26:20] iter = 03320, loss = 1.6809
[2023-10-02 05:26:21] iter = 03330, loss = 1.8357
[2023-10-02 05:26:22] iter = 03340, loss = 1.6947
[2023-10-02 05:26:23] iter = 03350, loss = 1.8085
[2023-10-02 05:26:24] iter = 03360, loss = 1.8002
[2023-10-02 05:26:25] iter = 03370, loss = 1.7257
[2023-10-02 05:26:26] iter = 03380, loss = 1.6370
[2023-10-02 05:26:26] iter = 03390, loss = 1.6005
[2023-10-02 05:26:27] iter = 03400, loss = 1.7874
[2023-10-02 05:26:28] iter = 03410, loss = 1.7385
[2023-10-02 05:26:29] iter = 03420, loss = 1.7606
[2023-10-02 05:26:30] iter = 03430, loss = 1.7128
[2023-10-02 05:26:31] iter = 03440, loss = 1.7079
[2023-10-02 05:26:32] iter = 03450, loss = 1.7659
[2023-10-02 05:26:33] iter = 03460, loss = 1.7606
[2023-10-02 05:26:34] iter = 03470, loss = 1.6618
[2023-10-02 05:26:35] iter = 03480, loss = 1.8240
[2023-10-02 05:26:35] iter = 03490, loss = 1.8120
[2023-10-02 05:26:36] iter = 03500, loss = 1.7194
[2023-10-02 05:26:37] iter = 03510, loss = 1.9473
[2023-10-02 05:26:38] iter = 03520, loss = 1.6163
[2023-10-02 05:26:39] iter = 03530, loss = 1.6807
[2023-10-02 05:26:40] iter = 03540, loss = 1.8248
[2023-10-02 05:26:41] iter = 03550, loss = 1.7740
[2023-10-02 05:26:42] iter = 03560, loss = 1.7795
[2023-10-02 05:26:43] iter = 03570, loss = 1.8483
[2023-10-02 05:26:44] iter = 03580, loss = 1.6422
[2023-10-02 05:26:45] iter = 03590, loss = 1.8537
[2023-10-02 05:26:45] iter = 03600, loss = 1.6897
[2023-10-02 05:26:46] iter = 03610, loss = 1.6331
[2023-10-02 05:26:47] iter = 03620, loss = 1.7086
[2023-10-02 05:26:48] iter = 03630, loss = 1.7726
[2023-10-02 05:26:49] iter = 03640, loss = 1.6801
[2023-10-02 05:26:50] iter = 03650, loss = 1.6913
[2023-10-02 05:26:51] iter = 03660, loss = 1.8100
[2023-10-02 05:26:52] iter = 03670, loss = 1.7170
[2023-10-02 05:26:53] iter = 03680, loss = 1.6926
[2023-10-02 05:26:54] iter = 03690, loss = 1.6720
[2023-10-02 05:26:55] iter = 03700, loss = 1.8335
[2023-10-02 05:26:56] iter = 03710, loss = 1.6887
[2023-10-02 05:26:57] iter = 03720, loss = 1.7294
[2023-10-02 05:26:57] iter = 03730, loss = 1.8879
[2023-10-02 05:26:58] iter = 03740, loss = 1.5452
[2023-10-02 05:26:59] iter = 03750, loss = 1.7462
[2023-10-02 05:27:00] iter = 03760, loss = 1.6909
[2023-10-02 05:27:01] iter = 03770, loss = 1.6050
[2023-10-02 05:27:02] iter = 03780, loss = 1.6814
[2023-10-02 05:27:03] iter = 03790, loss = 1.7415
[2023-10-02 05:27:04] iter = 03800, loss = 1.6988
[2023-10-02 05:27:05] iter = 03810, loss = 1.6930
[2023-10-02 05:27:05] iter = 03820, loss = 1.7533
[2023-10-02 05:27:06] iter = 03830, loss = 1.7364
[2023-10-02 05:27:07] iter = 03840, loss = 1.7717
[2023-10-02 05:27:08] iter = 03850, loss = 1.6731
[2023-10-02 05:27:09] iter = 03860, loss = 1.7529
[2023-10-02 05:27:10] iter = 03870, loss = 1.5452
[2023-10-02 05:27:11] iter = 03880, loss = 1.7533
[2023-10-02 05:27:12] iter = 03890, loss = 1.7772
[2023-10-02 05:27:13] iter = 03900, loss = 1.7107
[2023-10-02 05:27:14] iter = 03910, loss = 1.8300
[2023-10-02 05:27:14] iter = 03920, loss = 1.7447
[2023-10-02 05:27:15] iter = 03930, loss = 1.6821
[2023-10-02 05:27:16] iter = 03940, loss = 1.7815
[2023-10-02 05:27:17] iter = 03950, loss = 1.6227
[2023-10-02 05:27:18] iter = 03960, loss = 1.7455
[2023-10-02 05:27:19] iter = 03970, loss = 1.6960
[2023-10-02 05:27:20] iter = 03980, loss = 1.7815
[2023-10-02 05:27:21] iter = 03990, loss = 1.6144
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 41983}
[2023-10-02 05:27:46] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.010649 train acc = 1.0000, test acc = 0.6127
[2023-10-02 05:28:10] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.002799 train acc = 1.0000, test acc = 0.6051
[2023-10-02 05:28:34] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.009580 train acc = 1.0000, test acc = 0.6117
[2023-10-02 05:28:58] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.013498 train acc = 1.0000, test acc = 0.6093
[2023-10-02 05:29:22] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.009278 train acc = 1.0000, test acc = 0.6071
[2023-10-02 05:29:46] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.009416 train acc = 1.0000, test acc = 0.6063
[2023-10-02 05:30:11] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004006 train acc = 1.0000, test acc = 0.6065
[2023-10-02 05:30:35] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.001767 train acc = 1.0000, test acc = 0.5985
[2023-10-02 05:30:59] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.004291 train acc = 1.0000, test acc = 0.6091
[2023-10-02 05:31:24] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.008953 train acc = 1.0000, test acc = 0.6083
[2023-10-02 05:31:48] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.001505 train acc = 1.0000, test acc = 0.6067
[2023-10-02 05:32:12] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.010251 train acc = 1.0000, test acc = 0.6086
[2023-10-02 05:32:36] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.012149 train acc = 1.0000, test acc = 0.6173
[2023-10-02 05:33:00] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.018253 train acc = 1.0000, test acc = 0.6117
[2023-10-02 05:33:25] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.004190 train acc = 1.0000, test acc = 0.6115
[2023-10-02 05:33:49] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004667 train acc = 1.0000, test acc = 0.6054
[2023-10-02 05:34:13] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.008075 train acc = 1.0000, test acc = 0.6065
[2023-10-02 05:34:37] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.010400 train acc = 1.0000, test acc = 0.6078
[2023-10-02 05:35:01] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.001496 train acc = 1.0000, test acc = 0.6072
[2023-10-02 05:35:25] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.016138 train acc = 0.9980, test acc = 0.6065
Evaluate 20 random ConvNet, mean = 0.6082 std = 0.0037
-------------------------
[2023-10-02 05:35:26] iter = 04000, loss = 1.7109
[2023-10-02 05:35:26] iter = 04010, loss = 1.6729
[2023-10-02 05:35:27] iter = 04020, loss = 1.7156
[2023-10-02 05:35:28] iter = 04030, loss = 1.6178
[2023-10-02 05:35:29] iter = 04040, loss = 1.6096
[2023-10-02 05:35:30] iter = 04050, loss = 1.6543
[2023-10-02 05:35:31] iter = 04060, loss = 1.6958
[2023-10-02 05:35:32] iter = 04070, loss = 1.6771
[2023-10-02 05:35:33] iter = 04080, loss = 1.8182
[2023-10-02 05:35:34] iter = 04090, loss = 1.6078
[2023-10-02 05:35:35] iter = 04100, loss = 1.7401
[2023-10-02 05:35:35] iter = 04110, loss = 1.6483
[2023-10-02 05:35:36] iter = 04120, loss = 1.6854
[2023-10-02 05:35:37] iter = 04130, loss = 1.7064
[2023-10-02 05:35:38] iter = 04140, loss = 1.8519
[2023-10-02 05:35:39] iter = 04150, loss = 1.8213
[2023-10-02 05:35:40] iter = 04160, loss = 1.5134
[2023-10-02 05:35:41] iter = 04170, loss = 1.7213
[2023-10-02 05:35:42] iter = 04180, loss = 1.6635
[2023-10-02 05:35:43] iter = 04190, loss = 1.7967
[2023-10-02 05:35:44] iter = 04200, loss = 1.5109
[2023-10-02 05:35:44] iter = 04210, loss = 1.6513
[2023-10-02 05:35:45] iter = 04220, loss = 1.6609
[2023-10-02 05:35:46] iter = 04230, loss = 1.6247
[2023-10-02 05:35:47] iter = 04240, loss = 1.6639
[2023-10-02 05:35:48] iter = 04250, loss = 1.6471
[2023-10-02 05:35:49] iter = 04260, loss = 1.7255
[2023-10-02 05:35:50] iter = 04270, loss = 1.6281
[2023-10-02 05:35:51] iter = 04280, loss = 1.6663
[2023-10-02 05:35:52] iter = 04290, loss = 1.6634
[2023-10-02 05:35:53] iter = 04300, loss = 1.6884
[2023-10-02 05:35:54] iter = 04310, loss = 1.6328
[2023-10-02 05:35:55] iter = 04320, loss = 1.7544
[2023-10-02 05:35:55] iter = 04330, loss = 1.6321
[2023-10-02 05:35:56] iter = 04340, loss = 1.6591
[2023-10-02 05:35:57] iter = 04350, loss = 1.6387
[2023-10-02 05:35:58] iter = 04360, loss = 1.6561
[2023-10-02 05:35:59] iter = 04370, loss = 1.5950
[2023-10-02 05:36:00] iter = 04380, loss = 1.6193
[2023-10-02 05:36:01] iter = 04390, loss = 1.7561
[2023-10-02 05:36:02] iter = 04400, loss = 1.6118
[2023-10-02 05:36:03] iter = 04410, loss = 1.6168
[2023-10-02 05:36:04] iter = 04420, loss = 1.6526
[2023-10-02 05:36:04] iter = 04430, loss = 1.5288
[2023-10-02 05:36:05] iter = 04440, loss = 1.6053
[2023-10-02 05:36:06] iter = 04450, loss = 1.7736
[2023-10-02 05:36:07] iter = 04460, loss = 1.5677
[2023-10-02 05:36:08] iter = 04470, loss = 1.6299
[2023-10-02 05:36:09] iter = 04480, loss = 1.6719
[2023-10-02 05:36:10] iter = 04490, loss = 1.4998
[2023-10-02 05:36:11] iter = 04500, loss = 1.6145
[2023-10-02 05:36:12] iter = 04510, loss = 1.7556
[2023-10-02 05:36:13] iter = 04520, loss = 1.5729
[2023-10-02 05:36:13] iter = 04530, loss = 1.6060
[2023-10-02 05:36:14] iter = 04540, loss = 1.7031
[2023-10-02 05:36:15] iter = 04550, loss = 1.6882
[2023-10-02 05:36:16] iter = 04560, loss = 1.5917
[2023-10-02 05:36:17] iter = 04570, loss = 1.6961
[2023-10-02 05:36:18] iter = 04580, loss = 1.6823
[2023-10-02 05:36:19] iter = 04590, loss = 1.8630
[2023-10-02 05:36:20] iter = 04600, loss = 1.8533
[2023-10-02 05:36:21] iter = 04610, loss = 1.5959
[2023-10-02 05:36:22] iter = 04620, loss = 1.5290
[2023-10-02 05:36:23] iter = 04630, loss = 1.5803
[2023-10-02 05:36:23] iter = 04640, loss = 1.6684
[2023-10-02 05:36:24] iter = 04650, loss = 1.6510
[2023-10-02 05:36:25] iter = 04660, loss = 1.5484
[2023-10-02 05:36:26] iter = 04670, loss = 1.6457
[2023-10-02 05:36:27] iter = 04680, loss = 1.6838
[2023-10-02 05:36:28] iter = 04690, loss = 1.7908
[2023-10-02 05:36:29] iter = 04700, loss = 1.4533
[2023-10-02 05:36:30] iter = 04710, loss = 1.6413
[2023-10-02 05:36:31] iter = 04720, loss = 1.7169
[2023-10-02 05:36:32] iter = 04730, loss = 1.7912
[2023-10-02 05:36:32] iter = 04740, loss = 1.5377
[2023-10-02 05:36:33] iter = 04750, loss = 1.8813
[2023-10-02 05:36:34] iter = 04760, loss = 1.6441
[2023-10-02 05:36:35] iter = 04770, loss = 1.5826
[2023-10-02 05:36:36] iter = 04780, loss = 1.6638
[2023-10-02 05:36:37] iter = 04790, loss = 1.6622
[2023-10-02 05:36:38] iter = 04800, loss = 1.6837
[2023-10-02 05:36:39] iter = 04810, loss = 1.6403
[2023-10-02 05:36:40] iter = 04820, loss = 1.7029
[2023-10-02 05:36:41] iter = 04830, loss = 1.6321
[2023-10-02 05:36:41] iter = 04840, loss = 1.7122
[2023-10-02 05:36:42] iter = 04850, loss = 1.6416
[2023-10-02 05:36:43] iter = 04860, loss = 1.6370
[2023-10-02 05:36:44] iter = 04870, loss = 1.5970
[2023-10-02 05:36:45] iter = 04880, loss = 1.6597
[2023-10-02 05:36:46] iter = 04890, loss = 1.6871
[2023-10-02 05:36:47] iter = 04900, loss = 1.5638
[2023-10-02 05:36:48] iter = 04910, loss = 1.7142
[2023-10-02 05:36:49] iter = 04920, loss = 1.6623
[2023-10-02 05:36:49] iter = 04930, loss = 1.7088
[2023-10-02 05:36:50] iter = 04940, loss = 1.5653
[2023-10-02 05:36:51] iter = 04950, loss = 1.6802
[2023-10-02 05:36:52] iter = 04960, loss = 1.9164
[2023-10-02 05:36:53] iter = 04970, loss = 1.7699
[2023-10-02 05:36:54] iter = 04980, loss = 1.7020
[2023-10-02 05:36:55] iter = 04990, loss = 1.6326
[2023-10-02 05:36:56] iter = 05000, loss = 1.5405
[2023-10-02 05:36:57] iter = 05010, loss = 1.5659
[2023-10-02 05:36:58] iter = 05020, loss = 1.8099
[2023-10-02 05:36:59] iter = 05030, loss = 1.5277
[2023-10-02 05:36:59] iter = 05040, loss = 1.7900
[2023-10-02 05:37:00] iter = 05050, loss = 1.7047
[2023-10-02 05:37:01] iter = 05060, loss = 1.8182
[2023-10-02 05:37:02] iter = 05070, loss = 1.6767
[2023-10-02 05:37:03] iter = 05080, loss = 1.6393
[2023-10-02 05:37:04] iter = 05090, loss = 1.5961
[2023-10-02 05:37:05] iter = 05100, loss = 1.5299
[2023-10-02 05:37:06] iter = 05110, loss = 1.5099
[2023-10-02 05:37:07] iter = 05120, loss = 1.7793
[2023-10-02 05:37:08] iter = 05130, loss = 1.5932
[2023-10-02 05:37:09] iter = 05140, loss = 1.7347
[2023-10-02 05:37:09] iter = 05150, loss = 1.6327
[2023-10-02 05:37:10] iter = 05160, loss = 1.6955
[2023-10-02 05:37:11] iter = 05170, loss = 1.5763
[2023-10-02 05:37:12] iter = 05180, loss = 1.7618
[2023-10-02 05:37:13] iter = 05190, loss = 1.5988
[2023-10-02 05:37:14] iter = 05200, loss = 1.7160
[2023-10-02 05:37:15] iter = 05210, loss = 1.6058
[2023-10-02 05:37:16] iter = 05220, loss = 1.7459
[2023-10-02 05:37:17] iter = 05230, loss = 1.7637
[2023-10-02 05:37:17] iter = 05240, loss = 1.7951
[2023-10-02 05:37:18] iter = 05250, loss = 1.5776
[2023-10-02 05:37:19] iter = 05260, loss = 1.7055
[2023-10-02 05:37:20] iter = 05270, loss = 1.5875
[2023-10-02 05:37:21] iter = 05280, loss = 1.5672
[2023-10-02 05:37:22] iter = 05290, loss = 1.6623
[2023-10-02 05:37:23] iter = 05300, loss = 1.5804
[2023-10-02 05:37:24] iter = 05310, loss = 1.5593
[2023-10-02 05:37:25] iter = 05320, loss = 1.4971
[2023-10-02 05:37:25] iter = 05330, loss = 1.6276
[2023-10-02 05:37:26] iter = 05340, loss = 1.7547
[2023-10-02 05:37:27] iter = 05350, loss = 1.7636
[2023-10-02 05:37:28] iter = 05360, loss = 1.5749
[2023-10-02 05:37:29] iter = 05370, loss = 1.6028
[2023-10-02 05:37:30] iter = 05380, loss = 1.6382
[2023-10-02 05:37:31] iter = 05390, loss = 1.6211
[2023-10-02 05:37:32] iter = 05400, loss = 1.7685
[2023-10-02 05:37:33] iter = 05410, loss = 1.7619
[2023-10-02 05:37:33] iter = 05420, loss = 1.6087
[2023-10-02 05:37:34] iter = 05430, loss = 1.6798
[2023-10-02 05:37:35] iter = 05440, loss = 1.6512
[2023-10-02 05:37:36] iter = 05450, loss = 1.7174
[2023-10-02 05:37:37] iter = 05460, loss = 1.6620
[2023-10-02 05:37:38] iter = 05470, loss = 1.5800
[2023-10-02 05:37:39] iter = 05480, loss = 1.5616
[2023-10-02 05:37:40] iter = 05490, loss = 1.6162
[2023-10-02 05:37:41] iter = 05500, loss = 1.6318
[2023-10-02 05:37:42] iter = 05510, loss = 1.6439
[2023-10-02 05:37:43] iter = 05520, loss = 1.6960
[2023-10-02 05:37:43] iter = 05530, loss = 1.5841
[2023-10-02 05:37:44] iter = 05540, loss = 1.6496
[2023-10-02 05:37:45] iter = 05550, loss = 1.6037
[2023-10-02 05:37:46] iter = 05560, loss = 1.6940
[2023-10-02 05:37:47] iter = 05570, loss = 1.6114
[2023-10-02 05:37:48] iter = 05580, loss = 1.6527
[2023-10-02 05:37:49] iter = 05590, loss = 1.7701
[2023-10-02 05:37:50] iter = 05600, loss = 1.6491
[2023-10-02 05:37:51] iter = 05610, loss = 1.7035
[2023-10-02 05:37:51] iter = 05620, loss = 1.6268
[2023-10-02 05:37:52] iter = 05630, loss = 1.5767
[2023-10-02 05:37:53] iter = 05640, loss = 1.6935
[2023-10-02 05:37:54] iter = 05650, loss = 1.7266
[2023-10-02 05:37:55] iter = 05660, loss = 1.6314
[2023-10-02 05:37:56] iter = 05670, loss = 1.6339
[2023-10-02 05:37:57] iter = 05680, loss = 1.5686
[2023-10-02 05:37:58] iter = 05690, loss = 1.4489
[2023-10-02 05:37:59] iter = 05700, loss = 1.7054
[2023-10-02 05:37:59] iter = 05710, loss = 1.6391
[2023-10-02 05:38:00] iter = 05720, loss = 1.6517
[2023-10-02 05:38:01] iter = 05730, loss = 1.5796
[2023-10-02 05:38:02] iter = 05740, loss = 1.6008
[2023-10-02 05:38:03] iter = 05750, loss = 1.6224
[2023-10-02 05:38:04] iter = 05760, loss = 1.6183
[2023-10-02 05:38:05] iter = 05770, loss = 1.5736
[2023-10-02 05:38:06] iter = 05780, loss = 1.8765
[2023-10-02 05:38:07] iter = 05790, loss = 1.5386
[2023-10-02 05:38:07] iter = 05800, loss = 1.5933
[2023-10-02 05:38:08] iter = 05810, loss = 1.6160
[2023-10-02 05:38:09] iter = 05820, loss = 1.5992
[2023-10-02 05:38:10] iter = 05830, loss = 1.6078
[2023-10-02 05:38:11] iter = 05840, loss = 1.5632
[2023-10-02 05:38:12] iter = 05850, loss = 1.5331
[2023-10-02 05:38:13] iter = 05860, loss = 1.6477
[2023-10-02 05:38:14] iter = 05870, loss = 1.6279
[2023-10-02 05:38:15] iter = 05880, loss = 1.5709
[2023-10-02 05:38:16] iter = 05890, loss = 1.7810
[2023-10-02 05:38:17] iter = 05900, loss = 1.5896
[2023-10-02 05:38:17] iter = 05910, loss = 1.6547
[2023-10-02 05:38:18] iter = 05920, loss = 1.6438
[2023-10-02 05:38:19] iter = 05930, loss = 1.4390
[2023-10-02 05:38:20] iter = 05940, loss = 1.6663
[2023-10-02 05:38:21] iter = 05950, loss = 1.6513
[2023-10-02 05:38:22] iter = 05960, loss = 1.8289
[2023-10-02 05:38:23] iter = 05970, loss = 1.5957
[2023-10-02 05:38:24] iter = 05980, loss = 1.6877
[2023-10-02 05:38:25] iter = 05990, loss = 1.7365
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 5934}
[2023-10-02 05:38:49] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.006652 train acc = 1.0000, test acc = 0.6147
[2023-10-02 05:39:14] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.022341 train acc = 1.0000, test acc = 0.6169
[2023-10-02 05:39:38] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002942 train acc = 1.0000, test acc = 0.6182
[2023-10-02 05:40:02] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001788 train acc = 1.0000, test acc = 0.6091
[2023-10-02 05:40:26] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003432 train acc = 1.0000, test acc = 0.6141
[2023-10-02 05:40:50] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013388 train acc = 1.0000, test acc = 0.6180
[2023-10-02 05:41:15] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.007958 train acc = 1.0000, test acc = 0.6134
[2023-10-02 05:41:39] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.014320 train acc = 1.0000, test acc = 0.6162
[2023-10-02 05:42:03] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.021089 train acc = 1.0000, test acc = 0.6134
[2023-10-02 05:42:27] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.003499 train acc = 1.0000, test acc = 0.6135
[2023-10-02 05:42:51] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.003666 train acc = 1.0000, test acc = 0.6171
[2023-10-02 05:43:15] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.003577 train acc = 1.0000, test acc = 0.6202
[2023-10-02 05:43:40] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.025959 train acc = 1.0000, test acc = 0.6203
[2023-10-02 05:44:04] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.016950 train acc = 0.9980, test acc = 0.6119
[2023-10-02 05:44:28] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.026716 train acc = 1.0000, test acc = 0.6094
[2023-10-02 05:44:52] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.013621 train acc = 1.0000, test acc = 0.6155
[2023-10-02 05:45:16] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.010863 train acc = 1.0000, test acc = 0.6143
[2023-10-02 05:45:41] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003444 train acc = 1.0000, test acc = 0.6128
[2023-10-02 05:46:05] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.006516 train acc = 0.9980, test acc = 0.6180
[2023-10-02 05:46:29] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.006797 train acc = 1.0000, test acc = 0.6152
Evaluate 20 random ConvNet, mean = 0.6151 std = 0.0030
-------------------------
[2023-10-02 05:46:29] iter = 06000, loss = 1.6602
[2023-10-02 05:46:30] iter = 06010, loss = 1.6020
[2023-10-02 05:46:31] iter = 06020, loss = 1.5602
[2023-10-02 05:46:32] iter = 06030, loss = 1.7345
[2023-10-02 05:46:33] iter = 06040, loss = 1.4395
[2023-10-02 05:46:34] iter = 06050, loss = 1.6341
[2023-10-02 05:46:34] iter = 06060, loss = 1.5957
[2023-10-02 05:46:35] iter = 06070, loss = 1.4680
[2023-10-02 05:46:36] iter = 06080, loss = 1.6111
[2023-10-02 05:46:37] iter = 06090, loss = 1.5911
[2023-10-02 05:46:38] iter = 06100, loss = 1.7506
[2023-10-02 05:46:39] iter = 06110, loss = 1.6545
[2023-10-02 05:46:40] iter = 06120, loss = 1.7284
[2023-10-02 05:46:41] iter = 06130, loss = 1.6263
[2023-10-02 05:46:42] iter = 06140, loss = 1.7644
[2023-10-02 05:46:43] iter = 06150, loss = 1.7086
[2023-10-02 05:46:44] iter = 06160, loss = 1.5359
[2023-10-02 05:46:44] iter = 06170, loss = 1.5592
[2023-10-02 05:46:45] iter = 06180, loss = 1.6527
[2023-10-02 05:46:46] iter = 06190, loss = 1.6128
[2023-10-02 05:46:47] iter = 06200, loss = 1.6784
[2023-10-02 05:46:48] iter = 06210, loss = 1.7834
[2023-10-02 05:46:49] iter = 06220, loss = 1.6226
[2023-10-02 05:46:50] iter = 06230, loss = 1.5863
[2023-10-02 05:46:51] iter = 06240, loss = 1.4044
[2023-10-02 05:46:52] iter = 06250, loss = 1.6656
[2023-10-02 05:46:53] iter = 06260, loss = 1.5824
[2023-10-02 05:46:53] iter = 06270, loss = 1.8137
[2023-10-02 05:46:54] iter = 06280, loss = 1.6650
[2023-10-02 05:46:55] iter = 06290, loss = 1.4928
[2023-10-02 05:46:56] iter = 06300, loss = 1.6482
[2023-10-02 05:46:57] iter = 06310, loss = 1.5792
[2023-10-02 05:46:58] iter = 06320, loss = 1.6355
[2023-10-02 05:46:59] iter = 06330, loss = 1.6686
[2023-10-02 05:47:00] iter = 06340, loss = 1.5555
[2023-10-02 05:47:01] iter = 06350, loss = 1.5708
[2023-10-02 05:47:02] iter = 06360, loss = 1.6751
[2023-10-02 05:47:02] iter = 06370, loss = 1.4594
[2023-10-02 05:47:03] iter = 06380, loss = 1.4778
[2023-10-02 05:47:04] iter = 06390, loss = 1.6051
[2023-10-02 05:47:05] iter = 06400, loss = 1.5665
[2023-10-02 05:47:06] iter = 06410, loss = 1.7129
[2023-10-02 05:47:07] iter = 06420, loss = 1.7142
[2023-10-02 05:47:08] iter = 06430, loss = 1.5595
[2023-10-02 05:47:09] iter = 06440, loss = 1.5497
[2023-10-02 05:47:10] iter = 06450, loss = 1.5296
[2023-10-02 05:47:11] iter = 06460, loss = 1.5565
[2023-10-02 05:47:12] iter = 06470, loss = 1.7183
[2023-10-02 05:47:12] iter = 06480, loss = 1.6136
[2023-10-02 05:47:13] iter = 06490, loss = 1.6322
[2023-10-02 05:47:14] iter = 06500, loss = 1.6055
[2023-10-02 05:47:15] iter = 06510, loss = 1.5206
[2023-10-02 05:47:16] iter = 06520, loss = 1.6686
[2023-10-02 05:47:17] iter = 06530, loss = 1.6826
[2023-10-02 05:47:18] iter = 06540, loss = 1.6691
[2023-10-02 05:47:19] iter = 06550, loss = 1.6766
[2023-10-02 05:47:20] iter = 06560, loss = 1.6056
[2023-10-02 05:47:21] iter = 06570, loss = 1.5675
[2023-10-02 05:47:22] iter = 06580, loss = 1.6230
[2023-10-02 05:47:23] iter = 06590, loss = 1.5632
[2023-10-02 05:47:23] iter = 06600, loss = 1.6510
[2023-10-02 05:47:24] iter = 06610, loss = 1.5918
[2023-10-02 05:47:25] iter = 06620, loss = 1.5596
[2023-10-02 05:47:26] iter = 06630, loss = 1.4445
[2023-10-02 05:47:27] iter = 06640, loss = 1.5312
[2023-10-02 05:47:28] iter = 06650, loss = 1.5418
[2023-10-02 05:47:29] iter = 06660, loss = 1.5750
[2023-10-02 05:47:30] iter = 06670, loss = 1.5816
[2023-10-02 05:47:31] iter = 06680, loss = 1.4759
[2023-10-02 05:47:32] iter = 06690, loss = 1.4844
[2023-10-02 05:47:33] iter = 06700, loss = 1.6977
[2023-10-02 05:47:33] iter = 06710, loss = 1.6448
[2023-10-02 05:47:34] iter = 06720, loss = 1.6875
[2023-10-02 05:47:35] iter = 06730, loss = 1.6450
[2023-10-02 05:47:36] iter = 06740, loss = 1.7660
[2023-10-02 05:47:37] iter = 06750, loss = 1.7454
[2023-10-02 05:47:38] iter = 06760, loss = 1.5072
[2023-10-02 05:47:39] iter = 06770, loss = 1.5377
[2023-10-02 05:47:40] iter = 06780, loss = 1.5033
[2023-10-02 05:47:41] iter = 06790, loss = 1.5584
[2023-10-02 05:47:41] iter = 06800, loss = 1.5016
[2023-10-02 05:47:42] iter = 06810, loss = 1.5448
[2023-10-02 05:47:43] iter = 06820, loss = 1.7597
[2023-10-02 05:47:44] iter = 06830, loss = 1.6400
[2023-10-02 05:47:45] iter = 06840, loss = 1.5465
[2023-10-02 05:47:46] iter = 06850, loss = 1.6423
[2023-10-02 05:47:47] iter = 06860, loss = 1.6404
[2023-10-02 05:47:48] iter = 06870, loss = 1.5942
[2023-10-02 05:47:49] iter = 06880, loss = 1.6376
[2023-10-02 05:47:50] iter = 06890, loss = 1.5302
[2023-10-02 05:47:50] iter = 06900, loss = 1.6175
[2023-10-02 05:47:51] iter = 06910, loss = 1.6463
[2023-10-02 05:47:52] iter = 06920, loss = 1.6667
[2023-10-02 05:47:53] iter = 06930, loss = 1.5285
[2023-10-02 05:47:54] iter = 06940, loss = 1.6472
[2023-10-02 05:47:55] iter = 06950, loss = 1.5666
[2023-10-02 05:47:56] iter = 06960, loss = 1.6719
[2023-10-02 05:47:57] iter = 06970, loss = 1.6056
[2023-10-02 05:47:58] iter = 06980, loss = 1.4851
[2023-10-02 05:47:59] iter = 06990, loss = 1.4538
[2023-10-02 05:47:59] iter = 07000, loss = 1.7750
[2023-10-02 05:48:00] iter = 07010, loss = 1.4787
[2023-10-02 05:48:01] iter = 07020, loss = 1.7130
[2023-10-02 05:48:02] iter = 07030, loss = 1.5515
[2023-10-02 05:48:03] iter = 07040, loss = 1.5595
[2023-10-02 05:48:04] iter = 07050, loss = 1.4659
[2023-10-02 05:48:05] iter = 07060, loss = 1.4976
[2023-10-02 05:48:06] iter = 07070, loss = 1.6046
[2023-10-02 05:48:07] iter = 07080, loss = 1.5812
[2023-10-02 05:48:08] iter = 07090, loss = 1.7155
[2023-10-02 05:48:09] iter = 07100, loss = 1.6104
[2023-10-02 05:48:10] iter = 07110, loss = 1.6325
[2023-10-02 05:48:10] iter = 07120, loss = 1.5444
[2023-10-02 05:48:11] iter = 07130, loss = 1.6478
[2023-10-02 05:48:12] iter = 07140, loss = 1.4841
[2023-10-02 05:48:13] iter = 07150, loss = 1.5856
[2023-10-02 05:48:14] iter = 07160, loss = 1.7338
[2023-10-02 05:48:15] iter = 07170, loss = 1.5417
[2023-10-02 05:48:16] iter = 07180, loss = 1.6329
[2023-10-02 05:48:17] iter = 07190, loss = 1.5386
[2023-10-02 05:48:18] iter = 07200, loss = 1.7425
[2023-10-02 05:48:19] iter = 07210, loss = 1.5271
[2023-10-02 05:48:19] iter = 07220, loss = 1.5784
[2023-10-02 05:48:20] iter = 07230, loss = 1.7163
[2023-10-02 05:48:21] iter = 07240, loss = 1.6866
[2023-10-02 05:48:22] iter = 07250, loss = 1.6303
[2023-10-02 05:48:23] iter = 07260, loss = 1.6089
[2023-10-02 05:48:24] iter = 07270, loss = 1.5295
[2023-10-02 05:48:25] iter = 07280, loss = 1.5022
[2023-10-02 05:48:26] iter = 07290, loss = 1.4730
[2023-10-02 05:48:27] iter = 07300, loss = 1.4389
[2023-10-02 05:48:28] iter = 07310, loss = 1.6490
[2023-10-02 05:48:29] iter = 07320, loss = 1.5451
[2023-10-02 05:48:29] iter = 07330, loss = 1.6528
[2023-10-02 05:48:30] iter = 07340, loss = 1.7290
[2023-10-02 05:48:31] iter = 07350, loss = 1.7242
[2023-10-02 05:48:32] iter = 07360, loss = 1.7671
[2023-10-02 05:48:33] iter = 07370, loss = 1.6158
[2023-10-02 05:48:34] iter = 07380, loss = 1.5400
[2023-10-02 05:48:35] iter = 07390, loss = 1.6807
[2023-10-02 05:48:36] iter = 07400, loss = 1.6231
[2023-10-02 05:48:37] iter = 07410, loss = 1.4790
[2023-10-02 05:48:38] iter = 07420, loss = 1.7272
[2023-10-02 05:48:39] iter = 07430, loss = 1.7235
[2023-10-02 05:48:39] iter = 07440, loss = 1.4451
[2023-10-02 05:48:40] iter = 07450, loss = 1.6422
[2023-10-02 05:48:41] iter = 07460, loss = 1.5369
[2023-10-02 05:48:42] iter = 07470, loss = 1.4924
[2023-10-02 05:48:43] iter = 07480, loss = 1.5851
[2023-10-02 05:48:44] iter = 07490, loss = 1.6198
[2023-10-02 05:48:45] iter = 07500, loss = 1.6198
[2023-10-02 05:48:45] iter = 07510, loss = 1.4756
[2023-10-02 05:48:46] iter = 07520, loss = 1.5284
[2023-10-02 05:48:47] iter = 07530, loss = 1.5812
[2023-10-02 05:48:48] iter = 07540, loss = 1.5690
[2023-10-02 05:48:49] iter = 07550, loss = 1.5791
[2023-10-02 05:48:50] iter = 07560, loss = 1.4766
[2023-10-02 05:48:51] iter = 07570, loss = 1.5664
[2023-10-02 05:48:52] iter = 07580, loss = 1.5394
[2023-10-02 05:48:53] iter = 07590, loss = 1.8867
[2023-10-02 05:48:54] iter = 07600, loss = 1.6139
[2023-10-02 05:48:55] iter = 07610, loss = 1.5894
[2023-10-02 05:48:56] iter = 07620, loss = 1.5592
[2023-10-02 05:48:56] iter = 07630, loss = 1.6017
[2023-10-02 05:48:57] iter = 07640, loss = 1.6636
[2023-10-02 05:48:58] iter = 07650, loss = 1.6218
[2023-10-02 05:48:59] iter = 07660, loss = 1.6038
[2023-10-02 05:49:00] iter = 07670, loss = 1.5841
[2023-10-02 05:49:01] iter = 07680, loss = 1.4960
[2023-10-02 05:49:02] iter = 07690, loss = 1.5968
[2023-10-02 05:49:03] iter = 07700, loss = 1.5965
[2023-10-02 05:49:04] iter = 07710, loss = 1.5069
[2023-10-02 05:49:05] iter = 07720, loss = 1.6261
[2023-10-02 05:49:06] iter = 07730, loss = 1.5165
[2023-10-02 05:49:07] iter = 07740, loss = 1.5123
[2023-10-02 05:49:08] iter = 07750, loss = 1.6850
[2023-10-02 05:49:08] iter = 07760, loss = 1.5326
[2023-10-02 05:49:09] iter = 07770, loss = 1.5716
[2023-10-02 05:49:10] iter = 07780, loss = 1.6894
[2023-10-02 05:49:11] iter = 07790, loss = 1.5599
[2023-10-02 05:49:12] iter = 07800, loss = 1.5039
[2023-10-02 05:49:13] iter = 07810, loss = 1.5400
[2023-10-02 05:49:14] iter = 07820, loss = 1.5177
[2023-10-02 05:49:15] iter = 07830, loss = 1.5457
[2023-10-02 05:49:16] iter = 07840, loss = 1.7109
[2023-10-02 05:49:16] iter = 07850, loss = 1.6661
[2023-10-02 05:49:17] iter = 07860, loss = 1.5013
[2023-10-02 05:49:18] iter = 07870, loss = 1.6133
[2023-10-02 05:49:19] iter = 07880, loss = 1.6347
[2023-10-02 05:49:20] iter = 07890, loss = 1.6226
[2023-10-02 05:49:21] iter = 07900, loss = 1.5573
[2023-10-02 05:49:22] iter = 07910, loss = 1.6780
[2023-10-02 05:49:23] iter = 07920, loss = 1.5233
[2023-10-02 05:49:24] iter = 07930, loss = 1.5463
[2023-10-02 05:49:24] iter = 07940, loss = 1.5560
[2023-10-02 05:49:25] iter = 07950, loss = 1.6552
[2023-10-02 05:49:26] iter = 07960, loss = 1.5756
[2023-10-02 05:49:27] iter = 07970, loss = 1.4594
[2023-10-02 05:49:28] iter = 07980, loss = 1.7829
[2023-10-02 05:49:29] iter = 07990, loss = 1.6307
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 70061}
[2023-10-02 05:49:54] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.022652 train acc = 0.9980, test acc = 0.6168
[2023-10-02 05:50:18] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.001687 train acc = 1.0000, test acc = 0.6189
[2023-10-02 05:50:42] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.016055 train acc = 0.9980, test acc = 0.6224
[2023-10-02 05:51:06] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.001450 train acc = 1.0000, test acc = 0.6130
[2023-10-02 05:51:30] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.005526 train acc = 1.0000, test acc = 0.6133
[2023-10-02 05:51:55] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.013317 train acc = 1.0000, test acc = 0.6216
[2023-10-02 05:52:19] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.017333 train acc = 1.0000, test acc = 0.6200
[2023-10-02 05:52:43] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.004907 train acc = 1.0000, test acc = 0.6257
[2023-10-02 05:53:07] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.012104 train acc = 1.0000, test acc = 0.6230
[2023-10-02 05:53:31] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002629 train acc = 1.0000, test acc = 0.6180
[2023-10-02 05:53:55] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.002130 train acc = 1.0000, test acc = 0.6156
[2023-10-02 05:54:19] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.002263 train acc = 1.0000, test acc = 0.6205
[2023-10-02 05:54:43] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.002462 train acc = 1.0000, test acc = 0.6180
[2023-10-02 05:55:07] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.011353 train acc = 1.0000, test acc = 0.6291
[2023-10-02 05:55:31] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.008729 train acc = 1.0000, test acc = 0.6171
[2023-10-02 05:55:56] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004549 train acc = 1.0000, test acc = 0.6229
[2023-10-02 05:56:20] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.012056 train acc = 1.0000, test acc = 0.6230
[2023-10-02 05:56:44] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.012232 train acc = 1.0000, test acc = 0.6089
[2023-10-02 05:57:08] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.011120 train acc = 1.0000, test acc = 0.6233
[2023-10-02 05:57:32] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.001700 train acc = 1.0000, test acc = 0.6188
Evaluate 20 random ConvNet, mean = 0.6195 std = 0.0046
-------------------------
[2023-10-02 05:57:32] iter = 08000, loss = 1.6883
[2023-10-02 05:57:33] iter = 08010, loss = 1.6268
[2023-10-02 05:57:34] iter = 08020, loss = 1.5185
[2023-10-02 05:57:35] iter = 08030, loss = 1.5626
[2023-10-02 05:57:36] iter = 08040, loss = 1.6128
[2023-10-02 05:57:37] iter = 08050, loss = 1.6297
[2023-10-02 05:57:38] iter = 08060, loss = 1.5763
[2023-10-02 05:57:38] iter = 08070, loss = 1.5068
[2023-10-02 05:57:39] iter = 08080, loss = 1.5836
[2023-10-02 05:57:40] iter = 08090, loss = 1.5986
[2023-10-02 05:57:41] iter = 08100, loss = 1.7291
[2023-10-02 05:57:42] iter = 08110, loss = 1.6079
[2023-10-02 05:57:43] iter = 08120, loss = 1.5716
[2023-10-02 05:57:44] iter = 08130, loss = 1.7742
[2023-10-02 05:57:45] iter = 08140, loss = 1.5679
[2023-10-02 05:57:45] iter = 08150, loss = 1.5874
[2023-10-02 05:57:46] iter = 08160, loss = 1.6286
[2023-10-02 05:57:47] iter = 08170, loss = 1.5381
[2023-10-02 05:57:48] iter = 08180, loss = 1.6176
[2023-10-02 05:57:49] iter = 08190, loss = 1.6272
[2023-10-02 05:57:50] iter = 08200, loss = 1.4406
[2023-10-02 05:57:51] iter = 08210, loss = 1.6094
[2023-10-02 05:57:52] iter = 08220, loss = 1.5221
[2023-10-02 05:57:53] iter = 08230, loss = 1.4791
[2023-10-02 05:57:54] iter = 08240, loss = 1.6507
[2023-10-02 05:57:55] iter = 08250, loss = 1.7112
[2023-10-02 05:57:56] iter = 08260, loss = 1.5697
[2023-10-02 05:57:56] iter = 08270, loss = 1.3995
[2023-10-02 05:57:57] iter = 08280, loss = 1.6873
[2023-10-02 05:57:58] iter = 08290, loss = 1.5159
[2023-10-02 05:57:59] iter = 08300, loss = 1.7050
[2023-10-02 05:58:00] iter = 08310, loss = 1.6940
[2023-10-02 05:58:01] iter = 08320, loss = 1.5724
[2023-10-02 05:58:02] iter = 08330, loss = 1.5507
[2023-10-02 05:58:03] iter = 08340, loss = 1.6555
[2023-10-02 05:58:04] iter = 08350, loss = 1.6234
[2023-10-02 05:58:05] iter = 08360, loss = 1.7021
[2023-10-02 05:58:06] iter = 08370, loss = 1.5412
[2023-10-02 05:58:07] iter = 08380, loss = 1.6400
[2023-10-02 05:58:08] iter = 08390, loss = 1.6146
[2023-10-02 05:58:08] iter = 08400, loss = 1.4990
[2023-10-02 05:58:09] iter = 08410, loss = 1.5135
[2023-10-02 05:58:10] iter = 08420, loss = 1.7037
[2023-10-02 05:58:11] iter = 08430, loss = 1.6753
[2023-10-02 05:58:12] iter = 08440, loss = 1.4976
[2023-10-02 05:58:13] iter = 08450, loss = 1.5600
[2023-10-02 05:58:14] iter = 08460, loss = 1.6052
[2023-10-02 05:58:15] iter = 08470, loss = 1.7121
[2023-10-02 05:58:16] iter = 08480, loss = 1.3974
[2023-10-02 05:58:17] iter = 08490, loss = 1.5657
[2023-10-02 05:58:18] iter = 08500, loss = 1.6585
[2023-10-02 05:58:18] iter = 08510, loss = 1.6914
[2023-10-02 05:58:19] iter = 08520, loss = 1.6209
[2023-10-02 05:58:20] iter = 08530, loss = 1.5641
[2023-10-02 05:58:21] iter = 08540, loss = 1.5283
[2023-10-02 05:58:22] iter = 08550, loss = 1.6445
[2023-10-02 05:58:23] iter = 08560, loss = 1.5650
[2023-10-02 05:58:24] iter = 08570, loss = 1.6890
[2023-10-02 05:58:25] iter = 08580, loss = 1.5665
[2023-10-02 05:58:26] iter = 08590, loss = 1.5158
[2023-10-02 05:58:27] iter = 08600, loss = 1.6216
[2023-10-02 05:58:27] iter = 08610, loss = 1.6202
[2023-10-02 05:58:28] iter = 08620, loss = 1.4700
[2023-10-02 05:58:29] iter = 08630, loss = 1.5970
[2023-10-02 05:58:30] iter = 08640, loss = 1.5429
[2023-10-02 05:58:31] iter = 08650, loss = 1.4610
[2023-10-02 05:58:32] iter = 08660, loss = 1.4513
[2023-10-02 05:58:33] iter = 08670, loss = 1.6564
[2023-10-02 05:58:34] iter = 08680, loss = 1.5872
[2023-10-02 05:58:35] iter = 08690, loss = 1.4237
[2023-10-02 05:58:36] iter = 08700, loss = 1.4685
[2023-10-02 05:58:37] iter = 08710, loss = 1.4394
[2023-10-02 05:58:38] iter = 08720, loss = 1.6200
[2023-10-02 05:58:39] iter = 08730, loss = 1.6380
[2023-10-02 05:58:39] iter = 08740, loss = 1.7187
[2023-10-02 05:58:40] iter = 08750, loss = 1.5308
[2023-10-02 05:58:41] iter = 08760, loss = 1.5943
[2023-10-02 05:58:42] iter = 08770, loss = 1.5171
[2023-10-02 05:58:43] iter = 08780, loss = 1.6923
[2023-10-02 05:58:44] iter = 08790, loss = 1.6906
[2023-10-02 05:58:45] iter = 08800, loss = 1.5767
[2023-10-02 05:58:46] iter = 08810, loss = 1.6266
[2023-10-02 05:58:46] iter = 08820, loss = 1.7130
[2023-10-02 05:58:47] iter = 08830, loss = 1.5999
[2023-10-02 05:58:48] iter = 08840, loss = 1.5200
[2023-10-02 05:58:49] iter = 08850, loss = 1.5659
[2023-10-02 05:58:50] iter = 08860, loss = 1.6341
[2023-10-02 05:58:51] iter = 08870, loss = 1.7168
[2023-10-02 05:58:52] iter = 08880, loss = 1.4607
[2023-10-02 05:58:53] iter = 08890, loss = 1.6379
[2023-10-02 05:58:54] iter = 08900, loss = 1.5068
[2023-10-02 05:58:55] iter = 08910, loss = 1.4203
[2023-10-02 05:58:55] iter = 08920, loss = 1.5705
[2023-10-02 05:58:56] iter = 08930, loss = 1.7672
[2023-10-02 05:58:57] iter = 08940, loss = 1.6782
[2023-10-02 05:58:58] iter = 08950, loss = 1.5368
[2023-10-02 05:58:59] iter = 08960, loss = 1.5751
[2023-10-02 05:59:00] iter = 08970, loss = 1.6430
[2023-10-02 05:59:01] iter = 08980, loss = 1.3622
[2023-10-02 05:59:02] iter = 08990, loss = 1.6174
[2023-10-02 05:59:03] iter = 09000, loss = 1.6687
[2023-10-02 05:59:04] iter = 09010, loss = 1.5913
[2023-10-02 05:59:05] iter = 09020, loss = 1.7164
[2023-10-02 05:59:05] iter = 09030, loss = 1.7760
[2023-10-02 05:59:06] iter = 09040, loss = 1.6107
[2023-10-02 05:59:07] iter = 09050, loss = 1.4905
[2023-10-02 05:59:08] iter = 09060, loss = 1.5606
[2023-10-02 05:59:09] iter = 09070, loss = 1.6995
[2023-10-02 05:59:10] iter = 09080, loss = 1.8657
[2023-10-02 05:59:11] iter = 09090, loss = 1.5419
[2023-10-02 05:59:12] iter = 09100, loss = 1.5800
[2023-10-02 05:59:13] iter = 09110, loss = 1.5592
[2023-10-02 05:59:13] iter = 09120, loss = 1.5194
[2023-10-02 05:59:14] iter = 09130, loss = 1.5723
[2023-10-02 05:59:15] iter = 09140, loss = 1.5554
[2023-10-02 05:59:16] iter = 09150, loss = 1.4783
[2023-10-02 05:59:17] iter = 09160, loss = 1.4514
[2023-10-02 05:59:18] iter = 09170, loss = 1.4780
[2023-10-02 05:59:19] iter = 09180, loss = 1.5508
[2023-10-02 05:59:20] iter = 09190, loss = 1.6680
[2023-10-02 05:59:21] iter = 09200, loss = 1.5811
[2023-10-02 05:59:22] iter = 09210, loss = 1.6208
[2023-10-02 05:59:22] iter = 09220, loss = 1.5677
[2023-10-02 05:59:23] iter = 09230, loss = 1.4250
[2023-10-02 05:59:24] iter = 09240, loss = 1.7417
[2023-10-02 05:59:25] iter = 09250, loss = 1.5360
[2023-10-02 05:59:26] iter = 09260, loss = 1.5871
[2023-10-02 05:59:27] iter = 09270, loss = 1.5822
[2023-10-02 05:59:28] iter = 09280, loss = 1.5664
[2023-10-02 05:59:29] iter = 09290, loss = 1.4997
[2023-10-02 05:59:30] iter = 09300, loss = 1.5512
[2023-10-02 05:59:31] iter = 09310, loss = 1.4843
[2023-10-02 05:59:32] iter = 09320, loss = 1.5930
[2023-10-02 05:59:33] iter = 09330, loss = 1.8190
[2023-10-02 05:59:34] iter = 09340, loss = 1.4693
[2023-10-02 05:59:34] iter = 09350, loss = 1.4594
[2023-10-02 05:59:35] iter = 09360, loss = 1.5790
[2023-10-02 05:59:36] iter = 09370, loss = 1.5203
[2023-10-02 05:59:37] iter = 09380, loss = 1.5784
[2023-10-02 05:59:38] iter = 09390, loss = 1.5739
[2023-10-02 05:59:39] iter = 09400, loss = 1.6245
[2023-10-02 05:59:40] iter = 09410, loss = 1.5671
[2023-10-02 05:59:41] iter = 09420, loss = 1.4251
[2023-10-02 05:59:42] iter = 09430, loss = 1.5202
[2023-10-02 05:59:43] iter = 09440, loss = 1.4603
[2023-10-02 05:59:44] iter = 09450, loss = 1.5648
[2023-10-02 05:59:44] iter = 09460, loss = 1.5903
[2023-10-02 05:59:45] iter = 09470, loss = 1.3358
[2023-10-02 05:59:46] iter = 09480, loss = 1.5236
[2023-10-02 05:59:47] iter = 09490, loss = 1.5423
[2023-10-02 05:59:48] iter = 09500, loss = 1.6557
[2023-10-02 05:59:49] iter = 09510, loss = 1.5675
[2023-10-02 05:59:50] iter = 09520, loss = 1.6723
[2023-10-02 05:59:51] iter = 09530, loss = 1.4383
[2023-10-02 05:59:52] iter = 09540, loss = 1.4768
[2023-10-02 05:59:53] iter = 09550, loss = 1.4911
[2023-10-02 05:59:54] iter = 09560, loss = 1.3978
[2023-10-02 05:59:54] iter = 09570, loss = 1.4563
[2023-10-02 05:59:55] iter = 09580, loss = 1.3861
[2023-10-02 05:59:56] iter = 09590, loss = 1.5624
[2023-10-02 05:59:57] iter = 09600, loss = 1.5901
[2023-10-02 05:59:58] iter = 09610, loss = 1.4713
[2023-10-02 05:59:59] iter = 09620, loss = 1.6445
[2023-10-02 06:00:00] iter = 09630, loss = 1.4248
[2023-10-02 06:00:01] iter = 09640, loss = 1.6703
[2023-10-02 06:00:02] iter = 09650, loss = 1.5488
[2023-10-02 06:00:02] iter = 09660, loss = 1.5283
[2023-10-02 06:00:03] iter = 09670, loss = 1.5589
[2023-10-02 06:00:04] iter = 09680, loss = 1.5055
[2023-10-02 06:00:05] iter = 09690, loss = 1.6274
[2023-10-02 06:00:06] iter = 09700, loss = 1.4270
[2023-10-02 06:00:07] iter = 09710, loss = 1.5726
[2023-10-02 06:00:08] iter = 09720, loss = 1.5428
[2023-10-02 06:00:09] iter = 09730, loss = 1.5006
[2023-10-02 06:00:10] iter = 09740, loss = 1.4044
[2023-10-02 06:00:11] iter = 09750, loss = 1.5431
[2023-10-02 06:00:11] iter = 09760, loss = 1.6175
[2023-10-02 06:00:12] iter = 09770, loss = 1.5204
[2023-10-02 06:00:13] iter = 09780, loss = 1.7310
[2023-10-02 06:00:14] iter = 09790, loss = 1.5707
[2023-10-02 06:00:15] iter = 09800, loss = 1.7200
[2023-10-02 06:00:16] iter = 09810, loss = 1.5866
[2023-10-02 06:00:17] iter = 09820, loss = 1.5619
[2023-10-02 06:00:18] iter = 09830, loss = 1.6278
[2023-10-02 06:00:19] iter = 09840, loss = 1.5284
[2023-10-02 06:00:20] iter = 09850, loss = 1.5062
[2023-10-02 06:00:21] iter = 09860, loss = 1.6045
[2023-10-02 06:00:21] iter = 09870, loss = 1.4022
[2023-10-02 06:00:22] iter = 09880, loss = 1.8148
[2023-10-02 06:00:23] iter = 09890, loss = 1.4962
[2023-10-02 06:00:24] iter = 09900, loss = 1.6519
[2023-10-02 06:00:25] iter = 09910, loss = 1.4875
[2023-10-02 06:00:26] iter = 09920, loss = 1.4793
[2023-10-02 06:00:27] iter = 09930, loss = 1.5285
[2023-10-02 06:00:28] iter = 09940, loss = 1.4036
[2023-10-02 06:00:29] iter = 09950, loss = 1.4741
[2023-10-02 06:00:30] iter = 09960, loss = 1.4069
[2023-10-02 06:00:30] iter = 09970, loss = 1.7690
[2023-10-02 06:00:31] iter = 09980, loss = 1.5077
[2023-10-02 06:00:32] iter = 09990, loss = 1.4322
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 33541}
[2023-10-02 06:00:57] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.014408 train acc = 1.0000, test acc = 0.6196
[2023-10-02 06:01:21] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.006379 train acc = 1.0000, test acc = 0.6268
[2023-10-02 06:01:45] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.015576 train acc = 1.0000, test acc = 0.6211
[2023-10-02 06:02:09] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.026322 train acc = 0.9960, test acc = 0.6230
[2023-10-02 06:02:33] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.010205 train acc = 1.0000, test acc = 0.6289
[2023-10-02 06:02:57] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011389 train acc = 1.0000, test acc = 0.6277
[2023-10-02 06:03:22] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.001784 train acc = 1.0000, test acc = 0.6253
[2023-10-02 06:03:46] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.022229 train acc = 1.0000, test acc = 0.6304
[2023-10-02 06:04:10] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.003330 train acc = 1.0000, test acc = 0.6216
[2023-10-02 06:04:34] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.012088 train acc = 1.0000, test acc = 0.6261
[2023-10-02 06:04:58] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.016463 train acc = 0.9980, test acc = 0.6141
[2023-10-02 06:05:22] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.015275 train acc = 0.9980, test acc = 0.6269
[2023-10-02 06:05:47] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.010278 train acc = 1.0000, test acc = 0.6226
[2023-10-02 06:06:11] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.010551 train acc = 1.0000, test acc = 0.6206
[2023-10-02 06:06:35] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.016600 train acc = 1.0000, test acc = 0.6278
[2023-10-02 06:06:59] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004607 train acc = 1.0000, test acc = 0.6223
[2023-10-02 06:07:23] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004019 train acc = 1.0000, test acc = 0.6232
[2023-10-02 06:07:47] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.009004 train acc = 1.0000, test acc = 0.6279
[2023-10-02 06:08:11] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.015843 train acc = 0.9980, test acc = 0.6226
[2023-10-02 06:08:35] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.010564 train acc = 1.0000, test acc = 0.6272
Evaluate 20 random ConvNet, mean = 0.6243 std = 0.0038
-------------------------
[2023-10-02 06:08:36] iter = 10000, loss = 1.5047
[2023-10-02 06:08:37] iter = 10010, loss = 1.5107
[2023-10-02 06:08:38] iter = 10020, loss = 1.6029
[2023-10-02 06:08:39] iter = 10030, loss = 1.7203
[2023-10-02 06:08:39] iter = 10040, loss = 1.5129
[2023-10-02 06:08:40] iter = 10050, loss = 1.6575
[2023-10-02 06:08:41] iter = 10060, loss = 1.5499
[2023-10-02 06:08:42] iter = 10070, loss = 1.5206
[2023-10-02 06:08:43] iter = 10080, loss = 1.5489
[2023-10-02 06:08:44] iter = 10090, loss = 1.5912
[2023-10-02 06:08:45] iter = 10100, loss = 1.4304
[2023-10-02 06:08:46] iter = 10110, loss = 1.5124
[2023-10-02 06:08:47] iter = 10120, loss = 1.6560
[2023-10-02 06:08:47] iter = 10130, loss = 1.6823
[2023-10-02 06:08:48] iter = 10140, loss = 1.5107
[2023-10-02 06:08:49] iter = 10150, loss = 1.6559
[2023-10-02 06:08:50] iter = 10160, loss = 1.6277
[2023-10-02 06:08:51] iter = 10170, loss = 1.5154
[2023-10-02 06:08:52] iter = 10180, loss = 1.5577
[2023-10-02 06:08:53] iter = 10190, loss = 1.5311
[2023-10-02 06:08:54] iter = 10200, loss = 1.4910
[2023-10-02 06:08:55] iter = 10210, loss = 1.6869
[2023-10-02 06:08:56] iter = 10220, loss = 1.5965
[2023-10-02 06:08:57] iter = 10230, loss = 1.4657
[2023-10-02 06:08:57] iter = 10240, loss = 1.5786
[2023-10-02 06:08:58] iter = 10250, loss = 1.4568
[2023-10-02 06:08:59] iter = 10260, loss = 1.6738
[2023-10-02 06:09:00] iter = 10270, loss = 1.5454
[2023-10-02 06:09:01] iter = 10280, loss = 1.5950
[2023-10-02 06:09:02] iter = 10290, loss = 1.5308
[2023-10-02 06:09:03] iter = 10300, loss = 1.4635
[2023-10-02 06:09:04] iter = 10310, loss = 1.4503
[2023-10-02 06:09:05] iter = 10320, loss = 1.7035
[2023-10-02 06:09:06] iter = 10330, loss = 1.4864
[2023-10-02 06:09:07] iter = 10340, loss = 1.7095
[2023-10-02 06:09:08] iter = 10350, loss = 1.5239
[2023-10-02 06:09:09] iter = 10360, loss = 1.5891
[2023-10-02 06:09:09] iter = 10370, loss = 1.6542
[2023-10-02 06:09:10] iter = 10380, loss = 1.4866
[2023-10-02 06:09:11] iter = 10390, loss = 1.4911
[2023-10-02 06:09:12] iter = 10400, loss = 1.4072
[2023-10-02 06:09:13] iter = 10410, loss = 1.4641
[2023-10-02 06:09:14] iter = 10420, loss = 1.4521
[2023-10-02 06:09:15] iter = 10430, loss = 1.4330
[2023-10-02 06:09:16] iter = 10440, loss = 1.5008
[2023-10-02 06:09:17] iter = 10450, loss = 1.5849
[2023-10-02 06:09:18] iter = 10460, loss = 1.7544
[2023-10-02 06:09:18] iter = 10470, loss = 1.4287
[2023-10-02 06:09:19] iter = 10480, loss = 1.4601
[2023-10-02 06:09:20] iter = 10490, loss = 1.6206
[2023-10-02 06:09:21] iter = 10500, loss = 1.5039
[2023-10-02 06:09:22] iter = 10510, loss = 1.4617
[2023-10-02 06:09:23] iter = 10520, loss = 1.8281
[2023-10-02 06:09:24] iter = 10530, loss = 1.6139
[2023-10-02 06:09:25] iter = 10540, loss = 1.5185
[2023-10-02 06:09:26] iter = 10550, loss = 1.4410
[2023-10-02 06:09:26] iter = 10560, loss = 1.6467
[2023-10-02 06:09:27] iter = 10570, loss = 1.5355
[2023-10-02 06:09:28] iter = 10580, loss = 1.5547
[2023-10-02 06:09:29] iter = 10590, loss = 1.4962
[2023-10-02 06:09:30] iter = 10600, loss = 1.5716
[2023-10-02 06:09:31] iter = 10610, loss = 1.5716
[2023-10-02 06:09:32] iter = 10620, loss = 1.4550
[2023-10-02 06:09:33] iter = 10630, loss = 1.4632
[2023-10-02 06:09:33] iter = 10640, loss = 1.4967
[2023-10-02 06:09:34] iter = 10650, loss = 1.4796
[2023-10-02 06:09:35] iter = 10660, loss = 1.6593
[2023-10-02 06:09:36] iter = 10670, loss = 1.5659
[2023-10-02 06:09:37] iter = 10680, loss = 1.5802
[2023-10-02 06:09:38] iter = 10690, loss = 1.4871
[2023-10-02 06:09:39] iter = 10700, loss = 1.5398
[2023-10-02 06:09:40] iter = 10710, loss = 1.5923
[2023-10-02 06:09:41] iter = 10720, loss = 1.5633
[2023-10-02 06:09:42] iter = 10730, loss = 1.5578
[2023-10-02 06:09:43] iter = 10740, loss = 1.6359
[2023-10-02 06:09:44] iter = 10750, loss = 1.6468
[2023-10-02 06:09:44] iter = 10760, loss = 1.6099
[2023-10-02 06:09:45] iter = 10770, loss = 1.5731
[2023-10-02 06:09:46] iter = 10780, loss = 1.6208
[2023-10-02 06:09:47] iter = 10790, loss = 1.5009
[2023-10-02 06:09:48] iter = 10800, loss = 1.6289
[2023-10-02 06:09:49] iter = 10810, loss = 1.7095
[2023-10-02 06:09:50] iter = 10820, loss = 1.6317
[2023-10-02 06:09:51] iter = 10830, loss = 1.4830
[2023-10-02 06:09:52] iter = 10840, loss = 1.5780
[2023-10-02 06:09:53] iter = 10850, loss = 1.6847
[2023-10-02 06:09:54] iter = 10860, loss = 1.4333
[2023-10-02 06:09:55] iter = 10870, loss = 1.5145
[2023-10-02 06:09:55] iter = 10880, loss = 1.4854
[2023-10-02 06:09:56] iter = 10890, loss = 1.6462
[2023-10-02 06:09:57] iter = 10900, loss = 1.5124
[2023-10-02 06:09:58] iter = 10910, loss = 1.6429
[2023-10-02 06:09:59] iter = 10920, loss = 1.3363
[2023-10-02 06:10:00] iter = 10930, loss = 1.4777
[2023-10-02 06:10:01] iter = 10940, loss = 1.6873
[2023-10-02 06:10:02] iter = 10950, loss = 1.6123
[2023-10-02 06:10:03] iter = 10960, loss = 1.5648
[2023-10-02 06:10:03] iter = 10970, loss = 1.6274
[2023-10-02 06:10:04] iter = 10980, loss = 1.6832
[2023-10-02 06:10:05] iter = 10990, loss = 1.4606
[2023-10-02 06:10:06] iter = 11000, loss = 1.5203
[2023-10-02 06:10:07] iter = 11010, loss = 1.5244
[2023-10-02 06:10:08] iter = 11020, loss = 1.5215
[2023-10-02 06:10:09] iter = 11030, loss = 1.6140
[2023-10-02 06:10:10] iter = 11040, loss = 1.5395
[2023-10-02 06:10:11] iter = 11050, loss = 1.4232
[2023-10-02 06:10:12] iter = 11060, loss = 1.4658
[2023-10-02 06:10:12] iter = 11070, loss = 1.6111
[2023-10-02 06:10:13] iter = 11080, loss = 1.4537
[2023-10-02 06:10:14] iter = 11090, loss = 1.5983
[2023-10-02 06:10:15] iter = 11100, loss = 1.4522
[2023-10-02 06:10:16] iter = 11110, loss = 1.6326
[2023-10-02 06:10:17] iter = 11120, loss = 1.7229
[2023-10-02 06:10:18] iter = 11130, loss = 1.5092
[2023-10-02 06:10:19] iter = 11140, loss = 1.5353
[2023-10-02 06:10:20] iter = 11150, loss = 1.4940
[2023-10-02 06:10:20] iter = 11160, loss = 1.6086
[2023-10-02 06:10:21] iter = 11170, loss = 1.6005
[2023-10-02 06:10:22] iter = 11180, loss = 1.5529
[2023-10-02 06:10:23] iter = 11190, loss = 1.5008
[2023-10-02 06:10:24] iter = 11200, loss = 1.5424
[2023-10-02 06:10:25] iter = 11210, loss = 1.4860
[2023-10-02 06:10:26] iter = 11220, loss = 1.4710
[2023-10-02 06:10:26] iter = 11230, loss = 1.4741
[2023-10-02 06:10:27] iter = 11240, loss = 1.7153
[2023-10-02 06:10:28] iter = 11250, loss = 1.5771
[2023-10-02 06:10:29] iter = 11260, loss = 1.5163
[2023-10-02 06:10:30] iter = 11270, loss = 1.4875
[2023-10-02 06:10:31] iter = 11280, loss = 1.5014
[2023-10-02 06:10:32] iter = 11290, loss = 1.5096
[2023-10-02 06:10:33] iter = 11300, loss = 1.6430
[2023-10-02 06:10:34] iter = 11310, loss = 1.5843
[2023-10-02 06:10:35] iter = 11320, loss = 1.4833
[2023-10-02 06:10:36] iter = 11330, loss = 1.5934
[2023-10-02 06:10:37] iter = 11340, loss = 1.5122
[2023-10-02 06:10:37] iter = 11350, loss = 1.5573
[2023-10-02 06:10:38] iter = 11360, loss = 1.4918
[2023-10-02 06:10:39] iter = 11370, loss = 1.4597
[2023-10-02 06:10:40] iter = 11380, loss = 1.4823
[2023-10-02 06:10:41] iter = 11390, loss = 1.4618
[2023-10-02 06:10:42] iter = 11400, loss = 1.5493
[2023-10-02 06:10:43] iter = 11410, loss = 1.4736
[2023-10-02 06:10:44] iter = 11420, loss = 1.4618
[2023-10-02 06:10:45] iter = 11430, loss = 1.6292
[2023-10-02 06:10:46] iter = 11440, loss = 1.4885
[2023-10-02 06:10:47] iter = 11450, loss = 1.4520
[2023-10-02 06:10:48] iter = 11460, loss = 1.6458
[2023-10-02 06:10:48] iter = 11470, loss = 1.5098
[2023-10-02 06:10:49] iter = 11480, loss = 1.5746
[2023-10-02 06:10:50] iter = 11490, loss = 1.5267
[2023-10-02 06:10:51] iter = 11500, loss = 1.4960
[2023-10-02 06:10:52] iter = 11510, loss = 1.5762
[2023-10-02 06:10:53] iter = 11520, loss = 1.5321
[2023-10-02 06:10:54] iter = 11530, loss = 1.4309
[2023-10-02 06:10:55] iter = 11540, loss = 1.6163
[2023-10-02 06:10:56] iter = 11550, loss = 1.4845
[2023-10-02 06:10:57] iter = 11560, loss = 1.4170
[2023-10-02 06:10:58] iter = 11570, loss = 1.5101
[2023-10-02 06:10:58] iter = 11580, loss = 1.4420
[2023-10-02 06:10:59] iter = 11590, loss = 1.5459
[2023-10-02 06:11:00] iter = 11600, loss = 1.6960
[2023-10-02 06:11:01] iter = 11610, loss = 1.5415
[2023-10-02 06:11:02] iter = 11620, loss = 1.4179
[2023-10-02 06:11:03] iter = 11630, loss = 1.5672
[2023-10-02 06:11:04] iter = 11640, loss = 1.4916
[2023-10-02 06:11:04] iter = 11650, loss = 1.5426
[2023-10-02 06:11:05] iter = 11660, loss = 1.5572
[2023-10-02 06:11:06] iter = 11670, loss = 1.4544
[2023-10-02 06:11:07] iter = 11680, loss = 1.4967
[2023-10-02 06:11:08] iter = 11690, loss = 1.6124
[2023-10-02 06:11:09] iter = 11700, loss = 1.5798
[2023-10-02 06:11:10] iter = 11710, loss = 1.5435
[2023-10-02 06:11:11] iter = 11720, loss = 1.4952
[2023-10-02 06:11:12] iter = 11730, loss = 1.4718
[2023-10-02 06:11:13] iter = 11740, loss = 1.4897
[2023-10-02 06:11:14] iter = 11750, loss = 1.7310
[2023-10-02 06:11:15] iter = 11760, loss = 1.4501
[2023-10-02 06:11:16] iter = 11770, loss = 1.5380
[2023-10-02 06:11:16] iter = 11780, loss = 1.6608
[2023-10-02 06:11:17] iter = 11790, loss = 1.6404
[2023-10-02 06:11:18] iter = 11800, loss = 1.5639
[2023-10-02 06:11:19] iter = 11810, loss = 1.4558
[2023-10-02 06:11:20] iter = 11820, loss = 1.4844
[2023-10-02 06:11:21] iter = 11830, loss = 1.4953
[2023-10-02 06:11:22] iter = 11840, loss = 1.6375
[2023-10-02 06:11:23] iter = 11850, loss = 1.7593
[2023-10-02 06:11:24] iter = 11860, loss = 1.4926
[2023-10-02 06:11:24] iter = 11870, loss = 1.4485
[2023-10-02 06:11:25] iter = 11880, loss = 1.5196
[2023-10-02 06:11:26] iter = 11890, loss = 1.5260
[2023-10-02 06:11:27] iter = 11900, loss = 1.5085
[2023-10-02 06:11:28] iter = 11910, loss = 1.4469
[2023-10-02 06:11:29] iter = 11920, loss = 1.5187
[2023-10-02 06:11:30] iter = 11930, loss = 1.5749
[2023-10-02 06:11:31] iter = 11940, loss = 1.4881
[2023-10-02 06:11:32] iter = 11950, loss = 1.4350
[2023-10-02 06:11:32] iter = 11960, loss = 1.5380
[2023-10-02 06:11:33] iter = 11970, loss = 1.4503
[2023-10-02 06:11:34] iter = 11980, loss = 1.5572
[2023-10-02 06:11:35] iter = 11990, loss = 1.5939
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 96460}
[2023-10-02 06:12:00] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.018684 train acc = 1.0000, test acc = 0.6178
[2023-10-02 06:12:24] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.005916 train acc = 1.0000, test acc = 0.6248
[2023-10-02 06:12:48] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.028302 train acc = 0.9960, test acc = 0.6204
[2023-10-02 06:13:12] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.024843 train acc = 0.9980, test acc = 0.6302
[2023-10-02 06:13:36] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.018890 train acc = 0.9980, test acc = 0.6267
[2023-10-02 06:14:00] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.001890 train acc = 1.0000, test acc = 0.6270
[2023-10-02 06:14:24] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.004008 train acc = 1.0000, test acc = 0.6283
[2023-10-02 06:14:49] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.003861 train acc = 1.0000, test acc = 0.6294
[2023-10-02 06:15:13] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.016239 train acc = 1.0000, test acc = 0.6198
[2023-10-02 06:15:37] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.017359 train acc = 1.0000, test acc = 0.6224
[2023-10-02 06:16:01] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.014044 train acc = 1.0000, test acc = 0.6288
[2023-10-02 06:16:25] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.014448 train acc = 0.9980, test acc = 0.6253
[2023-10-02 06:16:49] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004361 train acc = 1.0000, test acc = 0.6260
[2023-10-02 06:17:14] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.016264 train acc = 0.9980, test acc = 0.6251
[2023-10-02 06:17:38] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.014943 train acc = 1.0000, test acc = 0.6279
[2023-10-02 06:18:02] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.004185 train acc = 1.0000, test acc = 0.6254
[2023-10-02 06:18:26] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.004737 train acc = 1.0000, test acc = 0.6281
[2023-10-02 06:18:50] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003756 train acc = 1.0000, test acc = 0.6183
[2023-10-02 06:19:14] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013697 train acc = 1.0000, test acc = 0.6215
[2023-10-02 06:19:38] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.018215 train acc = 1.0000, test acc = 0.6160
Evaluate 20 random ConvNet, mean = 0.6245 std = 0.0041
-------------------------
[2023-10-02 06:19:39] iter = 12000, loss = 1.5416
[2023-10-02 06:19:39] iter = 12010, loss = 1.7045
[2023-10-02 06:19:40] iter = 12020, loss = 1.5635
[2023-10-02 06:19:41] iter = 12030, loss = 1.5336
[2023-10-02 06:19:42] iter = 12040, loss = 1.4373
[2023-10-02 06:19:43] iter = 12050, loss = 1.4716
[2023-10-02 06:19:44] iter = 12060, loss = 1.5714
[2023-10-02 06:19:45] iter = 12070, loss = 1.6128
[2023-10-02 06:19:46] iter = 12080, loss = 1.4370
[2023-10-02 06:19:47] iter = 12090, loss = 1.5967
[2023-10-02 06:19:48] iter = 12100, loss = 1.4948
[2023-10-02 06:19:49] iter = 12110, loss = 1.6212
[2023-10-02 06:19:50] iter = 12120, loss = 1.6106
[2023-10-02 06:19:50] iter = 12130, loss = 1.6857
[2023-10-02 06:19:51] iter = 12140, loss = 1.7013
[2023-10-02 06:19:52] iter = 12150, loss = 1.4581
[2023-10-02 06:19:53] iter = 12160, loss = 1.5149
[2023-10-02 06:19:54] iter = 12170, loss = 1.7459
[2023-10-02 06:19:55] iter = 12180, loss = 1.4117
[2023-10-02 06:19:56] iter = 12190, loss = 1.6005
[2023-10-02 06:19:57] iter = 12200, loss = 1.6634
[2023-10-02 06:19:58] iter = 12210, loss = 1.4822
[2023-10-02 06:19:58] iter = 12220, loss = 1.5233
[2023-10-02 06:19:59] iter = 12230, loss = 1.4265
[2023-10-02 06:20:00] iter = 12240, loss = 1.6733
[2023-10-02 06:20:01] iter = 12250, loss = 1.5635
[2023-10-02 06:20:02] iter = 12260, loss = 1.5896
[2023-10-02 06:20:03] iter = 12270, loss = 1.5980
[2023-10-02 06:20:04] iter = 12280, loss = 1.4240
[2023-10-02 06:20:05] iter = 12290, loss = 1.4893
[2023-10-02 06:20:06] iter = 12300, loss = 1.5230
[2023-10-02 06:20:07] iter = 12310, loss = 1.5507
[2023-10-02 06:20:08] iter = 12320, loss = 1.4632
[2023-10-02 06:20:08] iter = 12330, loss = 1.3597
[2023-10-02 06:20:09] iter = 12340, loss = 1.5776
[2023-10-02 06:20:10] iter = 12350, loss = 1.4706
[2023-10-02 06:20:11] iter = 12360, loss = 1.5911
[2023-10-02 06:20:12] iter = 12370, loss = 1.5652
[2023-10-02 06:20:13] iter = 12380, loss = 1.4861
[2023-10-02 06:20:14] iter = 12390, loss = 1.6750
[2023-10-02 06:20:15] iter = 12400, loss = 1.5009
[2023-10-02 06:20:16] iter = 12410, loss = 1.3869
[2023-10-02 06:20:17] iter = 12420, loss = 1.6043
[2023-10-02 06:20:18] iter = 12430, loss = 1.5965
[2023-10-02 06:20:18] iter = 12440, loss = 1.5633
[2023-10-02 06:20:19] iter = 12450, loss = 1.6005
[2023-10-02 06:20:20] iter = 12460, loss = 1.5411
[2023-10-02 06:20:21] iter = 12470, loss = 1.5066
[2023-10-02 06:20:22] iter = 12480, loss = 1.4893
[2023-10-02 06:20:23] iter = 12490, loss = 1.4864
[2023-10-02 06:20:24] iter = 12500, loss = 1.5397
[2023-10-02 06:20:25] iter = 12510, loss = 1.4255
[2023-10-02 06:20:26] iter = 12520, loss = 1.5467
[2023-10-02 06:20:26] iter = 12530, loss = 1.4204
[2023-10-02 06:20:27] iter = 12540, loss = 1.5681
[2023-10-02 06:20:28] iter = 12550, loss = 1.3960
[2023-10-02 06:20:29] iter = 12560, loss = 1.3985
[2023-10-02 06:20:30] iter = 12570, loss = 1.4941
[2023-10-02 06:20:31] iter = 12580, loss = 1.5706
[2023-10-02 06:20:32] iter = 12590, loss = 1.5741
[2023-10-02 06:20:33] iter = 12600, loss = 1.4380
[2023-10-02 06:20:34] iter = 12610, loss = 1.4405
[2023-10-02 06:20:35] iter = 12620, loss = 1.6333
[2023-10-02 06:20:36] iter = 12630, loss = 1.4521
[2023-10-02 06:20:36] iter = 12640, loss = 1.4696
[2023-10-02 06:20:37] iter = 12650, loss = 1.5777
[2023-10-02 06:20:38] iter = 12660, loss = 1.3961
[2023-10-02 06:20:39] iter = 12670, loss = 1.5334
[2023-10-02 06:20:40] iter = 12680, loss = 1.4683
[2023-10-02 06:20:41] iter = 12690, loss = 1.4713
[2023-10-02 06:20:42] iter = 12700, loss = 1.6332
[2023-10-02 06:20:43] iter = 12710, loss = 1.7410
[2023-10-02 06:20:44] iter = 12720, loss = 1.5107
[2023-10-02 06:20:45] iter = 12730, loss = 1.4960
[2023-10-02 06:20:46] iter = 12740, loss = 1.5445
[2023-10-02 06:20:47] iter = 12750, loss = 1.4821
[2023-10-02 06:20:47] iter = 12760, loss = 1.4896
[2023-10-02 06:20:48] iter = 12770, loss = 1.4714
[2023-10-02 06:20:49] iter = 12780, loss = 1.4403
[2023-10-02 06:20:50] iter = 12790, loss = 1.4738
[2023-10-02 06:20:51] iter = 12800, loss = 1.4193
[2023-10-02 06:20:52] iter = 12810, loss = 1.5909
[2023-10-02 06:20:53] iter = 12820, loss = 1.5469
[2023-10-02 06:20:54] iter = 12830, loss = 1.4639
[2023-10-02 06:20:54] iter = 12840, loss = 1.5416
[2023-10-02 06:20:55] iter = 12850, loss = 1.6447
[2023-10-02 06:20:56] iter = 12860, loss = 1.5262
[2023-10-02 06:20:57] iter = 12870, loss = 1.5600
[2023-10-02 06:20:58] iter = 12880, loss = 1.4371
[2023-10-02 06:20:59] iter = 12890, loss = 1.4263
[2023-10-02 06:21:00] iter = 12900, loss = 1.4218
[2023-10-02 06:21:01] iter = 12910, loss = 1.5372
[2023-10-02 06:21:02] iter = 12920, loss = 1.6294
[2023-10-02 06:21:02] iter = 12930, loss = 1.4165
[2023-10-02 06:21:03] iter = 12940, loss = 1.5321
[2023-10-02 06:21:04] iter = 12950, loss = 1.4786
[2023-10-02 06:21:05] iter = 12960, loss = 1.5469
[2023-10-02 06:21:06] iter = 12970, loss = 1.6652
[2023-10-02 06:21:07] iter = 12980, loss = 1.4715
[2023-10-02 06:21:08] iter = 12990, loss = 1.5882
[2023-10-02 06:21:09] iter = 13000, loss = 1.5753
[2023-10-02 06:21:10] iter = 13010, loss = 1.4333
[2023-10-02 06:21:11] iter = 13020, loss = 1.5123
[2023-10-02 06:21:11] iter = 13030, loss = 1.5231
[2023-10-02 06:21:12] iter = 13040, loss = 1.3283
[2023-10-02 06:21:13] iter = 13050, loss = 1.4431
[2023-10-02 06:21:14] iter = 13060, loss = 1.5527
[2023-10-02 06:21:15] iter = 13070, loss = 1.4540
[2023-10-02 06:21:16] iter = 13080, loss = 1.6063
[2023-10-02 06:21:17] iter = 13090, loss = 1.4542
[2023-10-02 06:21:18] iter = 13100, loss = 1.6516
[2023-10-02 06:21:19] iter = 13110, loss = 1.4882
[2023-10-02 06:21:20] iter = 13120, loss = 1.4851
[2023-10-02 06:21:21] iter = 13130, loss = 1.4709
[2023-10-02 06:21:22] iter = 13140, loss = 1.4998
[2023-10-02 06:21:23] iter = 13150, loss = 1.6440
[2023-10-02 06:21:23] iter = 13160, loss = 1.4777
[2023-10-02 06:21:24] iter = 13170, loss = 1.5704
[2023-10-02 06:21:25] iter = 13180, loss = 1.5636
[2023-10-02 06:21:26] iter = 13190, loss = 1.4872
[2023-10-02 06:21:27] iter = 13200, loss = 1.6520
[2023-10-02 06:21:28] iter = 13210, loss = 1.5128
[2023-10-02 06:21:29] iter = 13220, loss = 1.5801
[2023-10-02 06:21:30] iter = 13230, loss = 1.4826
[2023-10-02 06:21:31] iter = 13240, loss = 1.6193
[2023-10-02 06:21:32] iter = 13250, loss = 1.4220
[2023-10-02 06:21:33] iter = 13260, loss = 1.5037
[2023-10-02 06:21:34] iter = 13270, loss = 1.5320
[2023-10-02 06:21:35] iter = 13280, loss = 1.4694
[2023-10-02 06:21:35] iter = 13290, loss = 1.4736
[2023-10-02 06:21:36] iter = 13300, loss = 1.5120
[2023-10-02 06:21:37] iter = 13310, loss = 1.5827
[2023-10-02 06:21:38] iter = 13320, loss = 1.5117
[2023-10-02 06:21:39] iter = 13330, loss = 1.4628
[2023-10-02 06:21:40] iter = 13340, loss = 1.5079
[2023-10-02 06:21:41] iter = 13350, loss = 1.3492
[2023-10-02 06:21:42] iter = 13360, loss = 1.4164
[2023-10-02 06:21:43] iter = 13370, loss = 1.5670
[2023-10-02 06:21:43] iter = 13380, loss = 1.5869
[2023-10-02 06:21:44] iter = 13390, loss = 1.5569
[2023-10-02 06:21:45] iter = 13400, loss = 1.4552
[2023-10-02 06:21:46] iter = 13410, loss = 1.4358
[2023-10-02 06:21:47] iter = 13420, loss = 1.4619
[2023-10-02 06:21:48] iter = 13430, loss = 1.4829
[2023-10-02 06:21:49] iter = 13440, loss = 1.3960
[2023-10-02 06:21:50] iter = 13450, loss = 1.5172
[2023-10-02 06:21:51] iter = 13460, loss = 1.5949
[2023-10-02 06:21:52] iter = 13470, loss = 1.5860
[2023-10-02 06:21:53] iter = 13480, loss = 1.4082
[2023-10-02 06:21:54] iter = 13490, loss = 1.4695
[2023-10-02 06:21:54] iter = 13500, loss = 1.5942
[2023-10-02 06:21:55] iter = 13510, loss = 1.4679
[2023-10-02 06:21:56] iter = 13520, loss = 1.4689
[2023-10-02 06:21:57] iter = 13530, loss = 1.5700
[2023-10-02 06:21:58] iter = 13540, loss = 1.4188
[2023-10-02 06:21:59] iter = 13550, loss = 1.5167
[2023-10-02 06:22:00] iter = 13560, loss = 1.4712
[2023-10-02 06:22:01] iter = 13570, loss = 1.4891
[2023-10-02 06:22:02] iter = 13580, loss = 1.6195
[2023-10-02 06:22:03] iter = 13590, loss = 1.4996
[2023-10-02 06:22:03] iter = 13600, loss = 1.5049
[2023-10-02 06:22:04] iter = 13610, loss = 1.5045
[2023-10-02 06:22:05] iter = 13620, loss = 1.5422
[2023-10-02 06:22:06] iter = 13630, loss = 1.5363
[2023-10-02 06:22:07] iter = 13640, loss = 1.6056
[2023-10-02 06:22:08] iter = 13650, loss = 1.4878
[2023-10-02 06:22:09] iter = 13660, loss = 1.5888
[2023-10-02 06:22:10] iter = 13670, loss = 1.4822
[2023-10-02 06:22:11] iter = 13680, loss = 1.6516
[2023-10-02 06:22:11] iter = 13690, loss = 1.5110
[2023-10-02 06:22:12] iter = 13700, loss = 1.5457
[2023-10-02 06:22:13] iter = 13710, loss = 1.3711
[2023-10-02 06:22:14] iter = 13720, loss = 1.3665
[2023-10-02 06:22:15] iter = 13730, loss = 1.6261
[2023-10-02 06:22:16] iter = 13740, loss = 1.4945
[2023-10-02 06:22:17] iter = 13750, loss = 1.5612
[2023-10-02 06:22:18] iter = 13760, loss = 1.5870
[2023-10-02 06:22:19] iter = 13770, loss = 1.4342
[2023-10-02 06:22:20] iter = 13780, loss = 1.6005
[2023-10-02 06:22:21] iter = 13790, loss = 1.6103
[2023-10-02 06:22:21] iter = 13800, loss = 1.5362
[2023-10-02 06:22:22] iter = 13810, loss = 1.5369
[2023-10-02 06:22:23] iter = 13820, loss = 1.5369
[2023-10-02 06:22:24] iter = 13830, loss = 1.4910
[2023-10-02 06:22:25] iter = 13840, loss = 1.4062
[2023-10-02 06:22:26] iter = 13850, loss = 1.4334
[2023-10-02 06:22:27] iter = 13860, loss = 1.4446
[2023-10-02 06:22:27] iter = 13870, loss = 1.5064
[2023-10-02 06:22:28] iter = 13880, loss = 1.5094
[2023-10-02 06:22:29] iter = 13890, loss = 1.4386
[2023-10-02 06:22:30] iter = 13900, loss = 1.5768
[2023-10-02 06:22:31] iter = 13910, loss = 1.5458
[2023-10-02 06:22:32] iter = 13920, loss = 1.5889
[2023-10-02 06:22:33] iter = 13930, loss = 1.4970
[2023-10-02 06:22:34] iter = 13940, loss = 1.4108
[2023-10-02 06:22:34] iter = 13950, loss = 1.4868
[2023-10-02 06:22:35] iter = 13960, loss = 1.3643
[2023-10-02 06:22:36] iter = 13970, loss = 1.5397
[2023-10-02 06:22:37] iter = 13980, loss = 1.4369
[2023-10-02 06:22:38] iter = 13990, loss = 1.6506
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 59434}
[2023-10-02 06:23:03] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.012865 train acc = 1.0000, test acc = 0.6311
[2023-10-02 06:23:27] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.017527 train acc = 1.0000, test acc = 0.6237
[2023-10-02 06:23:52] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.011223 train acc = 1.0000, test acc = 0.6256
[2023-10-02 06:24:16] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.005017 train acc = 1.0000, test acc = 0.6257
[2023-10-02 06:24:40] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.027393 train acc = 1.0000, test acc = 0.6298
[2023-10-02 06:25:04] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.002444 train acc = 1.0000, test acc = 0.6247
[2023-10-02 06:25:28] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.001921 train acc = 1.0000, test acc = 0.6245
[2023-10-02 06:25:53] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.002186 train acc = 1.0000, test acc = 0.6266
[2023-10-02 06:26:17] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.006540 train acc = 1.0000, test acc = 0.6218
[2023-10-02 06:26:41] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.007676 train acc = 1.0000, test acc = 0.6319
[2023-10-02 06:27:05] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.009561 train acc = 1.0000, test acc = 0.6287
[2023-10-02 06:27:29] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004103 train acc = 1.0000, test acc = 0.6311
[2023-10-02 06:27:53] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.020071 train acc = 1.0000, test acc = 0.6209
[2023-10-02 06:28:17] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.003517 train acc = 1.0000, test acc = 0.6291
[2023-10-02 06:28:41] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.017584 train acc = 1.0000, test acc = 0.6333
[2023-10-02 06:29:05] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015158 train acc = 1.0000, test acc = 0.6226
[2023-10-02 06:29:30] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.010711 train acc = 1.0000, test acc = 0.6299
[2023-10-02 06:29:54] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.004036 train acc = 1.0000, test acc = 0.6270
[2023-10-02 06:30:18] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.012091 train acc = 1.0000, test acc = 0.6240
[2023-10-02 06:30:42] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.008117 train acc = 1.0000, test acc = 0.6219
Evaluate 20 random ConvNet, mean = 0.6267 std = 0.0036
-------------------------
[2023-10-02 06:30:42] iter = 14000, loss = 1.5289
[2023-10-02 06:30:43] iter = 14010, loss = 1.5147
[2023-10-02 06:30:44] iter = 14020, loss = 1.4920
[2023-10-02 06:30:45] iter = 14030, loss = 1.5531
[2023-10-02 06:30:46] iter = 14040, loss = 1.6010
[2023-10-02 06:30:47] iter = 14050, loss = 1.4648
[2023-10-02 06:30:48] iter = 14060, loss = 1.6227
[2023-10-02 06:30:48] iter = 14070, loss = 1.5659
[2023-10-02 06:30:49] iter = 14080, loss = 1.5426
[2023-10-02 06:30:50] iter = 14090, loss = 1.5353
[2023-10-02 06:30:51] iter = 14100, loss = 1.5107
[2023-10-02 06:30:52] iter = 14110, loss = 1.5015
[2023-10-02 06:30:53] iter = 14120, loss = 1.4947
[2023-10-02 06:30:54] iter = 14130, loss = 1.4616
[2023-10-02 06:30:55] iter = 14140, loss = 1.5232
[2023-10-02 06:30:56] iter = 14150, loss = 1.5641
[2023-10-02 06:30:57] iter = 14160, loss = 1.6145
[2023-10-02 06:30:57] iter = 14170, loss = 1.5908
[2023-10-02 06:30:58] iter = 14180, loss = 1.5362
[2023-10-02 06:30:59] iter = 14190, loss = 1.5372
[2023-10-02 06:31:00] iter = 14200, loss = 1.4728
[2023-10-02 06:31:01] iter = 14210, loss = 1.5228
[2023-10-02 06:31:02] iter = 14220, loss = 1.5435
[2023-10-02 06:31:03] iter = 14230, loss = 1.4493
[2023-10-02 06:31:04] iter = 14240, loss = 1.4705
[2023-10-02 06:31:05] iter = 14250, loss = 1.4949
[2023-10-02 06:31:05] iter = 14260, loss = 1.5293
[2023-10-02 06:31:06] iter = 14270, loss = 1.6496
[2023-10-02 06:31:07] iter = 14280, loss = 1.4955
[2023-10-02 06:31:08] iter = 14290, loss = 1.4174
[2023-10-02 06:31:09] iter = 14300, loss = 1.5573
[2023-10-02 06:31:10] iter = 14310, loss = 1.6470
[2023-10-02 06:31:11] iter = 14320, loss = 1.4972
[2023-10-02 06:31:12] iter = 14330, loss = 1.4898
[2023-10-02 06:31:13] iter = 14340, loss = 1.4724
[2023-10-02 06:31:14] iter = 14350, loss = 1.4879
[2023-10-02 06:31:14] iter = 14360, loss = 1.5605
[2023-10-02 06:31:15] iter = 14370, loss = 1.6452
[2023-10-02 06:31:16] iter = 14380, loss = 1.5350
[2023-10-02 06:31:17] iter = 14390, loss = 1.4324
[2023-10-02 06:31:18] iter = 14400, loss = 1.5317
[2023-10-02 06:31:19] iter = 14410, loss = 1.5406
[2023-10-02 06:31:20] iter = 14420, loss = 1.4715
[2023-10-02 06:31:21] iter = 14430, loss = 1.6609
[2023-10-02 06:31:22] iter = 14440, loss = 1.5610
[2023-10-02 06:31:23] iter = 14450, loss = 1.5489
[2023-10-02 06:31:24] iter = 14460, loss = 1.5393
[2023-10-02 06:31:24] iter = 14470, loss = 1.5838
[2023-10-02 06:31:25] iter = 14480, loss = 1.4420
[2023-10-02 06:31:26] iter = 14490, loss = 1.5501
[2023-10-02 06:31:27] iter = 14500, loss = 1.6314
[2023-10-02 06:31:28] iter = 14510, loss = 1.3689
[2023-10-02 06:31:29] iter = 14520, loss = 1.4750
[2023-10-02 06:31:30] iter = 14530, loss = 1.5155
[2023-10-02 06:31:31] iter = 14540, loss = 1.4719
[2023-10-02 06:31:31] iter = 14550, loss = 1.5020
[2023-10-02 06:31:32] iter = 14560, loss = 1.6062
[2023-10-02 06:31:33] iter = 14570, loss = 1.5162
[2023-10-02 06:31:34] iter = 14580, loss = 1.5225
[2023-10-02 06:31:35] iter = 14590, loss = 1.5696
[2023-10-02 06:31:36] iter = 14600, loss = 1.5255
[2023-10-02 06:31:37] iter = 14610, loss = 1.4366
[2023-10-02 06:31:38] iter = 14620, loss = 1.3377
[2023-10-02 06:31:39] iter = 14630, loss = 1.4245
[2023-10-02 06:31:40] iter = 14640, loss = 1.4615
[2023-10-02 06:31:40] iter = 14650, loss = 1.5726
[2023-10-02 06:31:41] iter = 14660, loss = 1.4987
[2023-10-02 06:31:42] iter = 14670, loss = 1.4131
[2023-10-02 06:31:43] iter = 14680, loss = 1.5602
[2023-10-02 06:31:44] iter = 14690, loss = 1.4109
[2023-10-02 06:31:45] iter = 14700, loss = 1.4545
[2023-10-02 06:31:46] iter = 14710, loss = 1.5151
[2023-10-02 06:31:47] iter = 14720, loss = 1.5735
[2023-10-02 06:31:48] iter = 14730, loss = 1.6277
[2023-10-02 06:31:49] iter = 14740, loss = 1.5093
[2023-10-02 06:31:49] iter = 14750, loss = 1.5304
[2023-10-02 06:31:50] iter = 14760, loss = 1.4074
[2023-10-02 06:31:51] iter = 14770, loss = 1.3842
[2023-10-02 06:31:52] iter = 14780, loss = 1.6114
[2023-10-02 06:31:53] iter = 14790, loss = 1.5893
[2023-10-02 06:31:54] iter = 14800, loss = 1.6357
[2023-10-02 06:31:55] iter = 14810, loss = 1.5420
[2023-10-02 06:31:56] iter = 14820, loss = 1.4390
[2023-10-02 06:31:57] iter = 14830, loss = 1.5475
[2023-10-02 06:31:58] iter = 14840, loss = 1.3088
[2023-10-02 06:31:59] iter = 14850, loss = 1.4970
[2023-10-02 06:32:00] iter = 14860, loss = 1.5234
[2023-10-02 06:32:01] iter = 14870, loss = 1.4130
[2023-10-02 06:32:01] iter = 14880, loss = 1.3986
[2023-10-02 06:32:02] iter = 14890, loss = 1.4664
[2023-10-02 06:32:03] iter = 14900, loss = 1.4071
[2023-10-02 06:32:04] iter = 14910, loss = 1.4959
[2023-10-02 06:32:05] iter = 14920, loss = 1.5144
[2023-10-02 06:32:06] iter = 14930, loss = 1.5736
[2023-10-02 06:32:07] iter = 14940, loss = 1.5909
[2023-10-02 06:32:08] iter = 14950, loss = 1.6343
[2023-10-02 06:32:09] iter = 14960, loss = 1.5183
[2023-10-02 06:32:10] iter = 14970, loss = 1.5363
[2023-10-02 06:32:11] iter = 14980, loss = 1.7314
[2023-10-02 06:32:11] iter = 14990, loss = 1.5934
[2023-10-02 06:32:12] iter = 15000, loss = 1.4475
[2023-10-02 06:32:13] iter = 15010, loss = 1.6308
[2023-10-02 06:32:14] iter = 15020, loss = 1.5229
[2023-10-02 06:32:15] iter = 15030, loss = 1.4172
[2023-10-02 06:32:16] iter = 15040, loss = 1.5067
[2023-10-02 06:32:17] iter = 15050, loss = 1.4393
[2023-10-02 06:32:18] iter = 15060, loss = 1.4757
[2023-10-02 06:32:19] iter = 15070, loss = 1.4895
[2023-10-02 06:32:20] iter = 15080, loss = 1.6866
[2023-10-02 06:32:20] iter = 15090, loss = 1.5625
[2023-10-02 06:32:21] iter = 15100, loss = 1.6010
[2023-10-02 06:32:22] iter = 15110, loss = 1.5195
[2023-10-02 06:32:23] iter = 15120, loss = 1.5249
[2023-10-02 06:32:24] iter = 15130, loss = 1.5706
[2023-10-02 06:32:25] iter = 15140, loss = 1.4982
[2023-10-02 06:32:26] iter = 15150, loss = 1.5435
[2023-10-02 06:32:27] iter = 15160, loss = 1.6120
[2023-10-02 06:32:27] iter = 15170, loss = 1.4150
[2023-10-02 06:32:28] iter = 15180, loss = 1.5275
[2023-10-02 06:32:29] iter = 15190, loss = 1.6021
[2023-10-02 06:32:30] iter = 15200, loss = 1.5539
[2023-10-02 06:32:31] iter = 15210, loss = 1.5107
[2023-10-02 06:32:32] iter = 15220, loss = 1.5285
[2023-10-02 06:32:33] iter = 15230, loss = 1.4561
[2023-10-02 06:32:34] iter = 15240, loss = 1.4749
[2023-10-02 06:32:35] iter = 15250, loss = 1.4225
[2023-10-02 06:32:35] iter = 15260, loss = 1.5428
[2023-10-02 06:32:36] iter = 15270, loss = 1.4316
[2023-10-02 06:32:37] iter = 15280, loss = 1.6691
[2023-10-02 06:32:38] iter = 15290, loss = 1.3895
[2023-10-02 06:32:39] iter = 15300, loss = 1.5515
[2023-10-02 06:32:40] iter = 15310, loss = 1.5179
[2023-10-02 06:32:41] iter = 15320, loss = 1.5092
[2023-10-02 06:32:42] iter = 15330, loss = 1.4897
[2023-10-02 06:32:43] iter = 15340, loss = 1.3195
[2023-10-02 06:32:44] iter = 15350, loss = 1.5108
[2023-10-02 06:32:44] iter = 15360, loss = 1.4551
[2023-10-02 06:32:45] iter = 15370, loss = 1.4937
[2023-10-02 06:32:46] iter = 15380, loss = 1.3463
[2023-10-02 06:32:47] iter = 15390, loss = 1.6933
[2023-10-02 06:32:48] iter = 15400, loss = 1.6221
[2023-10-02 06:32:49] iter = 15410, loss = 1.5141
[2023-10-02 06:32:50] iter = 15420, loss = 1.5260
[2023-10-02 06:32:51] iter = 15430, loss = 1.5136
[2023-10-02 06:32:52] iter = 15440, loss = 1.4893
[2023-10-02 06:32:53] iter = 15450, loss = 1.5436
[2023-10-02 06:32:53] iter = 15460, loss = 1.5134
[2023-10-02 06:32:54] iter = 15470, loss = 1.3954
[2023-10-02 06:32:55] iter = 15480, loss = 1.4065
[2023-10-02 06:32:56] iter = 15490, loss = 1.5605
[2023-10-02 06:32:57] iter = 15500, loss = 1.4835
[2023-10-02 06:32:58] iter = 15510, loss = 1.6402
[2023-10-02 06:32:59] iter = 15520, loss = 1.3809
[2023-10-02 06:33:00] iter = 15530, loss = 1.5392
[2023-10-02 06:33:01] iter = 15540, loss = 1.5083
[2023-10-02 06:33:02] iter = 15550, loss = 1.6060
[2023-10-02 06:33:03] iter = 15560, loss = 1.4972
[2023-10-02 06:33:04] iter = 15570, loss = 1.4430
[2023-10-02 06:33:05] iter = 15580, loss = 1.5907
[2023-10-02 06:33:05] iter = 15590, loss = 1.4881
[2023-10-02 06:33:06] iter = 15600, loss = 1.3451
[2023-10-02 06:33:07] iter = 15610, loss = 1.4484
[2023-10-02 06:33:08] iter = 15620, loss = 1.5876
[2023-10-02 06:33:09] iter = 15630, loss = 1.3561
[2023-10-02 06:33:10] iter = 15640, loss = 1.5690
[2023-10-02 06:33:11] iter = 15650, loss = 1.5215
[2023-10-02 06:33:12] iter = 15660, loss = 1.3770
[2023-10-02 06:33:13] iter = 15670, loss = 1.4711
[2023-10-02 06:33:14] iter = 15680, loss = 1.5001
[2023-10-02 06:33:14] iter = 15690, loss = 1.5780
[2023-10-02 06:33:15] iter = 15700, loss = 1.4218
[2023-10-02 06:33:16] iter = 15710, loss = 1.5360
[2023-10-02 06:33:17] iter = 15720, loss = 1.4546
[2023-10-02 06:33:18] iter = 15730, loss = 1.4312
[2023-10-02 06:33:19] iter = 15740, loss = 1.4130
[2023-10-02 06:33:20] iter = 15750, loss = 1.6468
[2023-10-02 06:33:20] iter = 15760, loss = 1.4287
[2023-10-02 06:33:21] iter = 15770, loss = 1.4197
[2023-10-02 06:33:22] iter = 15780, loss = 1.6445
[2023-10-02 06:33:23] iter = 15790, loss = 1.4454
[2023-10-02 06:33:24] iter = 15800, loss = 1.5904
[2023-10-02 06:33:25] iter = 15810, loss = 1.5947
[2023-10-02 06:33:26] iter = 15820, loss = 1.4102
[2023-10-02 06:33:27] iter = 15830, loss = 1.5081
[2023-10-02 06:33:28] iter = 15840, loss = 1.6261
[2023-10-02 06:33:29] iter = 15850, loss = 1.6087
[2023-10-02 06:33:30] iter = 15860, loss = 1.5939
[2023-10-02 06:33:30] iter = 15870, loss = 1.5378
[2023-10-02 06:33:31] iter = 15880, loss = 1.5057
[2023-10-02 06:33:32] iter = 15890, loss = 1.6220
[2023-10-02 06:33:33] iter = 15900, loss = 1.5169
[2023-10-02 06:33:34] iter = 15910, loss = 1.5705
[2023-10-02 06:33:35] iter = 15920, loss = 1.3868
[2023-10-02 06:33:36] iter = 15930, loss = 1.5060
[2023-10-02 06:33:37] iter = 15940, loss = 1.5869
[2023-10-02 06:33:38] iter = 15950, loss = 1.4602
[2023-10-02 06:33:39] iter = 15960, loss = 1.5879
[2023-10-02 06:33:40] iter = 15970, loss = 1.4587
[2023-10-02 06:33:41] iter = 15980, loss = 1.5658
[2023-10-02 06:33:42] iter = 15990, loss = 1.6053
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 22850}
[2023-10-02 06:34:07] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.020838 train acc = 0.9960, test acc = 0.6193
[2023-10-02 06:34:31] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.015529 train acc = 0.9980, test acc = 0.6328
[2023-10-02 06:34:55] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.004548 train acc = 1.0000, test acc = 0.6271
[2023-10-02 06:35:19] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.002934 train acc = 1.0000, test acc = 0.6283
[2023-10-02 06:35:43] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.003953 train acc = 1.0000, test acc = 0.6242
[2023-10-02 06:36:07] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.004142 train acc = 1.0000, test acc = 0.6235
[2023-10-02 06:36:32] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.012532 train acc = 1.0000, test acc = 0.6235
[2023-10-02 06:36:55] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.010871 train acc = 0.9980, test acc = 0.6239
[2023-10-02 06:37:20] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.004847 train acc = 1.0000, test acc = 0.6258
[2023-10-02 06:37:44] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.005677 train acc = 1.0000, test acc = 0.6308
[2023-10-02 06:38:08] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.015404 train acc = 1.0000, test acc = 0.6261
[2023-10-02 06:38:32] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004086 train acc = 1.0000, test acc = 0.6256
[2023-10-02 06:38:56] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.004638 train acc = 1.0000, test acc = 0.6216
[2023-10-02 06:39:20] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002142 train acc = 1.0000, test acc = 0.6221
[2023-10-02 06:39:45] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.013045 train acc = 0.9980, test acc = 0.6282
[2023-10-02 06:40:09] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.013072 train acc = 1.0000, test acc = 0.6282
[2023-10-02 06:40:33] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.012400 train acc = 1.0000, test acc = 0.6308
[2023-10-02 06:40:57] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003910 train acc = 1.0000, test acc = 0.6266
[2023-10-02 06:41:21] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.002248 train acc = 1.0000, test acc = 0.6279
[2023-10-02 06:41:45] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.012829 train acc = 1.0000, test acc = 0.6227
Evaluate 20 random ConvNet, mean = 0.6260 std = 0.0033
-------------------------
[2023-10-02 06:41:46] iter = 16000, loss = 1.4525
[2023-10-02 06:41:47] iter = 16010, loss = 1.4724
[2023-10-02 06:41:48] iter = 16020, loss = 1.4070
[2023-10-02 06:41:49] iter = 16030, loss = 1.4180
[2023-10-02 06:41:49] iter = 16040, loss = 1.5635
[2023-10-02 06:41:50] iter = 16050, loss = 1.4959
[2023-10-02 06:41:51] iter = 16060, loss = 1.6453
[2023-10-02 06:41:52] iter = 16070, loss = 1.5798
[2023-10-02 06:41:53] iter = 16080, loss = 1.4945
[2023-10-02 06:41:54] iter = 16090, loss = 1.5802
[2023-10-02 06:41:55] iter = 16100, loss = 1.5991
[2023-10-02 06:41:56] iter = 16110, loss = 1.4685
[2023-10-02 06:41:57] iter = 16120, loss = 1.5335
[2023-10-02 06:41:58] iter = 16130, loss = 1.4506
[2023-10-02 06:41:59] iter = 16140, loss = 1.4948
[2023-10-02 06:42:00] iter = 16150, loss = 1.3831
[2023-10-02 06:42:01] iter = 16160, loss = 1.5368
[2023-10-02 06:42:02] iter = 16170, loss = 1.4389
[2023-10-02 06:42:03] iter = 16180, loss = 1.4646
[2023-10-02 06:42:03] iter = 16190, loss = 1.3783
[2023-10-02 06:42:04] iter = 16200, loss = 1.5096
[2023-10-02 06:42:05] iter = 16210, loss = 1.5769
[2023-10-02 06:42:06] iter = 16220, loss = 1.5036
[2023-10-02 06:42:07] iter = 16230, loss = 1.5047
[2023-10-02 06:42:08] iter = 16240, loss = 1.5793
[2023-10-02 06:42:09] iter = 16250, loss = 1.5964
[2023-10-02 06:42:10] iter = 16260, loss = 1.5604
[2023-10-02 06:42:11] iter = 16270, loss = 1.5053
[2023-10-02 06:42:12] iter = 16280, loss = 1.4239
[2023-10-02 06:42:12] iter = 16290, loss = 1.6403
[2023-10-02 06:42:13] iter = 16300, loss = 1.4683
[2023-10-02 06:42:14] iter = 16310, loss = 1.3914
[2023-10-02 06:42:15] iter = 16320, loss = 1.4079
[2023-10-02 06:42:16] iter = 16330, loss = 1.4146
[2023-10-02 06:42:17] iter = 16340, loss = 1.4117
[2023-10-02 06:42:18] iter = 16350, loss = 1.5428
[2023-10-02 06:42:19] iter = 16360, loss = 1.4707
[2023-10-02 06:42:20] iter = 16370, loss = 1.6268
[2023-10-02 06:42:21] iter = 16380, loss = 1.4692
[2023-10-02 06:42:22] iter = 16390, loss = 1.3782
[2023-10-02 06:42:22] iter = 16400, loss = 1.4572
[2023-10-02 06:42:23] iter = 16410, loss = 1.4841
[2023-10-02 06:42:24] iter = 16420, loss = 1.4365
[2023-10-02 06:42:25] iter = 16430, loss = 1.4705
[2023-10-02 06:42:26] iter = 16440, loss = 1.5921
[2023-10-02 06:42:27] iter = 16450, loss = 1.4694
[2023-10-02 06:42:28] iter = 16460, loss = 1.4290
[2023-10-02 06:42:29] iter = 16470, loss = 1.5810
[2023-10-02 06:42:30] iter = 16480, loss = 1.5283
[2023-10-02 06:42:30] iter = 16490, loss = 1.4121
[2023-10-02 06:42:31] iter = 16500, loss = 1.4505
[2023-10-02 06:42:32] iter = 16510, loss = 1.5933
[2023-10-02 06:42:33] iter = 16520, loss = 1.4525
[2023-10-02 06:42:34] iter = 16530, loss = 1.4915
[2023-10-02 06:42:35] iter = 16540, loss = 1.4132
[2023-10-02 06:42:36] iter = 16550, loss = 1.5685
[2023-10-02 06:42:37] iter = 16560, loss = 1.3679
[2023-10-02 06:42:38] iter = 16570, loss = 1.4562
[2023-10-02 06:42:38] iter = 16580, loss = 1.3114
[2023-10-02 06:42:39] iter = 16590, loss = 1.5509
[2023-10-02 06:42:40] iter = 16600, loss = 1.5028
[2023-10-02 06:42:41] iter = 16610, loss = 1.4281
[2023-10-02 06:42:42] iter = 16620, loss = 1.5218
[2023-10-02 06:42:43] iter = 16630, loss = 1.5282
[2023-10-02 06:42:44] iter = 16640, loss = 1.5800
[2023-10-02 06:42:45] iter = 16650, loss = 1.5796
[2023-10-02 06:42:46] iter = 16660, loss = 1.5788
[2023-10-02 06:42:47] iter = 16670, loss = 1.7998
[2023-10-02 06:42:47] iter = 16680, loss = 1.6238
[2023-10-02 06:42:48] iter = 16690, loss = 1.4984
[2023-10-02 06:42:49] iter = 16700, loss = 1.4319
[2023-10-02 06:42:50] iter = 16710, loss = 1.5409
[2023-10-02 06:42:51] iter = 16720, loss = 1.4740
[2023-10-02 06:42:52] iter = 16730, loss = 1.3708
[2023-10-02 06:42:53] iter = 16740, loss = 1.4176
[2023-10-02 06:42:54] iter = 16750, loss = 1.6630
[2023-10-02 06:42:55] iter = 16760, loss = 1.4034
[2023-10-02 06:42:56] iter = 16770, loss = 1.3361
[2023-10-02 06:42:57] iter = 16780, loss = 1.3878
[2023-10-02 06:42:57] iter = 16790, loss = 1.5605
[2023-10-02 06:42:58] iter = 16800, loss = 1.4376
[2023-10-02 06:42:59] iter = 16810, loss = 1.5388
[2023-10-02 06:43:00] iter = 16820, loss = 1.4683
[2023-10-02 06:43:01] iter = 16830, loss = 1.4525
[2023-10-02 06:43:02] iter = 16840, loss = 1.4251
[2023-10-02 06:43:03] iter = 16850, loss = 1.5777
[2023-10-02 06:43:04] iter = 16860, loss = 1.4433
[2023-10-02 06:43:05] iter = 16870, loss = 1.4626
[2023-10-02 06:43:05] iter = 16880, loss = 1.3785
[2023-10-02 06:43:06] iter = 16890, loss = 1.5969
[2023-10-02 06:43:07] iter = 16900, loss = 1.5538
[2023-10-02 06:43:08] iter = 16910, loss = 1.3933
[2023-10-02 06:43:09] iter = 16920, loss = 1.4320
[2023-10-02 06:43:10] iter = 16930, loss = 1.4613
[2023-10-02 06:43:11] iter = 16940, loss = 1.4525
[2023-10-02 06:43:12] iter = 16950, loss = 1.4475
[2023-10-02 06:43:13] iter = 16960, loss = 1.3984
[2023-10-02 06:43:14] iter = 16970, loss = 1.5880
[2023-10-02 06:43:15] iter = 16980, loss = 1.4834
[2023-10-02 06:43:16] iter = 16990, loss = 1.4530
[2023-10-02 06:43:17] iter = 17000, loss = 1.6112
[2023-10-02 06:43:17] iter = 17010, loss = 1.3852
[2023-10-02 06:43:18] iter = 17020, loss = 1.4229
[2023-10-02 06:43:19] iter = 17030, loss = 1.4965
[2023-10-02 06:43:20] iter = 17040, loss = 1.5855
[2023-10-02 06:43:21] iter = 17050, loss = 1.3830
[2023-10-02 06:43:22] iter = 17060, loss = 1.4057
[2023-10-02 06:43:23] iter = 17070, loss = 1.3223
[2023-10-02 06:43:23] iter = 17080, loss = 1.4990
[2023-10-02 06:43:24] iter = 17090, loss = 1.4640
[2023-10-02 06:43:25] iter = 17100, loss = 1.4975
[2023-10-02 06:43:26] iter = 17110, loss = 1.4393
[2023-10-02 06:43:27] iter = 17120, loss = 1.4946
[2023-10-02 06:43:28] iter = 17130, loss = 1.6477
[2023-10-02 06:43:29] iter = 17140, loss = 1.4069
[2023-10-02 06:43:30] iter = 17150, loss = 1.4820
[2023-10-02 06:43:31] iter = 17160, loss = 1.4264
[2023-10-02 06:43:32] iter = 17170, loss = 1.4679
[2023-10-02 06:43:33] iter = 17180, loss = 1.3899
[2023-10-02 06:43:34] iter = 17190, loss = 1.4463
[2023-10-02 06:43:34] iter = 17200, loss = 1.5748
[2023-10-02 06:43:35] iter = 17210, loss = 1.5530
[2023-10-02 06:43:36] iter = 17220, loss = 1.6592
[2023-10-02 06:43:37] iter = 17230, loss = 1.5210
[2023-10-02 06:43:38] iter = 17240, loss = 1.4382
[2023-10-02 06:43:39] iter = 17250, loss = 1.6505
[2023-10-02 06:43:40] iter = 17260, loss = 1.4444
[2023-10-02 06:43:41] iter = 17270, loss = 1.4546
[2023-10-02 06:43:42] iter = 17280, loss = 1.6254
[2023-10-02 06:43:43] iter = 17290, loss = 1.5105
[2023-10-02 06:43:44] iter = 17300, loss = 1.4321
[2023-10-02 06:43:44] iter = 17310, loss = 1.4307
[2023-10-02 06:43:45] iter = 17320, loss = 1.4827
[2023-10-02 06:43:46] iter = 17330, loss = 1.4487
[2023-10-02 06:43:47] iter = 17340, loss = 1.6442
[2023-10-02 06:43:48] iter = 17350, loss = 1.6127
[2023-10-02 06:43:49] iter = 17360, loss = 1.3819
[2023-10-02 06:43:50] iter = 17370, loss = 1.4980
[2023-10-02 06:43:51] iter = 17380, loss = 1.5387
[2023-10-02 06:43:52] iter = 17390, loss = 1.5221
[2023-10-02 06:43:52] iter = 17400, loss = 1.5434
[2023-10-02 06:43:53] iter = 17410, loss = 1.4617
[2023-10-02 06:43:54] iter = 17420, loss = 1.5935
[2023-10-02 06:43:55] iter = 17430, loss = 1.6191
[2023-10-02 06:43:56] iter = 17440, loss = 1.3930
[2023-10-02 06:43:57] iter = 17450, loss = 1.5486
[2023-10-02 06:43:58] iter = 17460, loss = 1.5093
[2023-10-02 06:43:59] iter = 17470, loss = 1.4681
[2023-10-02 06:44:00] iter = 17480, loss = 1.4708
[2023-10-02 06:44:01] iter = 17490, loss = 1.4186
[2023-10-02 06:44:02] iter = 17500, loss = 1.4945
[2023-10-02 06:44:02] iter = 17510, loss = 1.4089
[2023-10-02 06:44:03] iter = 17520, loss = 1.5191
[2023-10-02 06:44:04] iter = 17530, loss = 1.4354
[2023-10-02 06:44:05] iter = 17540, loss = 1.5654
[2023-10-02 06:44:06] iter = 17550, loss = 1.3804
[2023-10-02 06:44:07] iter = 17560, loss = 1.4656
[2023-10-02 06:44:08] iter = 17570, loss = 1.4107
[2023-10-02 06:44:09] iter = 17580, loss = 1.5592
[2023-10-02 06:44:10] iter = 17590, loss = 1.5385
[2023-10-02 06:44:10] iter = 17600, loss = 1.5616
[2023-10-02 06:44:11] iter = 17610, loss = 1.4421
[2023-10-02 06:44:12] iter = 17620, loss = 1.4238
[2023-10-02 06:44:13] iter = 17630, loss = 1.4984
[2023-10-02 06:44:14] iter = 17640, loss = 1.5345
[2023-10-02 06:44:15] iter = 17650, loss = 1.5192
[2023-10-02 06:44:16] iter = 17660, loss = 1.5181
[2023-10-02 06:44:17] iter = 17670, loss = 1.4731
[2023-10-02 06:44:18] iter = 17680, loss = 1.3818
[2023-10-02 06:44:18] iter = 17690, loss = 1.6480
[2023-10-02 06:44:19] iter = 17700, loss = 1.5904
[2023-10-02 06:44:20] iter = 17710, loss = 1.4310
[2023-10-02 06:44:21] iter = 17720, loss = 1.4408
[2023-10-02 06:44:22] iter = 17730, loss = 1.4556
[2023-10-02 06:44:23] iter = 17740, loss = 1.5037
[2023-10-02 06:44:24] iter = 17750, loss = 1.6086
[2023-10-02 06:44:25] iter = 17760, loss = 1.5216
[2023-10-02 06:44:26] iter = 17770, loss = 1.4626
[2023-10-02 06:44:26] iter = 17780, loss = 1.4299
[2023-10-02 06:44:27] iter = 17790, loss = 1.3688
[2023-10-02 06:44:28] iter = 17800, loss = 1.4230
[2023-10-02 06:44:29] iter = 17810, loss = 1.4081
[2023-10-02 06:44:30] iter = 17820, loss = 1.4196
[2023-10-02 06:44:31] iter = 17830, loss = 1.3674
[2023-10-02 06:44:32] iter = 17840, loss = 1.4201
[2023-10-02 06:44:33] iter = 17850, loss = 1.4397
[2023-10-02 06:44:34] iter = 17860, loss = 1.5128
[2023-10-02 06:44:35] iter = 17870, loss = 1.3759
[2023-10-02 06:44:36] iter = 17880, loss = 1.4987
[2023-10-02 06:44:37] iter = 17890, loss = 1.3205
[2023-10-02 06:44:37] iter = 17900, loss = 1.4881
[2023-10-02 06:44:38] iter = 17910, loss = 1.3963
[2023-10-02 06:44:39] iter = 17920, loss = 1.5728
[2023-10-02 06:44:40] iter = 17930, loss = 1.5981
[2023-10-02 06:44:41] iter = 17940, loss = 1.5664
[2023-10-02 06:44:42] iter = 17950, loss = 1.5587
[2023-10-02 06:44:43] iter = 17960, loss = 1.4932
[2023-10-02 06:44:44] iter = 17970, loss = 1.3942
[2023-10-02 06:44:45] iter = 17980, loss = 1.3671
[2023-10-02 06:44:46] iter = 17990, loss = 1.5401
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 86871}
[2023-10-02 06:45:11] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.003097 train acc = 1.0000, test acc = 0.6316
[2023-10-02 06:45:35] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.004121 train acc = 1.0000, test acc = 0.6263
[2023-10-02 06:46:00] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.015532 train acc = 1.0000, test acc = 0.6306
[2023-10-02 06:46:24] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.006227 train acc = 1.0000, test acc = 0.6357
[2023-10-02 06:46:48] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.005056 train acc = 1.0000, test acc = 0.6322
[2023-10-02 06:47:12] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.006292 train acc = 1.0000, test acc = 0.6337
[2023-10-02 06:47:36] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.016538 train acc = 1.0000, test acc = 0.6326
[2023-10-02 06:48:00] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.011967 train acc = 1.0000, test acc = 0.6327
[2023-10-02 06:48:24] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.006370 train acc = 1.0000, test acc = 0.6274
[2023-10-02 06:48:48] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.013636 train acc = 1.0000, test acc = 0.6274
[2023-10-02 06:49:12] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004081 train acc = 1.0000, test acc = 0.6331
[2023-10-02 06:49:36] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004074 train acc = 1.0000, test acc = 0.6342
[2023-10-02 06:50:01] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.002780 train acc = 1.0000, test acc = 0.6362
[2023-10-02 06:50:25] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.002718 train acc = 1.0000, test acc = 0.6258
[2023-10-02 06:50:49] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.002446 train acc = 1.0000, test acc = 0.6340
[2023-10-02 06:51:13] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.015599 train acc = 0.9980, test acc = 0.6317
[2023-10-02 06:51:37] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.020875 train acc = 1.0000, test acc = 0.6242
[2023-10-02 06:52:01] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.016344 train acc = 1.0000, test acc = 0.6331
[2023-10-02 06:52:25] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.013969 train acc = 1.0000, test acc = 0.6274
[2023-10-02 06:52:50] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.018098 train acc = 1.0000, test acc = 0.6283
Evaluate 20 random ConvNet, mean = 0.6309 std = 0.0034
-------------------------
[2023-10-02 06:52:50] iter = 18000, loss = 1.4099
[2023-10-02 06:52:51] iter = 18010, loss = 1.4873
[2023-10-02 06:52:52] iter = 18020, loss = 1.5810
[2023-10-02 06:52:53] iter = 18030, loss = 1.6219
[2023-10-02 06:52:53] iter = 18040, loss = 1.5230
[2023-10-02 06:52:54] iter = 18050, loss = 1.5070
[2023-10-02 06:52:55] iter = 18060, loss = 1.3808
[2023-10-02 06:52:56] iter = 18070, loss = 1.6073
[2023-10-02 06:52:57] iter = 18080, loss = 1.4877
[2023-10-02 06:52:58] iter = 18090, loss = 1.4741
[2023-10-02 06:52:59] iter = 18100, loss = 1.4249
[2023-10-02 06:53:00] iter = 18110, loss = 1.5079
[2023-10-02 06:53:00] iter = 18120, loss = 1.4313
[2023-10-02 06:53:01] iter = 18130, loss = 1.5264
[2023-10-02 06:53:02] iter = 18140, loss = 1.4182
[2023-10-02 06:53:03] iter = 18150, loss = 1.3905
[2023-10-02 06:53:04] iter = 18160, loss = 1.6575
[2023-10-02 06:53:05] iter = 18170, loss = 1.5745
[2023-10-02 06:53:06] iter = 18180, loss = 1.4954
[2023-10-02 06:53:07] iter = 18190, loss = 1.5793
[2023-10-02 06:53:08] iter = 18200, loss = 1.5738
[2023-10-02 06:53:09] iter = 18210, loss = 1.4420
[2023-10-02 06:53:10] iter = 18220, loss = 1.3686
[2023-10-02 06:53:11] iter = 18230, loss = 1.3559
[2023-10-02 06:53:12] iter = 18240, loss = 1.4966
[2023-10-02 06:53:13] iter = 18250, loss = 1.3772
[2023-10-02 06:53:13] iter = 18260, loss = 1.3953
[2023-10-02 06:53:14] iter = 18270, loss = 1.4725
[2023-10-02 06:53:15] iter = 18280, loss = 1.6158
[2023-10-02 06:53:16] iter = 18290, loss = 1.4086
[2023-10-02 06:53:17] iter = 18300, loss = 1.4137
[2023-10-02 06:53:18] iter = 18310, loss = 1.5945
[2023-10-02 06:53:19] iter = 18320, loss = 1.5504
[2023-10-02 06:53:20] iter = 18330, loss = 1.4365
[2023-10-02 06:53:21] iter = 18340, loss = 1.5115
[2023-10-02 06:53:22] iter = 18350, loss = 1.5969
[2023-10-02 06:53:22] iter = 18360, loss = 1.3442
[2023-10-02 06:53:23] iter = 18370, loss = 1.7052
[2023-10-02 06:53:24] iter = 18380, loss = 1.4721
[2023-10-02 06:53:25] iter = 18390, loss = 1.4768
[2023-10-02 06:53:26] iter = 18400, loss = 1.4368
[2023-10-02 06:53:27] iter = 18410, loss = 1.5046
[2023-10-02 06:53:28] iter = 18420, loss = 1.4255
[2023-10-02 06:53:29] iter = 18430, loss = 1.4933
[2023-10-02 06:53:30] iter = 18440, loss = 1.3380
[2023-10-02 06:53:31] iter = 18450, loss = 1.5720
[2023-10-02 06:53:32] iter = 18460, loss = 1.4472
[2023-10-02 06:53:32] iter = 18470, loss = 1.4905
[2023-10-02 06:53:33] iter = 18480, loss = 1.4796
[2023-10-02 06:53:34] iter = 18490, loss = 1.3841
[2023-10-02 06:53:35] iter = 18500, loss = 1.6468
[2023-10-02 06:53:36] iter = 18510, loss = 1.5902
[2023-10-02 06:53:37] iter = 18520, loss = 1.4153
[2023-10-02 06:53:38] iter = 18530, loss = 1.4143
[2023-10-02 06:53:39] iter = 18540, loss = 1.4889
[2023-10-02 06:53:40] iter = 18550, loss = 1.5764
[2023-10-02 06:53:41] iter = 18560, loss = 1.3970
[2023-10-02 06:53:42] iter = 18570, loss = 1.4972
[2023-10-02 06:53:43] iter = 18580, loss = 1.4390
[2023-10-02 06:53:44] iter = 18590, loss = 1.5114
[2023-10-02 06:53:44] iter = 18600, loss = 1.4183
[2023-10-02 06:53:45] iter = 18610, loss = 1.6418
[2023-10-02 06:53:46] iter = 18620, loss = 1.5268
[2023-10-02 06:53:47] iter = 18630, loss = 1.6354
[2023-10-02 06:53:48] iter = 18640, loss = 1.4532
[2023-10-02 06:53:49] iter = 18650, loss = 1.5382
[2023-10-02 06:53:50] iter = 18660, loss = 1.5236
[2023-10-02 06:53:51] iter = 18670, loss = 1.5991
[2023-10-02 06:53:52] iter = 18680, loss = 1.5474
[2023-10-02 06:53:53] iter = 18690, loss = 1.4395
[2023-10-02 06:53:54] iter = 18700, loss = 1.5024
[2023-10-02 06:53:55] iter = 18710, loss = 1.3344
[2023-10-02 06:53:56] iter = 18720, loss = 1.3632
[2023-10-02 06:53:56] iter = 18730, loss = 1.4574
[2023-10-02 06:53:57] iter = 18740, loss = 1.4010
[2023-10-02 06:53:58] iter = 18750, loss = 1.3268
[2023-10-02 06:53:59] iter = 18760, loss = 1.6268
[2023-10-02 06:54:00] iter = 18770, loss = 1.3772
[2023-10-02 06:54:01] iter = 18780, loss = 1.4689
[2023-10-02 06:54:02] iter = 18790, loss = 1.3999
[2023-10-02 06:54:03] iter = 18800, loss = 1.4635
[2023-10-02 06:54:04] iter = 18810, loss = 1.4475
[2023-10-02 06:54:05] iter = 18820, loss = 1.4661
[2023-10-02 06:54:06] iter = 18830, loss = 1.4152
[2023-10-02 06:54:06] iter = 18840, loss = 1.5153
[2023-10-02 06:54:07] iter = 18850, loss = 1.5191
[2023-10-02 06:54:08] iter = 18860, loss = 1.5520
[2023-10-02 06:54:09] iter = 18870, loss = 1.3146
[2023-10-02 06:54:10] iter = 18880, loss = 1.4212
[2023-10-02 06:54:11] iter = 18890, loss = 1.4885
[2023-10-02 06:54:12] iter = 18900, loss = 1.4576
[2023-10-02 06:54:13] iter = 18910, loss = 1.5143
[2023-10-02 06:54:14] iter = 18920, loss = 1.3908
[2023-10-02 06:54:15] iter = 18930, loss = 1.4050
[2023-10-02 06:54:15] iter = 18940, loss = 1.5852
[2023-10-02 06:54:16] iter = 18950, loss = 1.3649
[2023-10-02 06:54:17] iter = 18960, loss = 1.5427
[2023-10-02 06:54:18] iter = 18970, loss = 1.3781
[2023-10-02 06:54:19] iter = 18980, loss = 1.2910
[2023-10-02 06:54:20] iter = 18990, loss = 1.6066
[2023-10-02 06:54:21] iter = 19000, loss = 1.6055
[2023-10-02 06:54:22] iter = 19010, loss = 1.4501
[2023-10-02 06:54:23] iter = 19020, loss = 1.5692
[2023-10-02 06:54:24] iter = 19030, loss = 1.5770
[2023-10-02 06:54:24] iter = 19040, loss = 1.5072
[2023-10-02 06:54:25] iter = 19050, loss = 1.4818
[2023-10-02 06:54:26] iter = 19060, loss = 1.3562
[2023-10-02 06:54:27] iter = 19070, loss = 1.4138
[2023-10-02 06:54:28] iter = 19080, loss = 1.4338
[2023-10-02 06:54:29] iter = 19090, loss = 1.6067
[2023-10-02 06:54:30] iter = 19100, loss = 1.4299
[2023-10-02 06:54:31] iter = 19110, loss = 1.4707
[2023-10-02 06:54:32] iter = 19120, loss = 1.4374
[2023-10-02 06:54:33] iter = 19130, loss = 1.4876
[2023-10-02 06:54:33] iter = 19140, loss = 1.4853
[2023-10-02 06:54:34] iter = 19150, loss = 1.5283
[2023-10-02 06:54:35] iter = 19160, loss = 1.4696
[2023-10-02 06:54:36] iter = 19170, loss = 1.3549
[2023-10-02 06:54:37] iter = 19180, loss = 1.3384
[2023-10-02 06:54:38] iter = 19190, loss = 1.5847
[2023-10-02 06:54:39] iter = 19200, loss = 1.6536
[2023-10-02 06:54:40] iter = 19210, loss = 1.5111
[2023-10-02 06:54:41] iter = 19220, loss = 1.5274
[2023-10-02 06:54:42] iter = 19230, loss = 1.5121
[2023-10-02 06:54:43] iter = 19240, loss = 1.5135
[2023-10-02 06:54:44] iter = 19250, loss = 1.4700
[2023-10-02 06:54:44] iter = 19260, loss = 1.4264
[2023-10-02 06:54:45] iter = 19270, loss = 1.4897
[2023-10-02 06:54:46] iter = 19280, loss = 1.5039
[2023-10-02 06:54:47] iter = 19290, loss = 1.5434
[2023-10-02 06:54:48] iter = 19300, loss = 1.5052
[2023-10-02 06:54:49] iter = 19310, loss = 1.5335
[2023-10-02 06:54:50] iter = 19320, loss = 1.5446
[2023-10-02 06:54:51] iter = 19330, loss = 1.3905
[2023-10-02 06:54:52] iter = 19340, loss = 1.3733
[2023-10-02 06:54:52] iter = 19350, loss = 1.5102
[2023-10-02 06:54:53] iter = 19360, loss = 1.6382
[2023-10-02 06:54:54] iter = 19370, loss = 1.5046
[2023-10-02 06:54:55] iter = 19380, loss = 1.5389
[2023-10-02 06:54:56] iter = 19390, loss = 1.3801
[2023-10-02 06:54:57] iter = 19400, loss = 1.4630
[2023-10-02 06:54:58] iter = 19410, loss = 1.3512
[2023-10-02 06:54:59] iter = 19420, loss = 1.4704
[2023-10-02 06:55:00] iter = 19430, loss = 1.6446
[2023-10-02 06:55:01] iter = 19440, loss = 1.4746
[2023-10-02 06:55:02] iter = 19450, loss = 1.5229
[2023-10-02 06:55:02] iter = 19460, loss = 1.5614
[2023-10-02 06:55:03] iter = 19470, loss = 1.5172
[2023-10-02 06:55:04] iter = 19480, loss = 1.4689
[2023-10-02 06:55:05] iter = 19490, loss = 1.5449
[2023-10-02 06:55:06] iter = 19500, loss = 1.4823
[2023-10-02 06:55:07] iter = 19510, loss = 1.5062
[2023-10-02 06:55:08] iter = 19520, loss = 1.3970
[2023-10-02 06:55:09] iter = 19530, loss = 1.4236
[2023-10-02 06:55:10] iter = 19540, loss = 1.6428
[2023-10-02 06:55:11] iter = 19550, loss = 1.7221
[2023-10-02 06:55:12] iter = 19560, loss = 1.5422
[2023-10-02 06:55:12] iter = 19570, loss = 1.6629
[2023-10-02 06:55:13] iter = 19580, loss = 1.5480
[2023-10-02 06:55:14] iter = 19590, loss = 1.6476
[2023-10-02 06:55:15] iter = 19600, loss = 1.5384
[2023-10-02 06:55:16] iter = 19610, loss = 1.5173
[2023-10-02 06:55:17] iter = 19620, loss = 1.4525
[2023-10-02 06:55:18] iter = 19630, loss = 1.4027
[2023-10-02 06:55:19] iter = 19640, loss = 1.4196
[2023-10-02 06:55:20] iter = 19650, loss = 1.5079
[2023-10-02 06:55:21] iter = 19660, loss = 1.4512
[2023-10-02 06:55:22] iter = 19670, loss = 1.4909
[2023-10-02 06:55:22] iter = 19680, loss = 1.5402
[2023-10-02 06:55:23] iter = 19690, loss = 1.4174
[2023-10-02 06:55:24] iter = 19700, loss = 1.4930
[2023-10-02 06:55:25] iter = 19710, loss = 1.5562
[2023-10-02 06:55:26] iter = 19720, loss = 1.5379
[2023-10-02 06:55:27] iter = 19730, loss = 1.6327
[2023-10-02 06:55:28] iter = 19740, loss = 1.5398
[2023-10-02 06:55:29] iter = 19750, loss = 1.3477
[2023-10-02 06:55:30] iter = 19760, loss = 1.5212
[2023-10-02 06:55:31] iter = 19770, loss = 1.4634
[2023-10-02 06:55:32] iter = 19780, loss = 1.5554
[2023-10-02 06:55:33] iter = 19790, loss = 1.3938
[2023-10-02 06:55:34] iter = 19800, loss = 1.4700
[2023-10-02 06:55:35] iter = 19810, loss = 1.5149
[2023-10-02 06:55:35] iter = 19820, loss = 1.5864
[2023-10-02 06:55:36] iter = 19830, loss = 1.6117
[2023-10-02 06:55:37] iter = 19840, loss = 1.5494
[2023-10-02 06:55:38] iter = 19850, loss = 1.6450
[2023-10-02 06:55:39] iter = 19860, loss = 1.5298
[2023-10-02 06:55:40] iter = 19870, loss = 1.4165
[2023-10-02 06:55:41] iter = 19880, loss = 1.4532
[2023-10-02 06:55:42] iter = 19890, loss = 1.6852
[2023-10-02 06:55:43] iter = 19900, loss = 1.4030
[2023-10-02 06:55:44] iter = 19910, loss = 1.5454
[2023-10-02 06:55:45] iter = 19920, loss = 1.4021
[2023-10-02 06:55:45] iter = 19930, loss = 1.5116
[2023-10-02 06:55:46] iter = 19940, loss = 1.4409
[2023-10-02 06:55:47] iter = 19950, loss = 1.4435
[2023-10-02 06:55:48] iter = 19960, loss = 1.4720
[2023-10-02 06:55:49] iter = 19970, loss = 1.4993
[2023-10-02 06:55:50] iter = 19980, loss = 1.5265
[2023-10-02 06:55:51] iter = 19990, loss = 1.4510
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
DSA augmentation strategy: 
 color_crop_cutout_flip_scale_rotate
DSA augmentation parameters: 
 {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 52010}
[2023-10-02 06:56:16] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.010022 train acc = 1.0000, test acc = 0.6302
[2023-10-02 06:56:40] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.013234 train acc = 1.0000, test acc = 0.6379
[2023-10-02 06:57:04] Evaluate_02: epoch = 1000 train time = 22 s train loss = 0.002156 train acc = 1.0000, test acc = 0.6227
[2023-10-02 06:57:28] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.006321 train acc = 1.0000, test acc = 0.6343
[2023-10-02 06:57:52] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.005231 train acc = 1.0000, test acc = 0.6369
[2023-10-02 06:58:16] Evaluate_05: epoch = 1000 train time = 22 s train loss = 0.011810 train acc = 1.0000, test acc = 0.6352
[2023-10-02 06:58:40] Evaluate_06: epoch = 1000 train time = 22 s train loss = 0.002145 train acc = 1.0000, test acc = 0.6305
[2023-10-02 06:59:05] Evaluate_07: epoch = 1000 train time = 22 s train loss = 0.031656 train acc = 0.9960, test acc = 0.6364
[2023-10-02 06:59:29] Evaluate_08: epoch = 1000 train time = 22 s train loss = 0.002456 train acc = 1.0000, test acc = 0.6269
[2023-10-02 06:59:53] Evaluate_09: epoch = 1000 train time = 22 s train loss = 0.002154 train acc = 1.0000, test acc = 0.6201
[2023-10-02 07:00:17] Evaluate_10: epoch = 1000 train time = 22 s train loss = 0.004044 train acc = 1.0000, test acc = 0.6286
[2023-10-02 07:00:41] Evaluate_11: epoch = 1000 train time = 22 s train loss = 0.004983 train acc = 1.0000, test acc = 0.6372
[2023-10-02 07:01:05] Evaluate_12: epoch = 1000 train time = 22 s train loss = 0.019091 train acc = 1.0000, test acc = 0.6318
[2023-10-02 07:01:30] Evaluate_13: epoch = 1000 train time = 22 s train loss = 0.001908 train acc = 1.0000, test acc = 0.6296
[2023-10-02 07:01:54] Evaluate_14: epoch = 1000 train time = 22 s train loss = 0.014776 train acc = 1.0000, test acc = 0.6259
[2023-10-02 07:02:18] Evaluate_15: epoch = 1000 train time = 22 s train loss = 0.019601 train acc = 1.0000, test acc = 0.6249
[2023-10-02 07:02:42] Evaluate_16: epoch = 1000 train time = 22 s train loss = 0.024388 train acc = 0.9980, test acc = 0.6268
[2023-10-02 07:03:06] Evaluate_17: epoch = 1000 train time = 22 s train loss = 0.003471 train acc = 1.0000, test acc = 0.6243
[2023-10-02 07:03:30] Evaluate_18: epoch = 1000 train time = 22 s train loss = 0.012850 train acc = 1.0000, test acc = 0.6273
[2023-10-02 07:03:54] Evaluate_19: epoch = 1000 train time = 22 s train loss = 0.005539 train acc = 1.0000, test acc = 0.6292
Evaluate 20 random ConvNet, mean = 0.6298 std = 0.0050
-------------------------
[2023-10-02 07:03:55] iter = 20000, loss = 1.5141

==================== Final Results ====================

Run 5 experiments, train on ConvNet, evaluate 100 random ConvNet, mean  = 62.78%  std = 0.42%
