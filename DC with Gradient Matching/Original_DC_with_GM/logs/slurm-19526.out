/data/happythgus/DatasetCondensation
/data/opt/anaconda3/bin/python
moana-r1
eval_it_pool:  [0, 500, 1000]
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz
  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 32768/170498071 [00:00<25:28, 111553.88it/s]  0%|          | 65536/170498071 [00:00<25:27, 111567.52it/s]  0%|          | 98304/170498071 [00:00<25:27, 111544.96it/s]  0%|          | 229376/170498071 [00:01<11:41, 242678.24it/s]  0%|          | 425984/170498071 [00:01<07:09, 396082.92it/s]  0%|          | 655360/170498071 [00:01<05:56, 476304.56it/s]  0%|          | 720896/170498071 [00:02<06:27, 437755.52it/s]  1%|          | 1146880/170498071 [00:02<02:54, 968595.34it/s]  1%|          | 1441792/170498071 [00:02<02:30, 1126392.02it/s]  1%|▏         | 2162688/170498071 [00:02<01:18, 2144912.50it/s]  1%|▏         | 2490368/170498071 [00:02<01:26, 1939293.29it/s]  2%|▏         | 2883584/170498071 [00:02<01:36, 1745877.77it/s]  2%|▏         | 3276800/170498071 [00:03<01:19, 2099918.41it/s]  2%|▏         | 3571712/170498071 [00:03<01:28, 1879119.61it/s]  2%|▏         | 3997696/170498071 [00:03<01:11, 2318245.72it/s]  3%|▎         | 4292608/170498071 [00:03<01:23, 1998170.47it/s]  3%|▎         | 4685824/170498071 [00:03<01:26, 1907996.02it/s]  3%|▎         | 5046272/170498071 [00:03<01:18, 2103327.46it/s]  3%|▎         | 5341184/170498071 [00:04<01:28, 1869149.43it/s]  3%|▎         | 5767168/170498071 [00:04<01:11, 2314677.74it/s]  4%|▎         | 6062080/170498071 [00:04<01:21, 2014837.31it/s]  4%|▍         | 6488064/170498071 [00:04<01:07, 2414201.19it/s]  4%|▍         | 6782976/170498071 [00:04<01:17, 2113420.02it/s]  4%|▍         | 7176192/170498071 [00:04<01:06, 2465362.22it/s]  4%|▍         | 7471104/170498071 [00:05<01:16, 2136541.70it/s]  5%|▍         | 7864320/170498071 [00:05<01:04, 2505009.20it/s]  5%|▍         | 8159232/170498071 [00:05<01:15, 2152135.13it/s]  5%|▌         | 8585216/170498071 [00:05<01:03, 2569191.75it/s]  5%|▌         | 8880128/170498071 [00:05<01:13, 2195483.40it/s]  5%|▌         | 9306112/170498071 [00:05<01:01, 2616437.52it/s]  6%|▌         | 9633792/170498071 [00:05<01:10, 2273862.78it/s]  6%|▌         | 10059776/170498071 [00:06<01:00, 2657021.35it/s]  6%|▌         | 10387456/170498071 [00:06<01:09, 2314400.93it/s]  6%|▋         | 10813440/170498071 [00:06<00:59, 2689873.73it/s]  7%|▋         | 11141120/170498071 [00:06<01:08, 2339907.36it/s]  7%|▋         | 11567104/170498071 [00:06<00:58, 2708815.34it/s]  7%|▋         | 11894784/170498071 [00:06<01:07, 2357765.40it/s]  7%|▋         | 12288000/170498071 [00:06<00:58, 2689034.03it/s]  7%|▋         | 12615680/170498071 [00:07<01:07, 2326088.28it/s]  8%|▊         | 13041664/170498071 [00:07<00:57, 2733494.84it/s]  8%|▊         | 13369344/170498071 [00:07<01:07, 2344113.49it/s]  8%|▊         | 13828096/170498071 [00:07<00:56, 2781124.25it/s]  8%|▊         | 14155776/170498071 [00:07<01:05, 2396337.95it/s]  9%|▊         | 14581760/170498071 [00:07<00:56, 2780922.69it/s]  9%|▊         | 14909440/170498071 [00:07<01:05, 2387478.22it/s]  9%|▉         | 15335424/170498071 [00:08<00:55, 2787506.38it/s]  9%|▉         | 15663104/170498071 [00:08<01:05, 2380853.63it/s]  9%|▉         | 16121856/170498071 [00:08<00:55, 2806816.30it/s] 10%|▉         | 16449536/170498071 [00:08<01:03, 2411025.85it/s] 10%|▉         | 16875520/170498071 [00:08<00:54, 2798396.51it/s] 10%|█         | 17203200/170498071 [00:08<01:03, 2397257.60it/s] 10%|█         | 17661952/170498071 [00:08<00:54, 2819493.37it/s] 11%|█         | 17989632/170498071 [00:09<01:02, 2425236.28it/s] 11%|█         | 18415616/170498071 [00:09<00:54, 2808048.73it/s] 11%|█         | 18743296/170498071 [00:09<01:03, 2401546.33it/s] 11%|█▏        | 19202048/170498071 [00:09<00:53, 2813875.50it/s] 11%|█▏        | 19529728/170498071 [00:09<01:02, 2424467.77it/s] 12%|█▏        | 19955712/170498071 [00:09<00:53, 2808791.97it/s] 12%|█▏        | 20283392/170498071 [00:10<01:02, 2396210.32it/s] 12%|█▏        | 20742144/170498071 [00:10<00:53, 2821125.04it/s] 12%|█▏        | 21069824/170498071 [00:10<01:01, 2423958.51it/s] 13%|█▎        | 21528576/170498071 [00:10<00:52, 2833594.08it/s] 13%|█▎        | 21856256/170498071 [00:10<01:01, 2436570.67it/s] 13%|█▎        | 22282240/170498071 [00:10<00:52, 2802498.47it/s] 13%|█▎        | 22609920/170498071 [00:10<01:01, 2405408.44it/s] 14%|█▎        | 23068672/170498071 [00:11<00:51, 2837332.16it/s] 14%|█▎        | 23396352/170498071 [00:11<01:00, 2428488.84it/s] 14%|█▍        | 23822336/170498071 [00:11<00:52, 2802142.16it/s] 14%|█▍        | 24150016/170498071 [00:11<01:00, 2403300.51it/s] 14%|█▍        | 24576000/170498071 [00:11<00:52, 2800785.74it/s] 15%|█▍        | 24903680/170498071 [00:11<01:00, 2402084.14it/s] 15%|█▍        | 25362432/170498071 [00:11<00:51, 2815835.62it/s] 15%|█▌        | 25690112/170498071 [00:12<00:59, 2416914.20it/s] 15%|█▌        | 26148864/170498071 [00:12<00:50, 2847625.50it/s] 16%|█▌        | 26476544/170498071 [00:12<00:59, 2435994.01it/s] 16%|█▌        | 26935296/170498071 [00:12<00:50, 2862505.28it/s] 16%|█▌        | 27262976/170498071 [00:12<00:58, 2446193.43it/s] 16%|█▋        | 27721728/170498071 [00:12<00:49, 2866810.03it/s] 16%|█▋        | 28049408/170498071 [00:13<00:58, 2442876.62it/s] 17%|█▋        | 28508160/170498071 [00:13<00:49, 2882303.23it/s] 17%|█▋        | 28835840/170498071 [00:13<00:57, 2449599.75it/s] 17%|█▋        | 29294592/170498071 [00:13<00:48, 2892038.81it/s] 17%|█▋        | 29655040/170498071 [00:13<00:56, 2505296.64it/s] 18%|█▊        | 30081024/170498071 [00:13<00:49, 2863057.68it/s] 18%|█▊        | 30408704/170498071 [00:13<00:57, 2450384.74it/s] 18%|█▊        | 30867456/170498071 [00:14<00:47, 2912331.84it/s] 18%|█▊        | 31227904/170498071 [00:14<00:55, 2507256.60it/s] 19%|█▊        | 31719424/170498071 [00:14<00:46, 2981835.31it/s] 19%|█▉        | 32079872/170498071 [00:14<00:53, 2578583.39it/s] 19%|█▉        | 32538624/170498071 [00:14<00:46, 2976297.97it/s] 19%|█▉        | 32899072/170498071 [00:14<00:53, 2583431.40it/s] 20%|█▉        | 33390592/170498071 [00:14<00:45, 3036611.45it/s] 20%|█▉        | 33751040/170498071 [00:15<00:52, 2623486.07it/s] 20%|██        | 34242560/170498071 [00:15<00:44, 3064124.24it/s] 20%|██        | 34603008/170498071 [00:15<00:51, 2651430.40it/s] 21%|██        | 35094528/170498071 [00:15<00:43, 3116844.78it/s] 21%|██        | 35454976/170498071 [00:15<00:50, 2669663.60it/s] 21%|██        | 35979264/170498071 [00:15<00:42, 3169857.33it/s] 21%|██▏       | 36339712/170498071 [00:15<00:49, 2719469.58it/s] 22%|██▏       | 36864000/170498071 [00:16<00:41, 3232421.32it/s] 22%|██▏       | 37257216/170498071 [00:16<00:47, 2793318.19it/s] 22%|██▏       | 37781504/170498071 [00:16<00:40, 3299490.91it/s] 22%|██▏       | 38174720/170498071 [00:16<00:46, 2846281.39it/s] 23%|██▎       | 38731776/170498071 [00:16<00:39, 3375759.68it/s] 23%|██▎       | 39124992/170498071 [00:16<00:45, 2912855.20it/s] 23%|██▎       | 39682048/170498071 [00:16<00:37, 3450057.25it/s] 24%|██▎       | 40075264/170498071 [00:17<00:44, 2940034.39it/s] 24%|██▍       | 40665088/170498071 [00:17<00:36, 3567770.94it/s] 24%|██▍       | 41091072/170498071 [00:17<00:42, 3046365.67it/s] 24%|██▍       | 41680896/170498071 [00:17<00:35, 3660606.29it/s] 25%|██▍       | 42106880/170498071 [00:17<00:42, 3037064.86it/s] 25%|██▍       | 42532864/170498071 [00:17<00:39, 3275851.07it/s] 25%|██▌       | 43024384/170498071 [00:18<00:39, 3249725.58it/s] 25%|██▌       | 43384832/170498071 [00:18<00:38, 3308473.27it/s] 26%|██▌       | 43909120/170498071 [00:18<00:33, 3738191.45it/s] 26%|██▌       | 44335104/170498071 [00:18<00:37, 3379250.36it/s] 26%|██▋       | 44826624/170498071 [00:18<00:33, 3742727.91it/s] 27%|██▋       | 45252608/170498071 [00:18<00:34, 3585979.83it/s] 27%|██▋       | 45645824/170498071 [00:18<00:34, 3663750.47it/s] 27%|██▋       | 46170112/170498071 [00:18<00:30, 4053391.30it/s] 27%|██▋       | 46596096/170498071 [00:18<00:33, 3673304.62it/s] 28%|██▊       | 47087616/170498071 [00:19<00:30, 3992857.36it/s] 28%|██▊       | 47611904/170498071 [00:19<00:29, 4170008.09it/s] 28%|██▊       | 48070656/170498071 [00:19<00:31, 3860584.07it/s] 29%|██▊       | 48693248/170498071 [00:19<00:27, 4390540.55it/s] 29%|██▉       | 49152000/170498071 [00:19<00:30, 3918132.59it/s] 29%|██▉       | 49741824/170498071 [00:19<00:27, 4383438.32it/s] 29%|██▉       | 50200576/170498071 [00:19<00:29, 4118026.91it/s] 30%|██▉       | 50659328/170498071 [00:19<00:28, 4238451.67it/s] 30%|███       | 51314688/170498071 [00:20<00:24, 4815320.25it/s] 30%|███       | 51838976/170498071 [00:20<00:27, 4285812.53it/s] 31%|███       | 52461568/170498071 [00:20<00:24, 4754454.06it/s] 31%|███       | 52985856/170498071 [00:20<00:26, 4434511.54it/s] 31%|███▏      | 53542912/170498071 [00:20<00:24, 4699598.40it/s] 32%|███▏      | 54231040/170498071 [00:20<00:22, 5239320.48it/s] 32%|███▏      | 54788096/170498071 [00:20<00:24, 4634765.05it/s] 33%|███▎      | 55476224/170498071 [00:20<00:22, 5191112.27it/s] 33%|███▎      | 56033280/170498071 [00:21<00:23, 4798859.56it/s] 33%|███▎      | 56655872/170498071 [00:21<00:22, 5142610.40it/s] 34%|███▎      | 57376768/170498071 [00:21<00:20, 5643963.47it/s] 34%|███▍      | 57966592/170498071 [00:21<00:22, 4993441.40it/s] 34%|███▍      | 58720256/170498071 [00:21<00:19, 5640271.68it/s] 35%|███▍      | 59342848/170498071 [00:21<00:21, 5213795.12it/s] 35%|███▌      | 60030976/170498071 [00:21<00:19, 5602534.61it/s] 36%|███▌      | 60817408/170498071 [00:21<00:17, 6120329.41it/s] 36%|███▌      | 61472768/170498071 [00:21<00:20, 5420920.83it/s] 37%|███▋      | 62324736/170498071 [00:22<00:17, 6188282.53it/s] 37%|███▋      | 62980096/170498071 [00:22<00:19, 5594215.68it/s] 37%|███▋      | 63766528/170498071 [00:22<00:17, 6155828.24it/s] 38%|███▊      | 64552960/170498071 [00:22<00:16, 6441924.18it/s] 38%|███▊      | 65241088/170498071 [00:22<00:17, 5930549.69it/s] 39%|███▉      | 66158592/170498071 [00:22<00:15, 6765234.80it/s] 39%|███▉      | 66879488/170498071 [00:22<00:17, 6077483.90it/s] 40%|███▉      | 67764224/170498071 [00:22<00:15, 6766725.27it/s] 40%|████      | 68550656/170498071 [00:23<00:14, 6916596.58it/s] 41%|████      | 69271552/170498071 [00:23<00:15, 6474081.46it/s] 41%|████      | 70287360/170498071 [00:23<00:13, 7364894.83it/s] 42%|████▏     | 71073792/170498071 [00:23<00:15, 6610424.55it/s] 42%|████▏     | 72056832/170498071 [00:23<00:13, 7381382.53it/s] 43%|████▎     | 72908800/170498071 [00:23<00:13, 7165103.17it/s] 43%|████▎     | 73662464/170498071 [00:23<00:13, 7145210.55it/s] 44%|████▍     | 74743808/170498071 [00:23<00:11, 8113806.82it/s] 44%|████▍     | 75595776/170498071 [00:23<00:13, 7282181.79it/s] 45%|████▍     | 76578816/170498071 [00:24<00:11, 7936571.25it/s] 46%|████▌     | 77627392/170498071 [00:24<00:11, 7881977.72it/s] 46%|████▌     | 78446592/170498071 [00:24<00:11, 7807004.99it/s] 47%|████▋     | 79560704/170498071 [00:24<00:10, 8680453.29it/s] 47%|████▋     | 80478208/170498071 [00:24<00:11, 7982679.90it/s] 48%|████▊     | 81362944/170498071 [00:24<00:11, 8086812.23it/s] 48%|████▊     | 82214912/170498071 [00:25<00:20, 4330618.31it/s] 49%|████▊     | 83001344/170498071 [00:25<00:18, 4698707.69it/s] 49%|████▉     | 84344832/170498071 [00:25<00:14, 6137943.74it/s] 50%|████▉     | 85131264/170498071 [00:26<00:29, 2943220.01it/s] 50%|█████     | 85721088/170498071 [00:27<00:52, 1630303.04it/s] 51%|█████     | 86179840/170498071 [00:27<00:57, 1454695.90it/s] 52%|█████▏    | 87883776/170498071 [00:27<00:35, 2327126.05it/s] 52%|█████▏    | 88309760/170498071 [00:28<00:47, 1726910.08it/s] 52%|█████▏    | 89096192/170498071 [00:28<00:42, 1936471.26it/s] 53%|█████▎    | 90046464/170498071 [00:29<00:44, 1814772.38it/s] 54%|█████▍    | 92241920/170498071 [00:29<00:26, 2990494.47it/s] 55%|█████▍    | 92930048/170498071 [00:29<00:27, 2840737.15it/s] 55%|█████▍    | 93650944/170498071 [00:30<00:28, 2739346.27it/s] 55%|█████▌    | 94371840/170498071 [00:30<00:25, 2973659.84it/s] 56%|█████▌    | 94732288/170498071 [00:30<00:26, 2815669.20it/s] 56%|█████▌    | 95125504/170498071 [00:30<00:30, 2464358.69it/s] 56%|█████▌    | 95879168/170498071 [00:30<00:25, 2916580.22it/s] 56%|█████▋    | 96206848/170498071 [00:31<00:27, 2665011.09it/s] 57%|█████▋    | 96665600/170498071 [00:31<00:31, 2371391.25it/s] 57%|█████▋    | 97419264/170498071 [00:31<00:23, 3098799.10it/s] 57%|█████▋    | 97812480/170498071 [00:31<00:27, 2655825.98it/s] 58%|█████▊    | 98238464/170498071 [00:31<00:30, 2356179.03it/s] 58%|█████▊    | 98697216/170498071 [00:32<00:26, 2716793.41it/s] 58%|█████▊    | 99024896/170498071 [00:32<00:27, 2561456.05it/s] 58%|█████▊    | 99319808/170498071 [00:32<00:28, 2536188.28it/s] 59%|█████▊    | 99844096/170498071 [00:32<00:26, 2667032.19it/s] 59%|█████▊    | 100139008/170498071 [00:32<00:26, 2606788.46it/s] 59%|█████▉    | 100532224/170498071 [00:32<00:24, 2850570.41it/s] 59%|█████▉    | 100859904/170498071 [00:32<00:27, 2568575.64it/s] 59%|█████▉    | 101253120/170498071 [00:32<00:24, 2857814.21it/s] 60%|█████▉    | 101580800/170498071 [00:33<00:27, 2552361.41it/s] 60%|█████▉    | 102006784/170498071 [00:33<00:23, 2889152.51it/s] 60%|██████    | 102334464/170498071 [00:33<00:26, 2545332.48it/s] 60%|██████    | 102760448/170498071 [00:33<00:23, 2915971.53it/s] 60%|██████    | 103120896/170498071 [00:33<00:24, 2747658.50it/s] 61%|██████    | 103415808/170498071 [00:33<00:24, 2752350.54it/s] 61%|██████    | 103809024/170498071 [00:33<00:22, 2997214.39it/s] 61%|██████    | 104136704/170498071 [00:34<00:25, 2643248.96it/s] 61%|██████▏   | 104529920/170498071 [00:34<00:22, 2936127.15it/s] 62%|██████▏   | 104857600/170498071 [00:34<00:25, 2598568.31it/s] 62%|██████▏   | 105283584/170498071 [00:34<00:21, 2988202.91it/s] 62%|██████▏   | 105611264/170498071 [00:34<00:23, 2757254.22it/s] 62%|██████▏   | 105906176/170498071 [00:34<00:23, 2754942.93it/s] 62%|██████▏   | 106299392/170498071 [00:34<00:21, 3024086.11it/s] 63%|██████▎   | 106627072/170498071 [00:34<00:24, 2654273.32it/s] 63%|██████▎   | 107020288/170498071 [00:35<00:21, 2961142.05it/s] 63%|██████▎   | 107347968/170498071 [00:35<00:24, 2598162.13it/s] 63%|██████▎   | 107773952/170498071 [00:35<00:20, 2992170.00it/s] 63%|██████▎   | 108101632/170498071 [00:35<00:21, 2886935.42it/s] 64%|██████▎   | 108429312/170498071 [00:35<00:22, 2714203.15it/s] 64%|██████▍   | 108822528/170498071 [00:35<00:20, 2987256.01it/s] 64%|██████▍   | 109150208/170498071 [00:35<00:23, 2605228.27it/s] 64%|██████▍   | 109576192/170498071 [00:35<00:20, 2959168.58it/s] 64%|██████▍   | 109903872/170498071 [00:36<00:23, 2609115.62it/s] 65%|██████▍   | 110329856/170498071 [00:36<00:20, 2941631.68it/s] 65%|██████▍   | 110657536/170498071 [00:36<00:23, 2588789.87it/s] 65%|██████▌   | 111050752/170498071 [00:36<00:20, 2853423.52it/s] 65%|██████▌   | 111443968/170498071 [00:36<00:22, 2613491.79it/s] 66%|██████▌   | 111837184/170498071 [00:36<00:20, 2886044.77it/s] 66%|██████▌   | 112230400/170498071 [00:36<00:18, 3104105.79it/s] 66%|██████▌   | 112590848/170498071 [00:37<00:21, 2723270.80it/s] 66%|██████▋   | 112984064/170498071 [00:37<00:19, 2994296.97it/s] 66%|██████▋   | 113311744/170498071 [00:37<00:21, 2647793.72it/s] 67%|██████▋   | 113704960/170498071 [00:37<00:19, 2947463.10it/s] 67%|██████▋   | 114032640/170498071 [00:37<00:21, 2611352.25it/s] 67%|██████▋   | 114425856/170498071 [00:37<00:19, 2904339.85it/s] 67%|██████▋   | 114786304/170498071 [00:37<00:21, 2603067.27it/s] 68%|██████▊   | 115179520/170498071 [00:37<00:19, 2907069.75it/s] 68%|██████▊   | 115572736/170498071 [00:38<00:17, 3132133.35it/s] 68%|██████▊   | 115933184/170498071 [00:38<00:19, 2739124.07it/s] 68%|██████▊   | 116359168/170498071 [00:38<00:17, 3047309.56it/s] 68%|██████▊   | 116686848/170498071 [00:38<00:20, 2662530.87it/s] 69%|██████▊   | 117112832/170498071 [00:38<00:17, 2984397.05it/s] 69%|██████▉   | 117440512/170498071 [00:38<00:20, 2648135.85it/s] 69%|██████▉   | 117866496/170498071 [00:38<00:17, 2987740.46it/s] 69%|██████▉   | 118194176/170498071 [00:39<00:19, 2644504.81it/s] 70%|██████▉   | 118587392/170498071 [00:39<00:17, 2938218.07it/s] 70%|██████▉   | 118980608/170498071 [00:39<00:16, 3163731.46it/s] 70%|██████▉   | 119341056/170498071 [00:39<00:18, 2778576.31it/s] 70%|███████   | 119767040/170498071 [00:39<00:16, 3093287.53it/s] 70%|███████   | 120127488/170498071 [00:39<00:18, 2745786.34it/s] 71%|███████   | 120553472/170498071 [00:39<00:16, 3053331.97it/s] 71%|███████   | 120881152/170498071 [00:39<00:18, 2708216.76it/s] 71%|███████   | 121307136/170498071 [00:40<00:16, 3054928.51it/s] 71%|███████▏  | 121634816/170498071 [00:40<00:18, 2705465.67it/s] 72%|███████▏  | 122060800/170498071 [00:40<00:15, 3029420.40it/s] 72%|███████▏  | 122486784/170498071 [00:40<00:16, 2922955.87it/s] 72%|███████▏  | 122814464/170498071 [00:40<00:16, 2958211.20it/s] 72%|███████▏  | 123240448/170498071 [00:40<00:14, 3262547.06it/s] 72%|███████▏  | 123600896/170498071 [00:40<00:16, 2851780.81it/s] 73%|███████▎  | 124059648/170498071 [00:40<00:14, 3246528.06it/s] 73%|███████▎  | 124420096/170498071 [00:41<00:16, 2867641.12it/s] 73%|███████▎  | 124878848/170498071 [00:41<00:14, 3230641.19it/s] 73%|███████▎  | 125239296/170498071 [00:41<00:15, 2854735.36it/s] 74%|███████▎  | 125698048/170498071 [00:41<00:13, 3211000.15it/s] 74%|███████▍  | 126124032/170498071 [00:41<00:12, 3469424.49it/s] 74%|███████▍  | 126517248/170498071 [00:41<00:14, 3052832.89it/s] 74%|███████▍  | 126976000/170498071 [00:41<00:12, 3376806.04it/s] 75%|███████▍  | 127336448/170498071 [00:42<00:14, 2990580.45it/s] 75%|███████▍  | 127827968/170498071 [00:42<00:12, 3411148.97it/s] 75%|███████▌  | 128221184/170498071 [00:42<00:13, 3043090.93it/s] 75%|███████▌  | 128679936/170498071 [00:42<00:12, 3402858.34it/s] 76%|███████▌  | 129105920/170498071 [00:42<00:13, 3063671.36it/s] 76%|███████▌  | 129597440/170498071 [00:42<00:11, 3473330.61it/s] 76%|███████▋  | 130056192/170498071 [00:42<00:10, 3710000.61it/s] 77%|███████▋  | 130482176/170498071 [00:42<00:12, 3326506.36it/s] 77%|███████▋  | 130973696/170498071 [00:43<00:10, 3693082.35it/s] 77%|███████▋  | 131366912/170498071 [00:43<00:12, 3260563.08it/s] 77%|███████▋  | 131891200/170498071 [00:43<00:10, 3706044.08it/s] 78%|███████▊  | 132317184/170498071 [00:43<00:11, 3308694.67it/s] 78%|███████▊  | 132841472/170498071 [00:43<00:10, 3721476.69it/s] 78%|███████▊  | 133365760/170498071 [00:43<00:10, 3417722.42it/s] 79%|███████▊  | 133890048/170498071 [00:43<00:09, 3828081.64it/s] 79%|███████▉  | 134414336/170498071 [00:43<00:08, 4146056.86it/s] 79%|███████▉  | 134873088/170498071 [00:44<00:09, 3663920.40it/s] 79%|███████▉  | 135430144/170498071 [00:44<00:08, 4114334.01it/s] 80%|███████▉  | 135888896/170498071 [00:44<00:09, 3649042.53it/s] 80%|████████  | 136478720/170498071 [00:44<00:08, 4165786.01it/s] 80%|████████  | 136937472/170498071 [00:44<00:09, 3694985.69it/s] 81%|████████  | 137527296/170498071 [00:44<00:07, 4218336.32it/s] 81%|████████  | 138084352/170498071 [00:44<00:07, 4515470.53it/s] 81%|████████▏ | 138575872/170498071 [00:44<00:07, 4014137.21it/s] 82%|████████▏ | 139198464/170498071 [00:45<00:06, 4495949.30it/s] 82%|████████▏ | 139689984/170498071 [00:45<00:07, 3998118.09it/s] 82%|████████▏ | 140345344/170498071 [00:45<00:06, 4584393.06it/s] 83%|████████▎ | 140836864/170498071 [00:45<00:07, 4062354.55it/s] 83%|████████▎ | 141492224/170498071 [00:45<00:06, 4655155.70it/s] 83%|████████▎ | 142082048/170498071 [00:45<00:05, 4946437.55it/s] 84%|████████▎ | 142606336/170498071 [00:45<00:06, 4371279.92it/s] 84%|████████▍ | 143294464/170498071 [00:45<00:05, 4978118.72it/s] 84%|████████▍ | 143851520/170498071 [00:46<00:06, 4434155.15it/s] 85%|████████▍ | 144572416/170498071 [00:46<00:05, 5052728.09it/s] 85%|████████▌ | 145129472/170498071 [00:46<00:05, 4512091.65it/s] 86%|████████▌ | 145850368/170498071 [00:46<00:04, 5159047.90it/s] 86%|████████▌ | 146538496/170498071 [00:46<00:04, 5551147.12it/s] 86%|████████▋ | 147128320/170498071 [00:46<00:04, 4800045.68it/s] 87%|████████▋ | 147750912/170498071 [00:46<00:04, 5121693.18it/s] 87%|████████▋ | 148307968/170498071 [00:46<00:04, 4943981.68it/s] 87%|████████▋ | 148930560/170498071 [00:47<00:04, 5202233.84it/s] 88%|████████▊ | 149553152/170498071 [00:47<00:03, 5418719.18it/s] 88%|████████▊ | 150142976/170498071 [00:47<00:03, 5346856.97it/s] 88%|████████▊ | 150765568/170498071 [00:47<00:03, 5541724.12it/s] 89%|████████▉ | 151388160/170498071 [00:47<00:03, 5725796.60it/s] 89%|████████▉ | 151977984/170498071 [00:47<00:03, 5649259.75it/s] 90%|████████▉ | 152633344/170498071 [00:47<00:03, 5856431.93it/s] 90%|████████▉ | 153288704/170498071 [00:47<00:02, 6035252.70it/s] 90%|█████████ | 153911296/170498071 [00:47<00:02, 5947182.14it/s] 91%|█████████ | 154599424/170498071 [00:48<00:02, 6174987.51it/s] 91%|█████████ | 155287552/170498071 [00:48<00:02, 6332989.74it/s] 91%|█████████▏| 155942912/170498071 [00:48<00:02, 6221164.72it/s] 92%|█████████▏| 156663808/170498071 [00:48<00:02, 6446355.45it/s] 92%|█████████▏| 157384704/170498071 [00:48<00:02, 6491805.77it/s] 93%|█████████▎| 158072832/170498071 [00:48<00:01, 6547617.60it/s] 93%|█████████▎| 158826496/170498071 [00:48<00:01, 6752357.47it/s] 94%|█████████▎| 159514624/170498071 [00:48<00:01, 6691321.15it/s] 94%|█████████▍| 160268288/170498071 [00:48<00:01, 6888432.57it/s] 94%|█████████▍| 161054720/170498071 [00:48<00:01, 7151157.21it/s] 95%|█████████▍| 161775616/170498071 [00:49<00:01, 7027301.23it/s] 95%|█████████▌| 162529280/170498071 [00:49<00:01, 7156914.31it/s] 96%|█████████▌| 163348480/170498071 [00:49<00:00, 7449544.68it/s] 96%|█████████▌| 164102144/170498071 [00:49<00:00, 7359188.62it/s] 97%|█████████▋| 164921344/170498071 [00:49<00:00, 7543282.44it/s] 97%|█████████▋| 165806080/170498071 [00:49<00:00, 7849868.72it/s] 98%|█████████▊| 166592512/170498071 [00:49<00:00, 7729842.35it/s] 98%|█████████▊| 167444480/170498071 [00:49<00:00, 7865119.98it/s] 99%|█████████▊| 168361984/170498071 [00:49<00:00, 8206079.23it/s] 99%|█████████▉| 169213952/170498071 [00:50<00:00, 8069692.47it/s]100%|█████████▉| 170033152/170498071 [00:51<00:00, 1674251.80it/s]100%|██████████| 170498071/170498071 [00:51<00:00, 3313200.85it/s]
/data/happythgus/DatasetCondensation/main.py:89: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
/data/happythgus/DatasetCondensation/main.py:89: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
Extracting data/cifar-10-python.tar.gz to data
Files already downloaded and verified

================== Exp 0 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fdc3bdd2370>, 'dsa': False}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-09-28 02:33:42] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:33:46] Evaluate_00: epoch = 0300 train time = 2 s train loss = 0.000428 train acc = 1.0000, test acc = 0.1035
[2023-09-28 02:33:49] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000454 train acc = 1.0000, test acc = 0.1119
[2023-09-28 02:33:52] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000447 train acc = 1.0000, test acc = 0.1011
[2023-09-28 02:33:55] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000444 train acc = 1.0000, test acc = 0.0934
[2023-09-28 02:33:57] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000473 train acc = 1.0000, test acc = 0.1134
[2023-09-28 02:34:00] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000436 train acc = 1.0000, test acc = 0.0953
[2023-09-28 02:34:03] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000438 train acc = 1.0000, test acc = 0.1054
[2023-09-28 02:34:06] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000434 train acc = 1.0000, test acc = 0.1010
[2023-09-28 02:34:08] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000451 train acc = 1.0000, test acc = 0.1013
[2023-09-28 02:34:11] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000443 train acc = 1.0000, test acc = 0.0926
[2023-09-28 02:34:14] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000459 train acc = 1.0000, test acc = 0.0881
[2023-09-28 02:34:17] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000447 train acc = 1.0000, test acc = 0.0981
[2023-09-28 02:34:19] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000421 train acc = 1.0000, test acc = 0.1061
[2023-09-28 02:34:22] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000466 train acc = 1.0000, test acc = 0.1112
[2023-09-28 02:34:25] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000443 train acc = 1.0000, test acc = 0.1033
[2023-09-28 02:34:27] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000442 train acc = 1.0000, test acc = 0.1132
[2023-09-28 02:34:30] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000450 train acc = 1.0000, test acc = 0.1095
[2023-09-28 02:34:33] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000466 train acc = 1.0000, test acc = 0.0974
[2023-09-28 02:34:36] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000439 train acc = 1.0000, test acc = 0.1276
[2023-09-28 02:34:38] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000431 train acc = 1.0000, test acc = 0.1171
Evaluate 20 random ConvNet, mean = 0.1045 std = 0.0093
-------------------------
[2023-09-28 02:34:39] iter = 0000, loss = 315.2334
[2023-09-28 02:34:41] iter = 0010, loss = 247.5128
[2023-09-28 02:34:42] iter = 0020, loss = 232.7183
[2023-09-28 02:34:44] iter = 0030, loss = 223.3587
[2023-09-28 02:34:46] iter = 0040, loss = 220.8712
[2023-09-28 02:34:47] iter = 0050, loss = 216.0963
[2023-09-28 02:34:49] iter = 0060, loss = 210.9982
[2023-09-28 02:34:51] iter = 0070, loss = 207.0883
[2023-09-28 02:34:53] iter = 0080, loss = 208.8844
[2023-09-28 02:34:54] iter = 0090, loss = 205.4155
[2023-09-28 02:34:56] iter = 0100, loss = 210.3521
[2023-09-28 02:34:58] iter = 0110, loss = 207.2524
[2023-09-28 02:35:00] iter = 0120, loss = 199.9740
[2023-09-28 02:35:01] iter = 0130, loss = 194.8163
[2023-09-28 02:35:03] iter = 0140, loss = 208.5944
[2023-09-28 02:35:05] iter = 0150, loss = 195.4276
[2023-09-28 02:35:07] iter = 0160, loss = 197.9169
[2023-09-28 02:35:08] iter = 0170, loss = 198.5404
[2023-09-28 02:35:10] iter = 0180, loss = 196.9012
[2023-09-28 02:35:12] iter = 0190, loss = 196.0781
[2023-09-28 02:35:13] iter = 0200, loss = 197.8524
[2023-09-28 02:35:15] iter = 0210, loss = 194.2569
[2023-09-28 02:35:17] iter = 0220, loss = 197.7269
[2023-09-28 02:35:19] iter = 0230, loss = 197.2054
[2023-09-28 02:35:20] iter = 0240, loss = 191.8234
[2023-09-28 02:35:22] iter = 0250, loss = 193.6198
[2023-09-28 02:35:24] iter = 0260, loss = 189.1959
[2023-09-28 02:35:26] iter = 0270, loss = 197.6626
[2023-09-28 02:35:27] iter = 0280, loss = 198.4164
[2023-09-28 02:35:29] iter = 0290, loss = 197.6837
[2023-09-28 02:35:31] iter = 0300, loss = 192.0965
[2023-09-28 02:35:33] iter = 0310, loss = 190.8308
[2023-09-28 02:35:34] iter = 0320, loss = 193.7819
[2023-09-28 02:35:36] iter = 0330, loss = 192.0638
[2023-09-28 02:35:38] iter = 0340, loss = 186.6055
[2023-09-28 02:35:40] iter = 0350, loss = 192.0770
[2023-09-28 02:35:41] iter = 0360, loss = 185.0358
[2023-09-28 02:35:43] iter = 0370, loss = 188.7771
[2023-09-28 02:35:45] iter = 0380, loss = 195.6446
[2023-09-28 02:35:46] iter = 0390, loss = 189.7583
[2023-09-28 02:35:48] iter = 0400, loss = 191.9742
[2023-09-28 02:35:50] iter = 0410, loss = 185.2086
[2023-09-28 02:35:52] iter = 0420, loss = 188.8721
[2023-09-28 02:35:53] iter = 0430, loss = 190.7895
[2023-09-28 02:35:55] iter = 0440, loss = 188.4864
[2023-09-28 02:35:57] iter = 0450, loss = 183.6777
[2023-09-28 02:35:59] iter = 0460, loss = 183.3981
[2023-09-28 02:36:00] iter = 0470, loss = 191.8594
[2023-09-28 02:36:02] iter = 0480, loss = 191.9140
[2023-09-28 02:36:04] iter = 0490, loss = 184.3151
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:36:08] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000566 train acc = 1.0000, test acc = 0.2827
[2023-09-28 02:36:11] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000547 train acc = 1.0000, test acc = 0.2641
[2023-09-28 02:36:14] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000547 train acc = 1.0000, test acc = 0.2811
[2023-09-28 02:36:16] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000598 train acc = 1.0000, test acc = 0.2781
[2023-09-28 02:36:19] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000586 train acc = 1.0000, test acc = 0.2802
[2023-09-28 02:36:22] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000577 train acc = 1.0000, test acc = 0.2926
[2023-09-28 02:36:25] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000565 train acc = 1.0000, test acc = 0.2911
[2023-09-28 02:36:28] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000577 train acc = 1.0000, test acc = 0.2784
[2023-09-28 02:36:30] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000623 train acc = 1.0000, test acc = 0.2728
[2023-09-28 02:36:33] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000536 train acc = 1.0000, test acc = 0.2729
[2023-09-28 02:36:36] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000549 train acc = 1.0000, test acc = 0.2806
[2023-09-28 02:36:39] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000571 train acc = 1.0000, test acc = 0.2745
[2023-09-28 02:36:41] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000516 train acc = 1.0000, test acc = 0.2861
[2023-09-28 02:36:44] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000532 train acc = 1.0000, test acc = 0.2799
[2023-09-28 02:36:47] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000573 train acc = 1.0000, test acc = 0.2698
[2023-09-28 02:36:50] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000568 train acc = 1.0000, test acc = 0.2772
[2023-09-28 02:36:52] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000539 train acc = 1.0000, test acc = 0.2724
[2023-09-28 02:36:55] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000570 train acc = 1.0000, test acc = 0.2935
[2023-09-28 02:36:58] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000564 train acc = 1.0000, test acc = 0.2842
[2023-09-28 02:37:01] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000507 train acc = 1.0000, test acc = 0.2776
Evaluate 20 random ConvNet, mean = 0.2795 std = 0.0074
-------------------------
[2023-09-28 02:37:01] iter = 0500, loss = 188.3258
[2023-09-28 02:37:03] iter = 0510, loss = 185.3112
[2023-09-28 02:37:04] iter = 0520, loss = 188.1769
[2023-09-28 02:37:06] iter = 0530, loss = 187.0645
[2023-09-28 02:37:08] iter = 0540, loss = 187.9252
[2023-09-28 02:37:09] iter = 0550, loss = 174.8921
[2023-09-28 02:37:11] iter = 0560, loss = 184.3685
[2023-09-28 02:37:13] iter = 0570, loss = 185.3927
[2023-09-28 02:37:15] iter = 0580, loss = 190.2081
[2023-09-28 02:37:16] iter = 0590, loss = 190.7025
[2023-09-28 02:37:18] iter = 0600, loss = 184.0526
[2023-09-28 02:37:20] iter = 0610, loss = 183.7376
[2023-09-28 02:37:22] iter = 0620, loss = 186.0824
[2023-09-28 02:37:23] iter = 0630, loss = 192.2454
[2023-09-28 02:37:25] iter = 0640, loss = 183.2644
[2023-09-28 02:37:27] iter = 0650, loss = 186.2046
[2023-09-28 02:37:29] iter = 0660, loss = 188.0357
[2023-09-28 02:37:30] iter = 0670, loss = 182.7743
[2023-09-28 02:37:32] iter = 0680, loss = 194.4923
[2023-09-28 02:37:34] iter = 0690, loss = 184.7032
[2023-09-28 02:37:35] iter = 0700, loss = 185.6436
[2023-09-28 02:37:37] iter = 0710, loss = 180.1290
[2023-09-28 02:37:39] iter = 0720, loss = 184.2197
[2023-09-28 02:37:41] iter = 0730, loss = 184.0170
[2023-09-28 02:37:42] iter = 0740, loss = 184.0574
[2023-09-28 02:37:44] iter = 0750, loss = 178.5125
[2023-09-28 02:37:46] iter = 0760, loss = 186.7656
[2023-09-28 02:37:48] iter = 0770, loss = 186.1809
[2023-09-28 02:37:49] iter = 0780, loss = 182.1284
[2023-09-28 02:37:51] iter = 0790, loss = 181.4777
[2023-09-28 02:37:53] iter = 0800, loss = 186.9126
[2023-09-28 02:37:55] iter = 0810, loss = 187.9980
[2023-09-28 02:37:56] iter = 0820, loss = 183.2851
[2023-09-28 02:37:58] iter = 0830, loss = 186.0219
[2023-09-28 02:38:00] iter = 0840, loss = 177.0910
[2023-09-28 02:38:02] iter = 0850, loss = 182.0109
[2023-09-28 02:38:03] iter = 0860, loss = 181.1792
[2023-09-28 02:38:05] iter = 0870, loss = 184.1674
[2023-09-28 02:38:07] iter = 0880, loss = 183.9243
[2023-09-28 02:38:09] iter = 0890, loss = 184.7880
[2023-09-28 02:38:10] iter = 0900, loss = 184.1546
[2023-09-28 02:38:12] iter = 0910, loss = 183.3633
[2023-09-28 02:38:14] iter = 0920, loss = 177.0729
[2023-09-28 02:38:16] iter = 0930, loss = 178.7143
[2023-09-28 02:38:17] iter = 0940, loss = 182.4137
[2023-09-28 02:38:19] iter = 0950, loss = 182.7559
[2023-09-28 02:38:21] iter = 0960, loss = 183.6706
[2023-09-28 02:38:23] iter = 0970, loss = 189.0564
[2023-09-28 02:38:24] iter = 0980, loss = 183.2625
[2023-09-28 02:38:26] iter = 0990, loss = 180.9333
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:38:30] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000587 train acc = 1.0000, test acc = 0.2834
[2023-09-28 02:38:33] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000591 train acc = 1.0000, test acc = 0.2726
[2023-09-28 02:38:36] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000598 train acc = 1.0000, test acc = 0.2878
[2023-09-28 02:38:39] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000588 train acc = 1.0000, test acc = 0.2850
[2023-09-28 02:38:41] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000545 train acc = 1.0000, test acc = 0.2873
[2023-09-28 02:38:44] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000590 train acc = 1.0000, test acc = 0.2771
[2023-09-28 02:38:47] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000562 train acc = 1.0000, test acc = 0.2900
[2023-09-28 02:38:50] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000574 train acc = 1.0000, test acc = 0.2842
[2023-09-28 02:38:52] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000566 train acc = 1.0000, test acc = 0.2811
[2023-09-28 02:38:55] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000546 train acc = 1.0000, test acc = 0.2856
[2023-09-28 02:38:58] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000527 train acc = 1.0000, test acc = 0.2822
[2023-09-28 02:39:01] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000638 train acc = 1.0000, test acc = 0.2764
[2023-09-28 02:39:03] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000553 train acc = 1.0000, test acc = 0.2735
[2023-09-28 02:39:06] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000607 train acc = 1.0000, test acc = 0.2802
[2023-09-28 02:39:09] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000647 train acc = 1.0000, test acc = 0.2852
[2023-09-28 02:39:12] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000637 train acc = 1.0000, test acc = 0.2774
[2023-09-28 02:39:14] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000565 train acc = 1.0000, test acc = 0.2907
[2023-09-28 02:39:17] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000607 train acc = 1.0000, test acc = 0.2798
[2023-09-28 02:39:20] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000606 train acc = 1.0000, test acc = 0.2775
[2023-09-28 02:39:23] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000596 train acc = 1.0000, test acc = 0.2917
Evaluate 20 random ConvNet, mean = 0.2824 std = 0.0055
-------------------------
[2023-09-28 02:39:23] iter = 1000, loss = 176.1969

================== Exp 1 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fdc3bdd2370>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-09-28 02:39:41] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:39:43] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000424 train acc = 1.0000, test acc = 0.1129
[2023-09-28 02:39:46] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000467 train acc = 1.0000, test acc = 0.1172
[2023-09-28 02:39:49] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000460 train acc = 1.0000, test acc = 0.1191
[2023-09-28 02:39:52] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000431 train acc = 1.0000, test acc = 0.0934
[2023-09-28 02:39:54] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000452 train acc = 1.0000, test acc = 0.1242
[2023-09-28 02:39:57] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000446 train acc = 1.0000, test acc = 0.1208
[2023-09-28 02:40:00] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000438 train acc = 1.0000, test acc = 0.1215
[2023-09-28 02:40:02] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000450 train acc = 1.0000, test acc = 0.0974
[2023-09-28 02:40:05] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000450 train acc = 1.0000, test acc = 0.1037
[2023-09-28 02:40:08] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000423 train acc = 1.0000, test acc = 0.0988
[2023-09-28 02:40:11] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000455 train acc = 1.0000, test acc = 0.1151
[2023-09-28 02:40:13] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000429 train acc = 1.0000, test acc = 0.1004
[2023-09-28 02:40:16] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000472 train acc = 1.0000, test acc = 0.1117
[2023-09-28 02:40:19] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000436 train acc = 1.0000, test acc = 0.1180
[2023-09-28 02:40:22] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000436 train acc = 1.0000, test acc = 0.0963
[2023-09-28 02:40:24] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000454 train acc = 1.0000, test acc = 0.1125
[2023-09-28 02:40:27] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000453 train acc = 1.0000, test acc = 0.1278
[2023-09-28 02:40:30] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000448 train acc = 1.0000, test acc = 0.1073
[2023-09-28 02:40:33] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000444 train acc = 1.0000, test acc = 0.1025
[2023-09-28 02:40:36] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000463 train acc = 1.0000, test acc = 0.1072
Evaluate 20 random ConvNet, mean = 0.1104 std = 0.0099
-------------------------
[2023-09-28 02:40:36] iter = 0000, loss = 321.9635
[2023-09-28 02:40:37] iter = 0010, loss = 247.5385
[2023-09-28 02:40:39] iter = 0020, loss = 229.9969
[2023-09-28 02:40:41] iter = 0030, loss = 220.4030
[2023-09-28 02:40:42] iter = 0040, loss = 222.7921
[2023-09-28 02:40:44] iter = 0050, loss = 209.6327
[2023-09-28 02:40:46] iter = 0060, loss = 208.3273
[2023-09-28 02:40:48] iter = 0070, loss = 215.8202
[2023-09-28 02:40:49] iter = 0080, loss = 202.2232
[2023-09-28 02:40:51] iter = 0090, loss = 210.8994
[2023-09-28 02:40:53] iter = 0100, loss = 205.8294
[2023-09-28 02:40:55] iter = 0110, loss = 205.6973
[2023-09-28 02:40:56] iter = 0120, loss = 202.9614
[2023-09-28 02:40:58] iter = 0130, loss = 206.1538
[2023-09-28 02:41:00] iter = 0140, loss = 202.5531
[2023-09-28 02:41:02] iter = 0150, loss = 200.7475
[2023-09-28 02:41:03] iter = 0160, loss = 200.2014
[2023-09-28 02:41:05] iter = 0170, loss = 204.3721
[2023-09-28 02:41:07] iter = 0180, loss = 199.8727
[2023-09-28 02:41:09] iter = 0190, loss = 201.1051
[2023-09-28 02:41:10] iter = 0200, loss = 198.0360
[2023-09-28 02:41:12] iter = 0210, loss = 194.0274
[2023-09-28 02:41:14] iter = 0220, loss = 197.8833
[2023-09-28 02:41:16] iter = 0230, loss = 196.9294
[2023-09-28 02:41:17] iter = 0240, loss = 197.5666
[2023-09-28 02:41:19] iter = 0250, loss = 192.9935
[2023-09-28 02:41:21] iter = 0260, loss = 190.0614
[2023-09-28 02:41:23] iter = 0270, loss = 195.0426
[2023-09-28 02:41:24] iter = 0280, loss = 201.0393
[2023-09-28 02:41:26] iter = 0290, loss = 192.3519
[2023-09-28 02:41:28] iter = 0300, loss = 194.6764
[2023-09-28 02:41:29] iter = 0310, loss = 194.5105
[2023-09-28 02:41:31] iter = 0320, loss = 188.5865
[2023-09-28 02:41:33] iter = 0330, loss = 194.1712
[2023-09-28 02:41:35] iter = 0340, loss = 185.8715
[2023-09-28 02:41:36] iter = 0350, loss = 190.5321
[2023-09-28 02:41:38] iter = 0360, loss = 183.9503
[2023-09-28 02:41:40] iter = 0370, loss = 194.5480
[2023-09-28 02:41:42] iter = 0380, loss = 183.6678
[2023-09-28 02:41:43] iter = 0390, loss = 187.2287
[2023-09-28 02:41:45] iter = 0400, loss = 181.8395
[2023-09-28 02:41:47] iter = 0410, loss = 185.8016
[2023-09-28 02:41:49] iter = 0420, loss = 184.8695
[2023-09-28 02:41:50] iter = 0430, loss = 184.3373
[2023-09-28 02:41:52] iter = 0440, loss = 183.3338
[2023-09-28 02:41:54] iter = 0450, loss = 188.9086
[2023-09-28 02:41:56] iter = 0460, loss = 188.0523
[2023-09-28 02:41:57] iter = 0470, loss = 191.1492
[2023-09-28 02:41:59] iter = 0480, loss = 190.2920
[2023-09-28 02:42:01] iter = 0490, loss = 191.4874
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:42:05] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000569 train acc = 1.0000, test acc = 0.2879
[2023-09-28 02:42:08] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000595 train acc = 1.0000, test acc = 0.2808
[2023-09-28 02:42:11] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000564 train acc = 1.0000, test acc = 0.2851
[2023-09-28 02:42:13] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000559 train acc = 1.0000, test acc = 0.2824
[2023-09-28 02:42:16] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000571 train acc = 1.0000, test acc = 0.2731
[2023-09-28 02:42:19] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000565 train acc = 1.0000, test acc = 0.2819
[2023-09-28 02:42:22] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000556 train acc = 1.0000, test acc = 0.2804
[2023-09-28 02:42:24] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000545 train acc = 1.0000, test acc = 0.2788
[2023-09-28 02:42:27] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000553 train acc = 1.0000, test acc = 0.2657
[2023-09-28 02:42:30] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000591 train acc = 1.0000, test acc = 0.2872
[2023-09-28 02:42:33] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000549 train acc = 1.0000, test acc = 0.2818
[2023-09-28 02:42:35] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000547 train acc = 1.0000, test acc = 0.2814
[2023-09-28 02:42:38] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000554 train acc = 1.0000, test acc = 0.2762
[2023-09-28 02:42:41] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000539 train acc = 1.0000, test acc = 0.2854
[2023-09-28 02:42:44] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000567 train acc = 1.0000, test acc = 0.2880
[2023-09-28 02:42:47] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000549 train acc = 1.0000, test acc = 0.2817
[2023-09-28 02:42:49] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000520 train acc = 1.0000, test acc = 0.2852
[2023-09-28 02:42:52] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000547 train acc = 1.0000, test acc = 0.2809
[2023-09-28 02:42:55] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000537 train acc = 1.0000, test acc = 0.2799
[2023-09-28 02:42:58] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000521 train acc = 1.0000, test acc = 0.2859
Evaluate 20 random ConvNet, mean = 0.2815 std = 0.0052
-------------------------
[2023-09-28 02:42:58] iter = 0500, loss = 184.1716
[2023-09-28 02:42:59] iter = 0510, loss = 190.1745
[2023-09-28 02:43:01] iter = 0520, loss = 181.0164
[2023-09-28 02:43:03] iter = 0530, loss = 186.3914
[2023-09-28 02:43:05] iter = 0540, loss = 182.8214
[2023-09-28 02:43:06] iter = 0550, loss = 184.1594
[2023-09-28 02:43:08] iter = 0560, loss = 182.1592
[2023-09-28 02:43:10] iter = 0570, loss = 178.4712
[2023-09-28 02:43:12] iter = 0580, loss = 183.2047
[2023-09-28 02:43:13] iter = 0590, loss = 187.8693
[2023-09-28 02:43:15] iter = 0600, loss = 194.0482
[2023-09-28 02:43:17] iter = 0610, loss = 181.7668
[2023-09-28 02:43:18] iter = 0620, loss = 183.0299
[2023-09-28 02:43:20] iter = 0630, loss = 187.2033
[2023-09-28 02:43:22] iter = 0640, loss = 181.9729
[2023-09-28 02:43:24] iter = 0650, loss = 181.0420
[2023-09-28 02:43:25] iter = 0660, loss = 180.3812
[2023-09-28 02:43:27] iter = 0670, loss = 181.4193
[2023-09-28 02:43:29] iter = 0680, loss = 189.4739
[2023-09-28 02:43:31] iter = 0690, loss = 184.2570
[2023-09-28 02:43:32] iter = 0700, loss = 182.4571
[2023-09-28 02:43:34] iter = 0710, loss = 181.4421
[2023-09-28 02:43:36] iter = 0720, loss = 184.4115
[2023-09-28 02:43:38] iter = 0730, loss = 185.3951
[2023-09-28 02:43:39] iter = 0740, loss = 179.4959
[2023-09-28 02:43:41] iter = 0750, loss = 186.8285
[2023-09-28 02:43:43] iter = 0760, loss = 183.4692
[2023-09-28 02:43:45] iter = 0770, loss = 183.5549
[2023-09-28 02:43:46] iter = 0780, loss = 181.6045
[2023-09-28 02:43:48] iter = 0790, loss = 182.1234
[2023-09-28 02:43:50] iter = 0800, loss = 181.9264
[2023-09-28 02:43:52] iter = 0810, loss = 189.5870
[2023-09-28 02:43:53] iter = 0820, loss = 181.6605
[2023-09-28 02:43:55] iter = 0830, loss = 185.0461
[2023-09-28 02:43:57] iter = 0840, loss = 175.2432
[2023-09-28 02:43:59] iter = 0850, loss = 183.3466
[2023-09-28 02:44:00] iter = 0860, loss = 186.1348
[2023-09-28 02:44:02] iter = 0870, loss = 182.4634
[2023-09-28 02:44:04] iter = 0880, loss = 182.5766
[2023-09-28 02:44:06] iter = 0890, loss = 184.4549
[2023-09-28 02:44:07] iter = 0900, loss = 185.5261
[2023-09-28 02:44:09] iter = 0910, loss = 183.8425
[2023-09-28 02:44:11] iter = 0920, loss = 180.0608
[2023-09-28 02:44:13] iter = 0930, loss = 180.3583
[2023-09-28 02:44:14] iter = 0940, loss = 181.4223
[2023-09-28 02:44:16] iter = 0950, loss = 176.9642
[2023-09-28 02:44:18] iter = 0960, loss = 181.1859
[2023-09-28 02:44:20] iter = 0970, loss = 187.1179
[2023-09-28 02:44:21] iter = 0980, loss = 184.7914
[2023-09-28 02:44:23] iter = 0990, loss = 180.2872
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:44:27] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000630 train acc = 1.0000, test acc = 0.2777
[2023-09-28 02:44:30] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000613 train acc = 1.0000, test acc = 0.2751
[2023-09-28 02:44:33] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000589 train acc = 1.0000, test acc = 0.2928
[2023-09-28 02:44:36] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000569 train acc = 1.0000, test acc = 0.2859
[2023-09-28 02:44:38] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000620 train acc = 1.0000, test acc = 0.2740
[2023-09-28 02:44:41] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000551 train acc = 1.0000, test acc = 0.2812
[2023-09-28 02:44:44] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000533 train acc = 1.0000, test acc = 0.2865
[2023-09-28 02:44:47] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000606 train acc = 1.0000, test acc = 0.2732
[2023-09-28 02:44:49] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000566 train acc = 1.0000, test acc = 0.2915
[2023-09-28 02:44:52] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000571 train acc = 1.0000, test acc = 0.2890
[2023-09-28 02:44:55] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000542 train acc = 1.0000, test acc = 0.2707
[2023-09-28 02:44:58] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000589 train acc = 1.0000, test acc = 0.2859
[2023-09-28 02:45:00] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000559 train acc = 1.0000, test acc = 0.2913
[2023-09-28 02:45:03] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000550 train acc = 1.0000, test acc = 0.2770
[2023-09-28 02:45:06] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000597 train acc = 1.0000, test acc = 0.2722
[2023-09-28 02:45:09] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000621 train acc = 1.0000, test acc = 0.2744
[2023-09-28 02:45:11] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000577 train acc = 1.0000, test acc = 0.2807
[2023-09-28 02:45:14] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000578 train acc = 1.0000, test acc = 0.2808
[2023-09-28 02:45:17] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000592 train acc = 1.0000, test acc = 0.2793
[2023-09-28 02:45:20] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000618 train acc = 1.0000, test acc = 0.2804
Evaluate 20 random ConvNet, mean = 0.2810 std = 0.0067
-------------------------
[2023-09-28 02:45:20] iter = 1000, loss = 183.1250

================== Exp 2 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fdc3bdd2370>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-09-28 02:45:37] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:45:40] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000460 train acc = 1.0000, test acc = 0.0977
[2023-09-28 02:45:43] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000439 train acc = 1.0000, test acc = 0.1122
[2023-09-28 02:45:46] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000442 train acc = 1.0000, test acc = 0.0983
[2023-09-28 02:45:48] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000458 train acc = 1.0000, test acc = 0.1120
[2023-09-28 02:45:51] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000467 train acc = 1.0000, test acc = 0.0968
[2023-09-28 02:45:54] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000458 train acc = 1.0000, test acc = 0.0943
[2023-09-28 02:45:57] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000444 train acc = 1.0000, test acc = 0.1250
[2023-09-28 02:45:59] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000428 train acc = 1.0000, test acc = 0.1130
[2023-09-28 02:46:02] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000470 train acc = 1.0000, test acc = 0.1110
[2023-09-28 02:46:05] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000460 train acc = 1.0000, test acc = 0.1178
[2023-09-28 02:46:08] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000456 train acc = 1.0000, test acc = 0.1071
[2023-09-28 02:46:10] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000476 train acc = 1.0000, test acc = 0.1096
[2023-09-28 02:46:13] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000461 train acc = 1.0000, test acc = 0.1104
[2023-09-28 02:46:16] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000427 train acc = 1.0000, test acc = 0.1104
[2023-09-28 02:46:19] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000452 train acc = 1.0000, test acc = 0.0923
[2023-09-28 02:46:21] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000459 train acc = 1.0000, test acc = 0.1227
[2023-09-28 02:46:24] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000455 train acc = 1.0000, test acc = 0.1096
[2023-09-28 02:46:27] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000451 train acc = 1.0000, test acc = 0.1066
[2023-09-28 02:46:30] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000456 train acc = 1.0000, test acc = 0.0991
[2023-09-28 02:46:32] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000437 train acc = 1.0000, test acc = 0.1058
Evaluate 20 random ConvNet, mean = 0.1076 std = 0.0087
-------------------------
[2023-09-28 02:46:33] iter = 0000, loss = 317.9426
[2023-09-28 02:46:34] iter = 0010, loss = 250.6235
[2023-09-28 02:46:36] iter = 0020, loss = 229.5541
[2023-09-28 02:46:38] iter = 0030, loss = 220.6421
[2023-09-28 02:46:40] iter = 0040, loss = 216.0553
[2023-09-28 02:46:41] iter = 0050, loss = 209.2952
[2023-09-28 02:46:43] iter = 0060, loss = 208.0520
[2023-09-28 02:46:45] iter = 0070, loss = 211.0546
[2023-09-28 02:46:47] iter = 0080, loss = 205.6250
[2023-09-28 02:46:48] iter = 0090, loss = 210.9296
[2023-09-28 02:46:50] iter = 0100, loss = 204.2109
[2023-09-28 02:46:52] iter = 0110, loss = 201.4410
[2023-09-28 02:46:54] iter = 0120, loss = 206.2659
[2023-09-28 02:46:55] iter = 0130, loss = 204.7638
[2023-09-28 02:46:57] iter = 0140, loss = 196.9118
[2023-09-28 02:46:59] iter = 0150, loss = 197.5369
[2023-09-28 02:47:01] iter = 0160, loss = 204.4443
[2023-09-28 02:47:02] iter = 0170, loss = 198.2377
[2023-09-28 02:47:04] iter = 0180, loss = 196.6390
[2023-09-28 02:47:06] iter = 0190, loss = 192.6575
[2023-09-28 02:47:07] iter = 0200, loss = 193.6732
[2023-09-28 02:47:09] iter = 0210, loss = 196.5919
[2023-09-28 02:47:11] iter = 0220, loss = 200.7215
[2023-09-28 02:47:13] iter = 0230, loss = 193.3558
[2023-09-28 02:47:14] iter = 0240, loss = 193.9332
[2023-09-28 02:47:16] iter = 0250, loss = 194.0623
[2023-09-28 02:47:18] iter = 0260, loss = 195.3730
[2023-09-28 02:47:20] iter = 0270, loss = 194.5575
[2023-09-28 02:47:21] iter = 0280, loss = 189.4273
[2023-09-28 02:47:23] iter = 0290, loss = 195.3084
[2023-09-28 02:47:25] iter = 0300, loss = 188.3842
[2023-09-28 02:47:27] iter = 0310, loss = 194.6461
[2023-09-28 02:47:28] iter = 0320, loss = 186.4229
[2023-09-28 02:47:30] iter = 0330, loss = 188.3882
[2023-09-28 02:47:32] iter = 0340, loss = 187.1994
[2023-09-28 02:47:34] iter = 0350, loss = 187.8813
[2023-09-28 02:47:35] iter = 0360, loss = 185.6745
[2023-09-28 02:47:37] iter = 0370, loss = 189.0926
[2023-09-28 02:47:39] iter = 0380, loss = 192.6358
[2023-09-28 02:47:41] iter = 0390, loss = 186.4310
[2023-09-28 02:47:42] iter = 0400, loss = 191.6787
[2023-09-28 02:47:44] iter = 0410, loss = 191.5763
[2023-09-28 02:47:46] iter = 0420, loss = 184.2309
[2023-09-28 02:47:48] iter = 0430, loss = 184.4852
[2023-09-28 02:47:49] iter = 0440, loss = 185.4381
[2023-09-28 02:47:51] iter = 0450, loss = 187.0806
[2023-09-28 02:47:53] iter = 0460, loss = 186.2468
[2023-09-28 02:47:54] iter = 0470, loss = 183.5837
[2023-09-28 02:47:56] iter = 0480, loss = 182.3646
[2023-09-28 02:47:58] iter = 0490, loss = 191.1096
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:48:02] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000563 train acc = 1.0000, test acc = 0.2895
[2023-09-28 02:48:05] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000578 train acc = 1.0000, test acc = 0.2834
[2023-09-28 02:48:07] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000538 train acc = 1.0000, test acc = 0.2835
[2023-09-28 02:48:10] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000562 train acc = 1.0000, test acc = 0.2820
[2023-09-28 02:48:13] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000543 train acc = 1.0000, test acc = 0.2773
[2023-09-28 02:48:16] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000540 train acc = 1.0000, test acc = 0.2890
[2023-09-28 02:48:18] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000550 train acc = 1.0000, test acc = 0.2759
[2023-09-28 02:48:21] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000539 train acc = 1.0000, test acc = 0.2798
[2023-09-28 02:48:24] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000549 train acc = 1.0000, test acc = 0.2866
[2023-09-28 02:48:27] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000539 train acc = 1.0000, test acc = 0.2730
[2023-09-28 02:48:29] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000568 train acc = 1.0000, test acc = 0.2665
[2023-09-28 02:48:32] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000545 train acc = 1.0000, test acc = 0.2769
[2023-09-28 02:48:35] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000519 train acc = 1.0000, test acc = 0.2879
[2023-09-28 02:48:38] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000552 train acc = 1.0000, test acc = 0.2835
[2023-09-28 02:48:40] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000570 train acc = 1.0000, test acc = 0.2794
[2023-09-28 02:48:43] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000545 train acc = 1.0000, test acc = 0.2768
[2023-09-28 02:48:46] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000569 train acc = 1.0000, test acc = 0.2803
[2023-09-28 02:48:49] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000545 train acc = 1.0000, test acc = 0.2804
[2023-09-28 02:48:51] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000539 train acc = 1.0000, test acc = 0.2830
[2023-09-28 02:48:54] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000528 train acc = 1.0000, test acc = 0.2911
Evaluate 20 random ConvNet, mean = 0.2813 std = 0.0059
-------------------------
[2023-09-28 02:48:54] iter = 0500, loss = 182.7340
[2023-09-28 02:48:56] iter = 0510, loss = 186.5856
[2023-09-28 02:48:58] iter = 0520, loss = 191.7095
[2023-09-28 02:48:59] iter = 0530, loss = 179.3472
[2023-09-28 02:49:01] iter = 0540, loss = 182.0779
[2023-09-28 02:49:03] iter = 0550, loss = 186.9878
[2023-09-28 02:49:04] iter = 0560, loss = 178.8574
[2023-09-28 02:49:06] iter = 0570, loss = 185.8187
[2023-09-28 02:49:08] iter = 0580, loss = 181.5162
[2023-09-28 02:49:09] iter = 0590, loss = 183.1586
[2023-09-28 02:49:11] iter = 0600, loss = 186.9673
[2023-09-28 02:49:13] iter = 0610, loss = 182.0353
[2023-09-28 02:49:14] iter = 0620, loss = 190.4641
[2023-09-28 02:49:16] iter = 0630, loss = 183.9816
[2023-09-28 02:49:18] iter = 0640, loss = 178.0546
[2023-09-28 02:49:19] iter = 0650, loss = 185.5306
[2023-09-28 02:49:21] iter = 0660, loss = 177.3199
[2023-09-28 02:49:23] iter = 0670, loss = 181.3488
[2023-09-28 02:49:24] iter = 0680, loss = 179.8921
[2023-09-28 02:49:26] iter = 0690, loss = 183.9617
[2023-09-28 02:49:28] iter = 0700, loss = 188.5683
[2023-09-28 02:49:30] iter = 0710, loss = 182.6616
[2023-09-28 02:49:31] iter = 0720, loss = 183.8012
[2023-09-28 02:49:33] iter = 0730, loss = 183.5934
[2023-09-28 02:49:35] iter = 0740, loss = 180.6171
[2023-09-28 02:49:37] iter = 0750, loss = 180.2021
[2023-09-28 02:49:38] iter = 0760, loss = 189.4135
[2023-09-28 02:49:40] iter = 0770, loss = 179.9740
[2023-09-28 02:49:42] iter = 0780, loss = 182.1395
[2023-09-28 02:49:44] iter = 0790, loss = 188.9638
[2023-09-28 02:49:45] iter = 0800, loss = 183.9878
[2023-09-28 02:49:47] iter = 0810, loss = 182.2068
[2023-09-28 02:49:49] iter = 0820, loss = 184.2093
[2023-09-28 02:49:51] iter = 0830, loss = 181.7424
[2023-09-28 02:49:52] iter = 0840, loss = 186.7165
[2023-09-28 02:49:54] iter = 0850, loss = 180.7053
[2023-09-28 02:49:56] iter = 0860, loss = 189.0834
[2023-09-28 02:49:58] iter = 0870, loss = 180.4575
[2023-09-28 02:49:59] iter = 0880, loss = 187.2894
[2023-09-28 02:50:01] iter = 0890, loss = 184.6336
[2023-09-28 02:50:03] iter = 0900, loss = 181.0935
[2023-09-28 02:50:05] iter = 0910, loss = 188.5914
[2023-09-28 02:50:06] iter = 0920, loss = 184.9727
[2023-09-28 02:50:08] iter = 0930, loss = 181.4675
[2023-09-28 02:50:10] iter = 0940, loss = 178.1783
[2023-09-28 02:50:12] iter = 0950, loss = 185.8017
[2023-09-28 02:50:13] iter = 0960, loss = 177.2547
[2023-09-28 02:50:15] iter = 0970, loss = 180.5007
[2023-09-28 02:50:17] iter = 0980, loss = 181.1960
[2023-09-28 02:50:19] iter = 0990, loss = 183.0811
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:50:23] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000592 train acc = 1.0000, test acc = 0.2707
[2023-09-28 02:50:26] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000570 train acc = 1.0000, test acc = 0.2786
[2023-09-28 02:50:28] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000579 train acc = 1.0000, test acc = 0.2898
[2023-09-28 02:50:31] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000591 train acc = 1.0000, test acc = 0.2787
[2023-09-28 02:50:34] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000559 train acc = 1.0000, test acc = 0.2807
[2023-09-28 02:50:37] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000582 train acc = 1.0000, test acc = 0.2956
[2023-09-28 02:50:39] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000581 train acc = 1.0000, test acc = 0.2827
[2023-09-28 02:50:42] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000522 train acc = 1.0000, test acc = 0.2816
[2023-09-28 02:50:45] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000565 train acc = 1.0000, test acc = 0.2902
[2023-09-28 02:50:48] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000571 train acc = 1.0000, test acc = 0.2829
[2023-09-28 02:50:50] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000561 train acc = 1.0000, test acc = 0.2907
[2023-09-28 02:50:53] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000575 train acc = 1.0000, test acc = 0.2767
[2023-09-28 02:50:56] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000545 train acc = 1.0000, test acc = 0.2831
[2023-09-28 02:50:59] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000550 train acc = 1.0000, test acc = 0.2890
[2023-09-28 02:51:01] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000549 train acc = 1.0000, test acc = 0.2824
[2023-09-28 02:51:04] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000590 train acc = 1.0000, test acc = 0.2804
[2023-09-28 02:51:07] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000541 train acc = 1.0000, test acc = 0.2737
[2023-09-28 02:51:10] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000586 train acc = 1.0000, test acc = 0.2793
[2023-09-28 02:51:12] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000581 train acc = 1.0000, test acc = 0.2825
[2023-09-28 02:51:15] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000565 train acc = 1.0000, test acc = 0.2809
Evaluate 20 random ConvNet, mean = 0.2825 std = 0.0059
-------------------------
[2023-09-28 02:51:15] iter = 1000, loss = 182.8275

================== Exp 3 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fdc3bdd2370>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-09-28 02:51:33] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:51:36] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000461 train acc = 1.0000, test acc = 0.0971
[2023-09-28 02:51:38] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000444 train acc = 1.0000, test acc = 0.0765
[2023-09-28 02:51:41] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000456 train acc = 1.0000, test acc = 0.0861
[2023-09-28 02:51:44] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000428 train acc = 1.0000, test acc = 0.0957
[2023-09-28 02:51:47] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000461 train acc = 1.0000, test acc = 0.0864
[2023-09-28 02:51:49] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000432 train acc = 1.0000, test acc = 0.0784
[2023-09-28 02:51:52] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000444 train acc = 1.0000, test acc = 0.0837
[2023-09-28 02:51:55] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000436 train acc = 1.0000, test acc = 0.0883
[2023-09-28 02:51:58] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000433 train acc = 1.0000, test acc = 0.0898
[2023-09-28 02:52:00] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000436 train acc = 1.0000, test acc = 0.0836
[2023-09-28 02:52:03] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000447 train acc = 1.0000, test acc = 0.0749
[2023-09-28 02:52:06] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000446 train acc = 1.0000, test acc = 0.0955
[2023-09-28 02:52:09] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000445 train acc = 1.0000, test acc = 0.0796
[2023-09-28 02:52:11] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000445 train acc = 1.0000, test acc = 0.0854
[2023-09-28 02:52:14] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000439 train acc = 1.0000, test acc = 0.0841
[2023-09-28 02:52:17] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000423 train acc = 1.0000, test acc = 0.0803
[2023-09-28 02:52:20] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000463 train acc = 1.0000, test acc = 0.0822
[2023-09-28 02:52:22] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000422 train acc = 1.0000, test acc = 0.0800
[2023-09-28 02:52:25] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000459 train acc = 1.0000, test acc = 0.0919
[2023-09-28 02:52:28] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000415 train acc = 1.0000, test acc = 0.0792
Evaluate 20 random ConvNet, mean = 0.0849 std = 0.0063
-------------------------
[2023-09-28 02:52:28] iter = 0000, loss = 323.8330
[2023-09-28 02:52:30] iter = 0010, loss = 250.9842
[2023-09-28 02:52:31] iter = 0020, loss = 233.1788
[2023-09-28 02:52:33] iter = 0030, loss = 222.0820
[2023-09-28 02:52:35] iter = 0040, loss = 218.6842
[2023-09-28 02:52:36] iter = 0050, loss = 216.1752
[2023-09-28 02:52:38] iter = 0060, loss = 209.2368
[2023-09-28 02:52:40] iter = 0070, loss = 208.7058
[2023-09-28 02:52:42] iter = 0080, loss = 206.8056
[2023-09-28 02:52:43] iter = 0090, loss = 209.9663
[2023-09-28 02:52:45] iter = 0100, loss = 205.7532
[2023-09-28 02:52:47] iter = 0110, loss = 209.1575
[2023-09-28 02:52:49] iter = 0120, loss = 202.1472
[2023-09-28 02:52:50] iter = 0130, loss = 202.7452
[2023-09-28 02:52:52] iter = 0140, loss = 199.5163
[2023-09-28 02:52:54] iter = 0150, loss = 199.2392
[2023-09-28 02:52:56] iter = 0160, loss = 201.6424
[2023-09-28 02:52:57] iter = 0170, loss = 198.1232
[2023-09-28 02:52:59] iter = 0180, loss = 198.0950
[2023-09-28 02:53:01] iter = 0190, loss = 193.6672
[2023-09-28 02:53:03] iter = 0200, loss = 194.8309
[2023-09-28 02:53:04] iter = 0210, loss = 186.1845
[2023-09-28 02:53:06] iter = 0220, loss = 194.0557
[2023-09-28 02:53:08] iter = 0230, loss = 192.7476
[2023-09-28 02:53:10] iter = 0240, loss = 196.0725
[2023-09-28 02:53:11] iter = 0250, loss = 194.7930
[2023-09-28 02:53:13] iter = 0260, loss = 187.4250
[2023-09-28 02:53:15] iter = 0270, loss = 188.0847
[2023-09-28 02:53:17] iter = 0280, loss = 193.5814
[2023-09-28 02:53:18] iter = 0290, loss = 193.2851
[2023-09-28 02:53:20] iter = 0300, loss = 192.6288
[2023-09-28 02:53:22] iter = 0310, loss = 187.8822
[2023-09-28 02:53:23] iter = 0320, loss = 190.8047
[2023-09-28 02:53:25] iter = 0330, loss = 187.2564
[2023-09-28 02:53:27] iter = 0340, loss = 193.3247
[2023-09-28 02:53:29] iter = 0350, loss = 194.7372
[2023-09-28 02:53:30] iter = 0360, loss = 186.0761
[2023-09-28 02:53:32] iter = 0370, loss = 192.2023
[2023-09-28 02:53:34] iter = 0380, loss = 189.4035
[2023-09-28 02:53:36] iter = 0390, loss = 186.7913
[2023-09-28 02:53:37] iter = 0400, loss = 194.3793
[2023-09-28 02:53:39] iter = 0410, loss = 189.4245
[2023-09-28 02:53:41] iter = 0420, loss = 191.0107
[2023-09-28 02:53:43] iter = 0430, loss = 186.0526
[2023-09-28 02:53:44] iter = 0440, loss = 183.8053
[2023-09-28 02:53:46] iter = 0450, loss = 188.5215
[2023-09-28 02:53:48] iter = 0460, loss = 182.2134
[2023-09-28 02:53:50] iter = 0470, loss = 190.6020
[2023-09-28 02:53:51] iter = 0480, loss = 190.2027
[2023-09-28 02:53:53] iter = 0490, loss = 180.0939
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:53:57] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000560 train acc = 1.0000, test acc = 0.2820
[2023-09-28 02:54:00] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000574 train acc = 1.0000, test acc = 0.2745
[2023-09-28 02:54:03] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000592 train acc = 1.0000, test acc = 0.2782
[2023-09-28 02:54:06] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000547 train acc = 1.0000, test acc = 0.2872
[2023-09-28 02:54:08] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000591 train acc = 1.0000, test acc = 0.2846
[2023-09-28 02:54:11] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000544 train acc = 1.0000, test acc = 0.2699
[2023-09-28 02:54:14] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000539 train acc = 1.0000, test acc = 0.2805
[2023-09-28 02:54:17] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000619 train acc = 1.0000, test acc = 0.2685
[2023-09-28 02:54:20] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000565 train acc = 1.0000, test acc = 0.2808
[2023-09-28 02:54:22] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000543 train acc = 1.0000, test acc = 0.2768
[2023-09-28 02:54:25] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000587 train acc = 1.0000, test acc = 0.2747
[2023-09-28 02:54:28] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000546 train acc = 1.0000, test acc = 0.2755
[2023-09-28 02:54:31] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000549 train acc = 1.0000, test acc = 0.2776
[2023-09-28 02:54:33] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000576 train acc = 1.0000, test acc = 0.2823
[2023-09-28 02:54:36] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000592 train acc = 1.0000, test acc = 0.2785
[2023-09-28 02:54:39] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000530 train acc = 1.0000, test acc = 0.2757
[2023-09-28 02:54:42] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000572 train acc = 1.0000, test acc = 0.2793
[2023-09-28 02:54:44] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000543 train acc = 1.0000, test acc = 0.2816
[2023-09-28 02:54:47] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000562 train acc = 1.0000, test acc = 0.2786
[2023-09-28 02:54:50] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000576 train acc = 1.0000, test acc = 0.2794
Evaluate 20 random ConvNet, mean = 0.2783 std = 0.0044
-------------------------
[2023-09-28 02:54:50] iter = 0500, loss = 182.7831
[2023-09-28 02:54:52] iter = 0510, loss = 186.7180
[2023-09-28 02:54:53] iter = 0520, loss = 186.1225
[2023-09-28 02:54:55] iter = 0530, loss = 187.4000
[2023-09-28 02:54:57] iter = 0540, loss = 194.2830
[2023-09-28 02:54:59] iter = 0550, loss = 191.6030
[2023-09-28 02:55:00] iter = 0560, loss = 187.9693
[2023-09-28 02:55:02] iter = 0570, loss = 185.4126
[2023-09-28 02:55:04] iter = 0580, loss = 178.7957
[2023-09-28 02:55:06] iter = 0590, loss = 187.1296
[2023-09-28 02:55:07] iter = 0600, loss = 186.7453
[2023-09-28 02:55:09] iter = 0610, loss = 180.5296
[2023-09-28 02:55:11] iter = 0620, loss = 186.2943
[2023-09-28 02:55:13] iter = 0630, loss = 182.5399
[2023-09-28 02:55:14] iter = 0640, loss = 182.2178
[2023-09-28 02:55:16] iter = 0650, loss = 178.9139
[2023-09-28 02:55:18] iter = 0660, loss = 184.2079
[2023-09-28 02:55:19] iter = 0670, loss = 179.2374
[2023-09-28 02:55:21] iter = 0680, loss = 180.5536
[2023-09-28 02:55:23] iter = 0690, loss = 189.9231
[2023-09-28 02:55:25] iter = 0700, loss = 180.8657
[2023-09-28 02:55:26] iter = 0710, loss = 184.1864
[2023-09-28 02:55:28] iter = 0720, loss = 186.9713
[2023-09-28 02:55:30] iter = 0730, loss = 182.1945
[2023-09-28 02:55:32] iter = 0740, loss = 186.9985
[2023-09-28 02:55:33] iter = 0750, loss = 181.8537
[2023-09-28 02:55:35] iter = 0760, loss = 187.1496
[2023-09-28 02:55:37] iter = 0770, loss = 179.7062
[2023-09-28 02:55:39] iter = 0780, loss = 179.9849
[2023-09-28 02:55:40] iter = 0790, loss = 186.4647
[2023-09-28 02:55:42] iter = 0800, loss = 181.5681
[2023-09-28 02:55:44] iter = 0810, loss = 183.5796
[2023-09-28 02:55:46] iter = 0820, loss = 182.3678
[2023-09-28 02:55:47] iter = 0830, loss = 182.0957
[2023-09-28 02:55:49] iter = 0840, loss = 184.8005
[2023-09-28 02:55:51] iter = 0850, loss = 185.0917
[2023-09-28 02:55:53] iter = 0860, loss = 176.9616
[2023-09-28 02:55:54] iter = 0870, loss = 173.9392
[2023-09-28 02:55:56] iter = 0880, loss = 186.8420
[2023-09-28 02:55:58] iter = 0890, loss = 180.8821
[2023-09-28 02:55:59] iter = 0900, loss = 186.3443
[2023-09-28 02:56:01] iter = 0910, loss = 183.8285
[2023-09-28 02:56:03] iter = 0920, loss = 178.0328
[2023-09-28 02:56:05] iter = 0930, loss = 182.9633
[2023-09-28 02:56:06] iter = 0940, loss = 183.4955
[2023-09-28 02:56:08] iter = 0950, loss = 177.2936
[2023-09-28 02:56:10] iter = 0960, loss = 186.7030
[2023-09-28 02:56:12] iter = 0970, loss = 185.7840
[2023-09-28 02:56:13] iter = 0980, loss = 181.9799
[2023-09-28 02:56:15] iter = 0990, loss = 180.2145
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:56:19] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000603 train acc = 1.0000, test acc = 0.2657
[2023-09-28 02:56:22] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000623 train acc = 1.0000, test acc = 0.2842
[2023-09-28 02:56:25] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000564 train acc = 1.0000, test acc = 0.2942
[2023-09-28 02:56:28] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000582 train acc = 1.0000, test acc = 0.2760
[2023-09-28 02:56:30] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000589 train acc = 1.0000, test acc = 0.2820
[2023-09-28 02:56:33] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000587 train acc = 1.0000, test acc = 0.2927
[2023-09-28 02:56:36] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000597 train acc = 1.0000, test acc = 0.2833
[2023-09-28 02:56:39] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000635 train acc = 1.0000, test acc = 0.2716
[2023-09-28 02:56:41] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000585 train acc = 1.0000, test acc = 0.2779
[2023-09-28 02:56:44] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000584 train acc = 1.0000, test acc = 0.2859
[2023-09-28 02:56:47] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000542 train acc = 1.0000, test acc = 0.2839
[2023-09-28 02:56:50] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000571 train acc = 1.0000, test acc = 0.2793
[2023-09-28 02:56:52] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000580 train acc = 1.0000, test acc = 0.2715
[2023-09-28 02:56:55] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000625 train acc = 1.0000, test acc = 0.2785
[2023-09-28 02:56:58] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000593 train acc = 1.0000, test acc = 0.2855
[2023-09-28 02:57:01] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000584 train acc = 1.0000, test acc = 0.2779
[2023-09-28 02:57:03] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000596 train acc = 1.0000, test acc = 0.2844
[2023-09-28 02:57:06] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000578 train acc = 1.0000, test acc = 0.2821
[2023-09-28 02:57:09] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000585 train acc = 1.0000, test acc = 0.2852
[2023-09-28 02:57:12] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000613 train acc = 1.0000, test acc = 0.2789
Evaluate 20 random ConvNet, mean = 0.2810 std = 0.0066
-------------------------
[2023-09-28 02:57:12] iter = 1000, loss = 185.1594

================== Exp 4 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fdc3bdd2370>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-09-28 02:57:29] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:57:32] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000447 train acc = 1.0000, test acc = 0.0941
[2023-09-28 02:57:35] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000435 train acc = 1.0000, test acc = 0.0917
[2023-09-28 02:57:38] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000418 train acc = 1.0000, test acc = 0.0806
[2023-09-28 02:57:40] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000466 train acc = 1.0000, test acc = 0.0865
[2023-09-28 02:57:43] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000468 train acc = 1.0000, test acc = 0.0898
[2023-09-28 02:57:46] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000432 train acc = 1.0000, test acc = 0.1012
[2023-09-28 02:57:49] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000438 train acc = 1.0000, test acc = 0.1012
[2023-09-28 02:57:51] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000415 train acc = 1.0000, test acc = 0.0844
[2023-09-28 02:57:54] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000441 train acc = 1.0000, test acc = 0.1023
[2023-09-28 02:57:57] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000427 train acc = 1.0000, test acc = 0.0881
[2023-09-28 02:58:00] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000436 train acc = 1.0000, test acc = 0.0893
[2023-09-28 02:58:02] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000440 train acc = 1.0000, test acc = 0.0884
[2023-09-28 02:58:05] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000440 train acc = 1.0000, test acc = 0.0978
[2023-09-28 02:58:08] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000435 train acc = 1.0000, test acc = 0.0876
[2023-09-28 02:58:11] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000449 train acc = 1.0000, test acc = 0.1091
[2023-09-28 02:58:13] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000456 train acc = 1.0000, test acc = 0.0876
[2023-09-28 02:58:16] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000455 train acc = 1.0000, test acc = 0.0728
[2023-09-28 02:58:19] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000454 train acc = 1.0000, test acc = 0.0997
[2023-09-28 02:58:22] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000443 train acc = 1.0000, test acc = 0.0905
[2023-09-28 02:58:24] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000421 train acc = 1.0000, test acc = 0.0861
Evaluate 20 random ConvNet, mean = 0.0914 std = 0.0083
-------------------------
[2023-09-28 02:58:25] iter = 0000, loss = 317.5574
[2023-09-28 02:58:26] iter = 0010, loss = 250.9307
[2023-09-28 02:58:28] iter = 0020, loss = 229.5663
[2023-09-28 02:58:30] iter = 0030, loss = 224.8398
[2023-09-28 02:58:31] iter = 0040, loss = 222.5635
[2023-09-28 02:58:33] iter = 0050, loss = 217.0706
[2023-09-28 02:58:35] iter = 0060, loss = 209.3439
[2023-09-28 02:58:36] iter = 0070, loss = 209.5242
[2023-09-28 02:58:38] iter = 0080, loss = 213.5475
[2023-09-28 02:58:40] iter = 0090, loss = 210.0417
[2023-09-28 02:58:41] iter = 0100, loss = 204.1596
[2023-09-28 02:58:43] iter = 0110, loss = 207.0714
[2023-09-28 02:58:45] iter = 0120, loss = 201.3908
[2023-09-28 02:58:47] iter = 0130, loss = 200.4906
[2023-09-28 02:58:48] iter = 0140, loss = 203.0850
[2023-09-28 02:58:50] iter = 0150, loss = 198.6587
[2023-09-28 02:58:52] iter = 0160, loss = 204.9088
[2023-09-28 02:58:54] iter = 0170, loss = 203.9575
[2023-09-28 02:58:55] iter = 0180, loss = 198.2161
[2023-09-28 02:58:57] iter = 0190, loss = 190.5829
[2023-09-28 02:58:59] iter = 0200, loss = 199.8066
[2023-09-28 02:59:01] iter = 0210, loss = 197.7611
[2023-09-28 02:59:02] iter = 0220, loss = 194.0707
[2023-09-28 02:59:04] iter = 0230, loss = 191.6153
[2023-09-28 02:59:06] iter = 0240, loss = 198.2833
[2023-09-28 02:59:08] iter = 0250, loss = 189.2955
[2023-09-28 02:59:09] iter = 0260, loss = 190.6052
[2023-09-28 02:59:11] iter = 0270, loss = 193.9488
[2023-09-28 02:59:13] iter = 0280, loss = 195.7394
[2023-09-28 02:59:14] iter = 0290, loss = 189.4464
[2023-09-28 02:59:16] iter = 0300, loss = 199.5112
[2023-09-28 02:59:18] iter = 0310, loss = 190.5315
[2023-09-28 02:59:20] iter = 0320, loss = 196.4161
[2023-09-28 02:59:21] iter = 0330, loss = 187.9423
[2023-09-28 02:59:23] iter = 0340, loss = 195.4843
[2023-09-28 02:59:25] iter = 0350, loss = 195.3781
[2023-09-28 02:59:27] iter = 0360, loss = 189.3052
[2023-09-28 02:59:28] iter = 0370, loss = 187.4248
[2023-09-28 02:59:30] iter = 0380, loss = 192.7244
[2023-09-28 02:59:32] iter = 0390, loss = 182.4591
[2023-09-28 02:59:34] iter = 0400, loss = 188.0054
[2023-09-28 02:59:35] iter = 0410, loss = 186.9107
[2023-09-28 02:59:37] iter = 0420, loss = 185.3579
[2023-09-28 02:59:39] iter = 0430, loss = 190.9447
[2023-09-28 02:59:41] iter = 0440, loss = 190.5554
[2023-09-28 02:59:42] iter = 0450, loss = 183.1529
[2023-09-28 02:59:44] iter = 0460, loss = 188.1352
[2023-09-28 02:59:46] iter = 0470, loss = 181.9067
[2023-09-28 02:59:48] iter = 0480, loss = 191.2497
[2023-09-28 02:59:49] iter = 0490, loss = 187.1689
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 02:59:54] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000563 train acc = 1.0000, test acc = 0.2717
[2023-09-28 02:59:56] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000544 train acc = 1.0000, test acc = 0.2851
[2023-09-28 02:59:59] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000555 train acc = 1.0000, test acc = 0.2834
[2023-09-28 03:00:02] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000577 train acc = 1.0000, test acc = 0.2748
[2023-09-28 03:00:05] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000564 train acc = 1.0000, test acc = 0.2728
[2023-09-28 03:00:07] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000544 train acc = 1.0000, test acc = 0.2840
[2023-09-28 03:00:10] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000596 train acc = 1.0000, test acc = 0.2705
[2023-09-28 03:00:13] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000596 train acc = 1.0000, test acc = 0.2719
[2023-09-28 03:00:16] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000559 train acc = 1.0000, test acc = 0.2791
[2023-09-28 03:00:18] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000554 train acc = 1.0000, test acc = 0.2866
[2023-09-28 03:00:21] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000578 train acc = 1.0000, test acc = 0.2720
[2023-09-28 03:00:24] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000586 train acc = 1.0000, test acc = 0.2753
[2023-09-28 03:00:27] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000603 train acc = 1.0000, test acc = 0.2764
[2023-09-28 03:00:29] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000613 train acc = 1.0000, test acc = 0.2620
[2023-09-28 03:00:32] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000577 train acc = 1.0000, test acc = 0.2794
[2023-09-28 03:00:35] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000560 train acc = 1.0000, test acc = 0.2786
[2023-09-28 03:00:38] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000544 train acc = 1.0000, test acc = 0.2795
[2023-09-28 03:00:40] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000591 train acc = 1.0000, test acc = 0.2759
[2023-09-28 03:00:43] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000560 train acc = 1.0000, test acc = 0.2774
[2023-09-28 03:00:46] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000569 train acc = 1.0000, test acc = 0.2660
Evaluate 20 random ConvNet, mean = 0.2761 std = 0.0061
-------------------------
[2023-09-28 03:00:46] iter = 0500, loss = 184.2376
[2023-09-28 03:00:48] iter = 0510, loss = 183.5397
[2023-09-28 03:00:49] iter = 0520, loss = 184.6386
[2023-09-28 03:00:51] iter = 0530, loss = 185.4302
[2023-09-28 03:00:53] iter = 0540, loss = 182.2741
[2023-09-28 03:00:55] iter = 0550, loss = 181.4678
[2023-09-28 03:00:56] iter = 0560, loss = 182.8177
[2023-09-28 03:00:58] iter = 0570, loss = 192.2502
[2023-09-28 03:01:00] iter = 0580, loss = 181.1956
[2023-09-28 03:01:02] iter = 0590, loss = 184.5341
[2023-09-28 03:01:03] iter = 0600, loss = 189.7234
[2023-09-28 03:01:05] iter = 0610, loss = 186.2457
[2023-09-28 03:01:07] iter = 0620, loss = 183.5261
[2023-09-28 03:01:09] iter = 0630, loss = 193.7599
[2023-09-28 03:01:10] iter = 0640, loss = 188.2075
[2023-09-28 03:01:12] iter = 0650, loss = 186.3223
[2023-09-28 03:01:14] iter = 0660, loss = 188.0126
[2023-09-28 03:01:16] iter = 0670, loss = 183.7768
[2023-09-28 03:01:17] iter = 0680, loss = 184.6513
[2023-09-28 03:01:19] iter = 0690, loss = 180.9132
[2023-09-28 03:01:21] iter = 0700, loss = 185.3904
[2023-09-28 03:01:23] iter = 0710, loss = 181.2737
[2023-09-28 03:01:24] iter = 0720, loss = 187.8420
[2023-09-28 03:01:26] iter = 0730, loss = 182.2916
[2023-09-28 03:01:28] iter = 0740, loss = 180.7162
[2023-09-28 03:01:29] iter = 0750, loss = 183.9976
[2023-09-28 03:01:31] iter = 0760, loss = 183.0413
[2023-09-28 03:01:33] iter = 0770, loss = 184.3749
[2023-09-28 03:01:35] iter = 0780, loss = 184.4771
[2023-09-28 03:01:36] iter = 0790, loss = 182.1873
[2023-09-28 03:01:38] iter = 0800, loss = 186.9925
[2023-09-28 03:01:40] iter = 0810, loss = 184.9034
[2023-09-28 03:01:42] iter = 0820, loss = 177.7402
[2023-09-28 03:01:43] iter = 0830, loss = 180.0736
[2023-09-28 03:01:45] iter = 0840, loss = 186.3229
[2023-09-28 03:01:47] iter = 0850, loss = 188.8464
[2023-09-28 03:01:49] iter = 0860, loss = 183.6369
[2023-09-28 03:01:50] iter = 0870, loss = 185.5095
[2023-09-28 03:01:52] iter = 0880, loss = 182.3282
[2023-09-28 03:01:54] iter = 0890, loss = 178.6002
[2023-09-28 03:01:56] iter = 0900, loss = 184.9525
[2023-09-28 03:01:57] iter = 0910, loss = 180.6145
[2023-09-28 03:01:59] iter = 0920, loss = 183.5442
[2023-09-28 03:02:01] iter = 0930, loss = 184.1732
[2023-09-28 03:02:03] iter = 0940, loss = 178.4409
[2023-09-28 03:02:04] iter = 0950, loss = 184.6994
[2023-09-28 03:02:06] iter = 0960, loss = 179.2286
[2023-09-28 03:02:08] iter = 0970, loss = 180.5220
[2023-09-28 03:02:10] iter = 0980, loss = 192.9744
[2023-09-28 03:02:11] iter = 0990, loss = 174.8540
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-09-28 03:02:16] Evaluate_00: epoch = 0300 train time = 0 s train loss = 0.000568 train acc = 1.0000, test acc = 0.2943
[2023-09-28 03:02:18] Evaluate_01: epoch = 0300 train time = 0 s train loss = 0.000576 train acc = 1.0000, test acc = 0.2805
[2023-09-28 03:02:21] Evaluate_02: epoch = 0300 train time = 0 s train loss = 0.000614 train acc = 1.0000, test acc = 0.2754
[2023-09-28 03:02:24] Evaluate_03: epoch = 0300 train time = 0 s train loss = 0.000543 train acc = 1.0000, test acc = 0.2753
[2023-09-28 03:02:27] Evaluate_04: epoch = 0300 train time = 0 s train loss = 0.000587 train acc = 1.0000, test acc = 0.2926
[2023-09-28 03:02:29] Evaluate_05: epoch = 0300 train time = 0 s train loss = 0.000591 train acc = 1.0000, test acc = 0.2778
[2023-09-28 03:02:32] Evaluate_06: epoch = 0300 train time = 0 s train loss = 0.000538 train acc = 1.0000, test acc = 0.2866
[2023-09-28 03:02:35] Evaluate_07: epoch = 0300 train time = 0 s train loss = 0.000554 train acc = 1.0000, test acc = 0.2725
[2023-09-28 03:02:38] Evaluate_08: epoch = 0300 train time = 0 s train loss = 0.000536 train acc = 1.0000, test acc = 0.2818
[2023-09-28 03:02:40] Evaluate_09: epoch = 0300 train time = 0 s train loss = 0.000582 train acc = 1.0000, test acc = 0.2797
[2023-09-28 03:02:43] Evaluate_10: epoch = 0300 train time = 0 s train loss = 0.000573 train acc = 1.0000, test acc = 0.2821
[2023-09-28 03:02:46] Evaluate_11: epoch = 0300 train time = 0 s train loss = 0.000612 train acc = 1.0000, test acc = 0.2705
[2023-09-28 03:02:49] Evaluate_12: epoch = 0300 train time = 0 s train loss = 0.000563 train acc = 1.0000, test acc = 0.2906
[2023-09-28 03:02:51] Evaluate_13: epoch = 0300 train time = 0 s train loss = 0.000577 train acc = 1.0000, test acc = 0.2832
[2023-09-28 03:02:54] Evaluate_14: epoch = 0300 train time = 0 s train loss = 0.000595 train acc = 1.0000, test acc = 0.2820
[2023-09-28 03:02:57] Evaluate_15: epoch = 0300 train time = 0 s train loss = 0.000559 train acc = 1.0000, test acc = 0.2802
[2023-09-28 03:03:00] Evaluate_16: epoch = 0300 train time = 0 s train loss = 0.000588 train acc = 1.0000, test acc = 0.2808
[2023-09-28 03:03:02] Evaluate_17: epoch = 0300 train time = 0 s train loss = 0.000535 train acc = 1.0000, test acc = 0.2893
[2023-09-28 03:03:05] Evaluate_18: epoch = 0300 train time = 0 s train loss = 0.000565 train acc = 1.0000, test acc = 0.2950
[2023-09-28 03:03:08] Evaluate_19: epoch = 0300 train time = 0 s train loss = 0.000530 train acc = 1.0000, test acc = 0.2823
Evaluate 20 random ConvNet, mean = 0.2826 std = 0.0068
-------------------------
[2023-09-28 03:03:08] iter = 1000, loss = 186.9727

==================== Final Results ====================

Run 5 experiments, train on ConvNet, evaluate 100 random ConvNet, mean  = 28.19%  std = 0.64%
