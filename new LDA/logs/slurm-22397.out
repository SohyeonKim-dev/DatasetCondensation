/data/happythgus/newLDA/DC
/data/opt/anaconda3/envs/pytorch1.12.1_p38/bin/python
moana-r3
eval_it_pool:  [0, 500, 1000]
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz
  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 32768/170498071 [00:00<25:38, 110766.67it/s]  0%|          | 65536/170498071 [00:00<25:30, 111364.26it/s]  0%|          | 98304/170498071 [00:00<25:29, 111376.37it/s]  0%|          | 229376/170498071 [00:01<11:43, 242031.67it/s]  0%|          | 425984/170498071 [00:01<07:10, 394877.53it/s]  0%|          | 851968/170498071 [00:01<03:46, 750545.06it/s]  1%|          | 1212416/170498071 [00:02<04:09, 679765.93it/s]  1%|          | 1933312/170498071 [00:02<02:19, 1209586.02it/s]  1%|          | 2097152/170498071 [00:03<03:08, 891758.20it/s]   1%|▏         | 2228224/170498071 [00:03<03:25, 818125.15it/s]  3%|▎         | 4521984/170498071 [00:03<01:03, 2617163.33it/s]  3%|▎         | 4882432/170498071 [00:04<01:34, 1747667.18it/s]  4%|▍         | 7208960/170498071 [00:04<00:51, 3188216.04it/s]  5%|▍         | 8028160/170498071 [00:04<00:43, 3699027.93it/s]  5%|▌         | 8552448/170498071 [00:04<00:49, 3245995.42it/s]  5%|▌         | 8978432/170498071 [00:05<00:56, 2846147.69it/s]  6%|▌         | 9797632/170498071 [00:05<00:53, 2997632.26it/s]  6%|▌         | 10223616/170498071 [00:05<00:50, 3172471.18it/s]  6%|▋         | 10715136/170498071 [00:05<00:57, 2775159.00it/s]  7%|▋         | 11599872/170498071 [00:05<00:42, 3743042.34it/s]  7%|▋         | 12091392/170498071 [00:06<00:51, 3104116.32it/s]  7%|▋         | 12550144/170498071 [00:06<00:58, 2705558.58it/s]  8%|▊         | 13402112/170498071 [00:06<00:43, 3646419.91it/s]  8%|▊         | 13893632/170498071 [00:06<00:51, 3054402.06it/s]  8%|▊         | 14450688/170498071 [00:06<00:56, 2775788.55it/s]  9%|▉         | 15335424/170498071 [00:07<00:41, 3765114.13it/s]  9%|▉         | 15859712/170498071 [00:07<00:48, 3168287.24it/s] 10%|▉         | 16384000/170498071 [00:07<00:54, 2819516.05it/s] 10%|█         | 17268736/170498071 [00:07<00:40, 3817445.41it/s] 10%|█         | 17793024/170498071 [00:07<00:47, 3197906.24it/s] 11%|█         | 18350080/170498071 [00:08<00:52, 2925203.53it/s] 11%|█         | 18743296/170498071 [00:08<00:50, 3029031.92it/s] 11%|█         | 19103744/170498071 [00:08<00:48, 3107111.45it/s] 11%|█▏        | 19464192/170498071 [00:08<00:50, 3012828.80it/s] 12%|█▏        | 19824640/170498071 [00:08<00:48, 3076170.02it/s] 12%|█▏        | 20185088/170498071 [00:08<00:47, 3152299.11it/s] 12%|█▏        | 20545536/170498071 [00:08<00:47, 3167324.58it/s] 12%|█▏        | 20905984/170498071 [00:08<00:46, 3237973.32it/s] 12%|█▏        | 21266432/170498071 [00:08<00:45, 3265631.06it/s] 13%|█▎        | 21626880/170498071 [00:09<00:45, 3245546.21it/s] 13%|█▎        | 21987328/170498071 [00:09<00:47, 3154512.14it/s] 13%|█▎        | 22315008/170498071 [00:09<00:47, 3118495.22it/s] 13%|█▎        | 22642688/170498071 [00:09<00:47, 3132007.39it/s] 13%|█▎        | 22970368/170498071 [00:09<00:47, 3108716.83it/s] 14%|█▎        | 23298048/170498071 [00:09<00:46, 3136709.94it/s] 14%|█▍        | 23724032/170498071 [00:09<00:43, 3369030.01it/s] 14%|█▍        | 24084480/170498071 [00:09<00:44, 3263491.33it/s] 14%|█▍        | 24444928/170498071 [00:09<00:43, 3332101.34it/s] 15%|█▍        | 24805376/170498071 [00:10<00:42, 3396907.14it/s] 15%|█▍        | 25165824/170498071 [00:10<00:44, 3299187.46it/s] 15%|█▍        | 25526272/170498071 [00:10<00:43, 3370015.82it/s] 15%|█▌        | 25886720/170498071 [00:10<00:42, 3409024.37it/s] 15%|█▌        | 26247168/170498071 [00:10<00:43, 3333607.69it/s] 16%|█▌        | 26607616/170498071 [00:10<00:43, 3326910.58it/s] 16%|█▌        | 26968064/170498071 [00:10<00:42, 3384378.80it/s] 16%|█▌        | 27328512/170498071 [00:10<00:42, 3331442.05it/s] 16%|█▋        | 27754496/170498071 [00:10<00:40, 3512623.10it/s] 16%|█▋        | 28114944/170498071 [00:11<00:42, 3342624.97it/s] 17%|█▋        | 28508160/170498071 [00:11<00:42, 3350941.01it/s] 17%|█▋        | 28901376/170498071 [00:11<00:40, 3466854.61it/s] 17%|█▋        | 29261824/170498071 [00:11<00:41, 3386098.23it/s] 17%|█▋        | 29622272/170498071 [00:11<00:42, 3341121.27it/s] 18%|█▊        | 29982720/170498071 [00:11<00:41, 3400760.93it/s] 18%|█▊        | 30343168/170498071 [00:11<00:41, 3375958.64it/s] 18%|█▊        | 30769152/170498071 [00:11<00:39, 3527919.75it/s] 18%|█▊        | 31129600/170498071 [00:11<00:41, 3335033.53it/s] 18%|█▊        | 31522816/170498071 [00:12<00:41, 3336578.04it/s] 19%|█▊        | 31916032/170498071 [00:12<00:40, 3464428.16it/s] 19%|█▉        | 32276480/170498071 [00:12<00:40, 3386516.80it/s] 19%|█▉        | 32636928/170498071 [00:12<00:41, 3344918.88it/s] 19%|█▉        | 32997376/170498071 [00:12<00:40, 3402224.82it/s] 20%|█▉        | 33357824/170498071 [00:12<00:40, 3373811.95it/s] 20%|█▉        | 33751040/170498071 [00:12<00:39, 3451391.61it/s] 20%|██        | 34111488/170498071 [00:12<00:39, 3432379.51it/s] 20%|██        | 34471936/170498071 [00:12<00:39, 3428277.34it/s] 20%|██        | 34832384/170498071 [00:13<00:39, 3470791.99it/s] 21%|██        | 35192832/170498071 [00:13<00:40, 3341383.84it/s] 21%|██        | 35553280/170498071 [00:13<00:40, 3310691.44it/s] 21%|██        | 35946496/170498071 [00:13<00:38, 3482049.73it/s] 21%|██▏       | 36306944/170498071 [00:13<00:39, 3406244.71it/s] 22%|██▏       | 36667392/170498071 [00:13<00:39, 3373468.16it/s] 22%|██▏       | 37027840/170498071 [00:13<00:39, 3402241.34it/s] 22%|██▏       | 37388288/170498071 [00:13<00:39, 3380085.01it/s] 22%|██▏       | 37748736/170498071 [00:13<00:39, 3372783.83it/s] 22%|██▏       | 38109184/170498071 [00:13<00:38, 3423959.16it/s] 23%|██▎       | 38469632/170498071 [00:14<00:39, 3359370.19it/s] 23%|██▎       | 38830080/170498071 [00:14<00:38, 3411794.21it/s] 23%|██▎       | 39223296/170498071 [00:14<00:38, 3452459.91it/s] 23%|██▎       | 39583744/170498071 [00:14<00:38, 3396234.59it/s] 23%|██▎       | 39944192/170498071 [00:14<00:38, 3429958.58it/s] 24%|██▎       | 40304640/170498071 [00:14<00:38, 3417413.38it/s] 24%|██▍       | 40665088/170498071 [00:14<00:38, 3396012.98it/s] 24%|██▍       | 41058304/170498071 [00:14<00:37, 3476346.29it/s] 24%|██▍       | 41418752/170498071 [00:14<00:38, 3375346.50it/s] 25%|██▍       | 41779200/170498071 [00:15<00:39, 3264581.74it/s] 25%|██▍       | 42106880/170498071 [00:15<00:39, 3216984.60it/s] 25%|██▍       | 42434560/170498071 [00:15<00:40, 3189342.82it/s] 25%|██▌       | 42827776/170498071 [00:15<00:37, 3387521.62it/s] 25%|██▌       | 43188224/170498071 [00:15<00:37, 3429713.28it/s] 26%|██▌       | 43548672/170498071 [00:15<00:38, 3331586.21it/s] 26%|██▌       | 43909120/170498071 [00:15<00:37, 3401924.85it/s] 26%|██▌       | 44302336/170498071 [00:15<00:35, 3531665.49it/s] 26%|██▌       | 44662784/170498071 [00:15<00:36, 3447566.16it/s] 26%|██▋       | 45056000/170498071 [00:16<00:35, 3498699.91it/s] 27%|██▋       | 45449216/170498071 [00:16<00:34, 3617304.70it/s] 27%|██▋       | 45842432/170498071 [00:16<00:35, 3465910.31it/s] 27%|██▋       | 46268416/170498071 [00:16<00:34, 3612762.45it/s] 27%|██▋       | 46661632/170498071 [00:16<00:33, 3667389.84it/s] 28%|██▊       | 47054848/170498071 [00:16<00:34, 3585729.70it/s] 28%|██▊       | 47480832/170498071 [00:16<00:33, 3709881.16it/s] 28%|██▊       | 47874048/170498071 [00:16<00:33, 3637396.17it/s] 28%|██▊       | 48267264/170498071 [00:16<00:33, 3687669.25it/s] 29%|██▊       | 48693248/170498071 [00:17<00:32, 3780452.06it/s] 29%|██▉       | 49086464/170498071 [00:17<00:32, 3680163.78it/s] 29%|██▉       | 49512448/170498071 [00:17<00:32, 3747592.87it/s] 29%|██▉       | 49938432/170498071 [00:17<00:31, 3850611.01it/s] 30%|██▉       | 50331648/170498071 [00:17<00:32, 3698294.36it/s] 30%|██▉       | 50790400/170498071 [00:17<00:30, 3865109.04it/s] 30%|███       | 51216384/170498071 [00:17<00:30, 3887259.69it/s] 30%|███       | 51609600/170498071 [00:17<00:30, 3848126.60it/s] 31%|███       | 52068352/170498071 [00:17<00:29, 3997501.16it/s] 31%|███       | 52494336/170498071 [00:17<00:30, 3896719.10it/s] 31%|███       | 52920320/170498071 [00:18<00:30, 3915462.32it/s] 31%|███▏      | 53379072/170498071 [00:18<00:28, 4096327.28it/s] 32%|███▏      | 53805056/170498071 [00:18<00:29, 3891222.84it/s] 32%|███▏      | 54263808/170498071 [00:18<00:28, 4072872.64it/s] 32%|███▏      | 54722560/170498071 [00:18<00:28, 4104525.06it/s] 32%|███▏      | 55148544/170498071 [00:18<00:28, 4033070.24it/s] 33%|███▎      | 55607296/170498071 [00:18<00:27, 4171553.67it/s] 33%|███▎      | 56033280/170498071 [00:18<00:27, 4187205.24it/s] 33%|███▎      | 56459264/170498071 [00:18<00:27, 4172503.24it/s] 33%|███▎      | 56950784/170498071 [00:19<00:26, 4351056.15it/s] 34%|███▎      | 57409536/170498071 [00:19<00:26, 4251153.50it/s] 34%|███▍      | 57868288/170498071 [00:19<00:26, 4303230.91it/s] 34%|███▍      | 58359808/170498071 [00:19<00:25, 4437628.92it/s] 34%|███▍      | 58818560/170498071 [00:19<00:25, 4347585.38it/s] 35%|███▍      | 59310080/170498071 [00:19<00:25, 4430515.74it/s] 35%|███▌      | 59801600/170498071 [00:19<00:24, 4557291.71it/s] 35%|███▌      | 60260352/170498071 [00:19<00:25, 4368246.56it/s] 36%|███▌      | 60784640/170498071 [00:19<00:23, 4595979.42it/s] 36%|███▌      | 61308928/170498071 [00:20<00:23, 4632756.55it/s] 36%|███▌      | 61800448/170498071 [00:20<00:23, 4572307.44it/s] 37%|███▋      | 62324736/170498071 [00:20<00:22, 4733914.43it/s] 37%|███▋      | 62816256/170498071 [00:20<00:22, 4718306.91it/s] 37%|███▋      | 63340544/170498071 [00:20<00:22, 4795572.77it/s] 37%|███▋      | 63864832/170498071 [00:20<00:21, 4919812.85it/s] 38%|███▊      | 64389120/170498071 [00:20<00:21, 4912727.11it/s] 38%|███▊      | 64913408/170498071 [00:20<00:21, 4935193.84it/s] 38%|███▊      | 65503232/170498071 [00:20<00:20, 5111909.97it/s] 39%|███▊      | 66027520/170498071 [00:20<00:20, 5010120.32it/s] 39%|███▉      | 66584576/170498071 [00:21<00:20, 5135518.10it/s] 39%|███▉      | 67174400/170498071 [00:21<00:19, 5297805.52it/s] 40%|███▉      | 67731456/170498071 [00:21<00:19, 5145255.60it/s] 40%|████      | 68321280/170498071 [00:21<00:19, 5285564.62it/s] 40%|████      | 68911104/170498071 [00:21<00:18, 5399909.33it/s] 41%|████      | 69468160/170498071 [00:21<00:19, 5298053.32it/s] 41%|████      | 70090752/170498071 [00:21<00:18, 5551235.28it/s] 41%|████▏     | 70680576/170498071 [00:21<00:17, 5557139.37it/s] 42%|████▏     | 71270400/170498071 [00:21<00:17, 5654364.45it/s] 42%|████▏     | 71860224/170498071 [00:22<00:17, 5709537.89it/s] 42%|████▏     | 72450048/170498071 [00:22<00:17, 5700977.39it/s] 43%|████▎     | 73072640/170498071 [00:22<00:16, 5831962.35it/s] 43%|████▎     | 73695232/170498071 [00:22<00:16, 5916065.14it/s] 44%|████▎     | 74350592/170498071 [00:22<00:15, 6012150.57it/s] 44%|████▍     | 74973184/170498071 [00:22<00:15, 6024663.09it/s] 44%|████▍     | 75694080/170498071 [00:22<00:15, 6295031.29it/s] 45%|████▍     | 76349440/170498071 [00:22<00:15, 6104796.21it/s] 45%|████▌     | 77070336/170498071 [00:22<00:14, 6332949.81it/s] 46%|████▌     | 77758464/170498071 [00:22<00:14, 6482192.85it/s] 46%|████▌     | 78413824/170498071 [00:23<00:14, 6302731.57it/s] 46%|████▋     | 79167488/170498071 [00:23<00:13, 6555369.74it/s] 47%|████▋     | 79888384/170498071 [00:23<00:13, 6680975.79it/s] 47%|████▋     | 80576512/170498071 [00:23<00:13, 6484886.99it/s] 48%|████▊     | 81362944/170498071 [00:23<00:12, 6869178.75it/s] 48%|████▊     | 82083840/170498071 [00:23<00:12, 6889613.23it/s] 49%|████▊     | 82804736/170498071 [00:23<00:12, 6927664.26it/s] 49%|████▉     | 83591168/170498071 [00:23<00:12, 7104382.59it/s] 49%|████▉     | 84312064/170498071 [00:23<00:12, 7112063.27it/s] 50%|████▉     | 85098496/170498071 [00:24<00:11, 7255909.60it/s] 50%|█████     | 85884928/170498071 [00:24<00:11, 7415860.57it/s] 51%|█████     | 86671360/170498071 [00:24<00:11, 7430732.26it/s] 51%|█████▏    | 87457792/170498071 [00:24<00:11, 7524280.33it/s] 52%|█████▏    | 88309760/170498071 [00:24<00:10, 7803052.66it/s] 52%|█████▏    | 89096192/170498071 [00:24<00:10, 7526471.42it/s] 53%|█████▎    | 90013696/170498071 [00:24<00:10, 7924883.04it/s] 53%|█████▎    | 90898432/170498071 [00:24<00:09, 8114336.90it/s] 54%|█████▍    | 91717632/170498071 [00:24<00:10, 7867658.33it/s] 54%|█████▍    | 92667904/170498071 [00:24<00:09, 8298376.68it/s] 55%|█████▍    | 93519872/170498071 [00:25<00:20, 3695420.47it/s] 56%|█████▌    | 94830592/170498071 [00:25<00:17, 4375554.16it/s] 57%|█████▋    | 96600064/170498071 [00:25<00:12, 6016965.23it/s] 57%|█████▋    | 97419264/170498071 [00:26<00:23, 3057286.40it/s] 57%|█████▋    | 98009088/170498071 [00:27<00:30, 2388688.58it/s] 58%|█████▊    | 98467840/170498071 [00:27<00:32, 2220374.26it/s] 59%|█████▉    | 100499456/170498071 [00:27<00:20, 3425025.08it/s] 59%|█████▉    | 100990976/170498071 [00:28<00:30, 2304463.98it/s] 60%|██████    | 102563840/170498071 [00:28<00:21, 3175148.85it/s] 61%|██████    | 103645184/170498071 [00:28<00:16, 3971526.76it/s] 61%|██████    | 104300544/170498071 [00:28<00:18, 3552139.74it/s] 62%|██████▏   | 105086976/170498071 [00:29<00:18, 3523979.12it/s] 62%|██████▏   | 105578496/170498071 [00:29<00:17, 3716689.02it/s] 62%|██████▏   | 106102784/170498071 [00:29<00:18, 3419096.63it/s] 63%|██████▎   | 106594304/170498071 [00:29<00:17, 3662180.04it/s] 63%|██████▎   | 107118592/170498071 [00:29<00:18, 3360650.82it/s] 63%|██████▎   | 107577344/170498071 [00:29<00:17, 3553642.40it/s] 63%|██████▎   | 108167168/170498071 [00:29<00:18, 3398851.52it/s] 64%|██████▎   | 108593152/170498071 [00:30<00:17, 3572057.13it/s] 64%|██████▍   | 109215744/170498071 [00:30<00:15, 4033979.36it/s] 64%|██████▍   | 109674496/170498071 [00:30<00:17, 3484221.94it/s] 65%|██████▍   | 110067712/170498071 [00:30<00:31, 1917645.05it/s] 65%|██████▌   | 111214592/170498071 [00:31<00:23, 2534118.36it/s] 66%|██████▌   | 112492544/170498071 [00:31<00:14, 3915651.05it/s] 66%|██████▋   | 113082368/170498071 [00:31<00:17, 3331270.73it/s] 67%|██████▋   | 113639424/170498071 [00:31<00:15, 3646413.95it/s] 67%|██████▋   | 114163712/170498071 [00:31<00:17, 3131590.67it/s] 67%|██████▋   | 114589696/170498071 [00:32<00:19, 2845437.66it/s] 67%|██████▋   | 115015680/170498071 [00:32<00:18, 3061676.11it/s] 68%|██████▊   | 115408896/170498071 [00:32<00:20, 2694433.55it/s] 68%|██████▊   | 115867648/170498071 [00:32<00:17, 3052037.22it/s] 68%|██████▊   | 116228096/170498071 [00:32<00:19, 2744809.77it/s] 68%|██████▊   | 116621312/170498071 [00:32<00:18, 2984273.26it/s] 69%|██████▊   | 116981760/170498071 [00:32<00:19, 2707728.14it/s] 69%|██████▉   | 117309440/170498071 [00:33<00:18, 2807211.37it/s] 69%|██████▉   | 117833728/170498071 [00:33<00:18, 2868586.31it/s] 69%|██████▉   | 118161408/170498071 [00:33<00:18, 2843644.49it/s] 70%|██████▉   | 118718464/170498071 [00:33<00:18, 2861461.42it/s] 70%|██████▉   | 119013376/170498071 [00:33<00:17, 2880307.11it/s] 70%|███████   | 119603200/170498071 [00:33<00:17, 2922391.52it/s] 70%|███████   | 119930880/170498071 [00:33<00:16, 2982047.94it/s] 71%|███████   | 120487936/170498071 [00:34<00:15, 3146252.20it/s] 71%|███████   | 120815616/170498071 [00:34<00:16, 2942980.45it/s] 71%|███████   | 121405440/170498071 [00:34<00:16, 2967272.94it/s] 71%|███████▏  | 121733120/170498071 [00:34<00:16, 3022227.30it/s] 72%|███████▏  | 122290176/170498071 [00:34<00:14, 3331149.98it/s] 72%|███████▏  | 122650624/170498071 [00:34<00:16, 2973226.87it/s] 72%|███████▏  | 123207680/170498071 [00:34<00:14, 3296878.77it/s] 72%|███████▏  | 123568128/170498071 [00:35<00:15, 2940249.01it/s] 73%|███████▎  | 124125184/170498071 [00:35<00:13, 3371932.97it/s] 73%|███████▎  | 124485632/170498071 [00:35<00:15, 2914116.23it/s] 73%|███████▎  | 125042688/170498071 [00:35<00:13, 3348925.41it/s] 74%|███████▎  | 125403136/170498071 [00:35<00:15, 2888362.95it/s] 74%|███████▍  | 125960192/170498071 [00:35<00:13, 3369168.34it/s] 74%|███████▍  | 126353408/170498071 [00:35<00:15, 2924265.41it/s] 74%|███████▍  | 126910464/170498071 [00:36<00:14, 2961734.40it/s] 75%|███████▍  | 127238144/170498071 [00:36<00:14, 2998690.45it/s] 75%|███████▍  | 127827968/170498071 [00:36<00:12, 3300926.10it/s] 75%|███████▌  | 128188416/170498071 [00:36<00:14, 3002675.33it/s] 76%|███████▌  | 128745472/170498071 [00:36<00:12, 3460490.58it/s] 76%|███████▌  | 129105920/170498071 [00:36<00:14, 2922457.59it/s] 76%|███████▌  | 129695744/170498071 [00:37<00:13, 3075953.26it/s] 76%|███████▋  | 130023424/170498071 [00:37<00:13, 2985541.30it/s] 77%|███████▋  | 130613248/170498071 [00:37<00:11, 3520115.22it/s] 77%|███████▋  | 131006464/170498071 [00:37<00:13, 3005209.20it/s] 77%|███████▋  | 131563520/170498071 [00:37<00:12, 3011261.09it/s] 77%|███████▋  | 131891200/170498071 [00:37<00:12, 3016684.06it/s] 78%|███████▊  | 132481024/170498071 [00:37<00:12, 3108883.21it/s] 78%|███████▊  | 132808704/170498071 [00:38<00:12, 3042514.13it/s] 78%|███████▊  | 133431296/170498071 [00:38<00:11, 3109428.64it/s] 78%|███████▊  | 133758976/170498071 [00:38<00:11, 3113276.64it/s] 79%|███████▉  | 134348800/170498071 [00:38<00:11, 3122764.94it/s] 79%|███████▉  | 134676480/170498071 [00:38<00:11, 3107953.88it/s] 79%|███████▉  | 135266304/170498071 [00:38<00:10, 3244902.89it/s] 80%|███████▉  | 135593984/170498071 [00:38<00:11, 3065835.03it/s] 80%|███████▉  | 136216576/170498071 [00:39<00:11, 3091878.72it/s] 80%|████████  | 136544256/170498071 [00:39<00:10, 3126771.12it/s] 80%|████████  | 137134080/170498071 [00:39<00:10, 3242109.34it/s] 81%|████████  | 137461760/170498071 [00:39<00:10, 3094770.24it/s] 81%|████████  | 138084352/170498071 [00:39<00:10, 3100981.04it/s] 81%|████████  | 138444800/170498071 [00:39<00:10, 3188862.13it/s] 82%|████████▏ | 139001856/170498071 [00:40<00:10, 3142466.30it/s] 82%|████████▏ | 139329536/170498071 [00:40<00:09, 3119528.11it/s] 82%|████████▏ | 139952128/170498071 [00:40<00:09, 3119797.08it/s] 82%|████████▏ | 140312576/170498071 [00:40<00:09, 3159537.55it/s] 83%|████████▎ | 140902400/170498071 [00:40<00:09, 3156186.84it/s] 83%|████████▎ | 141262848/170498071 [00:40<00:09, 3187701.16it/s] 83%|████████▎ | 141819904/170498071 [00:40<00:08, 3348881.88it/s] 83%|████████▎ | 142180352/170498071 [00:41<00:09, 3129437.18it/s] 84%|████████▎ | 142770176/170498071 [00:41<00:08, 3172001.64it/s] 84%|████████▍ | 143097856/170498071 [00:41<00:08, 3130974.93it/s] 84%|████████▍ | 143720448/170498071 [00:41<00:07, 3361491.18it/s] 85%|████████▍ | 144080896/170498071 [00:41<00:08, 3154844.05it/s] 85%|████████▍ | 144703488/170498071 [00:41<00:08, 3199134.34it/s] 85%|████████▌ | 145031168/170498071 [00:41<00:07, 3183434.79it/s] 85%|████████▌ | 145653760/170498071 [00:42<00:07, 3408680.16it/s] 86%|████████▌ | 146014208/170498071 [00:42<00:07, 3188609.00it/s] 86%|████████▌ | 146636800/170498071 [00:42<00:07, 3256082.56it/s] 86%|████████▌ | 146964480/170498071 [00:42<00:07, 3217465.23it/s] 87%|████████▋ | 147619840/170498071 [00:42<00:06, 3466927.30it/s] 87%|████████▋ | 147980288/170498071 [00:42<00:06, 3235865.13it/s] 87%|████████▋ | 148635648/170498071 [00:42<00:06, 3307258.66it/s] 87%|████████▋ | 148963328/170498071 [00:43<00:06, 3297465.61it/s] 88%|████████▊ | 149618688/170498071 [00:43<00:05, 3955762.16it/s] 88%|████████▊ | 150044672/170498071 [00:43<00:06, 3234833.04it/s] 88%|████████▊ | 150667264/170498071 [00:43<00:05, 3331975.87it/s] 89%|████████▊ | 151027712/170498071 [00:43<00:05, 3329615.85it/s] 89%|████████▉ | 151715840/170498071 [00:43<00:05, 3418501.84it/s] 89%|████████▉ | 152076288/170498071 [00:43<00:05, 3427200.94it/s] 90%|████████▉ | 152764416/170498071 [00:44<00:04, 3684283.90it/s] 90%|████████▉ | 153157632/170498071 [00:44<00:04, 3479235.55it/s] 90%|█████████ | 153845760/170498071 [00:44<00:04, 3787207.29it/s] 90%|█████████ | 154238976/170498071 [00:44<00:04, 3503243.18it/s] 91%|█████████ | 154927104/170498071 [00:44<00:03, 4252023.74it/s] 91%|█████████ | 155385856/170498071 [00:44<00:04, 3472642.97it/s] 92%|█████████▏| 156073984/170498071 [00:45<00:03, 3751566.80it/s] 92%|█████████▏| 156499968/170498071 [00:45<00:03, 3597599.97it/s] 92%|█████████▏| 157220864/170498071 [00:45<00:03, 4382971.45it/s] 93%|█████████▎| 157712384/170498071 [00:45<00:03, 3589747.52it/s] 93%|█████████▎| 158433280/170498071 [00:45<00:03, 3737723.84it/s] 93%|█████████▎| 158859264/170498071 [00:45<00:03, 3753682.32it/s] 94%|█████████▎| 159645696/170498071 [00:45<00:02, 3971003.17it/s] 94%|█████████▍| 160071680/170498071 [00:46<00:02, 3906161.62it/s] 94%|█████████▍| 160727040/170498071 [00:46<00:02, 4483203.41it/s] 95%|█████████▍| 161218560/170498071 [00:46<00:02, 3834535.00it/s] 95%|█████████▍| 161841152/170498071 [00:46<00:01, 4373697.66it/s] 95%|█████████▌| 162332672/170498071 [00:46<00:01, 4123726.41it/s] 95%|█████████▌| 162791424/170498071 [00:46<00:01, 4133104.63it/s] 96%|█████████▌| 163446784/170498071 [00:46<00:01, 4692532.30it/s] 96%|█████████▌| 163938304/170498071 [00:46<00:01, 4062721.17it/s] 97%|█████████▋| 164560896/170498071 [00:47<00:01, 4540182.12it/s] 97%|█████████▋| 165052416/170498071 [00:47<00:01, 4393743.02it/s] 97%|█████████▋| 165543936/170498071 [00:47<00:01, 4427574.86it/s] 97%|█████████▋| 166133760/170498071 [00:47<00:00, 4776250.41it/s] 98%|█████████▊| 166625280/170498071 [00:47<00:00, 4427768.79it/s] 98%|█████████▊| 167215104/170498071 [00:47<00:00, 4797785.22it/s] 98%|█████████▊| 167772160/170498071 [00:47<00:00, 4876461.96it/s] 99%|█████████▊| 168296448/170498071 [00:47<00:00, 4641336.76it/s] 99%|█████████▉| 168951808/170498071 [00:47<00:00, 5147072.92it/s] 99%|█████████▉| 169508864/170498071 [00:48<00:00, 4841633.48it/s]100%|█████████▉| 170131456/170498071 [00:48<00:00, 5179238.73it/s]100%|██████████| 170498071/170498071 [00:48<00:00, 3532830.77it/s]
main.py:94: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
main.py:94: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/utils/tensor_new.cpp:201.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
Extracting data/cifar-10-python.tar.gz to data
Files already downloaded and verified

================== Exp 0 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f57875b1d90>, 'dsa': False}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-10-26 03:34:00] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 03:34:06] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.003708 train acc = 1.0000, test acc = 0.1282
[2023-10-26 03:34:09] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003675 train acc = 1.0000, test acc = 0.1312
[2023-10-26 03:34:13] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003698 train acc = 1.0000, test acc = 0.1175
[2023-10-26 03:34:16] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003675 train acc = 1.0000, test acc = 0.1212
[2023-10-26 03:34:19] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003642 train acc = 1.0000, test acc = 0.1162
[2023-10-26 03:34:22] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.003768 train acc = 1.0000, test acc = 0.1177
[2023-10-26 03:34:25] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.003657 train acc = 1.0000, test acc = 0.1400
[2023-10-26 03:34:29] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.003771 train acc = 1.0000, test acc = 0.1219
[2023-10-26 03:34:32] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.003769 train acc = 1.0000, test acc = 0.1089
[2023-10-26 03:34:35] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.003733 train acc = 1.0000, test acc = 0.1143
[2023-10-26 03:34:38] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.003693 train acc = 1.0000, test acc = 0.1296
[2023-10-26 03:34:41] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.003705 train acc = 1.0000, test acc = 0.1168
[2023-10-26 03:34:45] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.003740 train acc = 1.0000, test acc = 0.1314
[2023-10-26 03:34:48] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.003759 train acc = 1.0000, test acc = 0.1087
[2023-10-26 03:34:51] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.003734 train acc = 1.0000, test acc = 0.1267
[2023-10-26 03:34:54] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.003652 train acc = 1.0000, test acc = 0.1267
[2023-10-26 03:34:57] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.003797 train acc = 1.0000, test acc = 0.1090
[2023-10-26 03:35:01] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.003792 train acc = 1.0000, test acc = 0.1294
[2023-10-26 03:35:04] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.003722 train acc = 1.0000, test acc = 0.1059
[2023-10-26 03:35:07] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.003767 train acc = 1.0000, test acc = 0.1074
Evaluate 20 random ConvNet, mean = 0.1204 std = 0.0095
-------------------------
[2023-10-26 03:35:12] iter = 0000, loss = 218.6661
[2023-10-26 03:36:00] iter = 0010, loss = 146.9835
[2023-10-26 03:36:47] iter = 0020, loss = 141.7727
[2023-10-26 03:37:34] iter = 0030, loss = 135.6184
[2023-10-26 03:38:21] iter = 0040, loss = 127.6047
[2023-10-26 03:39:09] iter = 0050, loss = 126.3960
[2023-10-26 03:39:56] iter = 0060, loss = 124.3409
[2023-10-26 03:40:44] iter = 0070, loss = 118.8542
[2023-10-26 03:41:31] iter = 0080, loss = 119.8703
[2023-10-26 03:42:19] iter = 0090, loss = 114.5571
[2023-10-26 03:43:07] iter = 0100, loss = 111.9569
[2023-10-26 03:43:54] iter = 0110, loss = 112.3313
[2023-10-26 03:44:42] iter = 0120, loss = 111.2038
[2023-10-26 03:45:29] iter = 0130, loss = 109.9914
[2023-10-26 03:46:16] iter = 0140, loss = 103.7215
[2023-10-26 03:47:04] iter = 0150, loss = 105.3957
[2023-10-26 03:47:52] iter = 0160, loss = 110.6177
[2023-10-26 03:48:39] iter = 0170, loss = 107.2239
[2023-10-26 03:49:27] iter = 0180, loss = 102.7972
[2023-10-26 03:50:15] iter = 0190, loss = 103.7129
[2023-10-26 03:51:02] iter = 0200, loss = 103.5327
[2023-10-26 03:51:50] iter = 0210, loss = 102.0990
[2023-10-26 03:52:37] iter = 0220, loss = 101.8016
[2023-10-26 03:53:25] iter = 0230, loss = 99.8038
[2023-10-26 03:54:13] iter = 0240, loss = 101.3912
[2023-10-26 03:55:01] iter = 0250, loss = 96.5681
[2023-10-26 03:55:48] iter = 0260, loss = 98.1667
[2023-10-26 03:56:36] iter = 0270, loss = 98.1508
[2023-10-26 03:57:24] iter = 0280, loss = 97.8185
[2023-10-26 03:58:11] iter = 0290, loss = 95.5244
[2023-10-26 03:58:59] iter = 0300, loss = 94.7982
[2023-10-26 03:59:47] iter = 0310, loss = 95.6030
[2023-10-26 04:00:34] iter = 0320, loss = 94.7006
[2023-10-26 04:01:22] iter = 0330, loss = 96.0487
[2023-10-26 04:02:10] iter = 0340, loss = 95.6160
[2023-10-26 04:02:57] iter = 0350, loss = 93.3366
[2023-10-26 04:03:45] iter = 0360, loss = 96.1565
[2023-10-26 04:04:33] iter = 0370, loss = 94.2059
[2023-10-26 04:05:20] iter = 0380, loss = 93.3021
[2023-10-26 04:06:08] iter = 0390, loss = 94.7586
[2023-10-26 04:06:55] iter = 0400, loss = 90.6151
[2023-10-26 04:07:43] iter = 0410, loss = 95.0849
[2023-10-26 04:08:30] iter = 0420, loss = 93.0566
[2023-10-26 04:09:18] iter = 0430, loss = 93.4263
[2023-10-26 04:10:06] iter = 0440, loss = 93.1327
[2023-10-26 04:10:53] iter = 0450, loss = 94.5227
[2023-10-26 04:11:41] iter = 0460, loss = 93.3088
[2023-10-26 04:12:29] iter = 0470, loss = 90.7526
[2023-10-26 04:13:16] iter = 0480, loss = 90.4069
[2023-10-26 04:14:04] iter = 0490, loss = 94.1920
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 04:14:50] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.005581 train acc = 1.0000, test acc = 0.4362
[2023-10-26 04:14:53] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.005956 train acc = 1.0000, test acc = 0.4291
[2023-10-26 04:14:57] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.005698 train acc = 1.0000, test acc = 0.4299
[2023-10-26 04:15:00] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.005765 train acc = 1.0000, test acc = 0.4200
[2023-10-26 04:15:03] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.005769 train acc = 1.0000, test acc = 0.4371
[2023-10-26 04:15:06] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.005582 train acc = 1.0000, test acc = 0.4390
[2023-10-26 04:15:10] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.005723 train acc = 1.0000, test acc = 0.4353
[2023-10-26 04:15:13] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.005916 train acc = 1.0000, test acc = 0.4364
[2023-10-26 04:15:16] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.005825 train acc = 1.0000, test acc = 0.4364
[2023-10-26 04:15:19] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.005643 train acc = 1.0000, test acc = 0.4253
[2023-10-26 04:15:22] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.005588 train acc = 1.0000, test acc = 0.4396
[2023-10-26 04:15:26] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.005732 train acc = 1.0000, test acc = 0.4257
[2023-10-26 04:15:29] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.005593 train acc = 1.0000, test acc = 0.4336
[2023-10-26 04:15:32] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.005647 train acc = 1.0000, test acc = 0.4337
[2023-10-26 04:15:35] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.005446 train acc = 1.0000, test acc = 0.4308
[2023-10-26 04:15:39] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.005689 train acc = 1.0000, test acc = 0.4231
[2023-10-26 04:15:42] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.005466 train acc = 1.0000, test acc = 0.4428
[2023-10-26 04:15:45] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.005547 train acc = 1.0000, test acc = 0.4365
[2023-10-26 04:15:48] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.005727 train acc = 1.0000, test acc = 0.4412
[2023-10-26 04:15:52] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.005644 train acc = 1.0000, test acc = 0.4316
Evaluate 20 random ConvNet, mean = 0.4332 std = 0.0060
-------------------------
[2023-10-26 04:15:56] iter = 0500, loss = 91.8920
[2023-10-26 04:16:44] iter = 0510, loss = 92.5607
[2023-10-26 04:17:31] iter = 0520, loss = 92.1929
[2023-10-26 04:18:19] iter = 0530, loss = 91.0626
[2023-10-26 04:19:07] iter = 0540, loss = 92.0631
[2023-10-26 04:19:54] iter = 0550, loss = 93.3037
[2023-10-26 04:20:42] iter = 0560, loss = 93.0039
[2023-10-26 04:21:30] iter = 0570, loss = 89.9357
[2023-10-26 04:22:17] iter = 0580, loss = 93.7121
[2023-10-26 04:23:05] iter = 0590, loss = 94.2157
[2023-10-26 04:23:53] iter = 0600, loss = 90.7486
[2023-10-26 04:24:41] iter = 0610, loss = 91.9949
[2023-10-26 04:25:28] iter = 0620, loss = 92.7606
[2023-10-26 04:26:16] iter = 0630, loss = 88.1427
[2023-10-26 04:27:03] iter = 0640, loss = 90.2980
[2023-10-26 04:27:51] iter = 0650, loss = 91.6646
[2023-10-26 04:28:39] iter = 0660, loss = 90.7641
[2023-10-26 04:29:26] iter = 0670, loss = 89.8752
[2023-10-26 04:30:14] iter = 0680, loss = 87.9977
[2023-10-26 04:31:02] iter = 0690, loss = 89.9028
[2023-10-26 04:31:49] iter = 0700, loss = 89.6947
[2023-10-26 04:32:36] iter = 0710, loss = 92.1105
[2023-10-26 04:33:23] iter = 0720, loss = 87.3095
[2023-10-26 04:34:11] iter = 0730, loss = 92.0499
[2023-10-26 04:34:59] iter = 0740, loss = 89.8208
[2023-10-26 04:35:46] iter = 0750, loss = 89.0760
[2023-10-26 04:36:33] iter = 0760, loss = 89.8550
[2023-10-26 04:37:20] iter = 0770, loss = 85.7230
[2023-10-26 04:38:08] iter = 0780, loss = 87.4372
[2023-10-26 04:38:56] iter = 0790, loss = 88.9155
[2023-10-26 04:39:43] iter = 0800, loss = 90.5409
[2023-10-26 04:40:31] iter = 0810, loss = 89.7177
[2023-10-26 04:41:18] iter = 0820, loss = 90.0045
[2023-10-26 04:42:06] iter = 0830, loss = 89.3372
[2023-10-26 04:42:54] iter = 0840, loss = 88.9586
[2023-10-26 04:43:41] iter = 0850, loss = 86.6762
[2023-10-26 04:44:28] iter = 0860, loss = 86.1558
[2023-10-26 04:45:16] iter = 0870, loss = 89.9307
[2023-10-26 04:46:04] iter = 0880, loss = 88.2176
[2023-10-26 04:46:51] iter = 0890, loss = 87.6210
[2023-10-26 04:47:39] iter = 0900, loss = 87.4966
[2023-10-26 04:48:27] iter = 0910, loss = 91.3320
[2023-10-26 04:49:14] iter = 0920, loss = 85.8790
[2023-10-26 04:50:02] iter = 0930, loss = 88.6399
[2023-10-26 04:50:50] iter = 0940, loss = 87.5552
[2023-10-26 04:51:37] iter = 0950, loss = 89.7605
[2023-10-26 04:52:25] iter = 0960, loss = 88.9359
[2023-10-26 04:53:13] iter = 0970, loss = 88.4066
[2023-10-26 04:54:00] iter = 0980, loss = 86.7126
[2023-10-26 04:54:48] iter = 0990, loss = 89.0166
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 04:55:34] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.006276 train acc = 1.0000, test acc = 0.4500
[2023-10-26 04:55:37] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.006233 train acc = 1.0000, test acc = 0.4460
[2023-10-26 04:55:41] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.006228 train acc = 1.0000, test acc = 0.4480
[2023-10-26 04:55:44] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.006149 train acc = 1.0000, test acc = 0.4433
[2023-10-26 04:55:47] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.006437 train acc = 1.0000, test acc = 0.4391
[2023-10-26 04:55:50] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.006257 train acc = 1.0000, test acc = 0.4408
[2023-10-26 04:55:53] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.006264 train acc = 1.0000, test acc = 0.4454
[2023-10-26 04:55:57] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.006032 train acc = 1.0000, test acc = 0.4444
[2023-10-26 04:56:00] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.006231 train acc = 1.0000, test acc = 0.4448
[2023-10-26 04:56:03] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.005974 train acc = 1.0000, test acc = 0.4512
[2023-10-26 04:56:06] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.006368 train acc = 1.0000, test acc = 0.4367
[2023-10-26 04:56:10] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.006097 train acc = 1.0000, test acc = 0.4468
[2023-10-26 04:56:13] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.006305 train acc = 1.0000, test acc = 0.4485
[2023-10-26 04:56:16] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.006647 train acc = 1.0000, test acc = 0.4429
[2023-10-26 04:56:19] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.006279 train acc = 1.0000, test acc = 0.4405
[2023-10-26 04:56:23] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.006303 train acc = 1.0000, test acc = 0.4362
[2023-10-26 04:56:26] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.006273 train acc = 1.0000, test acc = 0.4457
[2023-10-26 04:56:29] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.006240 train acc = 1.0000, test acc = 0.4438
[2023-10-26 04:56:32] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.006203 train acc = 1.0000, test acc = 0.4487
[2023-10-26 04:56:35] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.006501 train acc = 1.0000, test acc = 0.4519
Evaluate 20 random ConvNet, mean = 0.4447 std = 0.0043
-------------------------
[2023-10-26 04:56:40] iter = 1000, loss = 88.2494

================== Exp 1 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f57875b1d90>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-10-26 04:56:57] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 04:57:00] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003728 train acc = 1.0000, test acc = 0.0866
[2023-10-26 04:57:03] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003756 train acc = 1.0000, test acc = 0.0741
[2023-10-26 04:57:06] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003862 train acc = 1.0000, test acc = 0.0842
[2023-10-26 04:57:09] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003743 train acc = 1.0000, test acc = 0.0750
[2023-10-26 04:57:13] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003744 train acc = 1.0000, test acc = 0.0869
[2023-10-26 04:57:16] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.003706 train acc = 1.0000, test acc = 0.0782
[2023-10-26 04:57:19] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.003779 train acc = 1.0000, test acc = 0.0772
[2023-10-26 04:57:22] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.003797 train acc = 1.0000, test acc = 0.0842
[2023-10-26 04:57:26] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.003686 train acc = 1.0000, test acc = 0.0757
[2023-10-26 04:57:29] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.003669 train acc = 1.0000, test acc = 0.0774
[2023-10-26 04:57:32] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.003746 train acc = 1.0000, test acc = 0.0797
[2023-10-26 04:57:35] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.003704 train acc = 1.0000, test acc = 0.0754
[2023-10-26 04:57:38] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.003767 train acc = 1.0000, test acc = 0.0806
[2023-10-26 04:57:42] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.003806 train acc = 1.0000, test acc = 0.0720
[2023-10-26 04:57:45] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.003932 train acc = 1.0000, test acc = 0.0803
[2023-10-26 04:57:48] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.003787 train acc = 1.0000, test acc = 0.0772
[2023-10-26 04:57:51] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.003762 train acc = 1.0000, test acc = 0.0785
[2023-10-26 04:57:55] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.003778 train acc = 1.0000, test acc = 0.0731
[2023-10-26 04:57:58] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.003809 train acc = 1.0000, test acc = 0.0803
[2023-10-26 04:58:01] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.003852 train acc = 1.0000, test acc = 0.0807
Evaluate 20 random ConvNet, mean = 0.0789 std = 0.0041
-------------------------
[2023-10-26 04:58:06] iter = 0000, loss = 227.5758
[2023-10-26 04:58:53] iter = 0010, loss = 144.6382
[2023-10-26 04:59:41] iter = 0020, loss = 134.5937
[2023-10-26 05:00:29] iter = 0030, loss = 133.1564
[2023-10-26 05:01:16] iter = 0040, loss = 132.1960
[2023-10-26 05:02:04] iter = 0050, loss = 130.2061
[2023-10-26 05:02:52] iter = 0060, loss = 120.2658
[2023-10-26 05:03:39] iter = 0070, loss = 119.5410
[2023-10-26 05:04:27] iter = 0080, loss = 120.2658
[2023-10-26 05:05:15] iter = 0090, loss = 116.9464
[2023-10-26 05:06:02] iter = 0100, loss = 113.8327
[2023-10-26 05:06:49] iter = 0110, loss = 112.6311
[2023-10-26 05:07:37] iter = 0120, loss = 114.9471
[2023-10-26 05:08:25] iter = 0130, loss = 109.8223
[2023-10-26 05:09:12] iter = 0140, loss = 107.9153
[2023-10-26 05:10:00] iter = 0150, loss = 107.3338
[2023-10-26 05:10:48] iter = 0160, loss = 107.0503
[2023-10-26 05:11:35] iter = 0170, loss = 107.4758
[2023-10-26 05:12:23] iter = 0180, loss = 104.4647
[2023-10-26 05:13:11] iter = 0190, loss = 102.1863
[2023-10-26 05:13:58] iter = 0200, loss = 102.1974
[2023-10-26 05:14:46] iter = 0210, loss = 99.5797
[2023-10-26 05:15:34] iter = 0220, loss = 102.0813
[2023-10-26 05:16:22] iter = 0230, loss = 100.0321
[2023-10-26 05:17:09] iter = 0240, loss = 98.7200
[2023-10-26 05:17:57] iter = 0250, loss = 96.9565
[2023-10-26 05:18:45] iter = 0260, loss = 99.9384
[2023-10-26 05:19:32] iter = 0270, loss = 97.6674
[2023-10-26 05:20:20] iter = 0280, loss = 98.3148
[2023-10-26 05:21:08] iter = 0290, loss = 98.2782
[2023-10-26 05:21:55] iter = 0300, loss = 95.2360
[2023-10-26 05:22:42] iter = 0310, loss = 97.8768
[2023-10-26 05:23:30] iter = 0320, loss = 96.9923
[2023-10-26 05:24:18] iter = 0330, loss = 92.6933
[2023-10-26 05:25:05] iter = 0340, loss = 98.5609
[2023-10-26 05:25:53] iter = 0350, loss = 95.8197
[2023-10-26 05:26:41] iter = 0360, loss = 91.8654
[2023-10-26 05:27:29] iter = 0370, loss = 93.1942
[2023-10-26 05:28:16] iter = 0380, loss = 92.6211
[2023-10-26 05:29:04] iter = 0390, loss = 95.2938
[2023-10-26 05:29:51] iter = 0400, loss = 94.4905
[2023-10-26 05:30:39] iter = 0410, loss = 93.0148
[2023-10-26 05:31:27] iter = 0420, loss = 92.7778
[2023-10-26 05:32:14] iter = 0430, loss = 95.1954
[2023-10-26 05:33:02] iter = 0440, loss = 91.4693
[2023-10-26 05:33:49] iter = 0450, loss = 97.7931
[2023-10-26 05:34:37] iter = 0460, loss = 93.2047
[2023-10-26 05:35:24] iter = 0470, loss = 90.3536
[2023-10-26 05:36:12] iter = 0480, loss = 89.5944
[2023-10-26 05:36:59] iter = 0490, loss = 89.6120
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 05:37:46] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.005804 train acc = 1.0000, test acc = 0.4358
[2023-10-26 05:37:49] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.005632 train acc = 1.0000, test acc = 0.4439
[2023-10-26 05:37:52] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.005814 train acc = 1.0000, test acc = 0.4442
[2023-10-26 05:37:55] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.005680 train acc = 1.0000, test acc = 0.4332
[2023-10-26 05:37:59] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.005662 train acc = 1.0000, test acc = 0.4351
[2023-10-26 05:38:02] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.006002 train acc = 1.0000, test acc = 0.4329
[2023-10-26 05:38:05] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.005879 train acc = 1.0000, test acc = 0.4406
[2023-10-26 05:38:08] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.005912 train acc = 1.0000, test acc = 0.4374
[2023-10-26 05:38:11] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.005852 train acc = 1.0000, test acc = 0.4376
[2023-10-26 05:38:15] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.005842 train acc = 1.0000, test acc = 0.4297
[2023-10-26 05:38:18] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.005675 train acc = 1.0000, test acc = 0.4381
[2023-10-26 05:38:21] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.005823 train acc = 1.0000, test acc = 0.4371
[2023-10-26 05:38:24] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.005735 train acc = 1.0000, test acc = 0.4361
[2023-10-26 05:38:28] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.005830 train acc = 1.0000, test acc = 0.4418
[2023-10-26 05:38:31] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.005574 train acc = 1.0000, test acc = 0.4450
[2023-10-26 05:38:34] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.005799 train acc = 1.0000, test acc = 0.4354
[2023-10-26 05:38:37] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.006106 train acc = 1.0000, test acc = 0.4329
[2023-10-26 05:38:41] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.005836 train acc = 1.0000, test acc = 0.4238
[2023-10-26 05:38:44] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.005813 train acc = 1.0000, test acc = 0.4293
[2023-10-26 05:38:47] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.006071 train acc = 1.0000, test acc = 0.4400
Evaluate 20 random ConvNet, mean = 0.4365 std = 0.0052
-------------------------
[2023-10-26 05:38:52] iter = 0500, loss = 90.6642
[2023-10-26 05:39:40] iter = 0510, loss = 92.5865
[2023-10-26 05:40:27] iter = 0520, loss = 89.5457
[2023-10-26 05:41:15] iter = 0530, loss = 93.1726
[2023-10-26 05:42:03] iter = 0540, loss = 91.6223
[2023-10-26 05:42:50] iter = 0550, loss = 91.5017
[2023-10-26 05:43:37] iter = 0560, loss = 91.4369
[2023-10-26 05:44:25] iter = 0570, loss = 89.8908
[2023-10-26 05:45:13] iter = 0580, loss = 88.3819
[2023-10-26 05:46:00] iter = 0590, loss = 92.1232
[2023-10-26 05:46:48] iter = 0600, loss = 89.1825
[2023-10-26 05:47:36] iter = 0610, loss = 87.2251
[2023-10-26 05:48:24] iter = 0620, loss = 91.1099
[2023-10-26 05:49:11] iter = 0630, loss = 92.5556
[2023-10-26 05:49:58] iter = 0640, loss = 89.8947
[2023-10-26 05:50:46] iter = 0650, loss = 93.4135
[2023-10-26 05:51:34] iter = 0660, loss = 89.4143
[2023-10-26 05:52:21] iter = 0670, loss = 89.3844
[2023-10-26 05:53:09] iter = 0680, loss = 89.8442
[2023-10-26 05:53:57] iter = 0690, loss = 91.6108
[2023-10-26 05:54:44] iter = 0700, loss = 88.8240
[2023-10-26 05:55:32] iter = 0710, loss = 94.5027
[2023-10-26 05:56:20] iter = 0720, loss = 89.5532
[2023-10-26 05:57:07] iter = 0730, loss = 88.1955
[2023-10-26 05:57:55] iter = 0740, loss = 89.8150
[2023-10-26 05:58:43] iter = 0750, loss = 90.9520
[2023-10-26 05:59:31] iter = 0760, loss = 87.7826
[2023-10-26 06:00:18] iter = 0770, loss = 88.6969
[2023-10-26 06:01:06] iter = 0780, loss = 86.0632
[2023-10-26 06:01:53] iter = 0790, loss = 87.7862
[2023-10-26 06:02:41] iter = 0800, loss = 90.2832
[2023-10-26 06:03:29] iter = 0810, loss = 88.8912
[2023-10-26 06:04:16] iter = 0820, loss = 88.4074
[2023-10-26 06:05:04] iter = 0830, loss = 88.7476
[2023-10-26 06:05:52] iter = 0840, loss = 89.8231
[2023-10-26 06:06:39] iter = 0850, loss = 87.9351
[2023-10-26 06:07:27] iter = 0860, loss = 92.1288
[2023-10-26 06:08:15] iter = 0870, loss = 88.5984
[2023-10-26 06:09:02] iter = 0880, loss = 89.9096
[2023-10-26 06:09:50] iter = 0890, loss = 86.5012
[2023-10-26 06:10:37] iter = 0900, loss = 90.1045
[2023-10-26 06:11:25] iter = 0910, loss = 87.1050
[2023-10-26 06:12:13] iter = 0920, loss = 86.7559
[2023-10-26 06:13:01] iter = 0930, loss = 89.0270
[2023-10-26 06:13:48] iter = 0940, loss = 88.4185
[2023-10-26 06:14:36] iter = 0950, loss = 89.3907
[2023-10-26 06:15:24] iter = 0960, loss = 91.8947
[2023-10-26 06:16:11] iter = 0970, loss = 88.2831
[2023-10-26 06:16:59] iter = 0980, loss = 89.9562
[2023-10-26 06:17:46] iter = 0990, loss = 89.3143
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 06:18:33] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.006148 train acc = 1.0000, test acc = 0.4464
[2023-10-26 06:18:36] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.006273 train acc = 1.0000, test acc = 0.4484
[2023-10-26 06:18:39] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.006022 train acc = 1.0000, test acc = 0.4448
[2023-10-26 06:18:42] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.006278 train acc = 1.0000, test acc = 0.4448
[2023-10-26 06:18:45] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.006245 train acc = 1.0000, test acc = 0.4487
[2023-10-26 06:18:49] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.006305 train acc = 1.0000, test acc = 0.4442
[2023-10-26 06:18:52] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.006258 train acc = 1.0000, test acc = 0.4493
[2023-10-26 06:18:55] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.006215 train acc = 1.0000, test acc = 0.4456
[2023-10-26 06:18:58] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.006329 train acc = 1.0000, test acc = 0.4367
[2023-10-26 06:19:02] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.006260 train acc = 1.0000, test acc = 0.4510
[2023-10-26 06:19:05] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.006097 train acc = 1.0000, test acc = 0.4478
[2023-10-26 06:19:08] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.006326 train acc = 1.0000, test acc = 0.4393
[2023-10-26 06:19:11] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.006221 train acc = 1.0000, test acc = 0.4500
[2023-10-26 06:19:14] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.006224 train acc = 1.0000, test acc = 0.4456
[2023-10-26 06:19:18] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.006405 train acc = 1.0000, test acc = 0.4573
[2023-10-26 06:19:21] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.006302 train acc = 1.0000, test acc = 0.4514
[2023-10-26 06:19:24] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.006043 train acc = 1.0000, test acc = 0.4533
[2023-10-26 06:19:27] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.006324 train acc = 1.0000, test acc = 0.4540
[2023-10-26 06:19:31] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.006545 train acc = 1.0000, test acc = 0.4426
[2023-10-26 06:19:34] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.006373 train acc = 1.0000, test acc = 0.4459
Evaluate 20 random ConvNet, mean = 0.4474 std = 0.0048
-------------------------
[2023-10-26 06:19:39] iter = 1000, loss = 86.9589

================== Exp 2 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f57875b1d90>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-10-26 06:19:54] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 06:19:58] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003745 train acc = 1.0000, test acc = 0.1298
[2023-10-26 06:20:01] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003734 train acc = 1.0000, test acc = 0.1118
[2023-10-26 06:20:04] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003712 train acc = 1.0000, test acc = 0.1207
[2023-10-26 06:20:07] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003678 train acc = 1.0000, test acc = 0.1221
[2023-10-26 06:20:11] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003784 train acc = 1.0000, test acc = 0.1281
[2023-10-26 06:20:14] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.003732 train acc = 1.0000, test acc = 0.1090
[2023-10-26 06:20:17] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.003745 train acc = 1.0000, test acc = 0.1092
[2023-10-26 06:20:20] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.003685 train acc = 1.0000, test acc = 0.1075
[2023-10-26 06:20:24] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.003809 train acc = 1.0000, test acc = 0.1043
[2023-10-26 06:20:27] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.003763 train acc = 1.0000, test acc = 0.1121
[2023-10-26 06:20:30] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.003731 train acc = 1.0000, test acc = 0.1153
[2023-10-26 06:20:33] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.003739 train acc = 1.0000, test acc = 0.1154
[2023-10-26 06:20:36] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.003748 train acc = 1.0000, test acc = 0.1099
[2023-10-26 06:20:40] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.003773 train acc = 1.0000, test acc = 0.1106
[2023-10-26 06:20:43] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.003778 train acc = 1.0000, test acc = 0.1199
[2023-10-26 06:20:46] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.003746 train acc = 1.0000, test acc = 0.1181
[2023-10-26 06:20:49] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.003761 train acc = 1.0000, test acc = 0.1126
[2023-10-26 06:20:53] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.003664 train acc = 1.0000, test acc = 0.1132
[2023-10-26 06:20:56] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.003662 train acc = 1.0000, test acc = 0.1082
[2023-10-26 06:20:59] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.003708 train acc = 1.0000, test acc = 0.1078
Evaluate 20 random ConvNet, mean = 0.1143 std = 0.0067
-------------------------
[2023-10-26 06:21:04] iter = 0000, loss = 226.0934
[2023-10-26 06:21:52] iter = 0010, loss = 150.2500
[2023-10-26 06:22:39] iter = 0020, loss = 140.9996
[2023-10-26 06:23:27] iter = 0030, loss = 135.9178
[2023-10-26 06:24:15] iter = 0040, loss = 132.1360
[2023-10-26 06:25:02] iter = 0050, loss = 123.3826
[2023-10-26 06:25:50] iter = 0060, loss = 123.6841
[2023-10-26 06:26:38] iter = 0070, loss = 117.9491
[2023-10-26 06:27:25] iter = 0080, loss = 120.2547
[2023-10-26 06:28:13] iter = 0090, loss = 117.1977
[2023-10-26 06:29:01] iter = 0100, loss = 112.9356
[2023-10-26 06:29:49] iter = 0110, loss = 111.7120
[2023-10-26 06:30:36] iter = 0120, loss = 114.6270
[2023-10-26 06:31:23] iter = 0130, loss = 110.1100
[2023-10-26 06:32:11] iter = 0140, loss = 107.4411
[2023-10-26 06:32:59] iter = 0150, loss = 104.6468
[2023-10-26 06:33:47] iter = 0160, loss = 105.4648
[2023-10-26 06:34:34] iter = 0170, loss = 105.2153
[2023-10-26 06:35:22] iter = 0180, loss = 103.3272
[2023-10-26 06:36:10] iter = 0190, loss = 102.7567
[2023-10-26 06:36:57] iter = 0200, loss = 102.1782
[2023-10-26 06:37:45] iter = 0210, loss = 98.6443
[2023-10-26 06:38:33] iter = 0220, loss = 98.1480
[2023-10-26 06:39:21] iter = 0230, loss = 99.6914
[2023-10-26 06:40:08] iter = 0240, loss = 96.9631
[2023-10-26 06:40:56] iter = 0250, loss = 98.6639
[2023-10-26 06:41:44] iter = 0260, loss = 101.6824
[2023-10-26 06:42:31] iter = 0270, loss = 96.7182
[2023-10-26 06:43:19] iter = 0280, loss = 95.6014
[2023-10-26 06:44:06] iter = 0290, loss = 95.5706
[2023-10-26 06:44:54] iter = 0300, loss = 96.1484
[2023-10-26 06:45:42] iter = 0310, loss = 97.0634
[2023-10-26 06:46:30] iter = 0320, loss = 95.6669
[2023-10-26 06:47:17] iter = 0330, loss = 95.7074
[2023-10-26 06:48:05] iter = 0340, loss = 95.6297
[2023-10-26 06:48:53] iter = 0350, loss = 96.7896
[2023-10-26 06:49:40] iter = 0360, loss = 93.8300
[2023-10-26 06:50:28] iter = 0370, loss = 97.6701
[2023-10-26 06:51:15] iter = 0380, loss = 91.0108
[2023-10-26 06:52:03] iter = 0390, loss = 95.7308
[2023-10-26 06:52:50] iter = 0400, loss = 93.9990
[2023-10-26 06:53:38] iter = 0410, loss = 92.8009
[2023-10-26 06:54:25] iter = 0420, loss = 91.3375
[2023-10-26 06:55:13] iter = 0430, loss = 94.3839
[2023-10-26 06:56:01] iter = 0440, loss = 93.3088
[2023-10-26 06:56:48] iter = 0450, loss = 93.8024
[2023-10-26 06:57:36] iter = 0460, loss = 93.5940
[2023-10-26 06:58:24] iter = 0470, loss = 93.7181
[2023-10-26 06:59:11] iter = 0480, loss = 93.8335
[2023-10-26 06:59:59] iter = 0490, loss = 92.1802
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 07:00:45] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.005554 train acc = 1.0000, test acc = 0.4434
[2023-10-26 07:00:48] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.005675 train acc = 1.0000, test acc = 0.4354
[2023-10-26 07:00:52] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.005824 train acc = 1.0000, test acc = 0.4394
[2023-10-26 07:00:55] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.005789 train acc = 1.0000, test acc = 0.4401
[2023-10-26 07:00:58] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.005736 train acc = 1.0000, test acc = 0.4325
[2023-10-26 07:01:01] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.005870 train acc = 1.0000, test acc = 0.4273
[2023-10-26 07:01:04] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.005940 train acc = 1.0000, test acc = 0.4381
[2023-10-26 07:01:08] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.005659 train acc = 1.0000, test acc = 0.4408
[2023-10-26 07:01:11] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.005680 train acc = 1.0000, test acc = 0.4433
[2023-10-26 07:01:14] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.005422 train acc = 1.0000, test acc = 0.4451
[2023-10-26 07:01:17] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.005547 train acc = 1.0000, test acc = 0.4415
[2023-10-26 07:01:21] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.005760 train acc = 1.0000, test acc = 0.4390
[2023-10-26 07:01:24] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.005681 train acc = 1.0000, test acc = 0.4413
[2023-10-26 07:01:27] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.005783 train acc = 1.0000, test acc = 0.4381
[2023-10-26 07:01:30] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.005908 train acc = 1.0000, test acc = 0.4325
[2023-10-26 07:01:34] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.005857 train acc = 1.0000, test acc = 0.4366
[2023-10-26 07:01:37] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.005704 train acc = 1.0000, test acc = 0.4387
[2023-10-26 07:01:40] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.005473 train acc = 1.0000, test acc = 0.4393
[2023-10-26 07:01:43] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.005851 train acc = 1.0000, test acc = 0.4394
[2023-10-26 07:01:46] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.006018 train acc = 1.0000, test acc = 0.4274
Evaluate 20 random ConvNet, mean = 0.4380 std = 0.0047
-------------------------
[2023-10-26 07:01:51] iter = 0500, loss = 92.5906
[2023-10-26 07:02:39] iter = 0510, loss = 91.1712
[2023-10-26 07:03:27] iter = 0520, loss = 92.6276
[2023-10-26 07:04:14] iter = 0530, loss = 91.6278
[2023-10-26 07:05:02] iter = 0540, loss = 89.8340
[2023-10-26 07:05:49] iter = 0550, loss = 91.4815
[2023-10-26 07:06:37] iter = 0560, loss = 91.9511
[2023-10-26 07:07:24] iter = 0570, loss = 91.8605
[2023-10-26 07:08:11] iter = 0580, loss = 89.2154
[2023-10-26 07:08:59] iter = 0590, loss = 88.4908
[2023-10-26 07:09:46] iter = 0600, loss = 91.2143
[2023-10-26 07:10:34] iter = 0610, loss = 89.5803
[2023-10-26 07:11:22] iter = 0620, loss = 92.8916
[2023-10-26 07:12:09] iter = 0630, loss = 93.7823
[2023-10-26 07:12:57] iter = 0640, loss = 89.3915
[2023-10-26 07:13:44] iter = 0650, loss = 91.2141
[2023-10-26 07:14:32] iter = 0660, loss = 88.4467
[2023-10-26 07:15:20] iter = 0670, loss = 87.3190
[2023-10-26 07:16:07] iter = 0680, loss = 90.4730
[2023-10-26 07:16:55] iter = 0690, loss = 91.9843
[2023-10-26 07:17:43] iter = 0700, loss = 90.2358
[2023-10-26 07:18:29] iter = 0710, loss = 89.8871
[2023-10-26 07:19:17] iter = 0720, loss = 89.0666
[2023-10-26 07:20:04] iter = 0730, loss = 94.0253
[2023-10-26 07:20:52] iter = 0740, loss = 88.5832
[2023-10-26 07:21:40] iter = 0750, loss = 87.9122
[2023-10-26 07:22:27] iter = 0760, loss = 90.8064
[2023-10-26 07:23:16] iter = 0770, loss = 87.8301
[2023-10-26 07:24:04] iter = 0780, loss = 89.1302
[2023-10-26 07:24:51] iter = 0790, loss = 89.3264
[2023-10-26 07:25:39] iter = 0800, loss = 87.1930
[2023-10-26 07:26:27] iter = 0810, loss = 88.5987
[2023-10-26 07:27:15] iter = 0820, loss = 87.4399
[2023-10-26 07:28:02] iter = 0830, loss = 88.8883
[2023-10-26 07:28:50] iter = 0840, loss = 89.8067
[2023-10-26 07:29:37] iter = 0850, loss = 89.0319
[2023-10-26 07:30:25] iter = 0860, loss = 88.5552
[2023-10-26 07:31:13] iter = 0870, loss = 87.1589
[2023-10-26 07:32:00] iter = 0880, loss = 89.5660
[2023-10-26 07:32:48] iter = 0890, loss = 89.4113
[2023-10-26 07:33:36] iter = 0900, loss = 93.8390
[2023-10-26 07:34:23] iter = 0910, loss = 89.8481
[2023-10-26 07:35:11] iter = 0920, loss = 87.2607
[2023-10-26 07:35:59] iter = 0930, loss = 90.5494
[2023-10-26 07:36:46] iter = 0940, loss = 88.2977
[2023-10-26 07:37:34] iter = 0950, loss = 88.8461
[2023-10-26 07:38:22] iter = 0960, loss = 88.7747
[2023-10-26 07:39:09] iter = 0970, loss = 87.4855
[2023-10-26 07:39:57] iter = 0980, loss = 88.4958
[2023-10-26 07:40:45] iter = 0990, loss = 86.3030
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 07:41:31] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.006387 train acc = 1.0000, test acc = 0.4459
[2023-10-26 07:41:34] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.006475 train acc = 1.0000, test acc = 0.4473
[2023-10-26 07:41:37] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.006489 train acc = 1.0000, test acc = 0.4442
[2023-10-26 07:41:41] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.006310 train acc = 1.0000, test acc = 0.4485
[2023-10-26 07:41:44] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.006563 train acc = 1.0000, test acc = 0.4420
[2023-10-26 07:41:47] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.006377 train acc = 1.0000, test acc = 0.4396
[2023-10-26 07:41:50] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.006238 train acc = 1.0000, test acc = 0.4542
[2023-10-26 07:41:53] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.006265 train acc = 1.0000, test acc = 0.4475
[2023-10-26 07:41:57] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.006311 train acc = 1.0000, test acc = 0.4492
[2023-10-26 07:42:00] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.006259 train acc = 1.0000, test acc = 0.4397
[2023-10-26 07:42:03] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.006102 train acc = 1.0000, test acc = 0.4416
[2023-10-26 07:42:06] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.006217 train acc = 1.0000, test acc = 0.4563
[2023-10-26 07:42:09] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.006031 train acc = 1.0000, test acc = 0.4401
[2023-10-26 07:42:12] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.006434 train acc = 1.0000, test acc = 0.4461
[2023-10-26 07:42:16] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.006301 train acc = 1.0000, test acc = 0.4399
[2023-10-26 07:42:19] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.006133 train acc = 1.0000, test acc = 0.4457
[2023-10-26 07:42:22] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.006409 train acc = 1.0000, test acc = 0.4423
[2023-10-26 07:42:25] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.006376 train acc = 1.0000, test acc = 0.4376
[2023-10-26 07:42:29] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.006160 train acc = 1.0000, test acc = 0.4465
[2023-10-26 07:42:32] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.006374 train acc = 1.0000, test acc = 0.4445
Evaluate 20 random ConvNet, mean = 0.4449 std = 0.0047
-------------------------
[2023-10-26 07:42:36] iter = 1000, loss = 86.9596

================== Exp 3 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f57875b1d90>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-10-26 07:42:52] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 07:42:56] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003757 train acc = 1.0000, test acc = 0.1085
[2023-10-26 07:42:59] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003696 train acc = 1.0000, test acc = 0.1332
[2023-10-26 07:43:02] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003716 train acc = 1.0000, test acc = 0.1130
[2023-10-26 07:43:05] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003757 train acc = 1.0000, test acc = 0.1154
[2023-10-26 07:43:09] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003732 train acc = 1.0000, test acc = 0.1108
[2023-10-26 07:43:12] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.003771 train acc = 1.0000, test acc = 0.1081
[2023-10-26 07:43:15] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.003737 train acc = 1.0000, test acc = 0.1181
[2023-10-26 07:43:18] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.003888 train acc = 1.0000, test acc = 0.1084
[2023-10-26 07:43:21] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.003835 train acc = 1.0000, test acc = 0.1124
[2023-10-26 07:43:24] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.003794 train acc = 1.0000, test acc = 0.1242
[2023-10-26 07:43:27] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.003768 train acc = 1.0000, test acc = 0.1132
[2023-10-26 07:43:31] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.003734 train acc = 1.0000, test acc = 0.1170
[2023-10-26 07:43:34] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.003790 train acc = 1.0000, test acc = 0.1166
[2023-10-26 07:43:37] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.003778 train acc = 1.0000, test acc = 0.1146
[2023-10-26 07:43:40] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.003849 train acc = 1.0000, test acc = 0.1090
[2023-10-26 07:43:43] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.003742 train acc = 1.0000, test acc = 0.1140
[2023-10-26 07:43:47] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.003712 train acc = 1.0000, test acc = 0.1091
[2023-10-26 07:43:50] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.003736 train acc = 1.0000, test acc = 0.1118
[2023-10-26 07:43:53] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.003724 train acc = 1.0000, test acc = 0.1143
[2023-10-26 07:43:56] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.003802 train acc = 1.0000, test acc = 0.1221
Evaluate 20 random ConvNet, mean = 0.1147 std = 0.0060
-------------------------
[2023-10-26 07:44:01] iter = 0000, loss = 225.2221
[2023-10-26 07:44:49] iter = 0010, loss = 152.4669
[2023-10-26 07:45:36] iter = 0020, loss = 141.8075
[2023-10-26 07:46:23] iter = 0030, loss = 131.2970
[2023-10-26 07:47:10] iter = 0040, loss = 126.2277
[2023-10-26 07:47:58] iter = 0050, loss = 129.6102
[2023-10-26 07:48:46] iter = 0060, loss = 123.0859
[2023-10-26 07:49:33] iter = 0070, loss = 115.3605
[2023-10-26 07:50:21] iter = 0080, loss = 118.0906
[2023-10-26 07:51:09] iter = 0090, loss = 116.0738
[2023-10-26 07:51:57] iter = 0100, loss = 112.1348
[2023-10-26 07:52:44] iter = 0110, loss = 115.3981
[2023-10-26 07:53:32] iter = 0120, loss = 112.4843
[2023-10-26 07:54:20] iter = 0130, loss = 106.5557
[2023-10-26 07:55:07] iter = 0140, loss = 107.4539
[2023-10-26 07:55:55] iter = 0150, loss = 105.2762
[2023-10-26 07:56:43] iter = 0160, loss = 105.4973
[2023-10-26 07:57:30] iter = 0170, loss = 103.1537
[2023-10-26 07:58:18] iter = 0180, loss = 102.8998
[2023-10-26 07:59:06] iter = 0190, loss = 104.3796
[2023-10-26 07:59:53] iter = 0200, loss = 100.4879
[2023-10-26 08:00:41] iter = 0210, loss = 100.1160
[2023-10-26 08:01:29] iter = 0220, loss = 101.2120
[2023-10-26 08:02:16] iter = 0230, loss = 101.7143
[2023-10-26 08:03:04] iter = 0240, loss = 98.4743
[2023-10-26 08:03:52] iter = 0250, loss = 99.4914
[2023-10-26 08:04:37] iter = 0260, loss = 97.3130
[2023-10-26 08:05:24] iter = 0270, loss = 97.9091
[2023-10-26 08:06:12] iter = 0280, loss = 95.8750
[2023-10-26 08:07:00] iter = 0290, loss = 96.0069
[2023-10-26 08:07:47] iter = 0300, loss = 96.9928
[2023-10-26 08:08:35] iter = 0310, loss = 98.2377
[2023-10-26 08:09:23] iter = 0320, loss = 93.4725
[2023-10-26 08:10:09] iter = 0330, loss = 95.0063
[2023-10-26 08:10:56] iter = 0340, loss = 94.4829
[2023-10-26 08:11:44] iter = 0350, loss = 93.8941
[2023-10-26 08:12:31] iter = 0360, loss = 92.4060
[2023-10-26 08:13:19] iter = 0370, loss = 96.7337
[2023-10-26 08:14:06] iter = 0380, loss = 96.3533
[2023-10-26 08:14:54] iter = 0390, loss = 93.8760
[2023-10-26 08:15:42] iter = 0400, loss = 94.1492
[2023-10-26 08:16:29] iter = 0410, loss = 93.6727
[2023-10-26 08:17:17] iter = 0420, loss = 93.6469
[2023-10-26 08:18:05] iter = 0430, loss = 92.8923
[2023-10-26 08:18:53] iter = 0440, loss = 93.3018
[2023-10-26 08:19:40] iter = 0450, loss = 91.1974
[2023-10-26 08:20:28] iter = 0460, loss = 93.2823
[2023-10-26 08:21:15] iter = 0470, loss = 91.7690
[2023-10-26 08:22:03] iter = 0480, loss = 91.6470
[2023-10-26 08:22:50] iter = 0490, loss = 91.7958
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 08:23:36] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.005601 train acc = 1.0000, test acc = 0.4349
[2023-10-26 08:23:39] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.005597 train acc = 1.0000, test acc = 0.4338
[2023-10-26 08:23:42] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.005803 train acc = 1.0000, test acc = 0.4301
[2023-10-26 08:23:46] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.005897 train acc = 1.0000, test acc = 0.4417
[2023-10-26 08:23:49] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.005580 train acc = 1.0000, test acc = 0.4456
[2023-10-26 08:23:52] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.005721 train acc = 1.0000, test acc = 0.4358
[2023-10-26 08:23:55] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.005637 train acc = 1.0000, test acc = 0.4371
[2023-10-26 08:23:58] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.005788 train acc = 1.0000, test acc = 0.4318
[2023-10-26 08:24:02] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.005871 train acc = 1.0000, test acc = 0.4369
[2023-10-26 08:24:05] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.006008 train acc = 1.0000, test acc = 0.4345
[2023-10-26 08:24:08] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.005554 train acc = 1.0000, test acc = 0.4452
[2023-10-26 08:24:11] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.005916 train acc = 1.0000, test acc = 0.4262
[2023-10-26 08:24:15] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.006013 train acc = 1.0000, test acc = 0.4359
[2023-10-26 08:24:18] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.006033 train acc = 1.0000, test acc = 0.4350
[2023-10-26 08:24:21] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.005678 train acc = 1.0000, test acc = 0.4262
[2023-10-26 08:24:24] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.005889 train acc = 1.0000, test acc = 0.4268
[2023-10-26 08:24:28] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.006043 train acc = 1.0000, test acc = 0.4313
[2023-10-26 08:24:31] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.005795 train acc = 1.0000, test acc = 0.4295
[2023-10-26 08:24:34] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.005591 train acc = 1.0000, test acc = 0.4296
[2023-10-26 08:24:37] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.005873 train acc = 1.0000, test acc = 0.4263
Evaluate 20 random ConvNet, mean = 0.4337 std = 0.0057
-------------------------
[2023-10-26 08:24:42] iter = 0500, loss = 92.7452
[2023-10-26 08:25:29] iter = 0510, loss = 91.6746
[2023-10-26 08:26:17] iter = 0520, loss = 94.7055
[2023-10-26 08:27:05] iter = 0530, loss = 89.8283
[2023-10-26 08:27:53] iter = 0540, loss = 91.9932
[2023-10-26 08:28:39] iter = 0550, loss = 90.3024
[2023-10-26 08:29:27] iter = 0560, loss = 93.6226
[2023-10-26 08:30:15] iter = 0570, loss = 93.4715
[2023-10-26 08:31:02] iter = 0580, loss = 92.2252
[2023-10-26 08:31:50] iter = 0590, loss = 90.4027
[2023-10-26 08:32:38] iter = 0600, loss = 91.2521
[2023-10-26 08:33:25] iter = 0610, loss = 88.8779
[2023-10-26 08:34:13] iter = 0620, loss = 90.8085
[2023-10-26 08:35:01] iter = 0630, loss = 92.1620
[2023-10-26 08:35:48] iter = 0640, loss = 92.5108
[2023-10-26 08:36:36] iter = 0650, loss = 91.2450
[2023-10-26 08:37:24] iter = 0660, loss = 91.5489
[2023-10-26 08:38:12] iter = 0670, loss = 91.1821
[2023-10-26 08:38:59] iter = 0680, loss = 91.2128
[2023-10-26 08:39:47] iter = 0690, loss = 90.1586
[2023-10-26 08:40:35] iter = 0700, loss = 87.9810
[2023-10-26 08:41:22] iter = 0710, loss = 89.5445
[2023-10-26 08:42:10] iter = 0720, loss = 88.4517
[2023-10-26 08:42:58] iter = 0730, loss = 90.0895
[2023-10-26 08:43:46] iter = 0740, loss = 91.1789
[2023-10-26 08:44:33] iter = 0750, loss = 90.9996
[2023-10-26 08:45:21] iter = 0760, loss = 92.2018
[2023-10-26 08:46:09] iter = 0770, loss = 89.5090
[2023-10-26 08:46:54] iter = 0780, loss = 90.1649
[2023-10-26 08:47:42] iter = 0790, loss = 87.5000
[2023-10-26 08:48:30] iter = 0800, loss = 88.9928
[2023-10-26 08:49:18] iter = 0810, loss = 92.6746
[2023-10-26 08:50:05] iter = 0820, loss = 89.0475
[2023-10-26 08:50:53] iter = 0830, loss = 88.8422
[2023-10-26 08:51:41] iter = 0840, loss = 91.0001
[2023-10-26 08:52:29] iter = 0850, loss = 86.5672
[2023-10-26 08:53:16] iter = 0860, loss = 90.1973
[2023-10-26 08:54:04] iter = 0870, loss = 90.4821
[2023-10-26 08:54:52] iter = 0880, loss = 87.8079
[2023-10-26 08:55:40] iter = 0890, loss = 88.2985
[2023-10-26 08:56:27] iter = 0900, loss = 87.9191
[2023-10-26 08:57:15] iter = 0910, loss = 89.2681
[2023-10-26 08:58:03] iter = 0920, loss = 90.6581
[2023-10-26 08:58:51] iter = 0930, loss = 88.9653
[2023-10-26 08:59:38] iter = 0940, loss = 88.4906
[2023-10-26 09:00:26] iter = 0950, loss = 88.9580
[2023-10-26 09:01:14] iter = 0960, loss = 87.5546
[2023-10-26 09:02:01] iter = 0970, loss = 88.7659
[2023-10-26 09:02:49] iter = 0980, loss = 86.6452
[2023-10-26 09:03:36] iter = 0990, loss = 88.0686
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 09:04:23] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.006325 train acc = 1.0000, test acc = 0.4503
[2023-10-26 09:04:26] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.006318 train acc = 1.0000, test acc = 0.4485
[2023-10-26 09:04:29] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.006195 train acc = 1.0000, test acc = 0.4477
[2023-10-26 09:04:32] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.006120 train acc = 1.0000, test acc = 0.4481
[2023-10-26 09:04:36] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.006289 train acc = 1.0000, test acc = 0.4396
[2023-10-26 09:04:39] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.006456 train acc = 1.0000, test acc = 0.4409
[2023-10-26 09:04:42] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.006533 train acc = 1.0000, test acc = 0.4461
[2023-10-26 09:04:45] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.006130 train acc = 1.0000, test acc = 0.4516
[2023-10-26 09:04:48] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.006434 train acc = 1.0000, test acc = 0.4352
[2023-10-26 09:04:52] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.006002 train acc = 1.0000, test acc = 0.4528
[2023-10-26 09:04:55] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.006269 train acc = 1.0000, test acc = 0.4442
[2023-10-26 09:04:58] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.006224 train acc = 1.0000, test acc = 0.4443
[2023-10-26 09:05:01] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.006441 train acc = 1.0000, test acc = 0.4391
[2023-10-26 09:05:05] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.006362 train acc = 1.0000, test acc = 0.4461
[2023-10-26 09:05:08] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.006022 train acc = 1.0000, test acc = 0.4426
[2023-10-26 09:05:11] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.006273 train acc = 1.0000, test acc = 0.4478
[2023-10-26 09:05:14] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.005978 train acc = 1.0000, test acc = 0.4497
[2023-10-26 09:05:18] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.005876 train acc = 1.0000, test acc = 0.4503
[2023-10-26 09:05:21] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.006298 train acc = 1.0000, test acc = 0.4530
[2023-10-26 09:05:24] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.005775 train acc = 1.0000, test acc = 0.4544
Evaluate 20 random ConvNet, mean = 0.4466 std = 0.0050
-------------------------
[2023-10-26 09:05:29] iter = 1000, loss = 87.5011

================== Exp 4 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f57875b1d90>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5000 real images
class c = 1: 5000 real images
class c = 2: 5000 real images
class c = 3: 5000 real images
class c = 4: 5000 real images
class c = 5: 5000 real images
class c = 6: 5000 real images
class c = 7: 5000 real images
class c = 8: 5000 real images
class c = 9: 5000 real images
real images channel 0, mean = -0.0000, std = 1.2211
real images channel 1, mean = -0.0002, std = 1.2211
real images channel 2, mean = 0.0002, std = 1.3014
initialize synthetic data from random noise
[2023-10-26 09:05:44] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 09:05:48] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003774 train acc = 1.0000, test acc = 0.0769
[2023-10-26 09:05:51] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003812 train acc = 1.0000, test acc = 0.0960
[2023-10-26 09:05:54] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003737 train acc = 1.0000, test acc = 0.0798
[2023-10-26 09:05:57] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003681 train acc = 1.0000, test acc = 0.0944
[2023-10-26 09:06:01] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003789 train acc = 1.0000, test acc = 0.0787
[2023-10-26 09:06:04] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.003769 train acc = 1.0000, test acc = 0.0865
[2023-10-26 09:06:07] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.003789 train acc = 1.0000, test acc = 0.0831
[2023-10-26 09:06:10] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.003796 train acc = 1.0000, test acc = 0.0833
[2023-10-26 09:06:14] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.003708 train acc = 1.0000, test acc = 0.0852
[2023-10-26 09:06:17] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.003663 train acc = 1.0000, test acc = 0.0902
[2023-10-26 09:06:20] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.003752 train acc = 1.0000, test acc = 0.0818
[2023-10-26 09:06:23] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.003701 train acc = 1.0000, test acc = 0.0778
[2023-10-26 09:06:26] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.003619 train acc = 1.0000, test acc = 0.0847
[2023-10-26 09:06:30] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.003780 train acc = 1.0000, test acc = 0.0859
[2023-10-26 09:06:33] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.003697 train acc = 1.0000, test acc = 0.0807
[2023-10-26 09:06:36] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.003634 train acc = 1.0000, test acc = 0.0860
[2023-10-26 09:06:39] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.003782 train acc = 1.0000, test acc = 0.0821
[2023-10-26 09:06:43] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.003768 train acc = 1.0000, test acc = 0.0897
[2023-10-26 09:06:46] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.003720 train acc = 1.0000, test acc = 0.0823
[2023-10-26 09:06:49] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.003719 train acc = 1.0000, test acc = 0.0847
Evaluate 20 random ConvNet, mean = 0.0845 std = 0.0050
-------------------------
[2023-10-26 09:06:54] iter = 0000, loss = 230.8220
[2023-10-26 09:07:42] iter = 0010, loss = 146.2793
[2023-10-26 09:08:28] iter = 0020, loss = 137.2426
[2023-10-26 09:09:16] iter = 0030, loss = 131.5734
[2023-10-26 09:10:04] iter = 0040, loss = 128.1932
[2023-10-26 09:10:52] iter = 0050, loss = 129.4216
[2023-10-26 09:11:39] iter = 0060, loss = 122.4002
[2023-10-26 09:12:27] iter = 0070, loss = 122.0655
[2023-10-26 09:13:15] iter = 0080, loss = 114.1157
[2023-10-26 09:14:03] iter = 0090, loss = 118.6562
[2023-10-26 09:14:50] iter = 0100, loss = 110.2136
[2023-10-26 09:15:37] iter = 0110, loss = 117.5728
[2023-10-26 09:16:25] iter = 0120, loss = 110.4979
[2023-10-26 09:17:12] iter = 0130, loss = 108.5830
[2023-10-26 09:18:00] iter = 0140, loss = 107.8837
[2023-10-26 09:18:48] iter = 0150, loss = 107.1596
[2023-10-26 09:19:36] iter = 0160, loss = 105.5918
[2023-10-26 09:20:23] iter = 0170, loss = 104.8562
[2023-10-26 09:21:11] iter = 0180, loss = 102.8902
[2023-10-26 09:21:59] iter = 0190, loss = 105.0642
[2023-10-26 09:22:46] iter = 0200, loss = 102.2020
[2023-10-26 09:23:34] iter = 0210, loss = 104.7479
[2023-10-26 09:24:22] iter = 0220, loss = 101.5679
[2023-10-26 09:25:09] iter = 0230, loss = 100.7220
[2023-10-26 09:25:57] iter = 0240, loss = 99.3463
[2023-10-26 09:26:45] iter = 0250, loss = 100.9302
[2023-10-26 09:27:33] iter = 0260, loss = 97.2720
[2023-10-26 09:28:20] iter = 0270, loss = 97.3156
[2023-10-26 09:29:07] iter = 0280, loss = 94.9268
[2023-10-26 09:29:55] iter = 0290, loss = 95.1217
[2023-10-26 09:30:43] iter = 0300, loss = 98.1694
[2023-10-26 09:31:30] iter = 0310, loss = 96.0395
[2023-10-26 09:32:18] iter = 0320, loss = 97.7519
[2023-10-26 09:33:06] iter = 0330, loss = 94.3282
[2023-10-26 09:33:53] iter = 0340, loss = 97.0935
[2023-10-26 09:34:41] iter = 0350, loss = 97.5982
[2023-10-26 09:35:29] iter = 0360, loss = 92.5877
[2023-10-26 09:36:17] iter = 0370, loss = 92.9654
[2023-10-26 09:37:04] iter = 0380, loss = 95.3751
[2023-10-26 09:37:52] iter = 0390, loss = 93.7399
[2023-10-26 09:38:40] iter = 0400, loss = 95.5088
[2023-10-26 09:39:27] iter = 0410, loss = 91.6768
[2023-10-26 09:40:15] iter = 0420, loss = 94.2709
[2023-10-26 09:41:03] iter = 0430, loss = 91.0603
[2023-10-26 09:41:50] iter = 0440, loss = 91.7171
[2023-10-26 09:42:38] iter = 0450, loss = 95.9905
[2023-10-26 09:43:26] iter = 0460, loss = 91.2259
[2023-10-26 09:44:13] iter = 0470, loss = 91.5044
[2023-10-26 09:45:01] iter = 0480, loss = 89.8717
[2023-10-26 09:45:49] iter = 0490, loss = 93.9771
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 09:46:35] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.005671 train acc = 1.0000, test acc = 0.4376
[2023-10-26 09:46:38] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.006065 train acc = 1.0000, test acc = 0.4277
[2023-10-26 09:46:41] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.005781 train acc = 1.0000, test acc = 0.4325
[2023-10-26 09:46:45] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.005421 train acc = 1.0000, test acc = 0.4484
[2023-10-26 09:46:48] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.005660 train acc = 1.0000, test acc = 0.4435
[2023-10-26 09:46:51] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.005945 train acc = 1.0000, test acc = 0.4382
[2023-10-26 09:46:54] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.005645 train acc = 1.0000, test acc = 0.4344
[2023-10-26 09:46:58] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.005634 train acc = 1.0000, test acc = 0.4489
[2023-10-26 09:47:01] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.005771 train acc = 1.0000, test acc = 0.4443
[2023-10-26 09:47:04] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.005897 train acc = 1.0000, test acc = 0.4296
[2023-10-26 09:47:07] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.005731 train acc = 1.0000, test acc = 0.4510
[2023-10-26 09:47:11] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.005859 train acc = 1.0000, test acc = 0.4423
[2023-10-26 09:47:14] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.005953 train acc = 1.0000, test acc = 0.4359
[2023-10-26 09:47:17] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.005904 train acc = 1.0000, test acc = 0.4340
[2023-10-26 09:47:20] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.005660 train acc = 1.0000, test acc = 0.4321
[2023-10-26 09:47:24] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.005913 train acc = 1.0000, test acc = 0.4338
[2023-10-26 09:47:27] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.005944 train acc = 1.0000, test acc = 0.4280
[2023-10-26 09:47:30] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.005861 train acc = 1.0000, test acc = 0.4351
[2023-10-26 09:47:33] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.005856 train acc = 1.0000, test acc = 0.4331
[2023-10-26 09:47:36] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.005728 train acc = 1.0000, test acc = 0.4336
Evaluate 20 random ConvNet, mean = 0.4372 std = 0.0068
-------------------------
[2023-10-26 09:47:41] iter = 0500, loss = 94.0754
[2023-10-26 09:48:29] iter = 0510, loss = 94.0999
[2023-10-26 09:49:17] iter = 0520, loss = 92.4555
[2023-10-26 09:50:04] iter = 0530, loss = 88.2279
[2023-10-26 09:50:52] iter = 0540, loss = 92.7800
[2023-10-26 09:51:40] iter = 0550, loss = 89.4575
[2023-10-26 09:52:27] iter = 0560, loss = 90.5870
[2023-10-26 09:53:15] iter = 0570, loss = 90.2018
[2023-10-26 09:54:03] iter = 0580, loss = 90.9824
[2023-10-26 09:54:50] iter = 0590, loss = 89.4174
[2023-10-26 09:55:38] iter = 0600, loss = 90.5806
[2023-10-26 09:56:25] iter = 0610, loss = 91.0007
[2023-10-26 09:57:13] iter = 0620, loss = 87.1719
[2023-10-26 09:58:01] iter = 0630, loss = 89.5081
[2023-10-26 09:58:48] iter = 0640, loss = 89.2454
[2023-10-26 09:59:36] iter = 0650, loss = 90.8824
[2023-10-26 10:00:24] iter = 0660, loss = 88.3159
[2023-10-26 10:01:12] iter = 0670, loss = 90.5467
[2023-10-26 10:01:59] iter = 0680, loss = 86.8828
[2023-10-26 10:02:47] iter = 0690, loss = 89.4911
[2023-10-26 10:03:35] iter = 0700, loss = 90.8580
[2023-10-26 10:04:22] iter = 0710, loss = 87.3291
[2023-10-26 10:05:10] iter = 0720, loss = 87.2498
[2023-10-26 10:05:58] iter = 0730, loss = 89.4976
[2023-10-26 10:06:45] iter = 0740, loss = 88.9680
[2023-10-26 10:07:33] iter = 0750, loss = 93.3235
[2023-10-26 10:08:21] iter = 0760, loss = 89.2179
[2023-10-26 10:09:09] iter = 0770, loss = 88.3965
[2023-10-26 10:09:56] iter = 0780, loss = 88.0695
[2023-10-26 10:10:43] iter = 0790, loss = 88.4513
[2023-10-26 10:11:31] iter = 0800, loss = 92.8309
[2023-10-26 10:12:19] iter = 0810, loss = 87.2102
[2023-10-26 10:13:06] iter = 0820, loss = 87.3493
[2023-10-26 10:13:54] iter = 0830, loss = 90.6096
[2023-10-26 10:14:42] iter = 0840, loss = 87.7157
[2023-10-26 10:15:30] iter = 0850, loss = 89.8023
[2023-10-26 10:16:17] iter = 0860, loss = 90.0722
[2023-10-26 10:17:05] iter = 0870, loss = 89.2313
[2023-10-26 10:17:53] iter = 0880, loss = 89.5071
[2023-10-26 10:18:41] iter = 0890, loss = 86.3814
[2023-10-26 10:19:28] iter = 0900, loss = 89.9791
[2023-10-26 10:20:16] iter = 0910, loss = 91.8688
[2023-10-26 10:21:04] iter = 0920, loss = 88.0653
[2023-10-26 10:21:51] iter = 0930, loss = 88.6067
[2023-10-26 10:22:39] iter = 0940, loss = 89.4934
[2023-10-26 10:23:27] iter = 0950, loss = 89.1173
[2023-10-26 10:24:14] iter = 0960, loss = 88.7575
[2023-10-26 10:25:01] iter = 0970, loss = 88.7715
[2023-10-26 10:25:48] iter = 0980, loss = 86.3403
[2023-10-26 10:26:36] iter = 0990, loss = 86.6575
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
[2023-10-26 10:27:22] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.006129 train acc = 1.0000, test acc = 0.4468
[2023-10-26 10:27:25] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.006343 train acc = 1.0000, test acc = 0.4459
[2023-10-26 10:27:28] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.006109 train acc = 1.0000, test acc = 0.4513
[2023-10-26 10:27:31] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.006445 train acc = 1.0000, test acc = 0.4372
[2023-10-26 10:27:34] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.006413 train acc = 1.0000, test acc = 0.4488
[2023-10-26 10:27:37] Evaluate_05: epoch = 0300 train time = 1 s train loss = 0.006337 train acc = 1.0000, test acc = 0.4333
[2023-10-26 10:27:40] Evaluate_06: epoch = 0300 train time = 1 s train loss = 0.006406 train acc = 1.0000, test acc = 0.4427
[2023-10-26 10:27:44] Evaluate_07: epoch = 0300 train time = 1 s train loss = 0.006148 train acc = 1.0000, test acc = 0.4383
[2023-10-26 10:27:47] Evaluate_08: epoch = 0300 train time = 1 s train loss = 0.006290 train acc = 1.0000, test acc = 0.4398
[2023-10-26 10:27:50] Evaluate_09: epoch = 0300 train time = 1 s train loss = 0.006382 train acc = 1.0000, test acc = 0.4361
[2023-10-26 10:27:53] Evaluate_10: epoch = 0300 train time = 1 s train loss = 0.006553 train acc = 1.0000, test acc = 0.4485
[2023-10-26 10:27:56] Evaluate_11: epoch = 0300 train time = 1 s train loss = 0.006251 train acc = 1.0000, test acc = 0.4519
[2023-10-26 10:27:59] Evaluate_12: epoch = 0300 train time = 1 s train loss = 0.006036 train acc = 1.0000, test acc = 0.4522
[2023-10-26 10:28:02] Evaluate_13: epoch = 0300 train time = 1 s train loss = 0.006393 train acc = 1.0000, test acc = 0.4385
[2023-10-26 10:28:05] Evaluate_14: epoch = 0300 train time = 1 s train loss = 0.006430 train acc = 1.0000, test acc = 0.4356
[2023-10-26 10:28:09] Evaluate_15: epoch = 0300 train time = 1 s train loss = 0.006341 train acc = 1.0000, test acc = 0.4370
[2023-10-26 10:28:12] Evaluate_16: epoch = 0300 train time = 1 s train loss = 0.006261 train acc = 1.0000, test acc = 0.4310
[2023-10-26 10:28:15] Evaluate_17: epoch = 0300 train time = 1 s train loss = 0.006449 train acc = 1.0000, test acc = 0.4457
[2023-10-26 10:28:18] Evaluate_18: epoch = 0300 train time = 1 s train loss = 0.006210 train acc = 1.0000, test acc = 0.4348
[2023-10-26 10:28:21] Evaluate_19: epoch = 0300 train time = 1 s train loss = 0.006753 train acc = 1.0000, test acc = 0.4380
Evaluate 20 random ConvNet, mean = 0.4417 std = 0.0065
-------------------------
[2023-10-26 10:28:26] iter = 1000, loss = 88.5778

==================== Final Results ====================

Run 5 experiments, train on ConvNet, evaluate 100 random ConvNet, mean  = 44.51%  std = 0.55%
