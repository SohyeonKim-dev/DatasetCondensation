/data/happythgus/LDA/DC
/data/opt/anaconda3/bin/python
moana-r3
/data/happythgus/LDA/DC/main.py:90: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
/data/happythgus/LDA/DC/main.py:90: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
eval_it_pool:  [0, 500, 1000]

================== Exp 0 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2aefbc9cd0>, 'dsa': False}
Evaluation model pool:  ['ConvNet']
class c = 0: 5923 real images
class c = 1: 6742 real images
class c = 2: 5958 real images
class c = 3: 6131 real images
class c = 4: 5842 real images
class c = 5: 5421 real images
class c = 6: 5918 real images
class c = 7: 6265 real images
class c = 8: 5851 real images
class c = 9: 5949 real images
real images channel 0, mean = -0.0001, std = 1.0000
initialize synthetic data from random noise
[2023-10-16 01:59:16] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 01:59:23] Evaluate_00: epoch = 1000 train time = 5 s train loss = 0.000582 train acc = 1.0000, test acc = 0.1257
batchnorm
[2023-10-16 01:59:29] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001376 train acc = 1.0000, test acc = 0.1463
batchnorm
[2023-10-16 01:59:35] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.002649 train acc = 1.0000, test acc = 0.1349
batchnorm
[2023-10-16 01:59:41] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.005384 train acc = 1.0000, test acc = 0.1600
batchnorm
[2023-10-16 01:59:47] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000361 train acc = 1.0000, test acc = 0.1489
batchnorm
[2023-10-16 01:59:53] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.000607 train acc = 1.0000, test acc = 0.1178
batchnorm
[2023-10-16 01:59:59] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.002654 train acc = 1.0000, test acc = 0.1559
batchnorm
[2023-10-16 02:00:05] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000292 train acc = 1.0000, test acc = 0.1313
batchnorm
[2023-10-16 02:00:11] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.002284 train acc = 1.0000, test acc = 0.1321
batchnorm
[2023-10-16 02:00:17] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000552 train acc = 1.0000, test acc = 0.1368
batchnorm
[2023-10-16 02:00:23] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.000284 train acc = 1.0000, test acc = 0.1278
batchnorm
[2023-10-16 02:00:29] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001251 train acc = 1.0000, test acc = 0.1464
batchnorm
[2023-10-16 02:00:35] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001396 train acc = 1.0000, test acc = 0.1446
batchnorm
[2023-10-16 02:00:41] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000733 train acc = 1.0000, test acc = 0.1396
batchnorm
[2023-10-16 02:00:47] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.000696 train acc = 1.0000, test acc = 0.1019
batchnorm
[2023-10-16 02:00:53] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000954 train acc = 1.0000, test acc = 0.1509
batchnorm
[2023-10-16 02:00:59] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.005002 train acc = 1.0000, test acc = 0.1522
batchnorm
[2023-10-16 02:01:05] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001356 train acc = 1.0000, test acc = 0.1603
batchnorm
[2023-10-16 02:01:11] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.004390 train acc = 1.0000, test acc = 0.1215
batchnorm
[2023-10-16 02:01:17] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000326 train acc = 1.0000, test acc = 0.1278
Evaluate 20 random ConvNet, mean = 0.1381 std = 0.0148
-------------------------
batchnorm
[2023-10-16 02:01:17] iter = 0000, loss = 258.4313
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:19] iter = 0010, loss = 153.9776
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:20] iter = 0020, loss = 114.3057
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:22] iter = 0030, loss = 108.5705
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:23] iter = 0040, loss = 86.8103
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:25] iter = 0050, loss = 85.5706
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:27] iter = 0060, loss = 78.6921
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:28] iter = 0070, loss = 77.9605
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:30] iter = 0080, loss = 74.0481
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:32] iter = 0090, loss = 76.0281
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:33] iter = 0100, loss = 65.9601
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:35] iter = 0110, loss = 80.5972
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:37] iter = 0120, loss = 66.9101
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:38] iter = 0130, loss = 60.7310
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:40] iter = 0140, loss = 69.3387
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:41] iter = 0150, loss = 65.9934
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:43] iter = 0160, loss = 65.2617
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:45] iter = 0170, loss = 67.9426
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:46] iter = 0180, loss = 59.5402
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:48] iter = 0190, loss = 59.9637
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:50] iter = 0200, loss = 60.9246
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:51] iter = 0210, loss = 60.0633
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:53] iter = 0220, loss = 66.6846
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:54] iter = 0230, loss = 62.6565
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:56] iter = 0240, loss = 56.8320
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:58] iter = 0250, loss = 56.6513
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:01:59] iter = 0260, loss = 64.1832
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:01] iter = 0270, loss = 56.2698
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:03] iter = 0280, loss = 58.5711
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:04] iter = 0290, loss = 57.0860
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:06] iter = 0300, loss = 54.5972
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:07] iter = 0310, loss = 53.2856
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:09] iter = 0320, loss = 63.4594
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:11] iter = 0330, loss = 63.0400
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:12] iter = 0340, loss = 55.6440
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:14] iter = 0350, loss = 63.7989
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:16] iter = 0360, loss = 60.6368
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:17] iter = 0370, loss = 51.9039
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:19] iter = 0380, loss = 56.5833
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:21] iter = 0390, loss = 60.7075
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:22] iter = 0400, loss = 60.1225
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:24] iter = 0410, loss = 61.8651
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:25] iter = 0420, loss = 53.1505
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:27] iter = 0430, loss = 60.3841
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:29] iter = 0440, loss = 60.7427
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:30] iter = 0450, loss = 59.5098
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:32] iter = 0460, loss = 57.5563
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:34] iter = 0470, loss = 56.8744
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:35] iter = 0480, loss = 58.1698
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:02:37] iter = 0490, loss = 56.1789
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:02:44] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000479 train acc = 1.0000, test acc = 0.7998
batchnorm
[2023-10-16 02:02:50] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.005640 train acc = 1.0000, test acc = 0.7867
batchnorm
[2023-10-16 02:02:56] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000104 train acc = 1.0000, test acc = 0.7443
batchnorm
[2023-10-16 02:03:02] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000249 train acc = 1.0000, test acc = 0.8189
batchnorm
[2023-10-16 02:03:08] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000084 train acc = 1.0000, test acc = 0.7733
batchnorm
[2023-10-16 02:03:14] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.002348 train acc = 1.0000, test acc = 0.8329
batchnorm
[2023-10-16 02:03:20] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.000835 train acc = 1.0000, test acc = 0.8249
batchnorm
[2023-10-16 02:03:26] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000383 train acc = 1.0000, test acc = 0.7707
batchnorm
[2023-10-16 02:03:32] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001576 train acc = 1.0000, test acc = 0.7918
batchnorm
[2023-10-16 02:03:38] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000169 train acc = 1.0000, test acc = 0.7990
batchnorm
[2023-10-16 02:03:44] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001170 train acc = 1.0000, test acc = 0.7918
batchnorm
[2023-10-16 02:03:50] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.000397 train acc = 1.0000, test acc = 0.8057
batchnorm
[2023-10-16 02:03:56] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000493 train acc = 1.0000, test acc = 0.7622
batchnorm
[2023-10-16 02:04:02] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000117 train acc = 1.0000, test acc = 0.7713
batchnorm
[2023-10-16 02:04:08] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.000606 train acc = 1.0000, test acc = 0.8206
batchnorm
[2023-10-16 02:04:14] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000385 train acc = 1.0000, test acc = 0.7901
batchnorm
[2023-10-16 02:04:20] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.003201 train acc = 1.0000, test acc = 0.7540
batchnorm
[2023-10-16 02:04:26] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001883 train acc = 1.0000, test acc = 0.7862
batchnorm
[2023-10-16 02:04:32] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.000780 train acc = 1.0000, test acc = 0.7966
batchnorm
[2023-10-16 02:04:38] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000530 train acc = 1.0000, test acc = 0.7928
Evaluate 20 random ConvNet, mean = 0.7907 std = 0.0229
-------------------------
batchnorm
[2023-10-16 02:04:38] iter = 0500, loss = 61.3959
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:40] iter = 0510, loss = 57.9842
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:42] iter = 0520, loss = 56.6153
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:43] iter = 0530, loss = 60.9295
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:45] iter = 0540, loss = 57.1759
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:47] iter = 0550, loss = 55.8003
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:48] iter = 0560, loss = 58.5623
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:50] iter = 0570, loss = 54.3660
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:52] iter = 0580, loss = 64.8226
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:53] iter = 0590, loss = 62.6109
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:55] iter = 0600, loss = 61.8276
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:56] iter = 0610, loss = 60.4395
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:04:58] iter = 0620, loss = 54.9910
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:00] iter = 0630, loss = 56.2086
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:01] iter = 0640, loss = 53.6209
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:03] iter = 0650, loss = 60.4443
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:05] iter = 0660, loss = 53.8583
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:06] iter = 0670, loss = 63.0930
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:08] iter = 0680, loss = 52.2048
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:09] iter = 0690, loss = 59.5911
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:11] iter = 0700, loss = 60.4901
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:13] iter = 0710, loss = 62.2876
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:14] iter = 0720, loss = 55.8830
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:16] iter = 0730, loss = 54.0009
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:18] iter = 0740, loss = 59.6148
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:19] iter = 0750, loss = 61.8877
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:21] iter = 0760, loss = 55.9935
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:23] iter = 0770, loss = 59.9647
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:24] iter = 0780, loss = 58.5896
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:26] iter = 0790, loss = 62.5032
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:27] iter = 0800, loss = 59.5917
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:29] iter = 0810, loss = 58.3111
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:31] iter = 0820, loss = 56.3943
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:32] iter = 0830, loss = 58.5735
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:34] iter = 0840, loss = 62.2180
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:36] iter = 0850, loss = 59.5899
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:37] iter = 0860, loss = 65.0187
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:39] iter = 0870, loss = 58.4356
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:41] iter = 0880, loss = 61.9496
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:42] iter = 0890, loss = 54.0595
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:44] iter = 0900, loss = 60.7395
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:45] iter = 0910, loss = 58.4874
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:47] iter = 0920, loss = 53.2014
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:49] iter = 0930, loss = 65.2642
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:50] iter = 0940, loss = 62.4290
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:52] iter = 0950, loss = 64.8941
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:54] iter = 0960, loss = 61.2635
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:55] iter = 0970, loss = 54.9402
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:57] iter = 0980, loss = 55.3349
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:05:59] iter = 0990, loss = 59.5970
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:06:06] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001579 train acc = 1.0000, test acc = 0.7930
batchnorm
[2023-10-16 02:06:12] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.000340 train acc = 1.0000, test acc = 0.7673
batchnorm
[2023-10-16 02:06:18] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.003638 train acc = 1.0000, test acc = 0.8036
batchnorm
[2023-10-16 02:06:24] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000346 train acc = 1.0000, test acc = 0.7982
batchnorm
[2023-10-16 02:06:30] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000584 train acc = 1.0000, test acc = 0.7919
batchnorm
[2023-10-16 02:06:36] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.000353 train acc = 1.0000, test acc = 0.7955
batchnorm
[2023-10-16 02:06:42] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.000883 train acc = 1.0000, test acc = 0.7649
batchnorm
[2023-10-16 02:06:48] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000836 train acc = 1.0000, test acc = 0.7880
batchnorm
[2023-10-16 02:06:54] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000200 train acc = 1.0000, test acc = 0.7984
batchnorm
[2023-10-16 02:07:00] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000349 train acc = 1.0000, test acc = 0.7909
batchnorm
[2023-10-16 02:07:06] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.003793 train acc = 1.0000, test acc = 0.7737
batchnorm
[2023-10-16 02:07:12] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.007263 train acc = 1.0000, test acc = 0.7759
batchnorm
[2023-10-16 02:07:18] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000227 train acc = 1.0000, test acc = 0.7826
batchnorm
[2023-10-16 02:07:24] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001094 train acc = 1.0000, test acc = 0.7609
batchnorm
[2023-10-16 02:07:30] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001456 train acc = 1.0000, test acc = 0.7661
batchnorm
[2023-10-16 02:07:36] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001208 train acc = 1.0000, test acc = 0.7954
batchnorm
[2023-10-16 02:07:42] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000364 train acc = 1.0000, test acc = 0.8052
batchnorm
[2023-10-16 02:07:48] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000696 train acc = 1.0000, test acc = 0.7666
batchnorm
[2023-10-16 02:07:54] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.000634 train acc = 1.0000, test acc = 0.7747
batchnorm
[2023-10-16 02:08:00] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000498 train acc = 1.0000, test acc = 0.7421
Evaluate 20 random ConvNet, mean = 0.7817 std = 0.0165
-------------------------
batchnorm
[2023-10-16 02:08:00] iter = 1000, loss = 58.3396

================== Exp 1 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2aefbc9cd0>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5923 real images
class c = 1: 6742 real images
class c = 2: 5958 real images
class c = 3: 6131 real images
class c = 4: 5842 real images
class c = 5: 5421 real images
class c = 6: 5918 real images
class c = 7: 6265 real images
class c = 8: 5851 real images
class c = 9: 5949 real images
real images channel 0, mean = -0.0001, std = 1.0000
initialize synthetic data from random noise
[2023-10-16 02:08:18] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:08:24] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000379 train acc = 1.0000, test acc = 0.0711
batchnorm
[2023-10-16 02:08:30] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.000271 train acc = 1.0000, test acc = 0.0748
batchnorm
[2023-10-16 02:08:36] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.002427 train acc = 1.0000, test acc = 0.0749
batchnorm
[2023-10-16 02:08:42] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000269 train acc = 1.0000, test acc = 0.0796
batchnorm
[2023-10-16 02:08:48] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.003781 train acc = 1.0000, test acc = 0.0769
batchnorm
[2023-10-16 02:08:54] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001014 train acc = 1.0000, test acc = 0.0948
batchnorm
[2023-10-16 02:09:00] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001960 train acc = 1.0000, test acc = 0.0958
batchnorm
[2023-10-16 02:09:06] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.003376 train acc = 1.0000, test acc = 0.0786
batchnorm
[2023-10-16 02:09:12] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000383 train acc = 1.0000, test acc = 0.0761
batchnorm
[2023-10-16 02:09:18] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000287 train acc = 1.0000, test acc = 0.0930
batchnorm
[2023-10-16 02:09:24] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.000688 train acc = 1.0000, test acc = 0.0832
batchnorm
[2023-10-16 02:09:30] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.000746 train acc = 1.0000, test acc = 0.0923
batchnorm
[2023-10-16 02:09:36] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000284 train acc = 1.0000, test acc = 0.0763
batchnorm
[2023-10-16 02:09:42] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.002759 train acc = 1.0000, test acc = 0.0915
batchnorm
[2023-10-16 02:09:48] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.000899 train acc = 1.0000, test acc = 0.0809
batchnorm
[2023-10-16 02:09:54] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001268 train acc = 1.0000, test acc = 0.0923
batchnorm
[2023-10-16 02:10:00] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000559 train acc = 1.0000, test acc = 0.0557
batchnorm
[2023-10-16 02:10:06] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.002812 train acc = 1.0000, test acc = 0.0787
batchnorm
[2023-10-16 02:10:12] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001245 train acc = 1.0000, test acc = 0.0683
batchnorm
[2023-10-16 02:10:18] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000971 train acc = 1.0000, test acc = 0.0759
Evaluate 20 random ConvNet, mean = 0.0805 std = 0.0100
-------------------------
batchnorm
[2023-10-16 02:10:18] iter = 0000, loss = 262.1380
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:20] iter = 0010, loss = 151.9526
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:21] iter = 0020, loss = 118.2160
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:23] iter = 0030, loss = 99.2287
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:25] iter = 0040, loss = 92.2314
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:26] iter = 0050, loss = 77.9374
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:28] iter = 0060, loss = 72.6964
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:30] iter = 0070, loss = 79.8909
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:31] iter = 0080, loss = 76.7537
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:33] iter = 0090, loss = 69.5943
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:35] iter = 0100, loss = 71.5643
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:36] iter = 0110, loss = 67.5747
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:38] iter = 0120, loss = 69.8365
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:39] iter = 0130, loss = 69.5964
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:41] iter = 0140, loss = 64.3712
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:43] iter = 0150, loss = 64.7491
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:44] iter = 0160, loss = 68.0926
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:46] iter = 0170, loss = 65.8800
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:47] iter = 0180, loss = 64.0968
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:49] iter = 0190, loss = 61.3378
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:51] iter = 0200, loss = 56.7332
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:52] iter = 0210, loss = 61.7558
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:54] iter = 0220, loss = 64.6752
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:56] iter = 0230, loss = 64.5401
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:57] iter = 0240, loss = 61.1815
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:10:59] iter = 0250, loss = 57.5947
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:01] iter = 0260, loss = 55.6897
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:02] iter = 0270, loss = 59.5181
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:04] iter = 0280, loss = 66.6726
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:05] iter = 0290, loss = 65.7557
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:07] iter = 0300, loss = 62.4097
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:09] iter = 0310, loss = 61.0166
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:10] iter = 0320, loss = 61.7842
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:12] iter = 0330, loss = 52.8578
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:14] iter = 0340, loss = 59.3429
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:15] iter = 0350, loss = 52.8342
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:17] iter = 0360, loss = 54.6505
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:19] iter = 0370, loss = 54.1299
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:20] iter = 0380, loss = 60.4054
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:22] iter = 0390, loss = 62.4447
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:23] iter = 0400, loss = 60.4061
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:25] iter = 0410, loss = 57.8021
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:27] iter = 0420, loss = 60.4365
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:28] iter = 0430, loss = 60.6522
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:30] iter = 0440, loss = 58.0231
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:32] iter = 0450, loss = 62.7748
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:33] iter = 0460, loss = 62.5330
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:35] iter = 0470, loss = 65.3980
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:37] iter = 0480, loss = 64.1333
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:11:38] iter = 0490, loss = 54.4776
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:11:46] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000622 train acc = 1.0000, test acc = 0.7979
batchnorm
[2023-10-16 02:11:52] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.000548 train acc = 1.0000, test acc = 0.7972
batchnorm
[2023-10-16 02:11:58] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000213 train acc = 1.0000, test acc = 0.7812
batchnorm
[2023-10-16 02:12:04] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000441 train acc = 1.0000, test acc = 0.7878
batchnorm
[2023-10-16 02:12:10] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000570 train acc = 1.0000, test acc = 0.7839
batchnorm
[2023-10-16 02:12:16] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001444 train acc = 1.0000, test acc = 0.7848
batchnorm
[2023-10-16 02:12:22] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001613 train acc = 1.0000, test acc = 0.7958
batchnorm
[2023-10-16 02:12:27] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001308 train acc = 1.0000, test acc = 0.7647
batchnorm
[2023-10-16 02:12:33] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000565 train acc = 1.0000, test acc = 0.7607
batchnorm
[2023-10-16 02:12:39] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000547 train acc = 1.0000, test acc = 0.8048
batchnorm
[2023-10-16 02:12:45] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.000315 train acc = 1.0000, test acc = 0.7886
batchnorm
[2023-10-16 02:12:51] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001462 train acc = 1.0000, test acc = 0.7647
batchnorm
[2023-10-16 02:12:57] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001624 train acc = 1.0000, test acc = 0.7897
batchnorm
[2023-10-16 02:13:03] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000546 train acc = 1.0000, test acc = 0.7928
batchnorm
[2023-10-16 02:13:09] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001546 train acc = 1.0000, test acc = 0.7785
batchnorm
[2023-10-16 02:13:15] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000537 train acc = 1.0000, test acc = 0.7656
batchnorm
[2023-10-16 02:13:21] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000416 train acc = 1.0000, test acc = 0.7945
batchnorm
[2023-10-16 02:13:27] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000134 train acc = 1.0000, test acc = 0.7751
batchnorm
[2023-10-16 02:13:33] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.005411 train acc = 1.0000, test acc = 0.8038
batchnorm
[2023-10-16 02:13:39] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000479 train acc = 1.0000, test acc = 0.7830
Evaluate 20 random ConvNet, mean = 0.7848 std = 0.0129
-------------------------
batchnorm
[2023-10-16 02:13:39] iter = 0500, loss = 54.6338
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:41] iter = 0510, loss = 59.4405
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:43] iter = 0520, loss = 55.5849
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:44] iter = 0530, loss = 58.8735
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:46] iter = 0540, loss = 59.5709
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:48] iter = 0550, loss = 59.0307
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:49] iter = 0560, loss = 57.7656
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:51] iter = 0570, loss = 61.4985
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:53] iter = 0580, loss = 58.5091
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:54] iter = 0590, loss = 59.5784
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:56] iter = 0600, loss = 53.6633
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:57] iter = 0610, loss = 56.3909
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:13:59] iter = 0620, loss = 60.8037
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:01] iter = 0630, loss = 58.7731
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:02] iter = 0640, loss = 54.9057
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:04] iter = 0650, loss = 60.6412
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:06] iter = 0660, loss = 58.7885
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:07] iter = 0670, loss = 57.5714
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:09] iter = 0680, loss = 55.9986
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:11] iter = 0690, loss = 67.3610
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:12] iter = 0700, loss = 58.8490
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:14] iter = 0710, loss = 56.7724
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:15] iter = 0720, loss = 58.1707
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:17] iter = 0730, loss = 56.0846
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:19] iter = 0740, loss = 54.3289
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:20] iter = 0750, loss = 59.8611
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:22] iter = 0760, loss = 52.7678
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:24] iter = 0770, loss = 63.1639
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:25] iter = 0780, loss = 52.6968
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:27] iter = 0790, loss = 54.9808
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:28] iter = 0800, loss = 51.8075
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:30] iter = 0810, loss = 61.3676
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:32] iter = 0820, loss = 61.4488
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:33] iter = 0830, loss = 61.1854
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:35] iter = 0840, loss = 62.7468
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:37] iter = 0850, loss = 59.9778
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:38] iter = 0860, loss = 55.5900
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:40] iter = 0870, loss = 56.5152
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:42] iter = 0880, loss = 67.6115
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:43] iter = 0890, loss = 53.5677
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:45] iter = 0900, loss = 56.3710
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:46] iter = 0910, loss = 58.9799
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:48] iter = 0920, loss = 62.0560
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:50] iter = 0930, loss = 59.1424
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:51] iter = 0940, loss = 58.9007
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:53] iter = 0950, loss = 57.6127
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:55] iter = 0960, loss = 57.7633
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:56] iter = 0970, loss = 64.8779
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:14:58] iter = 0980, loss = 56.3809
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:15:00] iter = 0990, loss = 59.9783
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:15:07] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000336 train acc = 1.0000, test acc = 0.8079
batchnorm
[2023-10-16 02:15:13] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.004218 train acc = 1.0000, test acc = 0.7705
batchnorm
[2023-10-16 02:15:19] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000901 train acc = 1.0000, test acc = 0.7967
batchnorm
[2023-10-16 02:15:25] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.002173 train acc = 1.0000, test acc = 0.8077
batchnorm
[2023-10-16 02:15:31] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000274 train acc = 1.0000, test acc = 0.7494
batchnorm
[2023-10-16 02:15:37] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.000305 train acc = 1.0000, test acc = 0.8017
batchnorm
[2023-10-16 02:15:43] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.000295 train acc = 1.0000, test acc = 0.8207
batchnorm
[2023-10-16 02:15:49] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001873 train acc = 1.0000, test acc = 0.8103
batchnorm
[2023-10-16 02:15:55] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.008579 train acc = 1.0000, test acc = 0.7664
batchnorm
[2023-10-16 02:16:01] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000293 train acc = 1.0000, test acc = 0.7996
batchnorm
[2023-10-16 02:16:07] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001095 train acc = 1.0000, test acc = 0.8228
batchnorm
[2023-10-16 02:16:13] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.005260 train acc = 1.0000, test acc = 0.7955
batchnorm
[2023-10-16 02:16:19] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001063 train acc = 1.0000, test acc = 0.8144
batchnorm
[2023-10-16 02:16:25] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000084 train acc = 1.0000, test acc = 0.7842
batchnorm
[2023-10-16 02:16:31] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001065 train acc = 1.0000, test acc = 0.7817
batchnorm
[2023-10-16 02:16:37] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001009 train acc = 1.0000, test acc = 0.7946
batchnorm
[2023-10-16 02:16:43] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000293 train acc = 1.0000, test acc = 0.8049
batchnorm
[2023-10-16 02:16:49] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000051 train acc = 1.0000, test acc = 0.7712
batchnorm
[2023-10-16 02:16:55] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.000816 train acc = 1.0000, test acc = 0.8145
batchnorm
[2023-10-16 02:17:01] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000302 train acc = 1.0000, test acc = 0.8111
Evaluate 20 random ConvNet, mean = 0.7963 std = 0.0194
-------------------------
batchnorm
[2023-10-16 02:17:01] iter = 1000, loss = 60.9488

================== Exp 2 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2aefbc9cd0>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5923 real images
class c = 1: 6742 real images
class c = 2: 5958 real images
class c = 3: 6131 real images
class c = 4: 5842 real images
class c = 5: 5421 real images
class c = 6: 5918 real images
class c = 7: 6265 real images
class c = 8: 5851 real images
class c = 9: 5949 real images
real images channel 0, mean = -0.0001, std = 1.0000
initialize synthetic data from random noise
[2023-10-16 02:17:19] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:17:25] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.002231 train acc = 1.0000, test acc = 0.0898
batchnorm
[2023-10-16 02:17:31] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.000846 train acc = 1.0000, test acc = 0.0866
batchnorm
[2023-10-16 02:17:37] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000394 train acc = 1.0000, test acc = 0.1115
batchnorm
[2023-10-16 02:17:43] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000576 train acc = 1.0000, test acc = 0.1067
batchnorm
[2023-10-16 02:17:49] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001443 train acc = 1.0000, test acc = 0.1032
batchnorm
[2023-10-16 02:17:55] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.002184 train acc = 1.0000, test acc = 0.0738
batchnorm
[2023-10-16 02:18:01] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001788 train acc = 1.0000, test acc = 0.0931
batchnorm
[2023-10-16 02:18:07] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.001155 train acc = 1.0000, test acc = 0.1055
batchnorm
[2023-10-16 02:18:13] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000609 train acc = 1.0000, test acc = 0.0763
batchnorm
[2023-10-16 02:18:19] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000423 train acc = 1.0000, test acc = 0.0699
batchnorm
[2023-10-16 02:18:25] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001493 train acc = 1.0000, test acc = 0.0824
batchnorm
[2023-10-16 02:18:31] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001735 train acc = 1.0000, test acc = 0.0787
batchnorm
[2023-10-16 02:18:37] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.002263 train acc = 1.0000, test acc = 0.0853
batchnorm
[2023-10-16 02:18:43] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000364 train acc = 1.0000, test acc = 0.0854
batchnorm
[2023-10-16 02:18:49] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.002241 train acc = 1.0000, test acc = 0.0957
batchnorm
[2023-10-16 02:18:55] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000632 train acc = 1.0000, test acc = 0.1016
batchnorm
[2023-10-16 02:19:01] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000639 train acc = 1.0000, test acc = 0.0974
batchnorm
[2023-10-16 02:19:07] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000496 train acc = 1.0000, test acc = 0.1002
batchnorm
[2023-10-16 02:19:13] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001041 train acc = 1.0000, test acc = 0.0772
batchnorm
[2023-10-16 02:19:19] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000808 train acc = 1.0000, test acc = 0.0835
Evaluate 20 random ConvNet, mean = 0.0902 std = 0.0118
-------------------------
batchnorm
[2023-10-16 02:19:19] iter = 0000, loss = 253.5846
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:21] iter = 0010, loss = 151.9673
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:22] iter = 0020, loss = 120.2608
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:24] iter = 0030, loss = 106.4092
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:26] iter = 0040, loss = 87.4291
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:27] iter = 0050, loss = 78.2606
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:29] iter = 0060, loss = 90.1099
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:30] iter = 0070, loss = 78.5800
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:32] iter = 0080, loss = 76.3518
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:34] iter = 0090, loss = 75.5538
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:35] iter = 0100, loss = 69.2573
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:37] iter = 0110, loss = 76.2408
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:39] iter = 0120, loss = 70.2504
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:40] iter = 0130, loss = 67.3719
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:42] iter = 0140, loss = 69.5961
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:43] iter = 0150, loss = 66.5119
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:45] iter = 0160, loss = 65.4793
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:47] iter = 0170, loss = 63.0660
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:48] iter = 0180, loss = 65.4778
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:50] iter = 0190, loss = 66.4213
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:52] iter = 0200, loss = 63.5661
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:53] iter = 0210, loss = 64.3557
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:55] iter = 0220, loss = 60.2911
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:57] iter = 0230, loss = 60.6491
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:19:58] iter = 0240, loss = 53.3474
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:00] iter = 0250, loss = 61.9485
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:01] iter = 0260, loss = 60.7214
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:03] iter = 0270, loss = 57.1640
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:05] iter = 0280, loss = 58.0850
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:06] iter = 0290, loss = 55.9967
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:08] iter = 0300, loss = 51.6043
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:10] iter = 0310, loss = 62.5106
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:11] iter = 0320, loss = 53.4022
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:13] iter = 0330, loss = 57.7624
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:15] iter = 0340, loss = 56.9151
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:16] iter = 0350, loss = 58.8077
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:18] iter = 0360, loss = 63.7330
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:19] iter = 0370, loss = 59.6491
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:21] iter = 0380, loss = 60.7611
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:23] iter = 0390, loss = 61.8444
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:24] iter = 0400, loss = 57.2924
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:26] iter = 0410, loss = 53.9811
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:28] iter = 0420, loss = 59.3728
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:29] iter = 0430, loss = 58.8522
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:31] iter = 0440, loss = 59.7492
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:33] iter = 0450, loss = 57.5359
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:34] iter = 0460, loss = 54.3513
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:36] iter = 0470, loss = 59.5476
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:37] iter = 0480, loss = 54.3891
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:20:39] iter = 0490, loss = 64.5257
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:20:47] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000790 train acc = 1.0000, test acc = 0.7749
batchnorm
[2023-10-16 02:20:53] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001570 train acc = 1.0000, test acc = 0.7822
batchnorm
[2023-10-16 02:20:59] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000872 train acc = 1.0000, test acc = 0.7922
batchnorm
[2023-10-16 02:21:05] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000744 train acc = 1.0000, test acc = 0.7757
batchnorm
[2023-10-16 02:21:11] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000063 train acc = 1.0000, test acc = 0.7930
batchnorm
[2023-10-16 02:21:16] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.000432 train acc = 1.0000, test acc = 0.7891
batchnorm
[2023-10-16 02:21:22] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.000729 train acc = 1.0000, test acc = 0.8045
batchnorm
[2023-10-16 02:21:28] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.005968 train acc = 1.0000, test acc = 0.7949
batchnorm
[2023-10-16 02:21:34] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001623 train acc = 1.0000, test acc = 0.8163
batchnorm
[2023-10-16 02:21:40] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001429 train acc = 1.0000, test acc = 0.7766
batchnorm
[2023-10-16 02:21:46] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.000349 train acc = 1.0000, test acc = 0.7882
batchnorm
[2023-10-16 02:21:52] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.000935 train acc = 1.0000, test acc = 0.7829
batchnorm
[2023-10-16 02:21:58] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000323 train acc = 1.0000, test acc = 0.7935
batchnorm
[2023-10-16 02:22:04] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000269 train acc = 1.0000, test acc = 0.8135
batchnorm
[2023-10-16 02:22:10] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001648 train acc = 1.0000, test acc = 0.7979
batchnorm
[2023-10-16 02:22:16] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000548 train acc = 1.0000, test acc = 0.8001
batchnorm
[2023-10-16 02:22:22] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000450 train acc = 1.0000, test acc = 0.7777
batchnorm
[2023-10-16 02:22:28] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000257 train acc = 1.0000, test acc = 0.7855
batchnorm
[2023-10-16 02:22:34] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.000104 train acc = 1.0000, test acc = 0.7836
batchnorm
[2023-10-16 02:22:40] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000529 train acc = 1.0000, test acc = 0.8163
Evaluate 20 random ConvNet, mean = 0.7919 std = 0.0127
-------------------------
batchnorm
[2023-10-16 02:22:40] iter = 0500, loss = 59.0869
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:42] iter = 0510, loss = 59.2712
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:44] iter = 0520, loss = 55.0227
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:45] iter = 0530, loss = 53.5808
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:47] iter = 0540, loss = 60.3521
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:49] iter = 0550, loss = 58.5933
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:50] iter = 0560, loss = 65.3292
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:52] iter = 0570, loss = 48.5214
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:53] iter = 0580, loss = 61.2778
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:55] iter = 0590, loss = 56.2860
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:57] iter = 0600, loss = 65.3039
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:22:58] iter = 0610, loss = 50.1758
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:00] iter = 0620, loss = 57.5915
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:02] iter = 0630, loss = 60.9420
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:03] iter = 0640, loss = 65.9203
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:05] iter = 0650, loss = 59.9101
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:06] iter = 0660, loss = 55.1128
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:08] iter = 0670, loss = 53.8110
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:10] iter = 0680, loss = 58.0367
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:11] iter = 0690, loss = 62.3073
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:13] iter = 0700, loss = 61.2703
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:15] iter = 0710, loss = 53.1385
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:16] iter = 0720, loss = 56.8491
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:18] iter = 0730, loss = 58.5075
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:20] iter = 0740, loss = 55.0911
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:21] iter = 0750, loss = 56.6844
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:23] iter = 0760, loss = 59.9888
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:24] iter = 0770, loss = 59.6212
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:26] iter = 0780, loss = 55.5163
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:28] iter = 0790, loss = 57.0320
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:29] iter = 0800, loss = 51.8416
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:31] iter = 0810, loss = 59.1996
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:33] iter = 0820, loss = 59.5624
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:34] iter = 0830, loss = 52.3462
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:36] iter = 0840, loss = 53.0649
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:38] iter = 0850, loss = 60.3790
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:39] iter = 0860, loss = 55.9551
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:41] iter = 0870, loss = 63.2625
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:43] iter = 0880, loss = 61.3564
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:44] iter = 0890, loss = 55.9155
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:46] iter = 0900, loss = 61.9603
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:47] iter = 0910, loss = 60.5685
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:49] iter = 0920, loss = 48.3310
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:51] iter = 0930, loss = 59.2436
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:52] iter = 0940, loss = 57.7873
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:54] iter = 0950, loss = 57.6662
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:56] iter = 0960, loss = 64.0445
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:57] iter = 0970, loss = 53.0722
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:23:59] iter = 0980, loss = 55.2255
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:24:01] iter = 0990, loss = 62.2063
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:24:08] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000967 train acc = 1.0000, test acc = 0.8324
batchnorm
[2023-10-16 02:24:14] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.009058 train acc = 1.0000, test acc = 0.8144
batchnorm
[2023-10-16 02:24:20] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000442 train acc = 1.0000, test acc = 0.8096
batchnorm
[2023-10-16 02:24:26] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.004257 train acc = 1.0000, test acc = 0.8160
batchnorm
[2023-10-16 02:24:32] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001646 train acc = 1.0000, test acc = 0.7843
batchnorm
[2023-10-16 02:24:38] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.002659 train acc = 1.0000, test acc = 0.8108
batchnorm
[2023-10-16 02:24:44] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.000585 train acc = 1.0000, test acc = 0.8054
batchnorm
[2023-10-16 02:24:50] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000612 train acc = 1.0000, test acc = 0.8028
batchnorm
[2023-10-16 02:24:56] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000596 train acc = 1.0000, test acc = 0.8075
batchnorm
[2023-10-16 02:25:02] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000543 train acc = 1.0000, test acc = 0.8304
batchnorm
[2023-10-16 02:25:08] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001185 train acc = 1.0000, test acc = 0.8064
batchnorm
[2023-10-16 02:25:14] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.002703 train acc = 1.0000, test acc = 0.7955
batchnorm
[2023-10-16 02:25:20] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.003542 train acc = 1.0000, test acc = 0.8002
batchnorm
[2023-10-16 02:25:26] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000560 train acc = 1.0000, test acc = 0.8075
batchnorm
[2023-10-16 02:25:32] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.002273 train acc = 1.0000, test acc = 0.8041
batchnorm
[2023-10-16 02:25:38] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.002510 train acc = 1.0000, test acc = 0.8064
batchnorm
[2023-10-16 02:25:44] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001591 train acc = 1.0000, test acc = 0.7868
batchnorm
[2023-10-16 02:25:50] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000283 train acc = 1.0000, test acc = 0.8139
batchnorm
[2023-10-16 02:25:56] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.000866 train acc = 1.0000, test acc = 0.7879
batchnorm
[2023-10-16 02:26:02] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001082 train acc = 1.0000, test acc = 0.7918
Evaluate 20 random ConvNet, mean = 0.8057 std = 0.0124
-------------------------
batchnorm
[2023-10-16 02:26:02] iter = 1000, loss = 61.2547

================== Exp 3 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2aefbc9cd0>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5923 real images
class c = 1: 6742 real images
class c = 2: 5958 real images
class c = 3: 6131 real images
class c = 4: 5842 real images
class c = 5: 5421 real images
class c = 6: 5918 real images
class c = 7: 6265 real images
class c = 8: 5851 real images
class c = 9: 5949 real images
real images channel 0, mean = -0.0001, std = 1.0000
initialize synthetic data from random noise
[2023-10-16 02:26:20] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:26:26] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000642 train acc = 1.0000, test acc = 0.0439
batchnorm
[2023-10-16 02:26:32] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001681 train acc = 1.0000, test acc = 0.0447
batchnorm
[2023-10-16 02:26:38] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000174 train acc = 1.0000, test acc = 0.0460
batchnorm
[2023-10-16 02:26:44] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001259 train acc = 1.0000, test acc = 0.0511
batchnorm
[2023-10-16 02:26:50] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.014213 train acc = 1.0000, test acc = 0.0585
batchnorm
[2023-10-16 02:26:56] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001136 train acc = 1.0000, test acc = 0.0482
batchnorm
[2023-10-16 02:27:02] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.000990 train acc = 1.0000, test acc = 0.0466
batchnorm
[2023-10-16 02:27:08] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000800 train acc = 1.0000, test acc = 0.0633
batchnorm
[2023-10-16 02:27:14] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.002608 train acc = 1.0000, test acc = 0.0535
batchnorm
[2023-10-16 02:27:20] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.009386 train acc = 1.0000, test acc = 0.0446
batchnorm
[2023-10-16 02:27:26] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001510 train acc = 1.0000, test acc = 0.0446
batchnorm
[2023-10-16 02:27:32] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.003854 train acc = 1.0000, test acc = 0.0397
batchnorm
[2023-10-16 02:27:38] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000605 train acc = 1.0000, test acc = 0.0624
batchnorm
[2023-10-16 02:27:44] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000655 train acc = 1.0000, test acc = 0.0450
batchnorm
[2023-10-16 02:27:50] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.001743 train acc = 1.0000, test acc = 0.0402
batchnorm
[2023-10-16 02:27:56] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.001064 train acc = 1.0000, test acc = 0.0401
batchnorm
[2023-10-16 02:28:02] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.002855 train acc = 1.0000, test acc = 0.0425
batchnorm
[2023-10-16 02:28:08] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.002101 train acc = 1.0000, test acc = 0.0523
batchnorm
[2023-10-16 02:28:14] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001273 train acc = 1.0000, test acc = 0.0461
batchnorm
[2023-10-16 02:28:20] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.001711 train acc = 1.0000, test acc = 0.0622
Evaluate 20 random ConvNet, mean = 0.0488 std = 0.0074
-------------------------
batchnorm
[2023-10-16 02:28:20] iter = 0000, loss = 247.6284
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:22] iter = 0010, loss = 148.6484
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:23] iter = 0020, loss = 128.4954
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:25] iter = 0030, loss = 112.4315
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:27] iter = 0040, loss = 96.3662
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:28] iter = 0050, loss = 91.6880
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:30] iter = 0060, loss = 90.4081
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:31] iter = 0070, loss = 75.7980
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:33] iter = 0080, loss = 77.1449
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:35] iter = 0090, loss = 79.6523
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:36] iter = 0100, loss = 71.6524
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:38] iter = 0110, loss = 65.2013
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:40] iter = 0120, loss = 62.7358
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:41] iter = 0130, loss = 67.6118
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:43] iter = 0140, loss = 65.4152
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:45] iter = 0150, loss = 62.0446
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:46] iter = 0160, loss = 63.0743
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:48] iter = 0170, loss = 65.2459
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:49] iter = 0180, loss = 67.3389
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:51] iter = 0190, loss = 74.3023
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:53] iter = 0200, loss = 60.6736
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:54] iter = 0210, loss = 64.3911
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:56] iter = 0220, loss = 68.4711
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:58] iter = 0230, loss = 59.7337
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:28:59] iter = 0240, loss = 57.9574
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:01] iter = 0250, loss = 66.3188
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:02] iter = 0260, loss = 63.2030
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:04] iter = 0270, loss = 63.6438
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:06] iter = 0280, loss = 59.3581
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:07] iter = 0290, loss = 56.0907
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:09] iter = 0300, loss = 54.4434
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:11] iter = 0310, loss = 54.5768
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:12] iter = 0320, loss = 61.3028
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:14] iter = 0330, loss = 57.0540
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:16] iter = 0340, loss = 66.7987
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:17] iter = 0350, loss = 65.0215
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:19] iter = 0360, loss = 61.3386
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:20] iter = 0370, loss = 53.9591
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:22] iter = 0380, loss = 58.8737
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:24] iter = 0390, loss = 56.1866
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:25] iter = 0400, loss = 60.7956
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:27] iter = 0410, loss = 57.6829
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:29] iter = 0420, loss = 62.7102
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:30] iter = 0430, loss = 52.3535
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:32] iter = 0440, loss = 55.6256
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:34] iter = 0450, loss = 61.6989
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:35] iter = 0460, loss = 60.5285
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:37] iter = 0470, loss = 58.6488
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:38] iter = 0480, loss = 56.4603
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:29:40] iter = 0490, loss = 57.5777
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:29:48] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000305 train acc = 1.0000, test acc = 0.7876
batchnorm
[2023-10-16 02:29:54] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.001330 train acc = 1.0000, test acc = 0.7832
batchnorm
[2023-10-16 02:30:00] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000789 train acc = 1.0000, test acc = 0.7771
batchnorm
[2023-10-16 02:30:05] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001536 train acc = 1.0000, test acc = 0.7833
batchnorm
[2023-10-16 02:30:11] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000337 train acc = 1.0000, test acc = 0.7445
batchnorm
[2023-10-16 02:30:17] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001887 train acc = 1.0000, test acc = 0.7901
batchnorm
[2023-10-16 02:30:23] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001125 train acc = 1.0000, test acc = 0.7621
batchnorm
[2023-10-16 02:30:29] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000934 train acc = 1.0000, test acc = 0.8023
batchnorm
[2023-10-16 02:30:35] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000968 train acc = 1.0000, test acc = 0.7573
batchnorm
[2023-10-16 02:30:41] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000745 train acc = 1.0000, test acc = 0.7595
batchnorm
[2023-10-16 02:30:47] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001126 train acc = 1.0000, test acc = 0.7742
batchnorm
[2023-10-16 02:30:53] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.000347 train acc = 1.0000, test acc = 0.8004
batchnorm
[2023-10-16 02:30:59] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.005068 train acc = 1.0000, test acc = 0.7816
batchnorm
[2023-10-16 02:31:05] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000788 train acc = 1.0000, test acc = 0.7959
batchnorm
[2023-10-16 02:31:11] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.000365 train acc = 1.0000, test acc = 0.7633
batchnorm
[2023-10-16 02:31:17] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.002154 train acc = 1.0000, test acc = 0.7731
batchnorm
[2023-10-16 02:31:23] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000074 train acc = 1.0000, test acc = 0.7924
batchnorm
[2023-10-16 02:31:29] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000894 train acc = 1.0000, test acc = 0.7972
batchnorm
[2023-10-16 02:31:35] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.000155 train acc = 1.0000, test acc = 0.7863
batchnorm
[2023-10-16 02:31:41] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000456 train acc = 1.0000, test acc = 0.7753
Evaluate 20 random ConvNet, mean = 0.7793 std = 0.0154
-------------------------
batchnorm
[2023-10-16 02:31:41] iter = 0500, loss = 57.4234
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:43] iter = 0510, loss = 59.2654
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:44] iter = 0520, loss = 60.5118
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:46] iter = 0530, loss = 58.8722
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:48] iter = 0540, loss = 59.1693
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:49] iter = 0550, loss = 60.2650
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:51] iter = 0560, loss = 60.2365
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:53] iter = 0570, loss = 54.2010
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:54] iter = 0580, loss = 51.3017
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:56] iter = 0590, loss = 59.9303
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:57] iter = 0600, loss = 53.4269
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:31:59] iter = 0610, loss = 55.9721
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:01] iter = 0620, loss = 56.0382
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:02] iter = 0630, loss = 57.8531
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:04] iter = 0640, loss = 54.9277
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:06] iter = 0650, loss = 57.8165
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:07] iter = 0660, loss = 55.8949
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:09] iter = 0670, loss = 58.9677
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:11] iter = 0680, loss = 59.6236
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:12] iter = 0690, loss = 53.8360
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:14] iter = 0700, loss = 62.0627
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:15] iter = 0710, loss = 55.8398
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:17] iter = 0720, loss = 58.7266
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:19] iter = 0730, loss = 52.9116
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:20] iter = 0740, loss = 61.8252
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:22] iter = 0750, loss = 58.4216
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:24] iter = 0760, loss = 60.2475
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:25] iter = 0770, loss = 59.6796
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:27] iter = 0780, loss = 56.6669
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:28] iter = 0790, loss = 59.6821
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:30] iter = 0800, loss = 57.3761
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:32] iter = 0810, loss = 49.3359
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:33] iter = 0820, loss = 56.4666
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:35] iter = 0830, loss = 50.2476
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:37] iter = 0840, loss = 63.8804
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:38] iter = 0850, loss = 55.6494
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:40] iter = 0860, loss = 53.4467
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:42] iter = 0870, loss = 49.9011
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:43] iter = 0880, loss = 56.9577
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:45] iter = 0890, loss = 60.2353
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:46] iter = 0900, loss = 55.5116
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:48] iter = 0910, loss = 51.7405
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:50] iter = 0920, loss = 61.1226
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:51] iter = 0930, loss = 66.7106
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:53] iter = 0940, loss = 56.9563
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:54] iter = 0950, loss = 53.0439
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:56] iter = 0960, loss = 65.9987
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:58] iter = 0970, loss = 56.0164
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:32:59] iter = 0980, loss = 66.1189
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:33:01] iter = 0990, loss = 61.0472
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:33:08] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.000446 train acc = 1.0000, test acc = 0.8039
batchnorm
[2023-10-16 02:33:14] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.000952 train acc = 1.0000, test acc = 0.7933
batchnorm
[2023-10-16 02:33:20] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000788 train acc = 1.0000, test acc = 0.7771
batchnorm
[2023-10-16 02:33:26] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001247 train acc = 1.0000, test acc = 0.8049
batchnorm
[2023-10-16 02:33:32] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000235 train acc = 1.0000, test acc = 0.8257
batchnorm
[2023-10-16 02:33:38] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.000680 train acc = 1.0000, test acc = 0.7862
batchnorm
[2023-10-16 02:33:44] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.002170 train acc = 1.0000, test acc = 0.7508
batchnorm
[2023-10-16 02:33:50] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000792 train acc = 1.0000, test acc = 0.7900
batchnorm
[2023-10-16 02:33:56] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001927 train acc = 1.0000, test acc = 0.7736
batchnorm
[2023-10-16 02:34:02] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000333 train acc = 1.0000, test acc = 0.7767
batchnorm
[2023-10-16 02:34:08] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.004709 train acc = 1.0000, test acc = 0.8145
batchnorm
[2023-10-16 02:34:14] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001810 train acc = 1.0000, test acc = 0.8158
batchnorm
[2023-10-16 02:34:20] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.001020 train acc = 1.0000, test acc = 0.8140
batchnorm
[2023-10-16 02:34:26] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.000539 train acc = 1.0000, test acc = 0.8162
batchnorm
[2023-10-16 02:34:32] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.000458 train acc = 1.0000, test acc = 0.8088
batchnorm
[2023-10-16 02:34:38] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.002065 train acc = 1.0000, test acc = 0.8037
batchnorm
[2023-10-16 02:34:44] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.004606 train acc = 1.0000, test acc = 0.8260
batchnorm
[2023-10-16 02:34:50] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000571 train acc = 1.0000, test acc = 0.7942
batchnorm
[2023-10-16 02:34:56] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.003596 train acc = 1.0000, test acc = 0.7865
batchnorm
[2023-10-16 02:35:02] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.002898 train acc = 1.0000, test acc = 0.8008
Evaluate 20 random ConvNet, mean = 0.7981 std = 0.0188
-------------------------
batchnorm
[2023-10-16 02:35:02] iter = 1000, loss = 64.0303

================== Exp 4 ==================
 
Hyper-parameters: 
 {'method': 'DC', 'dataset': 'MNIST', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2aefbc9cd0>, 'dsa': False, 'dc_aug_param': None}
Evaluation model pool:  ['ConvNet']
class c = 0: 5923 real images
class c = 1: 6742 real images
class c = 2: 5958 real images
class c = 3: 6131 real images
class c = 4: 5842 real images
class c = 5: 5421 real images
class c = 6: 5918 real images
class c = 7: 6265 real images
class c = 8: 5851 real images
class c = 9: 5949 real images
real images channel 0, mean = -0.0001, std = 1.0000
initialize synthetic data from random noise
[2023-10-16 02:35:20] training begins
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:35:26] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.002142 train acc = 1.0000, test acc = 0.0914
batchnorm
[2023-10-16 02:35:32] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.000229 train acc = 1.0000, test acc = 0.1015
batchnorm
[2023-10-16 02:35:38] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.001502 train acc = 1.0000, test acc = 0.0966
batchnorm
[2023-10-16 02:35:44] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.001243 train acc = 1.0000, test acc = 0.0903
batchnorm
[2023-10-16 02:35:50] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.003645 train acc = 1.0000, test acc = 0.0780
batchnorm
[2023-10-16 02:35:56] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.001840 train acc = 1.0000, test acc = 0.0885
batchnorm
[2023-10-16 02:36:02] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.000473 train acc = 1.0000, test acc = 0.0968
batchnorm
[2023-10-16 02:36:08] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000465 train acc = 1.0000, test acc = 0.0792
batchnorm
[2023-10-16 02:36:14] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000388 train acc = 1.0000, test acc = 0.0744
batchnorm
[2023-10-16 02:36:20] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.001074 train acc = 1.0000, test acc = 0.0951
batchnorm
[2023-10-16 02:36:26] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.001542 train acc = 1.0000, test acc = 0.1079
batchnorm
[2023-10-16 02:36:32] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.002399 train acc = 1.0000, test acc = 0.1084
batchnorm
[2023-10-16 02:36:38] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000139 train acc = 1.0000, test acc = 0.0803
batchnorm
[2023-10-16 02:36:44] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001098 train acc = 1.0000, test acc = 0.0806
batchnorm
[2023-10-16 02:36:50] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.000466 train acc = 1.0000, test acc = 0.1004
batchnorm
[2023-10-16 02:36:56] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000236 train acc = 1.0000, test acc = 0.1071
batchnorm
[2023-10-16 02:37:02] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000938 train acc = 1.0000, test acc = 0.0955
batchnorm
[2023-10-16 02:37:08] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.003173 train acc = 1.0000, test acc = 0.0898
batchnorm
[2023-10-16 02:37:14] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.000680 train acc = 1.0000, test acc = 0.1031
batchnorm
[2023-10-16 02:37:20] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000525 train acc = 1.0000, test acc = 0.1062
Evaluate 20 random ConvNet, mean = 0.0936 std = 0.0105
-------------------------
batchnorm
[2023-10-16 02:37:20] iter = 0000, loss = 263.7976
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:22] iter = 0010, loss = 156.0570
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:24] iter = 0020, loss = 116.5147
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:25] iter = 0030, loss = 102.3007
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:27] iter = 0040, loss = 81.6601
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:28] iter = 0050, loss = 86.7531
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:30] iter = 0060, loss = 84.2775
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:32] iter = 0070, loss = 80.1862
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:33] iter = 0080, loss = 74.7107
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:35] iter = 0090, loss = 67.3440
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:37] iter = 0100, loss = 68.8894
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:38] iter = 0110, loss = 69.8031
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:40] iter = 0120, loss = 74.4838
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:42] iter = 0130, loss = 61.3940
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:43] iter = 0140, loss = 64.9447
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:45] iter = 0150, loss = 58.6301
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:46] iter = 0160, loss = 71.8333
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:48] iter = 0170, loss = 62.1807
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:50] iter = 0180, loss = 57.5229
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:51] iter = 0190, loss = 63.5049
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:53] iter = 0200, loss = 56.4701
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:55] iter = 0210, loss = 62.7234
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:56] iter = 0220, loss = 63.8716
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:58] iter = 0230, loss = 60.2044
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:37:59] iter = 0240, loss = 61.8749
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:01] iter = 0250, loss = 64.8505
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:03] iter = 0260, loss = 59.2571
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:04] iter = 0270, loss = 58.7122
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:06] iter = 0280, loss = 56.1305
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:08] iter = 0290, loss = 61.7446
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:09] iter = 0300, loss = 57.2578
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:11] iter = 0310, loss = 60.9213
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:12] iter = 0320, loss = 53.3287
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:14] iter = 0330, loss = 57.6102
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:16] iter = 0340, loss = 56.9744
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:17] iter = 0350, loss = 64.2613
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:19] iter = 0360, loss = 60.1021
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:21] iter = 0370, loss = 55.1580
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:22] iter = 0380, loss = 58.9037
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:24] iter = 0390, loss = 60.9585
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:26] iter = 0400, loss = 62.0211
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:27] iter = 0410, loss = 64.2565
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:29] iter = 0420, loss = 59.2189
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:30] iter = 0430, loss = 61.2918
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:32] iter = 0440, loss = 63.9301
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:34] iter = 0450, loss = 54.5036
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:35] iter = 0460, loss = 57.8511
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:37] iter = 0470, loss = 65.7925
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:39] iter = 0480, loss = 56.0931
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:38:40] iter = 0490, loss = 53.6729
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:38:48] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.001212 train acc = 1.0000, test acc = 0.8106
batchnorm
[2023-10-16 02:38:54] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.003065 train acc = 1.0000, test acc = 0.8249
batchnorm
[2023-10-16 02:39:00] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000088 train acc = 1.0000, test acc = 0.8361
batchnorm
[2023-10-16 02:39:06] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000257 train acc = 1.0000, test acc = 0.7813
batchnorm
[2023-10-16 02:39:12] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.001128 train acc = 1.0000, test acc = 0.8024
batchnorm
[2023-10-16 02:39:18] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.000430 train acc = 1.0000, test acc = 0.7776
batchnorm
[2023-10-16 02:39:24] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001009 train acc = 1.0000, test acc = 0.7745
batchnorm
[2023-10-16 02:39:30] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000121 train acc = 1.0000, test acc = 0.8073
batchnorm
[2023-10-16 02:39:36] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.000278 train acc = 1.0000, test acc = 0.8119
batchnorm
[2023-10-16 02:39:41] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000256 train acc = 1.0000, test acc = 0.7957
batchnorm
[2023-10-16 02:39:47] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.000158 train acc = 1.0000, test acc = 0.8240
batchnorm
[2023-10-16 02:39:53] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.001197 train acc = 1.0000, test acc = 0.8222
batchnorm
[2023-10-16 02:39:59] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000163 train acc = 1.0000, test acc = 0.8396
batchnorm
[2023-10-16 02:40:05] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.001299 train acc = 1.0000, test acc = 0.8331
batchnorm
[2023-10-16 02:40:11] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.000627 train acc = 1.0000, test acc = 0.8007
batchnorm
[2023-10-16 02:40:17] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000532 train acc = 1.0000, test acc = 0.8092
batchnorm
[2023-10-16 02:40:23] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.001479 train acc = 1.0000, test acc = 0.8295
batchnorm
[2023-10-16 02:40:29] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.001349 train acc = 1.0000, test acc = 0.8234
batchnorm
[2023-10-16 02:40:35] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001228 train acc = 1.0000, test acc = 0.8047
batchnorm
[2023-10-16 02:40:41] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.002750 train acc = 1.0000, test acc = 0.8134
Evaluate 20 random ConvNet, mean = 0.8111 std = 0.0183
-------------------------
batchnorm
[2023-10-16 02:40:41] iter = 0500, loss = 55.7385
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:43] iter = 0510, loss = 60.2637
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:44] iter = 0520, loss = 59.7886
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:46] iter = 0530, loss = 64.7744
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:47] iter = 0540, loss = 54.5068
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:49] iter = 0550, loss = 54.3205
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:51] iter = 0560, loss = 62.1652
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:52] iter = 0570, loss = 60.7823
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:54] iter = 0580, loss = 62.4518
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:56] iter = 0590, loss = 62.2514
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:57] iter = 0600, loss = 64.1338
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:40:59] iter = 0610, loss = 57.9176
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:00] iter = 0620, loss = 65.3241
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:02] iter = 0630, loss = 59.5305
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:04] iter = 0640, loss = 59.0069
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:05] iter = 0650, loss = 61.3012
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:07] iter = 0660, loss = 52.3653
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:09] iter = 0670, loss = 59.5805
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:10] iter = 0680, loss = 55.2975
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:12] iter = 0690, loss = 54.3749
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:14] iter = 0700, loss = 57.0516
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:15] iter = 0710, loss = 56.7685
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:17] iter = 0720, loss = 57.7546
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:18] iter = 0730, loss = 60.1670
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:20] iter = 0740, loss = 61.0475
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:22] iter = 0750, loss = 61.0018
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:23] iter = 0760, loss = 61.4597
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:25] iter = 0770, loss = 58.0813
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:27] iter = 0780, loss = 64.1394
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:28] iter = 0790, loss = 55.4820
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:30] iter = 0800, loss = 63.0152
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:31] iter = 0810, loss = 58.9655
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:33] iter = 0820, loss = 56.8762
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:35] iter = 0830, loss = 60.7808
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:36] iter = 0840, loss = 61.6153
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:38] iter = 0850, loss = 63.7013
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:40] iter = 0860, loss = 63.1418
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:41] iter = 0870, loss = 61.4373
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:43] iter = 0880, loss = 62.4675
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:45] iter = 0890, loss = 60.5099
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:46] iter = 0900, loss = 58.0594
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:48] iter = 0910, loss = 58.6171
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:49] iter = 0920, loss = 60.9292
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:51] iter = 0930, loss = 63.6062
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:53] iter = 0940, loss = 55.6716
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:54] iter = 0950, loss = 59.9025
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:56] iter = 0960, loss = 60.4128
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:58] iter = 0970, loss = 54.8662
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:41:59] iter = 0980, loss = 62.6789
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
[2023-10-16 02:42:01] iter = 0990, loss = 60.6985
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
batchnorm
-------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
DC augmentation parameters: 
 {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}
batchnorm
[2023-10-16 02:42:08] Evaluate_00: epoch = 1000 train time = 4 s train loss = 0.002241 train acc = 1.0000, test acc = 0.8221
batchnorm
[2023-10-16 02:42:14] Evaluate_01: epoch = 1000 train time = 4 s train loss = 0.000747 train acc = 1.0000, test acc = 0.8053
batchnorm
[2023-10-16 02:42:20] Evaluate_02: epoch = 1000 train time = 4 s train loss = 0.000671 train acc = 1.0000, test acc = 0.7977
batchnorm
[2023-10-16 02:42:26] Evaluate_03: epoch = 1000 train time = 4 s train loss = 0.000330 train acc = 1.0000, test acc = 0.7854
batchnorm
[2023-10-16 02:42:32] Evaluate_04: epoch = 1000 train time = 4 s train loss = 0.000433 train acc = 1.0000, test acc = 0.8068
batchnorm
[2023-10-16 02:42:38] Evaluate_05: epoch = 1000 train time = 4 s train loss = 0.000342 train acc = 1.0000, test acc = 0.8065
batchnorm
[2023-10-16 02:42:44] Evaluate_06: epoch = 1000 train time = 4 s train loss = 0.001505 train acc = 1.0000, test acc = 0.8013
batchnorm
[2023-10-16 02:42:50] Evaluate_07: epoch = 1000 train time = 4 s train loss = 0.000513 train acc = 1.0000, test acc = 0.8168
batchnorm
[2023-10-16 02:42:56] Evaluate_08: epoch = 1000 train time = 4 s train loss = 0.001286 train acc = 1.0000, test acc = 0.8010
batchnorm
[2023-10-16 02:43:02] Evaluate_09: epoch = 1000 train time = 4 s train loss = 0.000208 train acc = 1.0000, test acc = 0.8121
batchnorm
[2023-10-16 02:43:08] Evaluate_10: epoch = 1000 train time = 4 s train loss = 0.000373 train acc = 1.0000, test acc = 0.7750
batchnorm
[2023-10-16 02:43:14] Evaluate_11: epoch = 1000 train time = 4 s train loss = 0.002530 train acc = 1.0000, test acc = 0.8085
batchnorm
[2023-10-16 02:43:20] Evaluate_12: epoch = 1000 train time = 4 s train loss = 0.000207 train acc = 1.0000, test acc = 0.7976
batchnorm
[2023-10-16 02:43:26] Evaluate_13: epoch = 1000 train time = 4 s train loss = 0.002331 train acc = 1.0000, test acc = 0.8295
batchnorm
[2023-10-16 02:43:32] Evaluate_14: epoch = 1000 train time = 4 s train loss = 0.002523 train acc = 1.0000, test acc = 0.8131
batchnorm
[2023-10-16 02:43:38] Evaluate_15: epoch = 1000 train time = 4 s train loss = 0.000550 train acc = 1.0000, test acc = 0.8084
batchnorm
[2023-10-16 02:43:44] Evaluate_16: epoch = 1000 train time = 4 s train loss = 0.000267 train acc = 1.0000, test acc = 0.8047
batchnorm
[2023-10-16 02:43:50] Evaluate_17: epoch = 1000 train time = 4 s train loss = 0.000541 train acc = 1.0000, test acc = 0.7770
batchnorm
[2023-10-16 02:43:56] Evaluate_18: epoch = 1000 train time = 4 s train loss = 0.001017 train acc = 1.0000, test acc = 0.8049
batchnorm
[2023-10-16 02:44:02] Evaluate_19: epoch = 1000 train time = 4 s train loss = 0.000881 train acc = 1.0000, test acc = 0.8135
Evaluate 20 random ConvNet, mean = 0.8044 std = 0.0131
-------------------------
batchnorm
[2023-10-16 02:44:03] iter = 1000, loss = 57.7628

==================== Final Results ====================

Run 5 experiments, train on ConvNet, evaluate 100 random ConvNet, mean  = 79.72%  std = 1.84%
